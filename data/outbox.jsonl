{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/tokenizers", "title": "Tokenization in Transformers v5: Simpler, Clearer, and More Modular", "url": "https://huggingface.co/blog/tokenizers", "published": "Thu, 18 Dec 2025 00:00:00 GMT", "text_source": "article", "article_fetch_error": null, "text": "Tokenization in Transformers v5: Simpler, Clearer, and More Modular\nTransformers v5 redesigns how tokenizers work. The big tokenizers reformat separates tokenizer design from trained vocabulary (much like how PyTorch separates neural network architecture from learned weights). The result is tokenizers you can inspect, customize, and train from scratch with far less friction.\nTL;DR: This blog explains how tokenization works in Transformers and why v5 is a major redesign, with clearer internals, a clean class hierarchy, and a single fast backend. It‚Äôs a practical guide for anyone who wants to understand, customize, or train model-specific tokenizers instead of treating them as black boxes.\nTable of Contents\n- What is Tokenization?\n- The Tokenization Pipeline\n- Tokenization Algorithms\n- Accessing\ntokenizers\nthroughtransformers\n- The Tokenizer Class Hierarchy in\ntransformers\nAutoTokenizer\nAutomatically Selects the Correct Tokenizer Class- v5 Separates Tokenizer Architecture from Trained Vocab\n- Summary\nFor experts: If you're already familiar with the concepts and want to understand the changes in v5, go to v5 Separates Tokenizer Architecture from Trained Vocab\nBefore diving into the changes, let's quickly cover what tokenization does and how the pieces fit together.\nWhat is tokenization?\nLanguage models don't read raw text. They consume sequences of integers usually called token IDs or input IDs. Tokenization is the process of converting raw text into these token IDs. (Try the tokenization playground here to visualize tokenization.)\nTokenization is a broad concept used across natural language processing and text processing generally. This post focuses specifically on tokenization for Large Language Models (LLMs) using the transformers\nand tokenizers\nlibraries.\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM3-3B\")\ntext = \"Hello world\"\ntokens = tokenizer(text)\nprint(tokens[\"input_ids\"])\n# [9906, 1917]\nprint(tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"]))\n# ['Hello', 'ƒ†world']\nƒ†world\n(above) is a single token that represents the character sequence \" world\" (with the space).\nA token is the smallest string unit the model sees. It can be a character, word, or subword chunk like \"play\" or \"##ing\" (\"##\" is a pattern, don't worry if you don't completely understand it now ü§ó). The vocabulary maps each unique token to the token ID.\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM3-3B\")\nprint(tokenizer.vocab)\n# {'√éƒπ√éƒæ': 106502, 'ƒ†Peel': 89694, '.languages': 91078, ...}\nA good tokenizer compresses text into the smallest amount of tokens. Fewer tokens means more usable context without increasing model size. Training a tokenizer boils down to finding the best compression rules for your datasets. For example, if you work on Chinese corpus you can sometimes find very nice surprises üòâ.\nThe tokenization pipeline\nTokenization happens in stages. Each stage transforms text before passing it to the next:\n| Stage | Purpose | Example |\n|---|---|---|\n| Normalizer | Standardizes text (lowercasing, unicode normalization, whitespace cleanup) | \"HELLO World\" ‚Üí \"hello world\" |\n| Pre-tokenizer | Splits text into preliminary chunks | \"hello world\" ‚Üí [\"hello\", \" world\"] |\n| Model | Applies the tokenization algorithm (BPE, Unigram, etc.) | [\"hello\", \" world\"] ‚Üí [9906, 1917] |\n| Post-processor | Adds special tokens (BOS, EOS, padding) | [9906, 1917] ‚Üí [1, 9906, 1917, 2] |\n| Decoder | Converts token IDs back to text | [9906, 1917] ‚Üí \"hello world\" |\nEach component is independent. You can swap normalizers or change the algorithm without rewriting everything else.\nYou can access the rust based tokenizer through\n_tokenizer\n. We go in more depth about it in this section\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-270m-it\")\nprint(f\"{tokenizer._tokenizer.normalizer=}\")\n# Replace(...)\nprint(f\"{tokenizer._tokenizer.pre_tokenizer=}\")\n# Split(...)\nprint(f\"{tokenizer._tokenizer.model=}\")\n# BPE(...)\nprint(f\"{tokenizer._tokenizer.post_processor=}\")\n# TemplateProcessing(...)\nprint(f\"{tokenizer._tokenizer.decoder=}\")\n# Sequence(decoders=[Replace(...), ByteFallback(), Fuse()])\nTokenization algorithms\nThe following algorithms dominate modern language model tokenizers:\n- Byte Pair Encoding (BPE) iteratively merges the most frequent character pairs. This algorithm is deterministic and widely used. (Read more about BPE)\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")\nprint(tokenizer._tokenizer.model)\n# BPE(...)\n- Unigram takes a probabilistic approach, selecting the most likely segmentation from a large initial vocabulary. This is more flexible than the strict BPE. (Read more about Unigram)\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\nprint(tokenizer._tokenizer.model)\n# Unigram(...)\n- WordPiece resembles BPE but uses different merge criteria based on likelihood. (Read more about WordPiece)\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nprint(tokenizer._tokenizer.model)\n# WordPiece(...)\nAccessing tokenizers through transformers\nThe tokenizers\nlibrary is a Rust-based tokenization engine. It is fast, efficient, and completely language model agnostic. The library handles the mechanics of converting text into token IDs and back. The tokenizers\nlibrary is a general-purpose tool that implements the tokenization algorithms, but does not implement the conventions that connect those algorithms to specific language models.\nConsider what happens when you use tokenizers\ndirectly with the SmolLM3-3B\nmodel:\nfrom tokenizers import Tokenizer\ntokenizer = Tokenizer.from_pretrained(\"HuggingFaceTB/SmolLM3-3B\")\ntext = \"Hello world\"\nencodings = tokenizer.encode(text)\nprint(encodings.ids)\n# [9906, 1917]\nprint(encodings.tokens)\n# ['Hello', 'ƒ†world']\nThe output is raw tokenization. You get token IDs and the string pieces they correspond to. Nothing more.\nNow consider what's missing. The SmolLM3-3B\nis a conversational model. When you interact with it, you typically structure your input as a conversation with roles like \"user\" and \"assistant\". The language model expects special formatting tokens to indicate these roles. The raw tokenizers\nlibrary has no concept of any of this.\nHow do you bridge the gap between raw tokenization and model requirements?\nThe transformers\nlibrary bridges this gap. The library is primarily known as a model definition library, but it also provides a tokenizer abstraction layer that wraps the raw tokenizers\nbackend and adds model-aware functionality.\nHere's the same tokenization with the transformers\nwrapper:\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM3-3B\")\n# Format a conversation using the model's chat template\nprompt = \"Give me a brief explanation of gravity in simple terms.\"\nmessages = [{\"role\": \"user\", \"content\": prompt}]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nprint(text)\n# <|im_start|>system\n# ...\n# <|im_start|>user\n# Give me a brief explanation of gravity in simple terms.<|im_end|>\n# <|im_start|>assistant\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\")\nNotice how the special tokens like <|im_start|>\nand <|im_end|>\nare applied to the prompt before tokenizing. This is useful for the model to learn where a new sequence starts and ends.\nThe transformers\ntokenizer adds everything the raw library lacks:\n- Chat template application. The\napply_chat_template\nmethod formats conversations according to the model's expected format, inserting the correct special tokens and delimiters. - Automatic special token insertion. Beginning-of-sequence and end-of-sequence tokens are added where the model expects them.\n- Truncation to context length. You can specify\ntruncation=True\nand the tokenizer will respect the model's maximum sequence length. - Batch encoding with padding. Multiple inputs can be padded to the same length with the correct padding token and direction.\n- Return format options. You can request PyTorch tensors (\nreturn_tensors=\"pt\"\n), NumPy arrays and others.\ntransformers\nimplements the tokenization API that is most commonly used in the entire ML community (encode\n,decode\n,convert_tokens_to_ids\n, etc.)\nThe tokenizer class hierarchy in transformers\nThe transformers\nlibrary organizes tokenizers into a class hierarchy. At the top sits a base class that defines the common interface. Below it, backend classes handle the actual tokenization using different engines. At the bottom, model-specific classes configure the backends for particular models.\nPreTrainedTokenizerBase\ndefines the common interface for all tokenizers\nPreTrainedTokenizerBase\nis the abstract base class for all tokenizers in transformers\n. It defines the interface that every tokenizer must implement.\nThe base class handles functionality that doesn't depend on the tokenization backend:\n- Special token properties. Properties like\nbos_token\n,eos_token\n,pad_token\n, andunk_token\nare defined here. These properties provide access to the special tokens that models use to mark sequence boundaries and handle unknown inputs. - Encoding interface. The\n__call__\nmethod,encode\n, andencode_plus\nmethods are defined here. These methods accept text input and return token IDs along with attention masks and other metadata. - Decoding interface. The\ndecode\nandbatch_decode\nmethods convert token IDs back to text. - Serialization. The\nsave_pretrained\nandfrom_pretrained\nmethods handle downloading the correct files, reading information, saving tokenizers to disk etc. - Chat template support. The\napply_chat_template\nmethod lives here, formatting conversations according to Jinja templates stored in the tokenizer configuration.\nEvery tokenizer in transformers\nultimately inherits from PreTrainedTokenizerBase\n. The base class ensures consistent behavior across all tokenizers, regardless of which backend they use for the actual tokenization.\nTokenizersBackend\nwraps the tokenizers\nlibrary\nTokenizersBackend\nis the primary backend class for most modern tokenizers. It inherits from PreTrainedTokenizerBase\nand wraps the Rust-based tokenizers\nlibrary.\nThe class stores the Rust tokenizer object internally:\nclass TokenizersBackend(PreTrainedTokenizerBase):\ndef __init__(self, tokenizer_object, ...):\nself._tokenizer = tokenizer_object # The Rust tokenizer\n...\nWhen you call encoding methods on a TokenizersBackend\ntokenizer, the class delegates the actual tokenization to the Rust backend:\ndef _batch_encode_plus(self, batch_text_or_text_pairs, ...):\nencodings = self._tokenizer.encode_batch(batch_text_or_text_pairs, ...)\n...\nThe Rust backend performs computationally intensive work, while the Python wrapper adds the model-aware features on top.\nMany model-specific tokenizers inherit from TokenizersBackend\n, examples include:\nLlamaTokenizer\nGemmaTokenizer\nThese model-specific classes configure the backend with the correct vocabulary, merge rules, special tokens, and normalization settings for their respective models.\nPythonBackend\nprovides a pure-Python mixin\nPythonBackend\ninherits from PreTrainedTokenizerBase\nand implements tokenization in pure Python. The class is aliased as PreTrainedTokenizer\n.\nThe pure-Python backend exists for several reasons:\n- Custom tokenization logic. Some models require tokenization behavior that doesn't fit the standard\ntokenizers\npipeline. - Legacy compatibility. Older model implementations may rely on Python-specific behavior.\nThe Python backend is slower than the Rust backend. For most use cases, the Rust-backed\nTokenizersBackend\nis preferred.\nModel-specific tokenizers that inherit from PythonBackend\n(or its alias PreTrainedTokenizer\n) include some older or specialized models, like:\nCTRLTokenizer\nCanineTokenizer\nSentencePieceBackend\nhandles SentencePiece models\nSentencePieceBackend\ninherits from PythonBackend\nand provides integration with Google's SentencePiece library. SentencePiece is a standalone tokenization library that many models use, particularly those trained by Google.\nThe backend wraps a SentencePiece processor:\nclass SentencePieceBackend(PythonBackend):\ndef __init__(self, vocab_file, ...):\nself.sp_model = spm.SentencePieceProcessor()\nself.sp_model.Load(vocab_file)\n...\nModels that use SentencePiece tokenization inherit from this backend. Examples include:\nSiglipTokenizer\nBartphoTokenizer\nThe SentencePiece backend inherits from PythonBackend\nrather than directly from PreTrainedTokenizerBase\nbecause it shares much of the same interface and padding/truncation logic.\nAutoTokenizer automatically selects the correct tokenizer class\nAutoTokenizer\nis the recommended entry point for loading tokenizers. It automatically determines which tokenizer class to use for a given model and returns an instance of that class.\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nBehind the scenes, AutoTokenizer\nperforms these steps:\n- Download the tokenizer configuration. The\nfrom_pretrained\nmethod fetchestokenizer_config.json\nfrom the Hub (or from a local directory). - Identify the model type. The configuration contains metadata that identifies the model type (e.g., \"gpt2\", \"llama\", \"bert\").\n- Look up the tokenizer class.\nAutoTokenizer\nmaintains a mapping calledTOKENIZER_MAPPING_NAMES\nthat maps model types to tokenizer class names:\nTOKENIZER_MAPPING_NAMES = {\n\"gpt2\": \"GPT2Tokenizer\",\n\"llama\": \"LlamaTokenizer\",\n\"bert\": \"BertTokenizer\",\n...\n}\n- Instantiate the correct class.\nAutoTokenizer\nimports the appropriate tokenizer class and calls itsfrom_pretrained\nmethod. - Return the configured tokenizer. You receive a fully configured, model-specific tokenizer ready for use.\nThe benefit of\nAutoTokenizer\nis that you don't need to know which tokenizer class a model uses. Whether a model usesLlamaTokenizer\n,GPT2Tokenizer\n, orBertTokenizer\n, the sameAutoTokenizer.from_pretrained(\"model-name\")\ncall works.\nThe tokenizer system in transformers\nforms a layered architecture:\n| Layer | Component | Responsibility |\n|---|---|---|\n| Entry Point | AutoTokenizer |\nAutomatically selects and instantiates the correct tokenizer class |\n| Model-Specific | LlamaTokenizer , GPT2Tokenizer , etc. |\nConfigures the backend with model-specific architecture of normalizer, pre tokenizer, etc, special tokens, and settings |\n| Backend | TokenizersBackend , PythonBackend , SentencePieceBackend |\nImplements the actual tokenization using a specific engine |\n| Base | PreTrainedTokenizerBase |\nDefines the common interface and shared functionality |\n| Engine | tokenizers (Rust), SentencePiece, Pure Python |\nPerforms raw tokenization |\nv5 Separates Tokenizer Architecture from Trained Vocab\nThe most significant change in Transformers v5 is a philosophical shift in how tokenizers are defined. Tokenizers now work like PyTorch's nn.Module\n: you define the architecture first, then fill it with learned parameters.\nThe problem with v4: tokenizers were opaque and tightly coupled\nIn v4, tokenizers were black boxes tied to pretrained checkpoint files. If you loaded LlamaTokenizerFast\n, you couldn't easily answer basic questions about it:\n- Is it BPE or Unigram?\n- How does it normalize text?\n- What pre-tokenization strategy does it use?\n- What are the special tokens and their positions?\nThe __init__\nmethod gave no clues. You had to dig through serialized files or external documentation to understand what the tokenizer actually did.\nv4 also maintained two parallel implementations for every model:\n- a \"slow\" Python tokenizer (\nLlamaTokenizer\ninheriting fromPreTrainedTokenizer\n) and - a \"fast\" Rust-backed tokenizer (\nLlamaTokenizerFast\ninheriting fromPreTrainedTokenizerFast\n).\nThis meant:\n- Two files per model (e.g.,\ntokenization_llama.py\nandtokenization_llama_fast.py\n) - Code duplication across hundreds of models\n- Behavioral discrepancies between slow and fast versions, leading to subtle bugs\n- A growing test suite dedicated to verifying that slow and fast tokenizers produced identical outputs\n- User confusion about which tokenizer to use and when\nWorst of all, you couldn't create an empty tokenizer architecture. If you wanted to train a LLaMA-style tokenizer on your own data, there was no clean way to instantiate a \"blank\" LLaMA tokenizer and fill it with your vocabulary and merges. Tokenizers existed only as loaded checkpoints, not as configurable templates.\nThe v5 solution: architecture and parameters are now separate\nv5 treats tokenizer architecture (normalizer, pre-tokenizer, model type, post-processor, decoder) as distinct from trained parameters (vocabulary, merges). This mirrors how PyTorch separates model architecture from learned weights.\nWith nn.Module\n, you define layers first:\nfrom torch import nn\nmodel = nn.Sequential(\nnn.Embedding(vocab_size, embed_dim),\nnn.Linear(embed_dim, hidden_dim),\n)\n# Architecture defined; weights initialized randomly or loaded later\nV5 tokenizers follow the same pattern:\nfrom transformers import LlamaTokenizer\n# Instantiate the architecture\ntokenizer = LlamaTokenizer()\n# Train on your own data to fill in vocab and merges\ntokenizer.train(files=[\"my_corpus.txt\"])\nThe tokenizer class now explicitly declares its structure. Looking at LlamaTokenizer\nin v5, you can immediately see:\n- It uses BPE as its tokenization model\n- It may add a prefix space before text\n- Its special tokens (\nunk\n,bos\n,eos\n) sit at specific vocabulary positions - It does not normalize input text\n- Its decoder replaces the metaspace character\n‚ñÅ\nwith spaces\nThis transparency was impossible in v4, where the same information was buried in serialized files.\nOne file, one backend, one recommended path\nv5 consolidates the two-file system into a single file per model. LlamaTokenizer\nnow inherits from TokenizersBackend\n, which wraps the Rust-based tokenizer that was previously exposed as the ‚Äúfast‚Äù implementation and is now the default.\nThe former ‚Äúslow‚Äù Python implementation lives explicitly behind PythonBackend\n, and SentencePieceBackend\nremains for models that require it, but Rust-backed tokenization is the preferred default.\nThis change eliminates:\n- Duplicate code across slow/fast implementations\n- The confusing\nTokenizer\nvsTokenizerFast\nnaming convention - Test suites dedicated to checking slow-fast parity\nUsers now have one clear entry point. Advanced users who need to customize can still access lower-level components, but the library no longer forces everyone to navigate two parallel implementations.\nYou can now train model specific tokenizers from scratch\nSuppose you want a tokenizer that behaves exactly like LLaMA's ‚Äì same normalization, same pre-tokenization, same BPE model type ‚Äì but trained on a domain-specific corpus (medical text, legal documents, a new language). In v4, this required manually reconstructing the tokenizer pipeline from low-level tokenizers\nlibrary primitives. In v5, you can instantiate the architecture directly and call train\n:\nfrom transformers import LlamaTokenizer\nfrom datasets import load_dataset\n# Initialize blank tokenizer\ntokenizer = LlamaTokenizer()\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\ndef get_training_corpus():\nbatch = 1000\nfor i in range(0, len(dataset), batch):\nyield dataset[i : i + batch][\"text\"]\ntrained_tokenizer = tokenizer.train_new_from_iterator(\ntext_iterator=get_training_corpus(),\nvocab_size=32000,\nlength=len(dataset),\nshow_progress=True,\n)\ntrained_tokenizer.push_to_hub(\"my_custom_tokenizer\")\ntokenizer = LlamaTokenizer.from_pretrained(\"my_custom_tokenizer\")\nThe resulting tokenizer will have your custom vocabulary and merge rules, but will process text identically to how a standard LLaMA tokenizer would with the same whitespace handling, same special token conventions, same decoding behavior.\n| Aspect | V4 | V5 |\n|---|---|---|\n| Files per model | Two (tokenization_X.py , tokenization_X_fast.py ) |\nOne (tokenization_X.py ) |\n| Default backend | Split between Python and Rust | Rust (TokenizersBackend ) preferred |\n| Architecture visibility | Hidden in serialized files | Explicit in class definition |\n| Training from scratch | Required manual pipeline construction | tokenizer.train(files=[...]) |\n| Component inspection | Difficult, undocumented | Direct properties (tokenizer.normalizer , etc.) |\n| Parent classes | PreTrainedTokenizer , PreTrainedTokenizerFast |\nTokenizersBackend (or SentencePieceBackend , PythonBackend ) |\nThe shift from \"tokenizers as loaded checkpoints\" to \"tokenizers as configurable architectures\" makes the library more modular, more transparent, and more aligned with how practitioners think about building ML systems.\nSummary\nTransformers v5 brings three improvements to tokenization:\n- One file per model instead of separate slow/fast implementations\n- Visible architecture so you can inspect normalizers, pre-tokenizers, and decoders\n- Trainable templates that let you create custom tokenizers matching any model's design\nThe wrapper layer between tokenizers\nand Transformers remains essential. It adds model awareness, context lengths, chat templates, special tokens, that raw tokenization doesn't provide. V5 just makes that layer clearer and more customizable.\nIf you are looking to learn more about tokenization here are some resources:"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe", "title": "The Open Evaluation Standard: Benchmarking NVIDIA Nemotron 3 Nano with NeMo Evaluator", "url": "https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe", "published": "Wed, 17 Dec 2025 13:22:18 GMT", "text_source": "article", "article_fetch_error": null, "text": "The Open Evaluation Standard: Benchmarking NVIDIA Nemotron 3 Nano with NeMo Evaluator\nIt has become increasingly challenging to assess whether a model‚Äôs reported improvements reflect genuine advances or variations in evaluation conditions, dataset composition, or training data that mirrors benchmark tasks. The NVIDIA Nemotron approach to openness addresses this by publishing transparent and reproducible evaluation recipes that make results independently verifiable.\nNVIDIA released Nemotron 3 Nano 30B A3B with an explicitly open evaluation approach to make that distinction clear. Alongside the model card, we are publishing the complete evaluation recipe used to generate the results, built with the NVIDIA NeMo Evaluator library, so anyone can rerun the evaluation pipeline, inspect the artifacts, and analyze the outcomes independently.\nWe believe that open innovation is the foundation of AI progress. This level of transparency matters because most model evaluations omit critical details. Configs, prompts, harness versions, runtime settings, and logs are often missing or underspecified, and even small differences in these parameters can materially change results. Without a complete recipe, it‚Äôs nearly impossible to tell whether a model is genuinely more intelligent or simply optimized for a benchmark.\nThis blog shows developers exactly how to reproduce the evaluation behind Nemotron 3 Nano 30B A3B using fully open tools, configurations, and artifacts. You‚Äôll learn how the evaluation was run, why the methodology matters, and how to execute the same end-to-end workflow using the NeMo Evaluator library so you can verify results, compare models consistently, and build transparent evaluation pipelines of your own.\nBuilding a consistent and transparent evaluation workflow with NeMo Evaluator\nA single, consistent evaluation system\nDevelopers and researchers need evaluation workflows they can rely on, not one-off scripts that behave differently from model to model. NeMo Evaluator provides a unified way to define benchmarks, prompts, configuration, and runtime behavior once, then reuse that methodology across models and releases. This avoids the common scenario where the evaluation setup quietly changes between runs, making comparisons over time difficult or misleading.\nMethodology independent of inference setup\nModel outputs can vary by inference backend and configuration, so evaluation tools should never be tied to a single inference solution. Locking an evaluation tool to one inference solution would limit its usefulness. NeMo Evaluator avoids this by separating the evaluation pipeline from the inference backend, allowing the same configuration to run against hosted endpoints, local deployments, or third-party providers. This separation enables meaningful comparisons even when you change infrastructure or inference engines.\nBuilt to scale beyond one-off experiments\nMany evaluation pipelines work once and then break down as the scope expands. NeMo Evaluator is designed to scale from quick, single-benchmark validation to full model card suites and repeated evaluations across multiple models. The launcher, artifact layout, and configuration model support ongoing workflows, not just isolated experiments, so teams can maintain consistent evaluation practices over time.\nAuditability with structured artifacts and logs\nTransparent evaluation requires more than final scores. Each evaluation run produces structured results and logs by default, making it easy to inspect how scores were computed, understand score calculations, debug unexpected behavior, and conduct deeper analysis. Each component of the evaluation is captured and reproducible.\nA shared evaluation standard\nBy releasing Nemotron 3 Nano 30B A3B with its full evaluation recipe, NVIDIA is providing a reference methodology that the community can run, inspect, and build upon. Using the same configuration and tools brings consistency to how benchmarks are selected, executed, and interpreted, enabling more reliable comparisons across models, providers, and releases.\nOpen evaluation for Nemotron 3 Nano\nOpen evaluation means publishing not just the final results, but the full methodology behind them, so benchmarks are run consistently, and results can be compared meaningfully over time. For Nemotron 3 Nano 30B A3B, this includes open‚Äësource tooling, transparent configurations, and reproducible artifacts that anyone can run end‚Äëto‚Äëend.\nOpen-source model evaluation tooling\nNeMo Evaluator is an open-source library designed for robust, reproducible, and scalable evaluation of generative models. Instead of introducing yet another standalone benchmark runner, it acts as a unifying orchestration layer that brings multiple evaluation harnesses under a single, consistent interface.\nUnder this architecture, NeMo Evaluator integrates and coordinates hundreds of benchmarks from many widely used evaluation harnesses, including NeMo Skills for Nemotron instruction-following, tool use, and agentic evaluations, as well as the LM Evaluation Harness for base model and pre-training benchmarks, and many more (full benchmark catalog). Each harness retains its native logic, datasets, and scoring semantics, while NeMo Evaluator standardizes how they are configured, executed, and logged.\nThis provides two practical advantages: teams can run diverse benchmark categories using a single configuration without rewriting custom evaluation scripts, and results from different harnesses are stored and inspected in a consistent, predictable way, even when the underlying tasks differ. The same orchestration framework used internally by NVIDIA‚Äôs Nemotron research and model‚Äëevaluation teams is now available to the community, enabling developers to run heterogeneous, multi‚Äëharness evaluations through a shared, auditable workflow.\nOpen configurations\nWe published the exact YAML configuration used for the Nemotron 3 Nano 30B A3B model card evaluation with NeMo Evaluator. This includes:\n- model inference and deployment settings\n- benchmark and task selection\n- benchmark-specific parameters such as sampling, repeats, and prompt templates\n- runtime controls including parallelism, timeouts, and retries\n- output paths and artifact layout\nUsing the same configuration means running the same evaluation methodology.\nOpen logs and artifacts\nEach evaluation run produces structured, inspectable outputs, including\nper‚Äëtask results.json\nfiles, execution logs for debugging and\nauditability, and artifacts organized by task for easy comparison. This\nstructure makes it possible to understand not only the final scores, but\nalso how those scores were produced and to perform deeper analysis of\nmodel behavior.\nThe reproducibility workflow\nReproducing Nemotron 3 Nano 30B A3B model card results follows a simple loop:\n- Start from the released model checkpoint or hosted endpoint\n- Use the published NeMo Evaluator config\n- Execute the evaluation with a single CLI command\n- Inspect logs and artifacts, and compare results to the model card\nThe same workflow applies to any model you evaluate using NeMo Evaluator. You can point the evaluation at a hosted endpoint or a local deployment, including common inference providers such as HuggingFace, build.nvidia.com, and OpenRouter. The key requirement is access to the model, either as weights you can serve or as an endpoint you can call. For this tutorial, we use the hosted endpoint on build.nvidia.com.\nReproducing Nemotron 3 Nano benchmark results\nThis tutorial reproduces the evaluation results for NVIDIA Nemotron 3 Nano 30B A3B using NeMo Evaluator. The step-by-step tutorial, including the published configs used for the model card evaluation, is available on GitHub. Although we have focused this tutorial on the Nemotron 3 Nano 30B A3B, we also published recipes for the base model evaluation.\nThis walkthrough runs a comprehensive evaluation suite of the published configs used for the model card evaluation for NVIDIA Nemotron 3 Nano 30B A3B using the following benchmarks:\n| Benchmark | Accuracy | Category | Description |\n|---|---|---|---|\n| BFCL v4 | 53.8 | Function Calling | Berkeley Function Calling Leaderboard v4 |\n| LiveCodeBench (v6 2025-08‚Äì2025-05) | 68.3 | Coding | Real-world coding problems evaluation |\n| MMLU-Pro | 78.3 | Knowledge | Multi-task language understanding (10-choice) |\n| GPQA | 73.0 | Science | Graduate-level science questions |\n| AIME 2025 | 89.1 | Mathematics | American Invitational Mathematics Exam |\n| SciCode | 33.3 | Scientific Coding | Scientific programming challenges |\n| IFBench | 71.5 | Instruction Following | Instruction following benchmark |\n| HLE | 10.6 | Humanity's Last Exam | Expert-level questions across domains |\nFor Model Card details, see the NVIDIA Nemotron 3 Nano 30B A3B Model Card. For a deep dive into the architecture, datasets, and benchmarks, read the full Nemotron 3 Nano Technical Report.\n1. Install NeMo Evaluator Launcher\npip install nemo-evaluator-launcher\n2. Set required environment variables\n# NVIDIA endpoint access\nexport NGC_API_KEY=\"your-ngc-api-key\"\n# Hugging Face access\nexport HF_TOKEN=\"your-huggingface-token\"\n# Required only for judge-based benchmarks such as HLE\nexport JUDGE_API_KEY=\"your-judge-api-key\"\nOptional but recommended for faster reruns:\nexport HF_HOME=\"/path/to/your/huggingface/cache\"\n3. Model endpoint\nThe evaluation uses the NVIDIA API endpoint hosted on build.nvidia.com:\ntarget:\napi_endpoint:\nmodel_id: nvidia/nemotron-nano-3-30b-a3b\nurl: https://integrate.api.nvidia.com/v1/chat/completions\napi_key_name: NGC_API_KEY\nEvaluations can be run against common inference providers such as HuggingFace, build.nvidia.com, or OpenRouter, or anywhere that the model has an available endpoint.\nIf you're hosting the model locally or using a different endpoint:\nnemo-evaluator-launcher run \\\n--config local_nvidia_nemotron_3_nano_30b_a3b.yaml \\\n-o target.api_endpoint.url=http://localhost:8000/v1/chat/completions\n4. Run the full evaluation suite\nPreview the run without executing using --dry-run\n:\nnemo-evaluator-launcher run \\\n--config local_nvidia_nemotron_3_nano_30b_a3b.yaml \\\n--dry-run\nFrom the examples directory, run the evaluation using the YAML configuration provided:\nnemo-evaluator-launcher run \\\n--config /path/to/examples/nemotron/local_nvidia_nemotron_3_nano_30b_a3b.yaml\nNote that for quick testing, you can limit the number\nof samples by setting limit_samples\n:\nnemo-evaluator-launcher run \\\n--config local_nvidia_nemotron_3_nano_30b_a3b.yaml \\\n-o evaluation.nemo_evaluator_config.config.params.limit_samples=10\n5. Running an individual benchmark\nYou can run specific benchmarks using the -t\nflag (from the examples/nemotron\ndirectory):\n# Run only MMLU-Pro\nnemo-evaluator-launcher run --config local_nvidia_nemotron_3_nano_30b_a3b.yaml -t ns_mmlu_pro\n# Run only coding benchmarks\nnemo-evaluator-launcher run --config local_nvidia_nemotron_3_nano_30b_a3b.yaml -t ns_livecodebench\n# Run multiple specific benchmarks\nnemo-evaluator-launcher run --config local_nvidia_nemotron_3_nano_30b_a3b.yaml -t ns_gpqa -t ns_aime2025\n6. Monitor execution and inspect results\n# Check status of a specific job\nnemo-evaluator-launcher status\n# Stream logs for a specific job\nnemo-evaluator-launcher logs <job-id>\nResults are written to the defined output directory:\nresults_nvidia_nemotron_3_nano_30b_a3b/\n‚îú‚îÄ‚îÄ artifacts/\n‚îÇ ‚îî‚îÄ‚îÄ <task_name>/\n‚îÇ ‚îî‚îÄ‚îÄ results.json\n‚îî‚îÄ‚îÄ logs/\n‚îî‚îÄ‚îÄ stdout.log\nInterpreting results\nWhen reproducing evaluations, you may observe small differences in final scores across runs. This variance reflects the probabilistic nature of LLMs rather than an issue with the evaluation pipeline. Modern evaluation introduces several sources of non‚Äëdeterminism: decoding settings, repeated trials, judge‚Äëbased scoring, parallel execution, and differences in serving infrastructure. All of which can lead to slight fluctuations.\nThe purpose of open evaluation is not to force bit-wise identical outputs, but to deliver methodological consistency with clear provenance of evaluation results. To ensure your evaluation aligns with the reference standard, verify the following:\n- Configuration: use the published NeMo Evaluator YAML without modification, or document any changes explicitly\n- Benchmark selection: run the intended tasks, task versions, and prompt templates\n- Inference target: verify you are evaluating the intended model and endpoint, including chat template behavior and reasoning settings when relevant\n- Execution settings: keep runtime parameters consistent, including repeats, parallelism, timeouts, and retry behavior\n- Outputs: confirm artifacts and logs are complete and follow the expected structure for each task\nWhen these elements are consistent, your results represent a valid reproduction of the methodology, even if individual runs differ slightly. NeMo Evaluator simplifies this process, tying benchmark definitions, prompts, runtime settings, and inference configuration into a single auditable workflow to minimize inconsistencies.\nConclusion: A more transparent standard for open models\nThe evaluation recipe released alongside Nemotron 3 Nano represents a meaningful step toward a more transparent and reliable approach to open-model evaluation. We are moving away from evaluation as a collection of bespoke, \"black box\" scripts, and towards a defined system where benchmark selection, prompts, and execution semantics are encoded into a transparent workflow.\nFor developers and researchers, this transparency changes what it means to share results. A score is only as trustworthy as the methodology behind it and making that methodology public is what enables the community to verify claims, compare models fairly, and continue building on shared foundations. With open evaluation configurations, open artifacts, and open tooling, Nemotron 3 Nano demonstrates what that commitment to openness looks like in practice.\nNeMo Evaluator supports this shift by providing a consistent benchmarking methodology across models, releases, and inference environments. The objective isn‚Äôt identical numbers on every run; it‚Äôs confidence in an evaluation methodology that is explicit, inspectable, and repeatable. And for organizations that need automated or large‚Äëscale evaluation pipelines, a separate microservice offering provides an enterprise‚Äëready NeMo Evaluator microservice built on the same evaluation principles.\nUse the published NeMo Evaluator evaluation configuration for an end-to-end walkthrough of the evaluation recipe.\nJoin the Community!\nNeMo Evaluator is fully open source, and community input is essential to shaping the future of open evaluation. If there‚Äôs a benchmark you‚Äôd like us to support or an improvement you want to propose, open an issue, or contribute directly on GitHub. Your contributions help strengthen the ecosystem and advance a shared, transparent standard for evaluating generative models."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/ibm-research/cuga-on-hugging-face", "title": "CUGA on Hugging Face: Democratizing Configurable AI Agents", "url": "https://huggingface.co/blog/ibm-research/cuga-on-hugging-face", "published": "Mon, 15 Dec 2025 16:01:04 GMT", "text_source": "article", "article_fetch_error": null, "text": "CUGA on Hugging Face: Democratizing Configurable AI Agents\nIntroduction\nAI agents are rapidly becoming essential for building intelligent applications, but creating robust, adaptable agents that scale across domains remains a challenge. Many existing frameworks struggle with brittleness, tool misuse, and failures when faced with complex workflows.\nCUGA (Configurable Generalist Agent) was designed to overcome these limitations. It's an open-source, AI Agent that combines flexibility, reliability, and ease of use for enterprise use cases. By abstracting orchestration complexity, CUGA empowers developers to focus on domain requirements rather than the internals of agent building. And now, with its integration into üöÄHugging Face SpacesüöÄ, experimenting with CUGA and open models has never been easier.\nWhat is CUGA?CUGA is a configurable, general-purpose AI agent that supports complex, multi-step tasks across web and API environments. It has achieved state-of-the-art performance on leading benchmarks:\nü•á #1 on AppWorld - a benchmark with 750 real-world tasks across 457 APIs\nü•à Top-tier on WebArena (#1 from 02/25 - 09/25) - showcases CUGA Computer Use capabilities with a complex benchmark for autonomous web agents across application domains\nAt its core, CUGA offers:\n- High-performing generalist agent: Benchmarked on complex web and API tasks, it combines best-of-breed agentic patterns (e.g. planner-executor, code-act) with structured planning and smart variable management to prevent hallucination and handle complexity\n- Configurable reasoning modes: Balance performance and cost/latency with flexible modes ranging from fast heuristics to deep planning, optimizing for your task requirements\n- Computer use: Effortlessly combine UI interactions with API invocations in a workflow\n- Multi-tool integration: Seamlessly integrate tools via OpenAPI specs, MCP servers, and LangChain, enabling rapid connection to REST APIs, custom protocols, and Python functions\n- Integrates with Langflow: A low-code visual build experience for designing and deploying agent workflows without extensive coding\n- Composable: CUGA can be exposed as a tool to other agents, enabling nested reasoning and multi-agent collaboration\nWe're also continuing to innovate with new experimental capabilities, including:\n- Configurable policy and human-in-the-loop instructions: Improve alignment and ensure safe agent behavior in enterprise contexts\n- Save-and-reuse capabilities: Capture and reuse successful execution paths (plans, code, and trajectories) for faster and consistent behavior across repeated tasks.\n- Configurable policy and human-in-the-loop instructions: Improve alignment and ensure safe agent behavior in enterprise contexts\nFigure 1: CUGA Agentic Architecture\nThe CUGA architecture begins with the user's message flowing into a chat layer that interprets intent and constructs the user's goal, based on context. A task planning and control component then decomposes this goal into structured subtasks, tracked programmatically through a dynamic task ledger. This ledger supports re-planning when needed, ensuring robust execution. Subtasks are delegated to specialized agents, such as the API agent, which uses an inner reasoning loop to generate pseudo-code instructions before invoking code in a secure sandbox. The system leverages a tool registry that goes beyond MCP protocols to parse and understand tool capabilities, enabling precise orchestration. Once all steps are completed, the final response is returned to the user, delivering reliable, policy-aligned outcomes.\nCUGA works best when inference is fast. When each call takes seconds, delays compound and force a tradeoff between agent capability and user experience. Running on high-performance inference platforms like Groq shows how fast inference fundamentally expands what agent architectures can achieve.\nOpen Source and Open Models\nCUGA is fully open source, under the Apache 2.0 license, and you can find us at cuga.dev.\nBy embracing open models, CUGA aligns with the Hugging Face ethos of democratizing AI-giving developers the freedom to choose models that best fit their needs, whether for experimentation or production.\nCUGA has been tested with a variety of open models, including gpt-oss-120b and Llama-4-Maverick-17B-128E-Instruct-fp8 (both hosted on Groq). Our Hugging Face Space uses gpt-oss-120b, with the model hosted on Groq, offering a rapid response time for LLM calls\nGroq runs open models on its custom‚Äëbuilt LPUs, which are designed for AI inference and optimal for repeated agent inferences required by CUGA's architecture, enabling planning, execution, and validation steps to finish fast. The result is strong cost and performance: open models are ~80-90% cheaper than closed alternatives; Groq's OpenAI-compatible APIs meet production latency needs, and CUGA stays fully configurable across models, providers, and deployment topologies.\nIntegration with Langflow: Visual Agent Design Made Simple\nTo make agent development even more accessible, CUGA integrates with Langflow, an open-source visual programming interface for building LLM-powered workflows. Its intuitive drag-and-drop interface reduces the barrier to entry for those who prefer low-code solutions.\nStarting with Langflow 1.7.0, CUGA ships with its own widget, enabling users to assemble complex, multi-tool agents visually and deploy with a click. Give it a try at langflow.org.\nTry the Hugging Face Demo: A Hands-On Preview\nWe've launched a CUGA demo on Hugging Face Spaces to give you a taste of what's possible. This demo showcases a small CRM system and equips CUGA with 20 preconfigured tools for handling sales related data queries and API interactions through the API Agent. To make experimentation even more powerful, the demo provides access to workspace files, enabling you to use predefined policies.\nGive it a try on Hugging Face Spaces and share your feedback!\nConclusion and Call to Action\nCUGA brings a new level of flexibility and openness to AI agent building. To engage with us:\n- Try the Hugging Face Spaces demo, experiment with the CRM setup and custom policies,\n- Try us on Langflow\n- Explore the CUGA GitHub repository to deploy your own instance, try Computer Use capabilities, dive deeper and contribute to the project\n- Please share your feedback! Your insights will help shape the next generation of configurable AI agents."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/ggml-org/model-management-in-llamacpp", "title": "New in llama.cpp: Model Management", "url": "https://huggingface.co/blog/ggml-org/model-management-in-llamacpp", "published": "Thu, 11 Dec 2025 15:47:44 GMT", "text_source": "article", "article_fetch_error": null, "text": "New in llama.cpp: Model Management\nllama.cpp server now ships with router mode, which lets you dynamically load, unload, and switch between multiple models without restarting.\nReminder: llama.cpp server is a lightweight, OpenAI-compatible HTTP server for running LLMs locally.\nThis feature was a popular request to bring Ollama-style model management to llama.cpp. It uses a multi-process architecture where each model runs in its own process, so if one model crashes, others remain unaffected.\nQuick Start\nStart the server in router mode by not specifying a model:\nllama-server\nThis auto-discovers models from your llama.cpp cache (LLAMA_CACHE\nor ~/.cache/llama.cpp\n). If you've previously downloaded models via llama-server -hf user/model\n, they'll be available automatically.\nYou can also point to a local directory of GGUF files:\nllama-server --models-dir ./my-models\nFeatures\n- Auto-discovery: Scans your llama.cpp cache (default) or a custom\n--models-dir\nfolder for GGUF files - On-demand loading: Models load automatically when first requested\n- LRU eviction: When you hit\n--models-max\n(default: 4), the least-recently-used model unloads - Request routing: The\nmodel\nfield in your request determines which model handles it\nExamples\nChat with a specific model\ncurl http://localhost:8080/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"ggml-org/gemma-3-4b-it-GGUF:Q4_K_M\",\n\"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n}'\nOn the first request, the server automatically loads the model into memory (loading time depends on model size). Subsequent requests to the same model are instant since it's already loaded.\nList available models\ncurl http://localhost:8080/models\nReturns all discovered models with their status (loaded\n, loading\n, or unloaded\n).\nManually load a model\ncurl -X POST http://localhost:8080/models/load \\\n-H \"Content-Type: application/json\" \\\n-d '{\"model\": \"my-model.gguf\"}'\nUnload a model to free VRAM\ncurl -X POST http://localhost:8080/models/unload \\\n-H \"Content-Type: application/json\" \\\n-d '{\"model\": \"my-model.gguf\"}'\nKey Options\n| Flag | Description |\n|---|---|\n--models-dir PATH |\nDirectory containing your GGUF files |\n--models-max N |\nMax models loaded simultaneously (default: 4) |\n--no-models-autoload |\nDisable auto-loading; require explicit /models/load calls |\nAll model instances inherit settings from the router:\nllama-server --models-dir ./models -c 8192 -ngl 99\nAll loaded models will use 8192 context and full GPU offload. You can also define per-model settings using presets:\nllama-server --models-preset config.ini\n[my-model]\nmodel = /path/to/model.gguf\nctx-size = 65536\ntemp = 0.7\nAlso available in the Web UI\nThe built-in web UI also supports model switching. Just select a model from the dropdown and it loads automatically.\nJoin the Conversation\nWe hope this feature makes it easier to A/B test different model versions, run multi-tenant deployments, or simply switch models during development without restarting the server.\nHave questions or feedback? Drop a comment below or open an issue on GitHub."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/hf-skills-training-codex", "title": "Codex is Open Sourcing AI models", "url": "https://huggingface.co/blog/hf-skills-training-codex", "published": "Thu, 11 Dec 2025 00:00:00 GMT", "text_source": "article", "article_fetch_error": null, "text": "Codex is Open Sourcing AI models\nBuilding on our work to get Claude Code to train open source models, we are now getting Codex to go further. We gave Codex access to the Hugging Face Skills repository, which contains skills for Machine Learning and AI tasks such as training or evaluating models. With HF skills, a coding agent can:\n- Fine-tune and apply RL alignment on language models\n- Review, explain, and act on live training metrics from Trackio\n- Evaluate checkpoints and act on evaluation results\n- Create reports from experiments\n- Export to and quantize models with GGUF for local deployment\n- Publish models to the Hub\nThis tutorial dives even deeper and shows you how it works and how to use it yourself. So let's get started.\nCodex uses\nAGENTS.md\nfiles to accomplish specialized tasks, whilst Claude Code uses 'Skills'. Fortunately, 'HF-skills' is compatible with both approaches and works with major coding agents like Claude Code, Codex, or Gemini CLI.\nWith HF-skills\n, you can tell Codex something like:\nFine-tune Qwen3-0.6B on the dataset open-r1/codeforces-cots\nAnd Codex will:\n- Validate your dataset format\n- Select appropriate hardware (t4-small for a 0.6B model)\n- Use and update a training script with Trackio monitoring\n- Submit the job to Hugging Face Jobs\n- Report the job ID and estimated cost\n- Check on progress when you ask\n- Help you debug if something goes wrong\nThe model trains on Hugging Face GPUs while you do other things. When it's done, your fine-tuned model appears on the Hub, ready to use.\nThis isn't a toy demo. The extension supports the same training methods used in production: supervised fine-tuning, direct preference optimization, and reinforcement learning with verifiable rewards. You can train models from 0.5B to 7B parameters, convert them to GGUF for local deployment, and run multi-stage pipelines that combine different techniques.\nGOAL: End-to-end Machine Learning experiments\nWe explored this single prompt approach in the Claude Code tutorial. However, we can now go further and get OpenAI Codex to do end-to-end Machine Learning experiments. For example, Codex should be able to monitor progress, evaluate the models, and maintain an up to date training report. This will allow engineers to delegate experiments to Codex and review reports in a more hands-off way. It will also allow Codex to make more decisions on its own based on the training report and evaluation results.\nSo let's get started!\nSetup and Install\nBefore starting, you'll need:\n- A Hugging Face account with a Pro or Team / Enterprise plan (Jobs require a paid plan)\n- A write-access token from huggingface.co/settings/tokens\n- Codex installed and configured\nInstall Codex\nCodex is OpenAI's AI coding agent included in ChatGPT Plus, Pro, Business, Edu, and Enterprise plans. Codex brings AI assistance directly into your development workflow.\nSee the Codex documentation for installation and setup instructions.\nInstall the Hugging Face Skills\nThe Hugging Face Skills repository includes an AGENTS.md\nfile that Codex automatically detects and uses.\nClone the repository:\ngit clone https://github.com/huggingface/skills.git\ncd skills\nCodex will automatically detect the AGENTS.md\nfile in the repository and load the skills. You can verify the instructions are loaded with:\ncodex --ask-for-approval never \"Summarize the current instructions.\"\nSee the Codex AGENTS guide for more details.\nConnect to Hugging Face\nAuthenticate with Hugging Face using the hf auth login\ncommand and a write-access token from hf.co/settings/tokens:\nhf auth login\nCodex supports MCP (Model Context Protocol) servers. You can configure the Hugging Face MCP server for additional Hub integration capabilities. You can add the Hugging Face MCP server to your Codex configuration by adding the following to your ~/.codex/config.toml\nfile:\n[mcp_servers.huggingface]\ncommand = \"npx\"\nargs = [\"-y\", \"mcp-remote\", \"https://huggingface.co/mcp?login\"]\nConfigure Hugging Face MCP Server to use relevant MCP servers like Jobs in the Settings page.\nThen start Codex and you'll be directed to the Hugging Face MCP authentication page.\nYour first AI Experiment\nLet's walk through a complete example. We'll fine-tune a small model to improve code solving abilities, using the open-r1/codeforces-cots dataset and the openai_humaneval benchmark.\nThe\nopen-r1/codeforces-cots\ndataset is a dataset of codeforces problems and solutions. It is a good dataset for instruction tuning a model to solve hard coding problems.\nInstruct Codex to do an end-to-end fine-tuning experiment\nStart Codex in your project directory. Then give it a simple and clear instruction:\nStart a new fine-tuning experiment to improve code solving abilities on using SFT.\n- Maintain a report for the experiment.\n- Evaluate models with the openai_humaneval benchmark\n- Use the open-r1/codeforces-cots dataset\nYou'll notice that we've gone a bit further than the single prompt approach in the Claude Code tutorial. We've added more details to the instruction but also added more steps to the experiment.\nWhy not try iterating on this experiment yourself with more open ended questions like \"What is the best model for code solving abilities?\" or \"What is the best dataset for code solving abilities?\"\nCodex analyzes your request and prepares a training configuration. For a 0.6B model on a demo dataset, it selects t4-small\n‚Äîenough GPU for this model size and the cheapest option available. Codex will start a new report at training_reports/<model>-<dataset>-<method>.md\nwhich looks like the example below. As the experiment progresses, Codex will update the report with the latest information and each run report.\nExample Training Report\n# Base Model & Dataset\n[Base Model](https://huggingface.co/Qwen/Qwen3-0.6B)\n[Dataset](https://huggingface.co/datasets/open-r1/codeforces-cots)\n---\n# `sft-a10g` - `TBD` - `In Progress`\n## Training Parameters\n| Parameter | Value |\n|-----------|-------|\n| Method | SFT (TRL) |\n| Model | `Qwen/Qwen3-0.6B` |\n| Dataset | `open-r1/codeforces-cots` (train, 5% eval split) |\n| Max Length | 2048 |\n| Epochs | 1 (extend to 3 after first check) |\n| Per-Device Batch Size | 1 |\n| Grad Accum Steps | 8 |\n| Effective Batch | 8 |\n| Learning Rate | 5e-5 |\n| Weight Decay | 0.01 |\n| Warmup Ratio | 0.03 |\n| Eval Strategy | steps (500) |\n| Save Strategy | steps (500), `hub_strategy=every_save`, limit=2 |\n| Precision | bf16 |\n| Gradient Checkpointing | true |\n| Packing | false |\n| Hub Model | `burtenshaw/qwen3-codeforces-cots-sft` |\n| Hardware | a10g-small |\n| Timeout | 2h |\n| Trackio | project `qwen3-codeforces-cots`, run `sft-a10g` |\n## Run Status\nIn Progress (queued to submit)\n## Run Logs\nPending submission (job link will be added)\n## Trackio Logs\nPending (will link after job starts)\n## Run Evaluations\nPending (lighteval `openai_humaneval` for base + checkpoints)\n---\n# Experiment Evaluations\n| Run Title | Benchmark | Score | Evaluation Job Link | Model Link |\n|-----------|-----------|-------|---------------------|------------|\n| `sft-a10g` - `TBD` - `In Progress` | HumanEval pass@1 | TBD | TBD | [burtenshaw/qwen3-codeforces-cots-sft](https://huggingface.co/burtenshaw/qwen3-codeforces-cots-sft)\nUpdating the Training Report\nAs the experiment progresses, Codex will update the report with the latest information and each run report. You can view the report in training_reports/<model>-<dataset>-<method>.md\nfile.\nFor example, codex will update the title of the report to sft-a10g\n- TBD\n- In Progress\nwhen the experiment is in progress.\n# `base-humaneval-a10g` - `2025-12-09 13:47:47 UTC` - `In Progress`\nIt can link to the run logs and trackio logs.\n## Run Logs\n[Run Logs](https://huggingface.co/jobs/burtenshaw/6938272ec67c9f186cfe1ae3)\n## Trackio Logs\n[Trackio Logs](https://burtenshaw-trackio.hf.space/?project=qwen3-codeforces-sft&metrics=train/loss&runs=sft-qwen3-codeforces-20251209-175806&sidebar=hidden&navbar=hidden)\nAnd it will update the evaluation results in a combined table.\n# Experiment Evaluations\n| Run Title | Benchmark | Score | Evaluation Job Link | Model Link |\n|-----------|-----------|-------|---------------------|------------|\n| `base-humaneval-a10g` - `2025-12-09 13:47:47 UTC` - `Completed` | HumanEval pass@1 | 0.304 | [Logs](https://huggingface.co/jobs/burtenshaw/69382863c67c9f186cfe1ae7) | [Qwen/Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B) |\n| `qwen3-0.6b-lora-v1` - `2025-12-09 13:47:47 UTC` - `In Progress` | HumanEval pass@1 | TBD | TBD | [burtenshaw/qwen3-codeforces-cots-sft](https://huggingface.co/burtenshaw/qwen3-codeforces-cots-sft)\nDataset Validation\nDataset format and processing is the most common source of training failures and usually a significant amount of work is done in the training script. Codex can validate datasets before the job starts and either define a configuration for TRL or process the dataset separately.\nIn most cases, Codex will validate the dataset before training, but you can always check the dataset validation before submitting the job.\nCheck if open-r1/codeforces-cots works for SFT training.\nCodex runs a quick inspection on CPU (fractions of a penny) and reports:\nDataset validation for my-org/conversation-data:\nSFT: ‚úì READY\nFound 'messages' column with conversation format\nDPO: ‚úó INCOMPATIBLE\nMissing 'chosen' and 'rejected' columns\nIf your dataset needs transformation, Codex can preprocess the dataset before training.\nPreprocess the dataset open-r1/codeforces-cots to use 'good_response' and 'bad_response' instead of 'chosen' and 'rejected'.\nCodex will preprocess the dataset and update the training script with the new configuration.\nReview Before Submitting\nBefore Codex submits anything, you'll see the configuration:\nI'll fine-tune Qwen/Qwen3-0.6B on open-r1/codeforces-cots using SFT.\nConfiguration:\n- Hardware: t4-small (~$0.75/hour)\n- Estimated time: ~20 minutes\n- Estimated cost: ~$0.30\n- Output: username/qwen-codeforces-cots-sft\nThe model will be pushed to Hub automatically. Should I submit?\nThis is your chance to adjust anything. Change the output repo name, pick different hardware, or ask Codex to modify training parameters. Once you approve, the job is submitted.\nFor example, you can ask Codex to try a test run:\nDo a quick test run on 100 examples.\nTrack Progress using the Training Report\nAfter submission, you can track progress using the training report and get links to the run logs and trackio logs. You can also tell Codex to fetch the logs, summarize progress, and update the training report. As codex runs the jobs, it will update the training report with the latest information.\nExample Codex Response\n‚îÄ Worked for 10m 38s ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n‚Ä¢ Training and eval jobs are live. Key details:\n- SFT launched on open-r1/codeforces-cots (solutions_py_decontaminated, 98/2 split) with LoRA on Qwen/Qwen3-0.6B, a10g-\nlarge, bf16, max_steps=1000, batch=1, grad_accum=8, lr=2e-5, save/eval every 200 steps, hub push burtenshaw/qwen3-\ncodeforces-cots-sft. Job: https://huggingface.co/jobs/burtenshaw/6938272ec67c9f186cfe1ae3 (Trackio project codeforces-\ncots-sft, run qwen3-0.6b-lora-v1).\n- Baseline lighteval (HumanEval) on the base model queued on a10g-large. Job: https://huggingface.co/jobs/\n- Experiment report updated with current runs and links: training_reports/qwen3-codeforces-cots-sft.md.\nTo evaluate the fine-tuned checkpoints once the SFT job pushes to the hub, reuse the same lighteval script:\nhf jobs uv run --flavor a10g-large --timeout 1h --secrets HF_TOKEN -e MODEL_ID=burtenshaw/qwen3-codeforces-cots-sft -e\nRESULTS_ORG=burtenshaw -d /tmp/lighteval_humaneval.py\n(or swap MODEL_ID for a specific checkpoint path). This will log to the same results directory on the hub.\n1. Monitor the two running jobs; I can fetch logs or metrics on request.\n2. After a checkpoint lands, trigger the lighteval run above to compare against the baseline; then we can fill in pass@1 in the report.\nTrackio will monitor the training and evaluation scores for you by running evaluation jobs and selecting the best checkpoint. If you want to know the status of the training job, you can ask Codex to fetch the logs and summarize progress in a table.\nAre models outperforming the base model?\n| Model | Benchmark | Score | Evaluation Job Link | Model Link |\n|-----------|-----------|-------|---------------------|------------|\n| `qwen3-0.6b-lora-v1` - `2025-12-09 13:47:47 UTC` - `Completed` | HumanEval pass@1 | 0.342 | [Logs](<link to training job>) | [burtenshaw/qwen3-codeforces-cots-sft](https://huggingface.co/burtenshaw/qwen3-codeforces-cots-sft)\n| `base-humaneval-a10g` - `2025-12-09 13:47:47 UTC` - `Completed` | HumanEval pass@1 | 0.306 | [Logs](<link to evaluation job>) | [Qwen/Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B)\nYou can also monitor the training loss in real-time.\nCodex fetches the logs and summarizes progress.\nClick here for an example Trackio dashboard with some completed runs.\nUse Your Model\nWhen training completes, your model is on the Hub:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"burtenshaw/qwen3-codeforces-cots-sft\")\ntokenizer = AutoTokenizer.from_pretrained(\"burtenshaw/qwen3-codeforces-cots-sft\")\nTransformers is great as a standard, and we can easily convert the trained model to GGUF for local deployment. This is because the training skill contains instructions and support scripts to convert models to GGUF.\nConvert my fine-tuned model to GGUF with Q4_K_M quantization.\nPush to username/my-model-gguf.\nCodex then converts to GGUF, applies quantization, and pushes to the Hub. If we trained a LoRA adapter, it will merge the LoRA adapters into the base model.\nThen use it locally:\nllama-server -hf <username>/<model-name>:<quantization>\n# For example, to run the Qwen3-1.7B-GGUF model on your local machine:\nllama-server -hf unsloth/Qwen3-1.7B-GGUF:Q4_K_M\nHardware and Cost\nCodex selects hardware based on your model size, but understanding the tradeoffs helps you make better decisions. You can use the Hardware Guide to see the hardware options and costs, but codex will do it for you and select the best option.\nFor tiny models under 1B parameters, t4-small\nworks well. These models train quickly‚Äîexpect $1-2 for a full run. This is perfect for educational or experimental runs.\nFor small models (1-3B), step up to t4-medium\nor a10g-small\n. Training takes a few hours and costs $5-15.\nFor medium models (3-7B), you need a10g-large\nor a100-large\nwith LoRA. Full fine-tuning doesn't fit, but LoRA makes these very trainable. Budget $15-40 for production.\nFor large models (7B+), this HF skills job is not suitable for this scale yet. But stay tuned because we are working on it!\nWhat's Next\nWe've shown that Codex can handle the full lifecycle of model fine-tuning: validating data, selecting hardware, generating scripts, submitting jobs, monitoring progress, and converting outputs.\nSome things to try:\n- Fine-tune a model on your own dataset\n- Try bigger experiments with more models and datasets and let the agent create a report for you.\n- Train a reasoning model with GRPO on math or code and let the agent create a report for you.\nThe extension is open source. You can extend it, customize it for your workflows, or use it as a starting point for other training scenarios.\nResources\nCodex\n- Codex Documentation ‚Äî OpenAI's AI coding agent\n- Codex Quickstart ‚Äî Get started with Codex\n- Codex AGENTS Guide ‚Äî Using AGENTS.md files\nHugging Face Skills\n- SKILL.md ‚Äî Full skill documentation\n- Training Methods ‚Äî SFT, DPO, GRPO explained\n- Hardware Guide ‚Äî GPU selection and costs\n- TRL Documentation ‚Äî The underlying training library\n- Hugging Face Jobs ‚Äî Cloud training infrastructure\n- Trackio ‚Äî Real-time training monitoring"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/ServiceNow-AI/apriel-1p6-15b-thinker", "title": "Apriel-1.6-15b-Thinker: Cost-efficient Frontier Multimodal Performance", "url": "https://huggingface.co/blog/ServiceNow-AI/apriel-1p6-15b-thinker", "published": "Tue, 09 Dec 2025 20:06:56 GMT", "text_source": "article", "article_fetch_error": null, "text": "Apriel-1.6-15b-Thinker: Cost-efficient Frontier Multimodal Performance\nWe release Apriel-1.6-15b-Thinker, a 15-billion parameter multimodal reasoning model in ServiceNow‚Äôs Apriel SLM series which achieves SOTA performance against models 10 times it's size. Apriel-1.6 builds on top of Apriel-1.5-15b-Thinker with an extensive focus on improving text and vision reasoning, while improving token efficiency. This version was trained on NVIDIA DGX‚Ñ¢ Cloud with GB200 Grace‚Ñ¢ Blackwell Superchips.\nApriel-1.6 scores 57 on the Artificial Analysis Index, outperforming models like Gemini 2.5 Flash, Claude Haiku 4.5 and GPT OSS 20b. It obtains a score on par with Qwen3 235B A22B, while being signficantly more efficient. This new release improves or maintains task performance in comparison with the previous Apriel-1.5-15B-Thinker [1], while reducing reasoning token usage by more than 30%.\nMid-Training\nWe follow the same overall training process used for Apriel-1.5-15B-Thinker, which includes a depth-upscaling phase followed by two Continual Pretraining (CPT) stages (detailed in [1]). The depth-upscaling corpus consists of 35% data from diverse sources, including high-quality web content, scientific and technical literature, mathematical problem sets, and programming code; 15% high-quality datasets from NVIDIA Nemotron‚Ñ¢; and the remaining 50% pretraining-style data serving as replay.\nFor Apriel-1.6-15B-Thinker, we expand the Stage-1 CPT mixture, which focuses on strengthening textual reasoning and image understanding, with additional text-only samples and image-text pairs. The new text data is fully synthetic, covering general reasoning, knowledge, coding, and creative writing, while the multimodal portion spans document and chart understanding, OCR, visual-reasoning tasks, and SVG/web-code synthesis.\nFollowing Stage-1, we perform a text-only CPT run at an extended 49K sequence length and then run Stage 2 to further refine the model‚Äôs visual-reasoning capabilities. This combination produced a strong base model that provided a solid foundation for subsequent post-training. Training for this mid-training pipeline required approximately 10,000 GPU hours on NVIDIA's GB200s, a small compute footprint enabled by their high throughput and aligned with our goal of building strong models with limited resources through careful data strategy and training methodology.\nPost-Training\nUsing the midtrained model, we perform post-training following a pipeline that consists of large scale Supervised Finetuning (SFT) and Reinforcement Learning (RL) targeting both vision and text abilities.\nSupervised Finetuning (SFT)\nOur Supervised Fine-Tuning (SFT) stage focuses on improving the reasoning quality of Apriel-1.6 by training on a meticulously curated dataset of 2.4 million high-signal text samples. Each example includes explicit, step-by-step reasoning traces, enabling the model to internalize transparent reasoning processes rather than merely reproducing final answers.\nTo construct this dataset, we combined execution-verifiable synthetic samples for math, coding, and scientific problem-solving with a broad mix of instruction-following, conversational, API/function-calling, creative writing, safety, and other knowledge-intensive samples. Data quality was treated as a first-class priority: every sample passed through multi-stage de-duplication, content filtering, heuristic quality pruning, LLM-as-Judge validation, execution-based verification (where applicable), and strict decontamination against evaluation benchmarks.\nSFT was carried out in two phases, both trained at a 32K context length. In the first phase, we ran a large-scale text-only training run on the 2.4M samples for 4 epochs. Compared to Apriel-1.5-15b-Thinker, we simplified the chat template by removing redundant tags and introduced four special tokens to the tokenizer (<tool_calls>\n, </tool_calls>\n, [BEGIN FINAL RESPONSE]\n, <|end|>\n) for easier output parsing.\nThe second phase was a lightweight, multimodal run trained for 3 epochs, using rejection-sampled data from Apriel-1.5-15b-Thinker to ensure the model maintained strong performance on image inputs after the introduction of these special tokens, while also preparing it for downstream RL stages.\nThis approach provided us with a robust, high-quality SFT foundation on top of which our RL pipeline could operate effectively. The resulting model exhibits strong multimodal understanding, improved text reasoning capabilities, and enhanced agentic behavior.\nReinforcement Learning (RL)\nWe adopt a multi-stage RL setup that focuses on simultaneously improving reasoning capability and efficiency. We train the model on image domains such as visual reasoning, general visual question answering (VQA) and optical character recognition (OCR). Our training data also consists of data across different domains, such as simple questions (to encourage short, direct answers on easy queries), math (numerical reasoning), STEM (multiple-choice scientific questions), and function calling (structured tool use).\nRewards are given for correctness of the response, along with penalties for undesirable behaviour, such as verbosity, incorrect formats, etc. Overall, our setup is designed to improve the model‚Äôs reasoning ability while using fewer reasoning tokens, encouraging it to avoid unnecessary intermediate steps, stop earlier when confident, and answer more directly for simpler queries.\nTraining is done with the Group Sequence Policy Optimization loss (GSPO) [2] using the VeRL framework and rule-based verification.\nEvaluation\nText Evaluation\nWe evaluate Apriel-1.6 on various domains such as tool use, math, coding, instruction following and long context.\nText benchmarks included in the Artificial Analysis Index v3.0 use scores reported by Artificial Analysis. All other benchmarks were evaluated internally.\nCategory Benchmark Apriel-1.6-15B-Thinker Apriel-1.5-15B-Thinker GPT OSS 120B DeepSeek R1 0528 Gemini 2.5 Flash (Sep) GPT 5 mini (high) Claude 4.5 Sonnet (thinking) o3-mini (high) Average Score** 53.22 46.56 52.56 51.92 50.71 62.58 60.37 48.85 Function Calling BFCL v3 only 63.50 51.88 50.62 39.75 39.75 17.62 - 50 Tau2 bench Telecom 69 57.8 66 37 32 68 50.8 31 Tau2 bench Retail 66.67 46.78 61.4 59.94 61.69 73.39 69.8 75.73 Tau2 bench Airline 58 52 45.3 47.33 56.66 59.33 58 61.33 ComplexFuncBench 33.2 19 24.6 24.2 26.3 37.5 24.6 18.9 Instruction Following Agent IF 57.2 55 54.20 52.20 49.70 57.60 54.50 54.90 Multi IF 83.34 76.91 82.95 73.76 82.49 85.37 84.32 87.28 Multi-Challenge 46.15 41.39 46.90 44.50 49.08 57.90 42.49 38.46 IF Bench 69 62 69 40 50 75 57 70.07 Math AIME 25 88 88 93 76 73 91 88 86.67 Coding Struct Eval 79 48.50 71 73 70 69.92 76 73 LCB 81 73 88 77 70 84 71 73 SciCode 37 35 39 40 41 39 45 40 Agentic DeepresearchBench 36.47 32.73 36.30 34.19 38.15 - - 33.40 GAIA 40 30.91 21.21 32.12 47.88 65.45 69.09 23.03 Work-Arena L1 50.2 51.5 50.9 63.9 51.8 65.5 62.7 52.4 OS World Small 16.70 13.90 16.70 25 19.40 22.20 30.60 19.40 SWE Bench Verified 23 16 31 29.60 34.20 61 64.2 22.60 Terminal Bench 14 10 22 15 13 31 33 5.67 Aider Polyglot 37.68 26.37 42 71.40 40 71.60 78 60.40 Knowledge MMLU Pro 79 77 81 85 83 84 88 80 Creative Writing Creative writing v3 / EQ Bench 59.73 60.24 53.70 79.40 74.25 75.25 80.70 30.40 Others GPQA Diamond 73 71 78 81 79 83 83 77 HLE 10 12 18.5 14.9 11.1 19.7 17.3 12.3 Long Context AA LCR 50* 20 51 55 62 68 66 30***\n* This score is with DCA enabled. Without this, the model scores 36.\n** The average score is calculated using all benchmarks except BFCL v3 Only and DeepResearchBench, since some models do not have scores for these two benchmarks.\n*** AA LCR score for o3-mini-high is projected score based on its AA Index score.\nImage Evaluation\nWe evaluate the Apriel-1.6 model on a representative set of evaluations with the prime focus on mathematical reasoning, visual question answering, logical reasoning, STEM related tasks and chart based reasoning. All evaluations are done using VLMEvalkit. Apriel-1.6 improves on its predecessor by 4 points on the average of 13 benchmarks of the Image Index comprising of the following benchmarks: MathVision, MathVista, MMMU (validation), MMMU-Pro (10 choice COT), MMMU-Pro (Vision only COT), MathVerse (Vision Dominant), MathVerse (Text Dominant), MMStar, BLINK, LogicVista, CharXiV (descriptive), CharXiV (reasoning), AI2D (test).\nCost-Efficient Frontier Performance\nApriel-1.6-15B-Thinker sits in the sweet spot of the cost-efficient frontier. It delivers intelligence scores that rival or surpass much larger models while using only 15B parameters. On the chart, it‚Äôs firmly inside the most attractive quadrant, balancing efficiency with top-tier reasoning. In practice, this means Apriel-1.6-15B-Thinker offers strong performance and deep reasoning at a fraction of the compute and deployment cost of heavyweight competitors, making it an exceptionally efficient choice for the real-world, especially in enterprise applications.\nOur post-training focuses heavily on improving reasoning-token efficiency. The image above showing intelligence score against token usage highlights the effectiveness of our post-training. Apriel-1.6-15B-Thinker again lands in most attractive quadrant. The model reaches a high Artificial Analysis Intelligence Index score while using far fewer tokens than many similarly capable or larger models. In comparison to Apriel-1.5-15b-Thinker [1], we reduce token usage by over 30%.\nOverall, Apriel-1.6 is a highly-capable reasoner, that maintains memory and efficiency characteristics required for enterprise deployment.\nNotes and Limitations\nWe are a small lab with big goals. While we are not GPU poor, our lab, in comparison has a tiny fraction of the compute available to other Frontier labs. Our goal with this work is to show that a SOTA model can be built with limited resources if you have the right data, design and solid methodology.\nWe set out to build a small but powerful model, aiming for capabilities on par with frontier models. Developing a 15B model with this level of performance requires tradeoffs, so we prioritized getting SOTA-level performance and improving reasoning token efficiency.\nThis model is trained to perform extensive reasoning for difficult questions and less reasoning effort for simpler questions. We are always actively working to make our models more efficient and concise in future releases.\nThe model has a few vision-related limitations to be aware of. Complex or low-quality images can reduce OCR accuracy, dense scenes (like crowds or many similar objects) can make subtle details and counting more challenging, and highly detailed or unusually formatted charts may occasionally lead to imperfect interpretations. It may also be less precise with fine-grained visual grounding, so bounding-box predictions can sometimes be approximate or inconsistent.\nReferences\n[1] Radhakrishna, S., Tiwari, A., Shukla, A., Hashemi, M., Maheshwary, R., Malay, S.K.R., Mehta, J., Pattnaik, P., Mittal, S., Slimi, K., Ogueji, K., Oladipo, A., Parikh, S., Bamgbose, O., Liang, T., Masry, A., Mahajan, K., Mudumba, S.R., Yadav, V., Madhusudhan, S.T., Scholak, T., Davasam, S., Sunkara, S. and Chapados, N., 2025. Apriel-1.5-15b-Thinker. arXiv preprint arXiv:2510.01141.\n[2] Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., Zhou, J. and Lin, J., 2025. Group Sequence Policy Optimization. arXiv preprint arXiv:2507.18071.\nContributors\nTraining\nShruthan Radhakrishna, Aman Tiwari, Jash Mehta, Toby Liang, Kelechi Ogueji, Akintunde Oladipo, Masoud Hashemi, Rishabh Maheshwary, Pulkit Pattnaik, Saloni Mittal, Khalil Slimi, Akshay Kalkunte, Shiva Krishna Reddy Malay\nBenchmarking & Evaluation\nAanjaneya Shukla, Oluwanifemi Bamgbose, Massimo Caccia, Dheeraj Vattikonda, Jishnu S Nair, Varun Pandey, Shashank Maiya, Dhruv Jhamb, Nicolas Gontier, Patrice Bechard, Tayfun Tuna, Kavya Sriram, Denis Akhiyarov, Hari Subramani, Tara Bogavelli\nLeads and Management\nSathwik Tejaswi Madhusudhan, Torsten Scholak, Sagar Davasam, Srinivas Sunkara"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/swift-huggingface", "title": "Introducing swift-huggingface: The Complete Swift Client for Hugging Face", "url": "https://huggingface.co/blog/swift-huggingface", "published": "Fri, 05 Dec 2025 00:00:00 GMT", "text_source": "article", "article_fetch_error": null, "text": "Introducing swift-huggingface: The Complete Swift Client for Hugging Face\nToday, we're announcing swift-huggingface, a new Swift package that provides a complete client for the Hugging Face Hub.\nYou can start using it today as a standalone package,\nand it will soon integrate into swift-transformers as a replacement for its current HubApi\nimplementation.\nThe Problem\nWhen we released swift-transformers 1.0 earlier this year, we heard loud and clear from the community:\n- Downloads were slow and unreliable. Large model files (often several gigabytes) would fail partway through with no way to resume. Developers resorted to manually downloading models and bundling them with their apps ‚Äî defeating the purpose of dynamic model loading.\n- No shared cache with the Python ecosystem.\nThe Python\ntransformers\nlibrary stores models in~/.cache/huggingface/hub\n. Swift apps downloaded to a different location with a different structure. If you'd already downloaded a model using the Python CLI, you'd download it again for your Swift app. - Authentication is confusing. Where should tokens come from? Environment variables? Files? Keychain? The answer is, \"It depends\", and the existing implementation didn't make the options clear.\nIntroducing swift-huggingface\nswift-huggingface is a ground-up rewrite focused on reliability and developer experience. It provides:\n- Complete Hub API coverage ‚Äî models, datasets, spaces, collections, discussions, and more\n- Robust file operations ‚Äî progress tracking, resume support, and proper error handling\n- Python-compatible cache ‚Äî share downloaded models between Swift and Python clients\n- Flexible authentication ‚Äî a\nTokenProvider\npattern that makes credential sources explicit - OAuth support ‚Äî first-class support for user-facing apps that need to authenticate users\n- Xet storage backend support (Coming soon!) ‚Äî chunk-based deduplication for significantly faster downloads\nLet's look at some examples.\nFlexible Authentication with TokenProvider\nOne of the biggest improvements is how authentication works. The TokenProvider\npattern makes it explicit where credentials come from:\nimport HuggingFace\n// For development: auto-detect from environment and standard locations\n// Checks HF_TOKEN, HUGGING_FACE_HUB_TOKEN, ~/.cache/huggingface/token, etc.\nlet client = HubClient.default\n// For CI/CD: explicit token\nlet client = HubClient(tokenProvider: .static(\"hf_xxx\"))\n// For production apps: read from Keychain\nlet client = HubClient(tokenProvider: .keychain(service: \"com.myapp\", account: \"hf_token\"))\nThe auto-detection follows the same conventions as the Python huggingface_hub\nlibrary:\nHF_TOKEN\nenvironment variableHUGGING_FACE_HUB_TOKEN\nenvironment variableHF_TOKEN_PATH\nenvironment variable (path to token file)$HF_HOME/token\nfile~/.cache/huggingface/token\n(standard HF CLI location)~/.huggingface/token\n(fallback location)\nThis means if you've already logged in with hf auth login\n,\nswift-huggingface will automatically find and use that token.\nOAuth for User-Facing Apps\nBuilding an app where users sign in with their Hugging Face account? swift-huggingface includes a complete OAuth 2.0 implementation:\nimport HuggingFace\n// Create authentication manager\nlet authManager = try HuggingFaceAuthenticationManager(\nclientID: \"your_client_id\",\nredirectURL: URL(string: \"yourapp://oauth/callback\")!,\nscope: [.openid, .profile, .email],\nkeychainService: \"com.yourapp.huggingface\",\nkeychainAccount: \"user_token\"\n)\n// Sign in user (presents system browser)\ntry await authManager.signIn()\n// Use with Hub client\nlet client = HubClient(tokenProvider: .oauth(manager: authManager))\n// Tokens are automatically refreshed when needed\nlet userInfo = try await client.whoami()\nprint(\"Signed in as: \\(userInfo.name)\")\nThe OAuth manager handles token storage in Keychain, automatic refresh, and secure sign-out. No more manual token management.\nReliable Downloads\nDownloading large models is now straightforward with proper progress tracking and resume support:\n// Download with progress tracking\nlet progress = Progress(totalUnitCount: 0)\nTask {\nfor await _ in progress.publisher(for: \\.fractionCompleted).values {\nprint(\"Download: \\(Int(progress.fractionCompleted * 100))%\")\n}\n}\nlet fileURL = try await client.downloadFile(\nat: \"model.safetensors\",\nfrom: \"microsoft/phi-2\",\nto: destinationURL,\nprogress: progress\n)\nIf a download is interrupted, you can resume it:\n// Resume from where you left off\nlet fileURL = try await client.resumeDownloadFile(\nresumeData: savedResumeData,\nto: destinationURL,\nprogress: progress\n)\nFor downloading entire model repositories,\ndownloadSnapshot\nhandles everything:\nlet modelDir = try await client.downloadSnapshot(\nof: \"mlx-community/Llama-3.2-1B-Instruct-4bit\",\nto: cacheDirectory,\nmatching: [\"*.safetensors\", \"*.json\"], // Only download what you need\nprogressHandler: { progress in\nprint(\"Downloaded \\(progress.completedUnitCount) of \\(progress.totalUnitCount) files\")\n}\n)\nThe snapshot function tracks metadata for each file, so subsequent calls only download files that have changed.\nShared Cache with Python\nRemember the second problem we mentioned? \"No shared cache with the Python ecosystem.\" That's now solved.\nswift-huggingface implements a Python-compatible cache structure that allows seamless sharing between Swift and Python clients:\n~/.cache/huggingface/hub/\n‚îú‚îÄ‚îÄ models--deepseek-ai--DeepSeek-V3.2/\n‚îÇ ‚îú‚îÄ‚îÄ blobs/\n‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ <etag> # actual file content\n‚îÇ ‚îú‚îÄ‚îÄ refs/\n‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ main # contains commit hash\n‚îÇ ‚îî‚îÄ‚îÄ snapshots/\n‚îÇ ‚îî‚îÄ‚îÄ <commit_hash>/\n‚îÇ ‚îî‚îÄ‚îÄ config.json # symlink ‚Üí ../../blobs/<etag>\nThis means:\n- Download once, use everywhere.\nIf you've already downloaded a model with the\nhf\nCLI or the Python library, swift-huggingface will find it automatically. - Content-addressed storage.\nFiles are stored by their ETag in the\nblobs/\ndirectory. If two revisions share the same file, it's only stored once. - Symlinks for efficiency. Snapshot directories contain symlinks to blobs, minimizing disk usage while maintaining a clean file structure.\nThe cache location follows the same environment variable conventions as Python:\nHF_HUB_CACHE\nenvironment variableHF_HOME\nenvironment variable +/hub\n~/.cache/huggingface/hub\n(default)\nYou can also use the cache directly:\nlet cache = HubCache.default\n// Check if a file is already cached\nif let cachedPath = cache.cachedFilePath(\nrepo: \"deepseek-ai/DeepSeek-V3.2\",\nkind: .model,\nrevision: \"main\",\nfilename: \"config.json\"\n) {\nlet data = try Data(contentsOf: cachedPath)\n// Use cached file without any network request\n}\nTo prevent race conditions when multiple processes access the same cache,\nswift-huggingface uses file locking\n(flock(2)\n).\nBefore and After\nHere's what downloading a model snapshot looked like with the old HubApi\n:\n// Before: HubApi in swift-transformers\nlet hub = HubApi()\nlet repo = Hub.Repo(id: \"mlx-community/Llama-3.2-1B-Instruct-4bit\")\n// No progress tracking, no resume, errors swallowed\nlet modelDir = try await hub.snapshot(\nfrom: repo,\nmatching: [\"*.safetensors\", \"*.json\"]\n) { progress in\n// Progress object exists but wasn't always accurate\nprint(progress.fractionCompleted)\n}\nAnd here's the same operation with swift-huggingface:\n// After: swift-huggingface\nlet client = HubClient.default\nlet modelDir = try await client.downloadSnapshot(\nof: \"mlx-community/Llama-3.2-1B-Instruct-4bit\",\nto: cacheDirectory,\nmatching: [\"*.safetensors\", \"*.json\"],\nprogressHandler: { progress in\n// Accurate progress per file\nprint(\"\\(progress.completedUnitCount)/\\(progress.totalUnitCount) files\")\n}\n)\nThe API is similar, but the implementation is completely different ‚Äî\nbuilt on URLSession\ndownload tasks with proper\ndelegate handling, resume data support, and metadata tracking.\nBeyond Downloads\nBut wait, there's more! swift-huggingface contains a complete Hub client:\n// List trending models\nlet models = try await client.listModels(\nfilter: \"library:mlx\",\nsort: \"trending\",\nlimit: 10\n)\n// Get model details\nlet model = try await client.getModel(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\nprint(\"Downloads: \\(model.downloads ?? 0)\")\nprint(\"Likes: \\(model.likes ?? 0)\")\n// Work with collections\nlet collections = try await client.listCollections(owner: \"huggingface\", sort: \"trending\")\n// Manage discussions\nlet discussions = try await client.listDiscussions(kind: .model, \"username/my-model\")\nAnd that's not all! swift-huggingface has everything you need to interact with Hugging Face Inference Providers, giving your app instant access to hundreds of machine learning models, powered by world-class inference providers:\nimport HuggingFace\n// Create a client (uses auto-detected credentials from environment)\nlet client = InferenceClient.default\n// Generate images from a text prompt\nlet response = try await client.textToImage(\nmodel: \"black-forest-labs/FLUX.1-schnell\",\nprompt: \"A serene Japanese garden with cherry blossoms\",\nprovider: .hfInference,\nwidth: 1024,\nheight: 1024,\nnumImages: 1,\nguidanceScale: 7.5,\nnumInferenceSteps: 50,\nseed: 42\n)\n// Save the generated image\ntry response.image.write(to: URL(fileURLWithPath: \"generated.png\"))\nCheck the README for a full list of everything that's supported.\nWhat's Next\nWe're actively working on two fronts:\nIntegration with swift-transformers.\nWe have a pull request in progress to replace HubApi\nwith swift-huggingface.\nThis will bring reliable downloads to everyone using swift-transformers,\nmlx-swift-lm,\nand the broader ecosystem.\nIf you maintain a Swift-based library or app and want help adopting swift-huggingface, reach out ‚Äî we're happy to help.\nFaster downloads with Xet. We're adding support for the Xet storage backend, which enables chunk-based deduplication and significantly faster downloads for large models. More on this soon.\nTry It Out\nAdd swift-huggingface to your project:\ndependencies: [\n.package(url: \"https://github.com/huggingface/swift-huggingface.git\", from: \"0.4.0\")\n]\nWe'd love your feedback. If you've been frustrated with model downloads in Swift, give this a try and let us know how it goes. Your experience reports will help us prioritize what to improve next.\nResources\nThanks to the swift-transformers community for the feedback that shaped this project, and to everyone who filed issues and shared their experiences. This is for you. ‚ù§Ô∏è"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/intel-deepmath", "title": "DeepMath: A lightweight math reasoning Agent with smolagents", "url": "https://huggingface.co/blog/intel-deepmath", "published": "Thu, 04 Dec 2025 00:00:00 GMT", "text_source": "article", "article_fetch_error": null, "text": "DeepMath: A lightweight math reasoning Agent with smolagents\nBy Intel AI Software Group\nDeepMath is an aligned math reasoning agent built on Qwen3-4B Thinking and fine-tuned with GRPO (Group Relative Policy Optimization). Instead of verbose text, the model emits tiny Python snippets for intermediate steps, runs them in a secure sandbox, and folds the results back into its reasoning, reducing errors and output length. The agent is implemented using the smolagents library.\nWe evaluate DeepMath on four math datasets: MATH500, AIME, HMMT, and HLE, and show that:\nü§ñ The math agent alone reduces output lengths by up to 66%, while often improving accuracy.\n‚ö° GRPO training improves the agent performance even further, in almost all benchmarks.\nüëâ Code and evaluation scripts: https://github.com/IntelLabs/DeepMath\nüëâ Model: https://huggingface.co/Intel/deepmath-v1\nWhy DeepMath?\nLarge language models (LLMs) have advanced reasoning capabilities, but mathematical problem-solving remains challenging; chain-of-thought traces can be lengthy and prone to arithmetic mistakes. Recent works[^1][^2] demonstrate that small models can reach strong performance, and other studies[^3] investigate tool use to improve reliability. What those papers generally do not emphasize is reducing trace verbosity or explicitly training models to prefer short, computation-oriented traces executed in a constrained, auditable environment.\nWe focused on two goals:\nOffload deterministic computation to a safe executor.\nTrain models to prefer concise, computation-oriented traces over verbose text.\nDeepMath tackles this by combining a small Python executor with a fine-tuned LLM, enabling concise, computation-driven reasoning. The model learns to generate short Python snippets, which are executed in a sandbox and reintegrated into the context. GRPO fine-tuning encourages this behavior by rewarding correctness and encouraging shorter outputs.\nHow It Works\n- Base model: Qwen3-4B Thinking.\n- Executor constraints: sandboxed environment, allow-list of imported modules, per-snippet timeout.\n- Inference: based on smolagents, a math agent was created. vLLM is used as the inference engine.\n- Training: based on the GRPO trainer in TRL, we modified TRL's vLLM client and server to generate GRPO completions using our DeepMath agent.\nFigure 1: The vLLM client and server were modified to use the DeepMath agent in generating the candidates, while using the vLLM backend.\nAgent Interface: During inference, the model can output normal tokens or special agent calls containing Python snippets.\nExecution: Snippets run in a sandboxed environment with strict safety constraints (no file I/O, no network, timeouts).\nDesign Goals:\nConcision: Replace multi-line textual calculations with short, focused snippets.\nDeterminism & Safety: Enforce strict execution limits.\nInterpretability: Snippets are readable and auditable.\nFigure 2: Output example where python code is generated, evaluated and the answer is inserted into the trace and used for context.\nTraining with GRPO\nWe fine-tune the model using GRPO, a reward-based optimization that balances:\nAccuracy Reward: +1 for correct answers.\nUsing code snippets: +1 for generating code snippets, weighted 10:1 vs. the accuracy reward.\nLength reduction: shorter lengths are encouraged by limiting the GRPO completion candidates to 5k tokens.\nTemperature Scheduling: We implemented linear temperature scheduling (T=1.2 ‚Üí T=0.7) to balance exploration and stability during training. This approach aims to enhance experimentation during the initial training phases, subsequently reducing the temperature as we refine our proficiency in the skill.\nIn-context Learning: we include 4 solved examples where the trace contains agent calls and executor outputs, so the model learns the syntax and the call/response pattern.\nDataset: we used the Tool-Integrated Reasoning (TIR) subset of the OpenMathReasoning dataset. Note that GRPO only uses the problem, not the solution in the data. This dataset was chosen to ensure the problems benefit from the external tool.\nEvaluation\nWe benchmarked DeepMath against baselines on four datasets. Metrics include:\nmajority@16: robustness across samples, as used in previous math reasoning works, see references.\nMean output length: brevity.\nWe compare a baseline configuration (Qwen3-4B-Thinking-2507, no agenting) with our DeepMath model. As ablation, we evaluate the agentic framework we developed running with the untrained Qwen3 model, denoted by +Agent. Additionally, we examine whether the GRPO training (for agentic use) improves non-agentic inference, denoted by +GRPO. Thus the two ablations are independent, not additive.\nWe observe the agentic inference reduces output lengths, with mixed accuracy results. The DeepMath model is both GRPO-trained and run in agentic mode, and shows the highest accuracy with shortened traces. We conclude both GRPO training and agentic inference are needed for best results.\nKey Insight: DeepMath reduces output length by up to 66% while improving accuracy on challenging datasets.\nWhy It Matters\nAccuracy: Offloading computation reduces arithmetic errors.\nEfficiency: Shorter outputs mean faster inference and easier interpretability.\nSafety: Sandbox execution mitigates risks of running arbitrary code.\nConclusion\nDeepMath demonstrates a practical and lightweight way to combine a small executor with an LLM and to train the model to prefer short, computation-driven traces. Offloading deterministic computation reduces arithmetic and numerical errors and shortens traces, and GRPO fine-tuning further encourages concise, correct answers. The result is a more accurate and more interpretable math-solving agent without requiring a massive model or heavyweight external tools.\nTry It Yourself\nCheck out the GitHub repo and share your feedback! Contributions welcome. üöÄ\nCitation\nIf you use DeepMath in your research, please cite:\n@software{deepmath2025,\nauthor = {Fleischer, Daniel and Berchansky, Moshe and Wasserblat, Moshe},\ntitle = {DeepMath: A Lightweight Math Reasoning Agent for LLMs},\nyear = {2025},\npublisher = {Intel AI Labs},\nurl = {https://github.com/IntelLabs/DeepMath}\n}\nLimitations & Future Work\nScope: we focused on a small model and on mathematical reasoning.\nGeneralization: evaluated on contest-style math; results may not transfer to open-ended mathematical creativity or formal proofs.\nExecuting generated code is inherently risky. DeepMath uses strict sandboxing and resource limits, but any deployment should carefully manage attack surfaces and enforce rate limits.\nReferences\n[1] Luo, Michael, Sijun Tan, Justin Wong, et al. 2025. ‚ÄúDeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL.‚Äù https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2\n[2] Liu, Mingjie, Shizhe Diao, Ximing Lu, et al. 2025. ‚ÄúProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models.‚Äù arXiv:2505.24864. Preprint, arXiv, May 30. https://doi.org/10.48550/arXiv.2505.24864\n[3] Moshkov, Ivan, Darragh Hanley, Ivan Sorokin, et al. 2025. ‚ÄúAIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning Dataset.‚Äù arXiv:2504.16891. Preprint, arXiv, April 23. https://doi.org/10.48550/arXiv.2504.16891"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/hf-skills-training", "title": "We Got Claude to Fine-Tune an Open Source LLM", "url": "https://huggingface.co/blog/hf-skills-training", "published": "Thu, 04 Dec 2025 00:00:00 GMT", "text_source": "article", "article_fetch_error": null, "text": "We Got Claude to Fine-Tune an Open Source LLM\nWe gave Claude the ability to fine-tune language models using a new tool called Hugging Face Skills. Not just write training scripts, but to actually submit jobs to cloud GPUs, monitor progress, and push finished models to the Hugging Face Hub. This tutorial shows you how it works and how to use it yourself.\nClaude Code can use \"skills\"‚Äîpackaged instructions, scripts, and domain knowledge‚Äîto accomplish specialized tasks. The\nhf-llm-trainer\nskill teaches Claude everything it needs to know about training: which GPU to pick for your model size, how to configure Hub authentication, when to use LoRA versus full fine-tuning, and how to handle the dozens of other decisions that go into a successful training run.\nWith this skill, you can tell Claude things like:\nFine-tune Qwen3-0.6B on the dataset open-r1/codeforces-cots\nAnd Claude will:\n- Validate your dataset format\n- Select appropriate hardware (t4-small for a 0.6B model)\n- Use and update a training script with Trackio monitoring\n- Submit the job to Hugging Face Jobs\n- Report the job ID and estimated cost\n- Check on progress when you ask\n- Help you debug if something goes wrong\nThe model trains on Hugging Face GPUs while you do other things. When it's done, your fine-tuned model appears on the Hub, ready to use.\nThis isn't a toy demo. The skill supports the same training methods used in production: supervised fine-tuning, direct preference optimization, and reinforcement learning with verifiable rewards. You can train models from 0.5B to 70B parameters, convert them to GGUF for local deployment, and run multi-stage pipelines that combine different techniques.\nSetup and Install\nBefore starting, you'll need:\n- A Hugging Face account with a Pro or Team / Enterprise plan (Jobs require a paid plan)\n- A write-access token from huggingface.co/settings/tokens\n- A coding agent like Claude Code, OpenAI Codex, or Google's Gemini CLI\nHugging Face skills are compatible with Claude Code, Codex, and Gemini CLI. With integrations on the way for Cursor, Windsurf, and Continue.\nClaude Code\n- Register the repository as a marketplace plugin:\n/plugin marketplace add huggingface/skills\n- To install a skill, run:\n/plugin install <skill-folder>@huggingface-skills\nFor example:\n/plugin install hf-llm-trainer@huggingface-skills\nCodex\n- Codex will identify the skills via the\nAGENTS.md\nfile. You can verify the instructions are loaded with:\ncodex --ask-for-approval never \"Summarize the current instructions.\"\n- For more details, see the Codex AGENTS guide.\nGemini CLI\nThis repo includes\ngemini-extension.json\nto integrate with the Gemini CLI.Install locally:\ngemini extensions install . --consent\nor use the GitHub URL:\ngemini extensions install https://github.com/huggingface/skills.git --consent\n- See Gemini CLI extensions docs for more help.\nConnect to Hugging Face\nYou have to authenticate to your Hugging Face account with a write-access token so that the job can create a model repo.\nSet up your token:\nhf auth login\n# or\nexport HF_TOKEN=hf_your_write_access_token_here\nConfigure Hugging Face MCP Server to use your write token by sending it in either the\nHF_TOKEN\norAuthorization: Bearer\nHTTP Headers.For Claude Code:\nclaude mcp add --transport http hf-skills https://huggingface.co/mcp?bouquet=skills --header \"Authorization: Bearer $HF_TOKEN\"\nYour First Training Run\nLet's walk through a complete example. We'll fine-tune a small model to see the full workflow, then explore more advanced capabilities.\nInstruct the coding agent to fine tune\nStart with a simple and clear instruction to fine tune a specific model\nFine-tune Qwen3-0.6B on the open-r1/codeforces-cots dataset for instruction following.\nThe coding agent analyzes your request and prepares a training configuration. For a 0.6B model on a demo dataset, it selects t4-small\n‚Äîenough GPU for this model size and the cheapest option available.\nThe\nopen-r1/codeforces-cots\ndataset is a dataset of codeforces problems and solutions. It is a good dataset for instruction tuning a model to solve hard coding problems.\nThis works for vision language models too! You can simply run \"Fine-tune Qwen/Qwen3-VL-2B-Instruct on llava-instruct-mix\"\nReview Before Submitting\nBefore your coding agent submits anything, you'll see the configuration:\nI'll fine-tune Qwen/Qwen3-0.6B on open-r1/codeforces-cots using SFT.\nConfiguration:\n- Hardware: t4-small (~$0.75/hour)\n- Estimated time: ~20 minutes\n- Estimated cost: ~$0.30\n- Output: username/qwen-codeforces-cots-sft\nThe model will be pushed to Hub automatically. Should I submit?\nThis is your chance to adjust anything. Change the output repo name, pick different hardware, or ask Claude to modify training parameters. Once you approve, the agent submits the job.\nFor example, you can ask the agent to try a test run:\nDo a quick test run on 100 examples.\nTrack Progress\nAfter submission, you get job details:\n‚úÖ Job submitted successfully!\nJob ID: abc123xyz\nMonitor: https://huggingface.co/jobs/username/abc123xyz\nExpected time: ~20 minutes\nEstimated cost: ~$0.30\nView real-time metrics at: https://huggingface.co/spaces/username/trackio\nThe skill includes Trackio integration, so you can watch training loss decrease in real-time. Jobs run asynchronously so you can close your terminal and come back later. When you want an update:\nHow's my training job doing?\nThen the agent fetches the logs and summarizes progress.\nUse Your Model\nWhen training completes, your model is on the Hub:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"username/qwen-codeforces-cots-sft\")\ntokenizer = AutoTokenizer.from_pretrained(\"username/qwen-codeforces-cots-sft\")\nThat's the full loop. You described what you wanted in plain English, and the agent handled GPU selection, script generation, job submission, authentication, and persistence. The whole thing cost about thirty cents.\nTraining Methods\nThe skill supports three training approaches. Understanding when to use each one helps you get better results.\nSupervised Fine-Tuning (SFT)\nSFT is where most projects start. You provide demonstration data‚Äîexamples of inputs and desired outputs‚Äîand training adjusts the model to match those patterns.\nUse SFT when you have high-quality examples of the behavior you want. Customer support conversations, code generation pairs, domain-specific Q&A‚Äîanything where you can show the model what good looks like.\nFine-tune Qwen3-0.6B on my-org/support-conversations for 3 epochs.\nThe agent validates the dataset, selects hardware (a10g-large with LoRA for a 7B model), and configures training with checkpoints and monitoring.\nFor models larger than 3B parameters, the agent automatically uses LoRA (Low-Rank Adaptation) to reduce memory requirements. This makes training 7B or 13B models feasible on single GPUs while preserving most of the quality of full fine-tuning.\nDirect Preference Optimization (DPO)\nDPO trains on preference pairs‚Äîresponses where one is \"chosen\" and another is \"rejected.\" This aligns model outputs with human preferences, typically after an initial SFT stage.\nUse DPO when you have preference annotations from human labelers or automated comparisons. DPO optimizes directly for the preferred response without needing a separate reward model.\nRun DPO on my-org/preference-data to align the SFT model I just trained.\nThe dataset has 'chosen' and 'rejected' columns.\nDPO is sensitive to dataset format. It requires columns named exactly\nchosen\nandrejected\n, or aprompt\ncolumn with the input. The agent validates this first and shows you how to map columns if your dataset uses different names.\nYou can run DPO using Skills on vision language models too! Try it out with openbmb/RLAIF-V-Dataset. Claude will apply minor modifications but will succeed in training.\nGroup Relative Policy Optimization (GRPO)\nGRPO is a reinforcement learning task that is proven to be effective on verifiable tasks like solving math problems, writing code, or any task with a programmatic success criterion.\nTrain a math reasoning model using GRPO on the openai/gsm8k dataset based on Qwen3-0.6B.\nThe model generates responses, receives rewards based on correctness, and learns from the outcomes. This is more complex than SFT or DPO, but the configuration is similar.\nHardware and Cost\nThe agent selects hardware based on your model size, but understanding the tradeoffs helps you make better decisions.\nModel Size to GPU Mapping\nFor tiny models under 1B parameters, t4-small\nworks well. These models train quickly‚Äîexpect $1-2 for a full run. This is perfect for educational or experimental runs.\nFor small models (1-3B), step up to t4-medium\nor a10g-small\n. Training takes a few hours and costs $5-15.\nFor medium models (3-7B), you need a10g-large\nor a100-large\nwith LoRA. Full fine-tuning doesn't fit, but LoRA makes these very trainable. Budget $15-40 for production.\nFor large models (7B+), this HF skills job is not suitable.\nDemo vs Production\nWhen testing a workflow, start small:\nDo a quick test run to SFT Qwen-0.6B with 100 examples of my-org/support-conversations.\nThe coding agent configures minimal training‚Äîenough to verify your pipeline works without real cost.\nFor production, be explicit:\nSFT Qwen-0.6B for production on the full my-org/support-conversations.\nCheckpoints every 500 steps, 3 epochs, cosine learning rate.\nAlways run a demo before committing to a multi-hour production job. A $0.50 demo that catches a format error saves a $30 failed run.\nDataset Validation\nDataset format is the most common source of training failures. The agent can validate datasets before you spend GPU time.\nCheck if my-org/conversation-data works for SFT training.\nThe agent runs a quick inspection on CPU (fractions of a penny) and reports:\nDataset validation for my-org/conversation-data:\nSFT: ‚úì READY\nFound 'messages' column with conversation format\nDPO: ‚úó INCOMPATIBLE\nMissing 'chosen' and 'rejected' columns\nIf your dataset needs transformation, the agent can show you how:\nMy DPO dataset uses 'good_response' and 'bad_response' instead\nof 'chosen' and 'rejected'. How do I fix this?\nThe agent provides mapping code and can incorporate it directly into your training script.\nMonitoring Training\nReal-time monitoring helps you catch problems early. The skill configures Trackio by default‚Äîafter submitting a job, you can watch metrics at:\nhttps://huggingface.co/spaces/username/trackio\nThis shows training loss, learning rate, and validation metrics. A healthy run shows steadily decreasing loss.\nAsk the agent about status anytime:\nWhat's the status of my training job?\nJob abc123xyz is running (45 minutes elapsed)\nCurrent step: 850/1200\nTraining loss: 1.23 (‚Üì from 2.41 at start)\nLearning rate: 1.2e-5\nEstimated completion: ~20 minutes\nIf something goes wrong, the agent helps diagnose. Out of memory? the agent suggests reducing batch size or upgrading hardware. Dataset error? The agent identifies the mismatch. Timeout? The agent recommends longer duration or faster training settings.\nConverting to GGUF\nAfter training, you might want to run your model locally. The GGUF format works with llama.cpp and dependent tools like LM Studio, Ollama, etc.\nConvert my fine-tuned model to GGUF with Q4_K_M quantization.\nPush to username/my-model-gguf.\nThe agent submits a conversion job that merges LoRA adapters, converts to GGUF, applies quantization, and pushes to Hub.\nThen use it locally:\nllama-server -hf <username>/<model-name>:<quantization>\n# For example, to run the Qwen3-1.7B-GGUF model on your local machine:\nllama-server -hf unsloth/Qwen3-1.7B-GGUF:Q4_K_M\nWhat's Next\nWe've shown that coding agents like Claude Code, Codex, or Gemini CLI can handle the full lifecycle of model fine-tuning: validating data, selecting hardware, generating scripts, submitting jobs, monitoring progress, and converting outputs. This turns what used to be a specialized skill into something you can do through conversation.\nSome things to try:\n- Fine-tune a model on your own dataset\n- Build a preference-aligned model with SFT ‚Üí DPO\n- Train a reasoning model with GRPO on math or code\n- Convert a model to GGUF and run it with Ollama\nThe skill is open source. You can extend it, customize it for your workflows, or use it as a starting point for other training scenarios.\nResources\n- SKILL.md ‚Äî Full skill documentation\n- Training Methods ‚Äî SFT, DPO, GRPO explained\n- Hardware Guide ‚Äî GPU selection and costs\n- TRL Documentation ‚Äî The underlying training library\n- Hugging Face Jobs ‚Äî Cloud training infrastructure\n- Trackio ‚Äî Real-time training monitoring"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/transformers-v5", "title": "Transformers v5: Simple model definitions powering the AI ecosystem", "url": "https://huggingface.co/blog/transformers-v5", "published": "Mon, 01 Dec 2025 00:00:00 GMT", "text_source": "article", "article_fetch_error": null, "text": "Transformers v5: Simple model definitions powering the AI ecosystem\nTransformers' version v4.0.0rc-1, the initial release candidate for version 4, was released on November 19th, 2020. Five years later, we now release v5.0.0rc-0.\nToday, as we launch v5, Transformers is installed more than 3 million times each day via pip - up from 20,000/day in v4 ü§Ø. Altogether, it has now surpassed 1.2 billion installs!\nThe ecosystem has expanded from 40 model architectures in v4 to over 400 today, and the community has contributed more than 750,000 model checkpoints on the Hub compatible with Transformers, up from roughly 1,000 at the time of v4.\nThis growth is powered by the evolution of the field and the now mainstream access to AI. As a leading model-definition library in the ecosystem, we need to continuously evolve and adapt the library to continue being relevant. Reinvention is key for longevity in AI.\nWe‚Äôre fortunate to collaborate with many libraries and apps built on transformers, in no specific order: llama.cpp, MLX, onnxruntime, Jan, LMStudio, vLLM, SGLang, Unsloth, LlamaFactory, dLLM, MaxText, TensorRT, Argmax, among many other friends.\nFor v5, we wanted to work on several notable aspects: simplicity, training, inference, and production. We detail the work that went into them in this post.\nSimplicity\nThe first focus of the team was on simplicity. Working on transformers, we see the code as the product. We want our model integrations to be clean, so that the ecosystem may depend on our model definitions and understand what‚Äôs really happening under the hood, how models differ from each other, and the key features of each new model. Simplicity results in wider standardization, generality, and wider support.\nModel Additions\nTransformers is the backbone of hundreds of thousands of projects, Unsloth included. We build on Transformers to help people fine-tune and train models efficiently, whether that‚Äôs BERT, text-to-speech (TTS), or others; to run fast inference for reinforcement learning (RL) even when models aren‚Äôt yet supported in other libraries. We're excited for Transformers v5 and are super happy to be working with the Hugging Face team!\n-- Michael Han at Unsloth\nTransformers, at the core, remains a model architecture toolkit. We aim to have all recent architectures and to be the ‚Äúsource of truth‚Äù for model definitions. We‚Äôve been adding between 1 - 3 new models every week for 5 years, shown in the timeline below:\nWe‚Äôve worked on improving that model-addition process.\nModular Approach\nOver the past year, we‚Äôve heavily pushed our modular design as a significant step forward. This allows for easier maintenance, faster integration, and better collaboration across the community.\nWe give a deeper overview in our Maintain the Unmaintainable blog post. For brevity, we aim to achieve a much easier model contribution process, as well as a lower maintenance burden. One metric we can highlight is that the number of lines of code to contribute (and review), drop significantly when modular is used:\nWhile we respect the ‚ÄúOne model, one file‚Äù philosophy, we continue introducing some abstractions making the management of common helpers simpler. The prime example of this is the introduction of the AttentionInterface\n, which offers a centralized abstraction for attention methods. The eager\nmethod will remain in the modeling file; others, such as FA1/2/3, FlexAttention, or SDPA, are moved to the interface.\nOver the past couple of years, the increasing amount of 0-day support for new model architectures and standardization of attention handling has helped to simplify our support for post-training modern LLMs.\n-- Wing Lian, Axolotl\nTooling for Model Conversion\nWe‚Äôre building tooling to help us identify which existing model architecture a new model resembles. This feature uses machine learning to find code similarities between independent modeling files. Going further, we aim to automate the conversion process by opening a draft PR for the model to be integrated into our transformers format. This process reduces manual effort and ensures consistency.\nCode Reduction\nStreamlining Modeling & Tokenization/Processing Files\nWe‚Äôve significantly refactored the modeling and tokenization files. Modeling files have been greatly improved thanks to the modular approach mentioned above, on top of standardization across models. Standardization contributes to abstracting most of the tools that don‚Äôt make up a model, so that the modeling code only contains the relevant parts for a model‚Äôs forward/backward passes.\nAlongside this work, we‚Äôre simplifying the tokenization and processing files: going forward, we‚Äôll only focus on the tokenizers\nbackend, removing the concept of ‚ÄúFast‚Äù and ‚ÄúSlow‚Äù tokenizers.\nWe'll use tokenizers as our main tokenization backend, just as we do for PyTorch-based models. We‚Äôll offer alternatives for Sentencepiece or MistralCommon backed tokenizers, which will be non-default but will be supported. Image processors will now only exist with their fast variant, which depends on the torchvision backend.\nFinally, we‚Äôre sunsetting our Flax/TensorFlow support in favor of focusing on PyTorch as the sole backend; however, we're also working with partners in the Jax ecosystem to ensure we have compatibility between our models and this ecosystem.\nWith its v5 release, transformers is going all in on PyTorch. Transformers acts as a source of truth and foundation for modeling across the field; we've been working with the team to ensure good performance across the stack.\nWe're excited to continue pushing for this in the future across training, inference, and deployment.\n-- Matt White, Executive Director, PyTorch Foundation. GM of AI, Linux Foundation\nTraining\nTraining remains a big focus of the team as we head into v5: whereas previously we would focus heavily on fine-tuning rather than pre-training/full-training at scale, we‚Äôve recently done significant work to improve our support for the latter as well.\nPre-training at scale\nSupporting pre-training meant reworking the initialization of our models, ensuring that they worked at scale with different parallelism paradigms, and shipping support for optimized kernels for both the forward and backward passes.\nGoing forward, we‚Äôre excited to have extended compatibility with torchtitan, megatron, nanotron, as well as any other pre-training tool that is interested in collaborating with us.\nFine-tuning & Post-training\nWe continue collaborating closely with all fine-tuning tools in the Python ecosystem. We aim to continue providing model implementations compatible with Unsloth, Axolotl, LlamaFactory, TRL and others in the PyTorch ecosystem; but we are also working with tools such as MaxText, in the JAX ecosystem, to have good interoperability between their frameworks and transformers\n.\nAll fine-tuning and post-training tools can now rely on transformers for model definitions; further enabling Agentic use-cases through OpenEnv or the Prime Environment Hub.\nInference\nWe‚Äôre putting a significant focus on inference for v5, with several paradigm changes: the introduction of specialized kernels, cleaner defaults, new APIs, support for optimized inference engines.\nSimilarly to training, we‚Äôve been putting some effort in packaging kernels\nso that they‚Äôre automatically used in case your hardware and software permits it. If you haven‚Äôt heard of kernels\nbefore, we recommend taking a look at this doc.\nAlongside this effort, we ship two new APIs dedicated to inference:\n- We ship support for continuous batching and paged attention mechanisms. This has now been used internally for some time, and we‚Äôre working on finalizing the rough edges and writing usage guides.\n- We introduce\ntransformers serve\nas the new transformers-specific serving system, which deploys an OpenAI API-compatible server.\nWe see this as a major step forward for use-cases such as evaluation, where a great number of inference requests are done simultaneously. We don‚Äôt aim to do specialized optimizations like the dedicated inference engines (vLLM, SGLang, TensorRT LLM). Instead, we aim to be perfectly inter-compatible with these, as detailed in the next section.\nThe Transformers backend in vLLM has been very enabling to get more architectures, like BERT and other encoders, available to more users. We've been working with the Transformers team to ensure many models are available across modalities with the best performance possible. This is just the start of our collaboration: we're happy to see the Transformers team will have this as a focus going into version 5.\n-- Simon Mo, Harry Mellor at vLLM\nStandardization is key to accelerating AI innovation. Transformers v5 empowers the SGLang team to spend less time on model reimplementation and more time on kernel optimization. We look forward to building a more efficient and unified AI ecosystem together!\n-- Chenyang Zhao at SGLang\nProduction & Local\nRecently, we've been working hand in hand with the most popular inference engines for them to use transformers\nas a backend. The value added is significant: as soon as a model is added to transformers\n, it becomes available in these inference engines, while taking advantage of the strengths each engine provides: inference optimizations, specialized kernels, dynamic batching, etc.\nWe've also been working very closely with ONNXRuntime, llama.cpp and MLX so that the implementations between transformers\nand these modeling libraries have great interoperability. For example, thanks to a significant community effort, it's now very easy to load GGUF files in transformers\nfor further fine-tuning. Conversely, transformers models can be easily converted to GGUF files for use with llama.cpp.\nThe Transformers framework is the go-to place for reference AI model implementations. The framework plays a crucial role in enabling modern AI across the entire stack. The team and the community behind the project truly understand and embrace the spirit of the open-source development and collaboration.\n-- Georgi Gerganov, ggml-org\nThe same is true for MLX, where the transformers' safetensors files are directly compatible with MLX's models.\nIt‚Äôs hard to overstate the importance of Transformers (and datasets, tokenizers, etc) to the open-source and overall AI ecosystem. I can‚Äôt count the number of times I‚Äôve personally used Transformers as a source-of-truth.\n-- Awni Hannun, MLX\nFinally, we‚Äôre pushing the boundaries of local inference and are working hand-in-hand with the executorch team to get the transformers models to be available on-device. We‚Äôre expanding the coverage to multimodal models (vision, audio) through optimum.\nQuantization\nQuantization is quickly emerging as the standard for state-of-the-art model development. Many SOTA models are now released in low-precision formats such as 8-bit and 4-bit (e.g., gpt-oss, Kimi-K2, Deepseek-r1), hardware is increasingly optimized for low-precision workloads, and the community is actively sharing high-quality quantized checkpoints. In v5, we're making quantization a central focus of Transformers support, ensuring full compatibility with all major features, and delivering a reliable framework for training and inference.\nWe introduce a significant change to the way we load weights in our models; and with this, we move to quantization being a first-class citizen.\nOur collaboration with the Transformers team was highly productive, marked by their proactive code reviews, feedback, and technical expertise. Their support was crucial in integrating TorchAO, expanding quantization features, and improving documentation for broader adoption in the V5.\n-- Jerry Zhang at TorchAO\nWe're excited that v5 has made quantization a first-class citizen. It provides the foundation for bitsandbytes to better support key features like TP and MoEs, and also makes it easier to integrate new quantization methods.\n-- Matthew Douglas & Titus von Koeller, bitsandbytes\nConclusion\nThe overarching theme of this version 5 release is ‚Äúinteroperability‚Äù. All refactors, performance improvements, and standardization are aligned with this theme. v5 plays nicely and end-to-end with the growing ecosystem: train a model with Unsloth/Axolotl/LlamaFactory/MaxText deploy it with vLLM/SGLang, and export it to llama.cpp/executorch/MLX to run locally!\nVersion 5 is undeniably an accomplishment of the past five years by a very large number of people in our community. We also see it as a promise, and as a beacon of the direction we want to go.\nWe took it as an opportunity to clean up the toolkit and isolate what mattered; we now have a clean slate on top of which to build. Thanks to the many changes from the community and team, improvements in performance, usability, and readability, will be simpler to ship.\nNow that v5.0.0's first RC is out there, we'll be eagerly awaiting your feedback. Please check our release notes for all the technical details, and we'll be awaiting your feedback in our GitHub issues!"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/flux-2", "title": "Diffusers welcomes FLUX-2", "url": "https://huggingface.co/blog/flux-2", "published": "Tue, 25 Nov 2025 00:00:00 GMT", "text_source": "article", "article_fetch_error": null, "text": "Welcome FLUX.2 - BFL‚Äôs new open image generation model ü§ó\nFLUX.2 is the recent series of image generation models from Black Forest Labs, preceded by the Flux.1 series. It is an entirely new model with a new architecture and pre-training done from scratch! In this post, we discuss the key changes introduced in FLUX.2, performing inference with it under various setups, and LoRA fine-tuning.\nüö® FLUX.2 is not meant to be a drop-in replacement of FLUX.1, but a new image generation and editing model.\nTable of contents\nFLUX.2: A Brief Introduction\nFLUX.2 can be used for both image-guided and text-guided image generation. Furthermore, it can take multiple images as reference inputs, while producing the final output image. Below, we briefly discuss the key changes introduced in FLUX.2.\nText encoder\nFirst, instead of two text encoders as in Flux.1, it uses a single text encoder ‚Äî Mistral Small 3.1. Using a single text encoder greatly simplifies the process of computing prompt embeddings. The pipeline allows for a max_sequence_length\nof 512. Instead of using a single-layer output for the prompt embedding, FLUX.2 stacks outputs from intermediate layers, which have been known to be more beneficial.\nDiT\nFLUX.2 follows the same general multimodel diffusion transformer (MM-DiT) + parallel DiT architecture as Flux.1. As a refresher, MM-DiT blocks first process the image latents and conditioning text in separate streams, only joining the two together for the attention operation, and are thus referred to as ‚Äúdouble-stream‚Äù blocks. The parallel blocks then operate on the concatenated image and text streams and can be regarded as ‚Äúsingle-stream‚Äù blocks.\nThe key DiT changes from Flux.1 to FLUX.2 are as follows:\nTime and guidance information (in the form of AdaLayerNorm-Zero modulation parameters) is shared across all double-stream and single-stream transformer blocks, respectively, rather than having individual modulation parameters for each block as in Flux.1.\nNone of the layers in the model use\nbias\nparameters. In particular, neither the attention nor feedforward (FF) sub-blocks of either transformer block usebias\nparameters in any of their layers.In Flux.1, the single-stream transformer blocks fused the attention output projection with the FF output projection. FLUX.2 single-stream blocks also fuse the attention QKV projections with the FF input projection, creating a fully parallel transformer block:\nNote that compared to the\nViT-22B\nblock depicted above, FLUX.2 uses a SwiGLU-style MLP activation rather than a GELU activation (and also doesn‚Äôt usebias\nparameters).A larger proportion of the transformer blocks in FLUX.2 are single-stream blocks (\n8\ndouble-stream blocks to48\nsingle-stream blocks, compared to19\n/38\nfor Flux.1). This also means that single-stream blocks make up a larger proportion of the DiT parameters:Flux.1[dev]-12B\nhas ~54% of its total parameters in the double-stream blocks, whereasFLUX.2[dev]-32B\nhas ~24% of its parameters in the double-stream blocks (and ~73% in the single-stream blocks).\nMisc\n- A new Autoencoder aka\nAutoencoderKLFlux2\n- Better way to incorporate resolution-dependent timestep schedules\nInference With Diffusers\nFLUX.2 uses a larger DiT and Mistral3 Small as its text encoder. When used together without any kind of offloading, the inference takes more than 80GB VRAM. In the following sections, we show how to perform inference with FLUX.2 in more accessible ways, under various system-level constraints.\nInstallation and Authentication\nBefore you try out the following code snippets, make sure you have installed diffusers\nfrom main\nand have run hf auth login\n.\npip uninstall diffusers -y && pip install git+https://github.com/huggingface/diffusers -U\nRegular Inference\nfrom diffusers import Flux2Pipeline\nimport torch\nrepo_id = \"black-forest-labs/FLUX.2-dev\"\npipe = Flux2Pipeline.from_pretrained(repo_id, torch_dtype=torch.bfloat16)\npipe.enable_model_cpu_offload()\nimage = pipe(\nprompt=\"dog dancing near the sun\",\nnum_inference_steps=50, # 28 is a good trade-off\nguidance_scale=4,\nheight=1024,\nwidth=1024\n).images[0]\nThe above code snippet was tested on an H100, and it isn‚Äôt sufficient to run inference on it without CPU offloading. With CPU offloading enabled, this setup takes ~62GB to run.\nUsers who have access to Hopper-series GPUs can take advantage of Flash Attention 3 to speed up inference:\nfrom diffusers import Flux2Pipeline\nimport torch\nrepo_id = \"black-forest-labs/FLUX.2-dev\"\npipe = Flux2Pipeline.from_pretrained(path, torch_dtype=torch.bfloat16)\n+ pipe.transformer.set_attention_backend(\"_flash_3_hub\")\npipe.enable_model_cpu_offload()\nimage = pipe(\nprompt=\"dog dancing near the sun\",\nnum_inference_steps=50,\nguidance_scale=2.5,\nheight=1024,\nwidth=1024\n).images[0]\nYou can check out the supported attention backends (we have many!) here.\nResource-constrained\nUsing 4-bit quantization\nUsing bitsandbytes\n, we can load the transformer and text encoder models in 4-bit, allowing owners of 24GB GPUs to use the model locally. You can run this snippet on a GPU with ~20 GB of free VRAM.\nUnfold\nimport torch\nfrom transformers import Mistral3ForConditionalGeneration\nfrom diffusers import Flux2Pipeline, Flux2Transformer2DModel\nrepo_id = \"diffusers/FLUX.2-dev-bnb-4bit\"\ndevice = \"cuda:0\"\ntorch_dtype = torch.bfloat16\ntransformer = Flux2Transformer2DModel.from_pretrained(\nrepo_id, subfolder=\"transformer\", torch_dtype=torch_dtype, device_map=\"cpu\"\n)\ntext_encoder = Mistral3ForConditionalGeneration.from_pretrained(\nrepo_id, subfolder=\"text_encoder\", dtype=torch_dtype, device_map=\"cpu\"\n)\npipe = Flux2Pipeline.from_pretrained(\nrepo_id, transformer=transformer, text_encoder=text_encoder, torch_dtype=torch_dtype\n)\npipe.enable_model_cpu_offload()\nprompt = \"Realistic macro photograph of a hermit crab using a soda can as its shell, partially emerging from the can, captured with sharp detail and natural colors, on a sunlit beach with soft shadows and a shallow depth of field, with blurred ocean waves in the background. The can has the text `BFL Diffusers` on it and it has a color gradient that start with #FF5733 at the top and transitions to #33FF57 at the bottom.\"\nimage = pipe(\nprompt=prompt,\ngenerator=torch.Generator(device=device).manual_seed(42),\nnum_inference_steps=50, # 28 is a good trade-off\nguidance_scale=4,\n).images[0]\nimage.save(\"flux2_t2i_nf4.png\")\nNotice that we're using a repository that contains the NF4-quantized versions of the FLUX.2 DiT and the Mistral text encoder.\nLocal + remote\nDue to the modular design of a Diffusers pipeline, we can isolate modules and work with them in sequence. We decouple the text encoder and deploy it to an Inference Endpoint. This helps us with freeing up the VRAM usage for the DiT and VAE only.\n‚ö†Ô∏è To use the remote text encoder, you need to have a valid token. If you are already authenticated, no further action is needed.\nThe example below uses a combination of local and remote inference. Additionally, we quantize the DiT with NF4 quantization through bitsandbytes\n.\nYou can run this snippet on a GPU with 18 GB of VRAM:\nUnfold\nfrom diffusers import Flux2Pipeline, Flux2Transformer2DModel\nfrom diffusers import BitsAndBytesConfig as DiffBitsAndBytesConfig\nfrom huggingface_hub import get_token\nimport requests\nimport torch\nimport io\ndef remote_text_encoder(prompts: str | list[str]):\ndef _encode_single(prompt: str):\nresponse = requests.post(\n\"https://remote-text-encoder-flux-2.huggingface.co/predict\",\njson={\"prompt\": prompt},\nheaders={\n\"Authorization\": f\"Bearer {get_token()}\",\n\"Content-Type\": \"application/json\"\n}\n)\nassert response.status_code == 200, f\"{response.status_code=}\"\nreturn torch.load(io.BytesIO(response.content))\nif isinstance(prompts, (list, tuple)):\nembeds = [_encode_single(p) for p in prompts]\nreturn torch.cat(embeds, dim=0)\nreturn _encode_single(prompts).to(\"cuda\")\nrepo_id = \"black-forest-labs/FLUX.2-dev\"\nquantized_dit_id = \"diffusers/FLUX.2-dev-bnb-4bit\"\ndit = Flux2Transformer2DModel.from_pretrained(\nquantized_dit_id, subfolder=\"transformer\", torch_dtype=torch_dtype, device_map=\"cpu\"\n)\npipe = Flux2Pipeline.from_pretrained(\nrepo_id,\ntext_encoder=None,\ntransformer=dit,\ntorch_dtype=torch.bfloat16,\n)\npipe.enable_model_cpu_offload()\nprint(\"Running remote text encoder ‚òÅÔ∏è\")\nprompt1 = \"a photo of a forest with mist swirling around the tree trunks. The word 'FLUX.2' is painted over it in big, red brush strokes with visible texture\"\nprompt2 = \"a photo of a dense forest with rain. The word 'FLUX.2' is painted over it in big, red brush strokes with visible texture\"\nprompt_embeds = remote_text_encoder([prompt1, prompt2])\nprint(\"Done ‚úÖ\")\nout = pipe(\nprompt_embeds=prompt_embeds,\ngenerator=torch.Generator(device=\"cuda\").manual_seed(42),\nnum_inference_steps=50, # 28 is a good trade-off\nguidance_scale=4,\nheight=1024,\nwidth=1024,\n)\nfor idx, image in enumerate(out.images):\nimage.save(f\"flux_out_{idx}.png\")\nFor GPUs with even lower VRAM, we have group_offloading\n, which allows GPUs with as little as 8GB of free VRAM to use this model. However, you'll need 32GB of free RAM. Alternatively, if you're willing to sacrifice some speed, you can set low_cpu_mem_usage=True\nto reduce the RAM requirement to just 10GB.\nUnfold\nimport io\nimport os\nimport requests\nimport torch\nfrom diffusers import Flux2Pipeline, Flux2Transformer2DModel\nrepo_id = \"diffusers/FLUX.2-dev-bnb-4bit\"\ntorch_dtype = torch.bfloat16\ndevice = \"cuda\"\ndef remote_text_encoder(prompts: str | list[str]):\ndef _encode_single(prompt: str):\nresponse = requests.post(\n\"https://remote-text-encoder-flux-2.huggingface.co/predict\",\njson={\"prompt\": prompt},\nheaders={\"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\", \"Content-Type\": \"application/json\"},\n)\nassert response.status_code == 200, f\"{response.status_code=}\"\nreturn torch.load(io.BytesIO(response.content))\nif isinstance(prompts, (list, tuple)):\nembeds = [_encode_single(p) for p in prompts]\nreturn torch.cat(embeds, dim=0)\nreturn _encode_single(prompts).to(\"cuda\")\ntransformer = Flux2Transformer2DModel.from_pretrained(\nrepo_id, subfolder=\"transformer\", torch_dtype=torch_dtype, device_map=\"cpu\"\n)\npipe = Flux2Pipeline.from_pretrained(\nrepo_id,\ntext_encoder=None,\ntransformer=transformer,\ntorch_dtype=torch_dtype,\n)\npipe.transformer.enable_group_offload(\nonload_device=device,\noffload_device=\"cpu\",\noffload_type=\"leaf_level\",\nuse_stream=True,\n# low_cpu_mem_usage=True # uncomment for lower RAM usage\n)\npipe.to(device)\nprompt = \"a photo of a forest with mist swirling around the tree trunks. The word 'FLUX.2' is painted over it in big, red brush strokes with visible texture\"\nprompt_embeds = remote_text_encoder(prompt)\nimage = pipe(\nprompt_embeds=prompt_embeds,\ngenerator=torch.Generator(device=device).manual_seed(42),\nnum_inference_steps=50,\nguidance_scale=4,\nheight=1024,\nwidth=1024,\n).images[0]\nYou can check out other supported quantization backends here and other memory-saving techniques here.\nTo check how different quantizations affect an image, you can play with the playground below or access it as standlone in the FLUX.2 Quantization experiments Space\nMultiple images as reference\nFLUX.2 supports using multiple images as inputs, allowing you to use up to 10 images. However, keep in mind that each additional image will require more VRAM. You can reference the images by index (e.g., image 1, image 2) or by natural language (e.g., the kangaroo, the turtle). For optimal results, the best approach is to use a combination of both methods.\nUnfold\nimport torch\nfrom transformers import Mistral3ForConditionalGeneration\nfrom diffusers import Flux2Pipeline, Flux2Transformer2DModel\nfrom diffusers.utils import load_image\nrepo_id = \"diffusers-internal-dev/new-model-image-final-weights\"\ndevice = \"cuda:0\"\ntorch_dtype = torch.bfloat16\npipe = Flux2Pipeline.from_pretrained(\nrepo_id, torch_dtype=torch_dtype\n)\npipe.enable_model_cpu_offload()\nimage_one = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/flux2_blog/kangaroo.png\")\nimage_two = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/flux2_blog/turtle.png\")\nprompt = \"the boxer kangaroo from image 1 and the martial artist turtle from image 2 are fighting in an epic battle scene at a beach of a tropical island, 35mm, depth of field, 50mm lens, f/3.5, cinematic lighting\"\nimage = pipe(\nprompt=prompt,\nimage=[image_one, image_two],\ngenerator=torch.Generator(device=device).manual_seed(42),\nnum_inference_steps=50,\nguidance_scale=2.5,\nwidth=1024,\nheight=768,\n).images[0]\nimage.save(f\"./flux2_t2i.png\")\nAdvanced Prompting\nFLUX.2 supports advanced prompting techniques like structured JSON prompting, precise hex color control, and multi-reference image editing.\nAside for the added control, this also allows for flexibility in changing specific attributes while maintaining others overall the same.\nFor example, let's start with this json as the base schema (taken from the official FLUX.2 prompting guide):\n{\n\"scene\": \"overall scene description\",\n\"subjects\": [\n{\n\"description\": \"detailed subject description\",\n\"position\": \"where in frame\",\n\"action\": \"what they're doing\"\n}\n],\n\"style\": \"artistic style\",\n\"color_palette\": [\"#hex1\", \"#hex2\", \"#hex3\"],\n\"lighting\": \"lighting description\",\n\"mood\": \"emotional tone\",\n\"background\": \"background details\",\n\"composition\": \"framing and layout\",\n\"camera\": {\n\"angle\": \"camera angle\",\n\"lens\": \"lens type\",\n\"depth_of_field\": \"focus behavior\"\n}\n}\nBuilding up on that, let's turn it into a prompt for a shot of a good old fashion walkman on a carpet (simply pass this prompt to your chosen diffusers inference example from above):\nprompt = \"\"\"\n{\n\"scene\": \"Professional studio product photography setup with soft-textured carpet surface\",\n\"subjects\": [\n{\n\"description\": \"Old silver Walkman placed on a carpet in the middle of an empty room\",\n\"pose\": \"Stationary, lying flat\",\n\"position\": \"Center foreground on carpeted surface\",\n\"color_palette\": [\"brushed silver\", \"dark gray accents\"]\n}\n],\n\"style\": \"Ultra-realistic product photography with commercial quality\",\n\"color_palette\": [\"brushed silver\", \"neutral beige\", \"soft white highlights\"],\n\"lighting\": \"Three-point softbox setup creating soft, diffused highlights with no harsh shadows\",\n\"mood\": \"Clean, professional, minimalist\",\n\"background\": \"Soft-textured carpet surface with subtle studio backdrop suggesting an empty room\",\n\"composition\": \"rule of thirds\",\n\"camera\": {\n\"angle\": \"high angle\",\n\"distance\": \"medium shot\",\n\"focus\": \"Sharp focus on metallic Walkman textures and physical controls\",\n\"lens-mm\": 85,\n\"f-number\": \"f/5.6\",\n\"ISO\": 200\n}\n}\n\"\"\"\nNow, let's change the color of the carpet to a specific teal-blue shade (#367588) and add wired headphones plugged into the walkman:\nprompt = \"\"\"\n{\n\"scene\": \"Professional studio product photography setup with soft-textured carpet surface\",\n\"subjects\": [\n{\n\"description\": \"Old silver Walkman placed on a teal-blue carpet (#367588) in the middle of an empty room, with wired headphones plugged in\",\n\"pose\": \"Stationary, lying flat\",\n\"position\": \"Center foreground on carpeted surface\",\n\"color_palette\": [\"brushed silver\", \"dark gray accents\", \"#367588\"]\n},\n{\n\"description\": \"Wired headphones connected to the Walkman, cable loosely coiled on the carpet\",\n\"pose\": \"Stationary\",\n\"position\": \"Next to and partially in front of the Walkman on the carpet\",\n\"color_palette\": [\"dark gray\", \"soft black\", \"#367588\"]\n}\n],\n\"style\": \"Ultra-realistic product photography with commercial quality\",\n\"color_palette\": [\"brushed silver\", \"#367588\", \"neutral beige\", \"soft white highlights\"],\n\"lighting\": \"Three-point softbox setup creating soft, diffused highlights with no harsh shadows\",\n\"mood\": \"Clean, professional, minimalist\",\n\"background\": \"Soft-textured teal-blue carpet surface (#367588) with subtle studio backdrop suggesting an empty room\",\n\"composition\": \"rule of thirds\",\n\"camera\": {\n\"angle\": \"high angle\",\n\"distance\": \"medium shot\",\n\"focus\": \"Sharp focus on metallic Walkman textures, wired headphones, and carpet fibers\",\n\"lens-mm\": 85,\n\"f-number\": \"f/5.6\",\n\"ISO\": 200\n}\n}\n\"\"\"\nThe carpet color now matches the hex code provided, and the headphones have been with small changes to the overall scene.\nCheck out the official prompting guide for more examples and details.\nLoRA fine-tuning\nBeing both a text-to-image and an image-to-image model, FLUX.2 makes the perfect fine-tuning candidate for many use-cases! However, as inference alone takes more than 80GB of VRAM, LoRA fine-tuning is even more challenging to run on consumer GPUs. To squeeze in as much memory saving as we can, we utilize some of the inference optimizations described above for training as well, together with shared memory saving techniques, to substantially reduce memory consumption. To train it, you can use either the diffusers code below or Ostris' AI Toolkit.\nWe provide both text-to-image and image-to-image training scripts, for the purpose of this blog will focus on a text-to-image training example.\nMemory optimizations for fine-tuning\nMany of these techniques complement each other and can be used together to reduce memory consumption further. However, some techniques may be mutually exclusive, so be sure to check before launching a training run.\nUnfold to check details on the memory-saving techniques used:\n- Remote Text Encoder: to leverage the remote text encoding for training, simply pass\n--remote_text_encoder\n. Note that you must either be logged in to your Hugging Face account (hf auth login\n) OR pass a token with--hub_token\n. - CPU Offloading: by passing\n--offload\nthe vae and text encoder to will be offloaded to CPU memory and only moved to GPU when needed. - Latent Caching: Pre-encode the training images with the vae, and then delete it to free up some memory. To enable\nlatent_caching\nsimply pass--cache_latents\n. - QLoRA: Low Precision Training with Quantization - using 8-bit or 4-bit quantization. You can use the following flags:\n- FP8 training with\ntorchao\n: enable FP8 training by passing--do_fp8_training\n. Since we are utilizing FP8 tensor cores, we need CUDA GPUs with compute capability at least 8.9 or greater. If you're looking for memory-efficient training on relatively older cards, we encourage you to check out other trainers likeSimpleTuner\n,ai-toolkit\n, etc. - NF4 training with\nbitsandbytes\n: Alternatively, you can use 8-bit or 4-bit quantization withbitsandbytes\nby passing:---bnb_quantization_config_path\nwith a corresponding path to a json file containing your config. see below for more details.\n- FP8 training with\n- Gradient Checkpointing and Accumulation:\n--gradient accumulation\nrefers to the number of updates steps to accumulate before performing a backward/update pass.by passing a value > 1 you can reduce the amount of backward/update passes and hence also memory reqs.* with--gradient checkpointing\nwe can save memory by not storing all intermediate activations during the forward pass.Instead, only a subset of these activations (the checkpoints) are stored and the rest is recomputed as needed during the backward pass. Note that this comes at the expanse of a slower backward pass. - 8-bit-Adam Optimizer: When training with\nAdamW\n(doesn't apply toprodigy\n) You can pass--use_8bit_adam\nto reduce the memory requirements of training. Make sure to installbitsandbytes\nif you want to do so.\nPlease make sure to check out the README for prerequisites before starting training.\nFor this example, we‚Äôll use multimodalart/1920-raider-waite-tarot-public-domain\ndataset with the following configuration using FP8 training. Feel free to experiment more with the hyper-parameters and share your results ü§ó\naccelerate launch train_dreambooth_lora_flux2.py \\\n--pretrained_model_name_or_path=\"black-forest-labs/FLUX.2-dev\" \\\n--mixed_precision=\"bf16\" \\\n--gradient_checkpointing \\\n--remote_text_encoder \\\n--cache_latents \\\n--caption_column=\"caption\"\\\n--do_fp8_training \\\n--dataset_name=\"multimodalart/1920-raider-waite-tarot-public-domain\" \\\n--output_dir=\"tarot_card_Flux2_LoRA\" \\\n--instance_prompt=\"trcrd tarot card\" \\\n--resolution=1024 \\\n--train_batch_size=2 \\\n--guidance_scale=1 \\\n--gradient_accumulation_steps=1 \\\n--optimizer=\"adamW\" \\\n--use_8bit_adam\\\n--learning_rate=1e-4 \\\n--report_to=\"wandb\" \\\n--lr_scheduler=\"constant_with_warmup\" \\\n--lr_warmup_steps=200 \\\n--checkpointing_steps=250\\\n--max_train_steps=1000 \\\n--rank=8\\\n--validation_prompt=\"a trtcrd of a person on a computer, on the computer you see a meme being made with an ancient looking trollface, 'the shitposter' arcana, in the style of TOK a trtcrd, tarot style\" \\\n--validation_epochs=25 \\\n--seed=\"0\"\\\n--push_to_hub\nThe left image was generated using the pre-trained FLUX.2 model, and the right image was produced the LoRA.\nIn case your hardware isn‚Äôt compatible with FP8 training, you can use QLoRA with bitsandbytes\n. You first need to define a config.json\nfile like so:\n{\n\"load_in_4bit\": true,\n\"bnb_4bit_quant_type\": \"nf4\"\n}\nAnd then pass its path to --bnb_quantization_config_path\n:\naccelerate launch train_dreambooth_lora_flux2.py \\\n--pretrained_model_name_or_path=\"black-forest-labs/FLUX.2-dev\" \\\n--mixed_precision=\"bf16\" \\\n--gradient_checkpointing \\\n--remote_text_encoder \\\n--cache_latents \\\n--caption_column=\"caption\"\\\n**--bnb_quantization_config_path=\"config.json\" \\**\n--dataset_name=\"multimodalart/1920-raider-waite-tarot-public-domain\" \\\n--output_dir=\"tarot_card_Flux2_LoRA\" \\\n--instance_prompt=\"a tarot card\" \\\n--resolution=1024 \\\n--train_batch_size=2 \\\n--guidance_scale=1 \\\n--gradient_accumulation_steps=1 \\\n--optimizer=\"adamW\" \\\n--use_8bit_adam\\\n--learning_rate=1e-4 \\\n--report_to=\"wandb\" \\\n--lr_scheduler=\"constant_with_warmup\" \\\n--lr_warmup_steps=200 \\\n--max_train_steps=1000 \\\n--rank=8\\\n--validation_prompt=\"a trtcrd of a person on a computer, on the computer you see a meme being made with an ancient looking trollface, 'the shitposter' arcana, in the style of TOK a trtcrd, tarot style\" \\\n--seed=\"0\"\nResources\n- FLUX.2 announcement post\n- Diffusers documentation\n- FLUX.2 official demo\n- FLUX.2 on the Hub\n- FLUX.2 original codebase"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/continuous_batching", "title": "Continuous batching from first principles", "url": "https://huggingface.co/blog/continuous_batching", "published": "Tue, 25 Nov 2025 00:00:00 GMT", "text_source": "article", "article_fetch_error": null, "text": "Continuous batching\nTL;DR: in this blog post, starting from attention mechanisms and KV caching, we derive continuous batching by optimizing for throughput.\nIf you've ever used Qwen, Claude, or any other AI chatbot, you've probably noticed something: it takes a while for the first word of the response to appear, and then words appear one-by-one on your screen with (hopefully) a regular and fast-paced frequency. That's because at the heart of it, all LLMs are just fancy next token predictors. An LLM first processes your entire prompt to produce one new token. Then it keeps adding tokens one by one, each time reading everything that came before, until it decides generation is over.\nThis generation process is computationally expensive: it requires passing the input through billions of parameters for each token generated. To make these models practical for real-world applications, particularly when serving many users simultaneously, researchers and engineers have developed a range of efficient inference techniques.\nOne of the most impactful optimizations is continuous batching, which attempts to maximize performance by processing multiple conversations in parallel and swapping them out when they are done.\nTo understand how continuous batching works and why it's so effective in high-load serving scenarios, we'll build up from the fundamentals of how LLMs process tokens.\nAttention\nThe attention mechanism is the central piece of how LLMs work. A language model processes text by breaking it down into pieces that we call tokens. We can conceptually think of \"tokens\" as \"words\", but sometimes a word might be composed of several tokens. For each token sequence, the network computes a prediction of what the next token should be.\nMany operations in the network are token-wise: each token is processed independently, and the output for a given token depends only on that token's content, not on any other tokens in the sequence. Operations like this include layer normalization or matrix multiplication. However, to create connections between words in a sentence, we need operations where tokens can influence each other.\nThis is where attention comes in. Attention layers are the only place where different tokens interact with each other. Understanding how a network connects tokens together means understanding attention.\nLet's see how this works in practice, in the case where there is only one input prompt.\nConsider the initial prompt I am sure this project\n, tokenized as 7 tokens: [<bos>, I, am, sure, this, pro, ject]\n. The <bos>\n, or \"Beginning of Sequence\", is a special token we add at the start of the prompt to tell the language model that a new conversation starts here.\nEach token is represented inside the network with a vector of length d\n(the hidden dimension). Therefore, the seven incoming tokens form a tensor with shape . 1\nis the number of sequences, or batch size, which is just one in our case. 7\nis the sequence length, and d\nis the hidden dimension, or the size of each token representation. Going forward, we'll use instead of 7\nas the sequence length.\nInput tensor is then projected by three matrices: the query projection , the key projection and the value projection . This produces three tensors , and , all of shape , where is the dimension of the attention head. We call them the query, key and value states, respectively. This is represented on the left in the figure below.\nNext, tensors and are multiplied together to measure similarity between tokens, producing a tensor of shape . This is why we say that attention has quadratic complexity in sequence length. Computing requires operations, so the cost is a square of the sequence length. It is represented on the right in the figure above.\nWe then apply a boolean attention mask to to control which tokens can interact, as represented in the figure below. In this figure, the attention mask is a causal mask, meaning each token only interacts with tokens that came before it. This follows the intuition that a cause must come before its consequence, hence the name causal mask. The attention mask is crucial because it dictates all token interactions in the network. Set all attention mask values to False and no token will ever interact with another in the whole network. We'll examine attention masks more closely in a few paragraphs.\nFinally, after applying the attention mask, we take a token-wise softmax (which is the same as saying a row-wise softmax) and multiply the result by the value projection to get the output of one attention head, of shape . We offer a visual summary of the whole process in the following figure.\nWe are going to use a lot of attention visualization in this post, so to simplify things, we are going to condense the figure above just a bit.\nWhy this matters: In continuous batching, , , and can have different numbers of tokens because, as we'll see, we'll be processing different stages (prefill and decode) at the same time. To make it more general, let's say has shape , has shape , and has shape .\nThe attention scores then have shape , and the attention mask has the same shape since it's applied point-wise to the scores.\nAfter applying the attention mask and row-wise softmax, we multiply by . Since we're multiplying a matrix of shape by one of shape , the inner dimensions must match: . This means and always have the same length, so we can simplify our visualizations by only showing .\nDon't worry if this seems abstract: the figures will make it concrete.\nFurthermore, since we know that the attention mask is applied to , we know they have the same shape. Instead of representing the attention scores, we will represent the attention mask in its place. Finally, since , and are direct projections of , no need to represent . This gives the simplified figure where we only represent , and the attention mask:\nThis representation also underlines how we can read an attention mask.\nWe read the mask row-by-row, which is the same as reading token-by-token: each row corresponds to one token's attention computation. A green square at position (row i, column j) means True\n: token j can influence token i. A white square means False\n: no interaction allowed.\nFor example, look at the third row for token \"am\". The \"I\" column is green, so \"I\" influences the computation of \"am\". The \"pro\" column is white, so \"pro\" doesn't influence \"am\" . This is causal masking at work: future tokens can't affect past ones.\nThe last layer of the model outputs a token prediction for each input token. In our context, generating the continuation of a single prompt, we only care about the next token prediction from the last token. The last token is \"ject\" in the figure above, and the associated prediction is \"will\".\nThe process we just described, where we take an entire input sequence, pass it through multiple attention layers and compute a score for the next token, is called prefill. This is because, as we'll see in a moment, much of the computation we performed can be cached and reused ‚Äì hence, we are prefilling the cache. Thanks to the use of this cache, sequence generation can proceed using much less compute in a phase called decoding. In the decoding phase, generating one new token will be much faster than the initial full-sequence computation. Let's see why.\nTo continue generation, we begin a new forward pass, which would naively look like this:\nTo compute the attention scores of the new token, we still need the key and value projections of the previous tokens. So we need to repeat the matrix multiplication of the old tokens (in grey in the figure above) with and to retrieve a result that was already computed once before. In other terms, we are wasting compute. Let's see how we can avoid that.\nKV-cache\nRight off the bat, we notice that the last token does not impact the attention calculation of the other tokens:\nThis follows the idea of the causal mask: since \"will\" comes after all previous tokens, it does not change their attention calculation. For text generation, causal attention is by far the most common, so we will focus on that case from now on. Keep in mind that non-causal attention schemes can also be used, especially when dealing with images. Considering we only need the next-token prediction for the \"will\" token, we can simplify the attention mechanism by only computing the output for this token.\nMoreover, we already computed the and states for the tokens \"<bos>\", ‚Ä¶ , \"ject\" during the previous forward pass: if they have been stored, we do not need to recompute them again. This is the KV cache: the list of key and value states created during generation. It essentially allows one to reduce the compute cost of generating token from to by avoiding recomputation of key and value projections, while paying a memory cost of .\nIn the figure above, only the tokens in white are computed: instead of computing the keys and values for 8 tokens, we compute them for 1. You can see that through KV caching, a lot of compute is saved.\nYou can check this post for more visualizations of KV caching, or this one for a practical implementation example.\nLet's be a bit more specific about the cache size, because it's a good opportunity to examine the shapes present in our model. For a model with attention layers and attention heads with head dimension , the total cache size needed to store one token will be with a factor of to account for both and .\nFor instance, Llama-2-7B with layers, heads, and requires values per token per layer. With float16\nprecision, this takes bytes KB in memory.\nKV caching is useful when we want to generate the next token, which is a stage we call decoding. But it can also be useful in the prefill stage, when we process the initial prompt and have many input tokens. Especially when there are large initial prompts that don't fit in GPU memory all at once.\nChunked prefill\nUp till now, we have looked at an example of prefill where we have tokens, but in practice initial prompts can be much longer. For instance, when using Cursor, you can add your repository to the prompt, where it acts as context: this significantly increases the prompt size. In such cases, the memory needed to store the activations for tokens can be larger than the available memory on the GPU. Thus we cannot perform prefill in a single forward pass: we have to split the prefill in chunks. This is called chunked prefill, and it's going to be one of the components needed to enable efficient inference.\nLet's pretend that the available memory is very constrained, and that we can only pass tokens per forward pass. If we have an initial prompt with tokens, we need to split it in chunks (rounding up 7/4 = 1.75 to 2). We illustrate the example below using the same and notations:\nWe can do that thanks to the KV cache. We store the KV states during the first prefill split, and during the second prefill split, we prepend the stored KV states to the new KV states. We also adapt the attention mask accordingly. Visually, it looks like we split the non-chunked prefill in the middle.\nThe key insight: cached KV states let us process the prompt incrementally without losing information.\nAlthough we showed here an example where we split the prefill into 2 chunks, chunked prefill can be used to split the prefill in any way we want, adapting flexibly to memory constraints.\nWe are now finally equipped with all the tools we need to understand Continuous Batching.\nContinuous batching\nIn our previous examples we have only considered the case of batch size one, i.e. we only generate tokens for one prompt at a time. In the context of evaluation or model serving, we want to generate tokens for a large number of prompts. To increase the throughput, which is the number of tokens generated per second, the best course of action is to generate tokens in parallel for a batch of several prompts.\nTo batch prompts together, the naive way is to add an axis to both input tensors: token sequence and attention mask. However, this comes with a constraint on the shape of the inputs: we need all prompts to have the same length, because tensors must be rectangular. To achieve this, we usually add padding on the left so the new token prediction always comes from the rightmost token. We also modify the attention mask of each prompt accordingly, as shown below:\nwhere the padding tokens <pad>\nare coloured in orange. Then we can perform the forward pass as we used to, with the added dimension of the batch size. This is called batched generation: efficient for same-length prompts, but wasteful when lengths vary.\nIt is illustrated below, through 4 steps of generation: one prefilling step (at the top) and 3 decoding steps (below each \"Forward pass\" lines).\nwhere <eos>\nmeans \"End Of Sequence\", this is a special token to indicate the model has reached the end of generation for the corresponding sequence.\nThe drawback of batched generation is that if one prompt finishes generation before the other one by generating an <eos>\ntoken, all further generated tokens are useless. And this goes on until the longest request of the batch finishes. Of course, we can remove the prompts that have reached an <eos>\ntoken from the batch and save some compute and memory, but saving resources is not the goal here: throughput is.\nInstead of just removing the finished prompt from the batch, we can replace it with a prompt that's waiting for generation. We will call this dynamic scheduling, or dynamic batching. Dynamic scheduling is great to maintain throughput while ensuring any token generated by a forward pass is relevant. But because of the way we batched prompts together, it has a major drawback: we need a lot of padding when swapping prompts. That's because the newly-inserted prompt needs to go through prefill while the other prompts are decoding one token at a time. So there is almost as much padding as there are tokens in the newly-inserted prompt.\nThe problem becomes even worse when batch size increases and initial prompts are long. The padding cost grows quadratically with both batch size and prompt length. If we have a batch of prompts that are in decoding phase and one finishes, dynamically introducing a prompt of initial tokens in the batch requires padding tokens. For instance, with and , we'd need padding tokens!\nFurthermore, practical optimizations like CUDA graphs or torch.compile\nrequire static tensor shapes. This forces us to pad all prompts to a fixed maximum length, dramatically increasing the padding waste.\nAt this point, our main problem is padding, which is a consequence of the axis we added to batch sentences together. Thus, the ideal would be to get rid of this axis entirely, a radical rethinking of batching. If we do so, the only way to batch prompts together is to concatenate them:\nBut we don't want tokens from prompt 0 to interact with the tokens of prompt 1! Luckily for us, we have a way to control how tokens interact with one another: the attention mask. How we do this is displayed below:\nAlthough we use different tints of green to illustrate the different parts of the attention mask, this is still a boolean mask with only greens for True\nand white for False\n.\nThis way of batching prompts together is called ragged batching (because sequence lengths are 'ragged' or uneven), and it offers the benefit of added throughput without introducing the need for padding tokens.\nIn the figure above, we use ragged batching to combine two full prompts together, but we can batch as many as memory allows. The only limit is , the number of tokens we can fit in a batch, with depending on the available memory on the GPU.\nRagged batching is one of the key components of continuous batching. To maximize throughput, we can combine prefill and decoding sequences following an algorithm like this:\n- We try to always reach our memory budget of tokens per batch\n- We first add all the prompts in decoding phase to the batch, each accounting for 1 token\n- We fill the remaining space with prefill phase prompts, relying on the flexibility of chunked prefill to split inputs as needed\nDynamic scheduling is the final piece that contributes to the continuous batching technique: we remove finished prompts from the batch as soon as they are done, and replace them with new chunked prompts that correspond to incoming requests.\nThis combination of ragged batching and dynamic scheduling is called continuous batching, and it's the technique that powers modern LLM serving systems.\nConclusion\nContinuous batching combines three key techniques to maximize throughput in LLM serving:\n- KV caching to avoid recomputing past token representations\n- Chunked prefill to handle variable-length prompts within memory constraints\n- Ragged batching with dynamic scheduling to eliminate padding waste and keep the GPU fully utilized\nBy removing the batch dimension and using attention masks to control token interactions, continuous batching allows mixing prefill and decode phases in the same batch, dramatically improving efficiency for serving multiple requests. This is why services like ChatGPT can handle thousands of concurrent users efficiently.\nIn the next article in this series, we'll explore efficient KV cache management through paged attention. If you'd like to see a deep dive on other continuous batching topics, please let us know in the comments!\nAcknowledgement: thanks to Arthur Zucker for producing the initial concept for the figures used in this article. And thanks to Arthur Zucker, Luc Georges, Lysandre Debut, Merve Noyan and Pedro Cuenca for all providing helpful reviews."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/Tavily/tavily-deep-research", "title": "Building Deep Research: How we Achieved State of the Art", "url": "https://huggingface.co/blog/Tavily/tavily-deep-research", "published": "Mon, 24 Nov 2025 17:40:14 GMT", "text_source": "article", "article_fetch_error": null, "text": "Building Deep Research: How we Achieved State of the Art\nResearch agents are rapidly becoming one of the most important applications of AI. Research is a foundational knowledge-work task: collecting, reading, and synthesizing information underpins everything from writing and decision-making to coding itself. Yet human-driven research is constrained by memory, reading speed, and time. AI research agents, by contrast, can process vast amounts of information, synthesize insights instantly, and scale effortlessly. Because of this, research agents are emerging as a top use case for AI today and will soon become a core subcomponent of broader agentic workflows across content generation, coding, sales, and more. In this post, we share the technical and philosophical lessons we‚Äôve learned building a state-of-the-art research agent, and where we believe the field is headed.\nBuilding for the Future\nAgent Harness\nThe task of building an agent harness is to create a software layer that enhances a model‚Äôs runtime execution through context management, tool invocations, loop control, orchestration, and error handling. Building applications on top of rapidly improving models is, however, a modern engineering challenge. How can we design software today that absorbs the performance gains from future model releases?\nThis requires forecasting how models will evolve, staying optimistic about their progress, limiting assumptions, and avoiding hand-crafted optimizations.\nWe learned this the hard way seven months ago, when we had to abandon our first attempt at deep research and rebuild the entire system from scratch. The first architecture was complicated and sophisticated (we thought this was a good thing), but its assumptions became bottlenecks when the next generation of models arrived.\nModels\nOver the last seven months, model capabilities have quietly but meaningfully evolved (especially in their tool-calling abilities). This single optimization focus has pushed us from workflows to agents. We believe future models will be trained to solve the current pain points of agent developers. Every model is ultimately consumed by a harness, so models should evolve in service of that harness. We hope to see models improve in high-recall summarization (for context compression), tool-calling reliability, and concision in writing.\nTools\nSimilarly, tools should evolve to support LLMs and widely adopted agent harnesses. The best tools should perform some context engineering on the tool side, abstracted away from the agent. They should return only the most relevant data instead of dumping large volumes of tokens into the context window. As a tool provider, we‚Äôve invested heavily in our advanced search feature, which has context engineering baked in. This in turn lowers hallucinations and latency for the downstream agent processes.\nTakeaways\nTo build agents that improve over time, we followed a few guiding principles:\n- Simplify orchestration logic and lean into autonomy.\n- Pay close attention to what models and tools are being optimized for, and leverage their emerging capabilities.\n- Focus on context engineering (more on this in the next section).\nContext Engineering ‚Äî An Exercise in Curation\nLong-horizon research tasks expose a fundamental challenge in current agent design: the task of maintaining a clean, optimized context window over time. If curating context is not a task the engineer pays close attention to, the agent is almost destined for failure. The following outlines our thinking around this concept within the deep research domain.\nContext-Managed Web Retrieval\nUsing Tavily‚Äôs Advanced Search is the natural first step to take in overcoming this challenge, in that it abstracts away the processing of raw web content and returns only the most relevant content chunks from each source. In leveraging this functionality, we let Tavily Search do the heavy lifting and allow Tavily Research to reap the benefit, gathering the most valuable content in a latency-efficient manner.\nEnsuring that the agent does not overfit to a single research thread is the next step towards an effective context-gathering pipeline. It is in this regard that global state persistence and source deduplication is paramount, and in our case, it helps threefold:\n- It ensures the agent is exposed only to fresh information.\n- It allows the engineer to recognize when the information scope is narrowing and to prompt the agent to explore untapped relevant domains.\n- It lends to effective source attribution later on in the generation process.\nAt Tavily, interacting with the web is our bread and butter. Architecting a refined web-retrieval system engineered for deep research was a foundational building block for our deep research agent design as a whole.\nModeling the Human-Web Interaction\nHumans research in an inherently unstructured, iterative way. We start by defining the task: what we‚Äôre trying to accomplish and what information we need. We next gather data from our sources, extracting the key insights and holding them in short-term memory, letting these distilled thoughts guide our subsequent actions.\nThis cycle repeats: collect information, distill it, decide what to do next. Only once we‚Äôve gathered enough understanding to produce the final deliverable do we return to the original sources, using them as references to assemble the finished product.\nWe believe that deep research agents should be designed in a similar manner, in that tool outputs should be distilled into reflections, and only the set of past reflections should be used as context for your tool caller. Similar to humans, it is only at the point when your agent begins to prepare the final deliverable that you must provide the raw information as context, so as to ensure there is no information loss.\nDoing More with Less\nThis approach differs from traditional context structuring in a ReAct agent-based architecture. Typically, tool calls and outputs are propagated through the tool calling loop, with previously retrieved/generated tokens being persisted in the context window on each subsequent iteration. This pattern can be seen in LangChain‚Äôs Open Deep Research agent implementation, and from a token consumption perspective, it can be modeled by the following quadratic series, where is the amount of tokens the tool calling model is invoked with on each tool calling iteration, and is the number of tool calling iterations.\nContrarily, our proposed method of context engineering removes this token propagation (as the knowledge distillations, even when aggregated, are negligible when compared to the quantity of tokens gathered from web) and can be modeled by the following linear series.\nWhen comparing the two approaches, tokens are saved on a per-agent basis by a factor of , and when extrapolating this over a multi-agent system and with consumption at scale, the absolute value of tokens saved becomes even more significant.\nThrough this methodology, we were able to reduce token consumption by 66% (when compared to Open Deep Research) while achieving SOTA on DeepResearch Bench ‚Äì the intersection of quality and efficiency in full effect.\nProductionizing Agents ‚Äî an Ongoing Challenge\nBuilding production-grade agents is a balancing act. We leaned into autonomy to maximize performance and quality, while still meeting strict requirements for latency, cost, and reliability.\nEngineering with Non-Determinism\nLLMs are inherently non-deterministic, and we found that giving them guard-railed freedom to reason and iterate produces the strongest results. Autonomy, when gone wrong, can cause agent behavior to go off track. Tools can be called incorrectly, LLMs can overfit to a subtopic, and expected reasoning patterns may break. No single safeguard will catch all of these issues.\nA shift in engineering mindset is required: treat failure modes as core design considerations, not afterthoughts. Simple guardrails like tool-call retries or model cascades help, but proactively anticipating anomalies, reinforcing proper patterns in prompting and edge-case testing is what enables production-grade, long-running agents.\nOptimal Tooling ‚Äî Less is More\nFrom our experience, it‚Äôs better to expose a small, essential toolset to the agent rather than a large, complex one. We were tempted to over-engineer by adding many tools that seemed useful in theory, but in practice this created new failure modes and made it harder for LLMs to consistently choose the right tool and iterate effectively.\nEvals\nWe used evals to steer our development process but also recognize their shortcomings. LLM-as-a-judge evals are difficult to trust: current models are non‚Äëdeterministic, uninterpretable in their reasoning, and can turn into bottlenecks, especially for long‚Äërunning agents where a single experiment can take days to complete.\nRather than optimizing for benchmark scores, we optimized for directional feedback. The core question was always: did this change make the agent more reliable and more useful in practice? Evals became a tool for validating that direction, not the optimization target. Intuition and careful agent‚Äëtrace monitoring consistently provided higher‚Äësignal feedback than any single eval score. Overall, the best outcome is rarely the highest numerical score. For production systems, improvements like reduced token usage, reliability, lower latency, and fewer failures are more valuable than a one‚Äëpoint bump on an eval.\nIf you‚Äôre interested in experiencing the result of these findings in practice, you can sign up for early access to Tavily Research here."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/OVHcloud/inference-providers-ovhcloud", "title": "OVHcloud on Hugging Face Inference Providers üî•", "url": "https://huggingface.co/blog/OVHcloud/inference-providers-ovhcloud", "published": "Mon, 24 Nov 2025 16:08:47 GMT", "text_source": "article", "article_fetch_error": null, "text": "OVHcloud on Hugging Face Inference Providers üî•\nWe're thrilled to share that OVHcloud is now a supported Inference Provider on the Hugging Face Hub! OVHcloud joins our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub's model pages. Inference Providers are also seamlessly integrated into our client SDKs (for both JS and Python), making it super easy to use a wide variety of models with your preferred providers.\nThis launch makes it easier than ever to access popular open-weight models like gpt-oss, Qwen3, DeepSeek R1, and Llama ‚Äî right from Hugging Face. You can browse OVHcloud's org on the Hub at https://huggingface.co/ovhcloud and try trending supported models at https://huggingface.co/models?inference_provider=ovhcloud&sort=trending.\nOVHcloud AI Endpoints are a fully managed, serverless service that provides access to frontier AI models from leading research labs via simple API calls. The service offers competitive pay-per-token pricing starting at ‚Ç¨0.04 per million tokens.\nThe service runs on secure infrastructure located in European data centers, ensuring data sovereignty and low latency for European users. The platform supports advanced features including structured outputs, function calling, and multimodal capabilities for both text and image processing.\nBuilt for production use, OVHcloud's inference infrastructure delivers sub-200ms response times for first tokens, making it ideal for interactive applications and agentic workflows. The service supports both text generation and embedding models. You can learn more about OVHcloud's platform and infrastructure at https://www.ovhcloud.com/en/public-cloud/ai-endpoints/catalog/.\nRead more about how to use OVHcloud as an Inference Provider in its dedicated documentation page.\nSee the list of supported models here.\nHow it works\nIn the website UI\nIn your user account settings, you are able to:\n- Set your own API keys for the providers you've signed up with. If no custom key is set, your requests will be routed through HF.\n- Order providers by preference. This applies to the widget and code snippets in the model pages.\nAs mentioned, there are two modes when calling Inference Providers:\n- Custom key (calls go directly to the inference provider, using your own API key of the corresponding inference provider)\n- Routed by HF (in that case, you don't need a token from the provider, and the charges are applied directly to your HF account rather than the provider's account)\nModel pages showcase third-party inference providers (the ones that are compatible with the current model, sorted by user preference)\nFrom the client SDKs\nFrom Python, using huggingface_hub\nThe following example shows how to use OpenAI's gpt-oss-120b using OVHcloud as the inference provider. You can use a Hugging Face token for automatic routing through Hugging Face, or your own OVHcloud AI Endpoints API key if you have one.\nNote: this requires using a recent version of huggingface_hub (>= 1.1.5).\nimport os\nfrom huggingface_hub import InferenceClient\nclient = InferenceClient(\napi_key=os.environ[\"HF_TOKEN\"],\n)\ncompletion = client.chat.completions.create(\nmodel=\"openai/gpt-oss-120b:ovhcloud\",\nmessages=[\n{\n\"role\": \"user\",\n\"content\": \"What is the capital of France?\"\n}\n],\n)\nprint(completion.choices[0].message)\nFrom JS using @huggingface/inference\nimport { InferenceClient } from \"@huggingface/inference\";\nconst client = new InferenceClient(process.env.HF_TOKEN);\nconst chatCompletion = await client.chatCompletion({\nmodel: \"openai/gpt-oss-120b:ovhcloud\",\nmessages: [\n{\nrole: \"user\",\ncontent: \"What is the capital of France?\",\n},\n],\n});\nconsole.log(chatCompletion.choices[0].message);\nBilling\nHere is how billing works:\nFor direct requests, i.e. when you use the key from an inference provider, you are billed by the corresponding provider. For instance, if you use an OVHcloud API key you're billed on your OVHcloud account.\nFor routed requests, i.e. when you authenticate via the Hugging Face Hub, you'll only pay the standard provider API rates. There's no additional markup from us; we just pass through the provider costs directly. (In the future, we may establish revenue-sharing agreements with our provider partners.)\nImportant Note ‚ÄºÔ∏è PRO users get $2 worth of Inference credits every month. You can use them across providers. üî•\nSubscribe to the Hugging Face PRO plan to get access to Inference credits, ZeroGPU, Spaces Dev Mode, 20x higher limits, and more.\nWe also provide free inference with a small quota for our signed-in free users, but please upgrade to PRO if you can!\nFeedback and next steps\nWe would love to get your feedback! Share your thoughts and/or comments here: https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/49"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/rapidfireai", "title": "20x Faster TRL Fine-tuning with RapidFire AI", "url": "https://huggingface.co/blog/rapidfireai", "published": "Fri, 21 Nov 2025 00:00:00 GMT", "text_source": "article", "article_fetch_error": null, "text": "20x Faster TRL Fine-tuning with RapidFire AI\nHugging Face TRL now officially integrates with RapidFire AI to accelerate your fine-tuning and post-training experiments. TRL users can now discover, install, and run RapidFire AI as the fastest way to compare multiple fine-tuning/post-training configurations to customize LLMs without major code changes and without bloating GPU requirements.\nWhy this matters\nWhen fine-tuning or post-training LLMs, teams often do not have the time and/or budget to compare multiple configs even though that can significantly boost eval metrics. RapidFire AI lets you launch multiple TRL configs concurrently--even on a single GPU--and compare them in near real time via a new adaptive, chunk-based scheduling and execution scheme. In internal benchmarks referenced in the TRL page, this delivers ~16‚Äì24√ó higher experimentation throughput than sequentially comparing configs one after another, enabling you to reach much better metrics much faster.\nRapidFire AI establishes live three-way communication between your IDE, a metrics dashboard, and a multi-GPU execution backend\nWhat you get, out of the box\nDrop-in TRL wrappers ‚Äî Use\nRFSFTConfig\n,RFDPOConfig\n, andRFGRPOConfig\nas near-zero-code replacements for TRL's SFT/DPO/GRPO configs.Adaptive chunk-based concurrent training ‚Äî RapidFire AI shards the dataset into a given number of chunks and cycles configs at chunk boundaries to enable earlier apples-to-apples comparisons and also maximize GPU utilization.\nInteractive Control Ops (IC Ops) ‚Äî From the dashboard itself, you can Stop, Resume, Delete, and Clone-Modify, possibly with Warm-Start, any runs in flight to avoid wasting resources on underperforming configs and double-down on better performing configs--no job restarts, no juggling separate GPUs or clusters, no resource bloat.\nClone promising configurations with modified hyperparameters, optionally warm-starting from the parent's weights, all from the live dashboard\nMulti-GPU orchestration ‚Äî The RapidFire AI scheduler automatically places and orchestrates configs across available GPUs on chunks of data via effcient shared-memory mechanisms. You focus on your models and eval metrics, not plumbing.\nMLflow-based dashboard ‚Äî Real-time metrics, logs, and IC Ops in one place as soon as you start your experiment. Support for more dashboards such as Trackio, W&B, and TensorBoard coming soon.\nHow it works\nRapidFire AI splits your dataset randomly into \"chunks\" and cycles LLM configurations through the GPUs at chunk boundaries. You get incremental signal on eval metrics across all configs much more quickly. The automatic checkpointing via an efficient shared-memory-based adapter/model spilling/loading mechanism keeps training smooth, stable, and consistent. Use IC Ops to adapt mid-flight to stop low-performers earlier and clone promising ones with tweaked config knobs, optionally warm-starting from the parent's weights.\nSequential vs. Task Parallel vs. RapidFire AI: The adaptive scheduler maximizes GPU utilization across multiple configs and GPUs. The bottom row shows IC Ops in action‚Äîstopping, cloning, and modifying runs mid-flight.\nGetting Started\nInstall RapidFire AI and get running in under a minute:\npip install rapidfireai\n# Authenticate with Hugging Face\nhuggingface-cli login --token YOUR_TOKEN\n# Workaround for current issue\npip uninstall -y hf-xet\n# Initialize and start RapidFire AI\nrapidfireai init\nrapidfireai start\nThe dashboard launches at http://localhost:3000\nwhere you can monitor and control all your experiments.\nSupported TRL trainers\n- SFT with\nRFSFTConfig\n- DPO with\nRFDPOConfig\n- GRPO with\nRFGRPOConfig\nThese are designed as drop-in replacements so that you can keep your TRL mental model while gaining far more concurrency and control for your fine-tuning/post-training applications.\nMinimal TRL SFT example\nHere's what it looks like to train multiple configurations concurrently even on a single GPU:\nfrom rapidfireai import Experiment\nfrom rapidfireai.automl import List, RFGridSearch, RFModelConfig, RFLoraConfig, RFSFTConfig\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n# Setup: load your dataset and define formatting\ndataset = load_dataset(\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\")\ntrain_dataset = dataset[\"train\"].select(range(128)).shuffle(seed=42)\ndef formatting_function(row):\nreturn {\n\"prompt\": [\n{\"role\": \"system\", \"content\": \"You are a helpful customer support assistant.\"},\n{\"role\": \"user\", \"content\": row[\"instruction\"]},\n],\n\"completion\": [{\"role\": \"assistant\", \"content\": row[\"response\"]}]\n}\ndataset = dataset.map(formatting_function)\n# Define multiple configs to compare\nconfig_set = List([\nRFModelConfig(\nmodel_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\npeft_config=RFLoraConfig(r=8, lora_alpha=16, target_modules=[\"q_proj\", \"v_proj\"]),\ntraining_args=RFSFTConfig(learning_rate=1e-3, max_steps=128, fp16=True),\n),\nRFModelConfig(\nmodel_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\npeft_config=RFLoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"]),\ntraining_args=RFSFTConfig(learning_rate=1e-4, max_steps=128, fp16=True),\nformatting_func=formatting_function,\n)\n])\n# Run all configs concurrently with chunk-based scheduling\nexperiment = Experiment(experiment_name=\"sft-comparison\")\nconfig_group = RFGridSearch(configs=config_set, trainer_type=\"SFT\")\ndef create_model(model_config):\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_config[\"model_name\"],\ndevice_map=\"auto\", torch_dtype=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_config[\"model_name\"])\nreturn (model, tokenizer)\nexperiment.run_fit(config_group, create_model, train_dataset, num_chunks=4, seed=42)\nexperiment.end()\nWhat happens when you run this?\nSuppose you run the above on a 2-GPU machine. Instead of training sequentially (Config 1 ‚Üí wait ‚Üí Config 2 ‚Üí wait), both configs train concurrently:\n| Approach | Time till Comparative Decision | GPU utilization |\n|---|---|---|\n| Sequential (traditional) | ~15 minutes | 60% utilization |\n| RapidFire AI (concurrent) | ~5 minutes | 95%+ utilization |\nYou can get to a comparative decision 3√ó sooner on the same resources after both configs finish processing the first data chunk instead of waiting for them to see the whole dataset one after another. Open http://localhost:3000\nto watch live metrics and use IC Ops to stop, clone, or tweak runs in real-time based on what you're seeing.\nBenchmarks: Real-World Speedups\nHere is what teams see on time to reach a comparable overall best training loss (across all tried configs) when switching from sequential comparisons to RapidFire AI-enabled hyperparallel experimentation:\n| Scenario | Sequential Time | RapidFire AI Time | Speedup |\n|---|---|---|---|\n| 4 configs, 1 GPU | 120 min | 7.5 min | 16√ó |\n| 8 configs, 1 GPU | 240 min | 12 min | 20√ó |\n| 4 configs, 2 GPUs | 60 min | 4 min | 15√ó |\nBenchmarks on NVIDIA A100 40GB with TinyLlama-1.1B and Llama-3.2-1B models\nGet Started Today\nüöÄ Try it hands-on: Interactive Colab Notebook ‚Äî Zero setup, runs in your browser\nüìö Full Documentation: oss-docs.rapidfire.ai ‚Äî Complete guides, examples, and API reference\nüíª GitHub: RapidFireAI/rapidfireai ‚Äî Open source, production-ready\nüì¶ Install via PyPI: pypi.org/project/rapidfireai ‚Äî pip install rapidfireai\nüí¨ Join the Community: Discord ‚Äî Get help, share results, request features\nRapidFire AI was built because the common status quo of trying one config at a time wastes both time and GPU cycles. With this official integration, every TRL user can fine-tune/post-train smarter, iterate faster, and ship better models.\nTry the integration and let us know: How much faster is your experimentation loop? What should we build next? We're just getting started, and your feedback shapes where we go from here."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/open-asr-leaderboard", "title": "Open ASR Leaderboard: Trends and Insights with New Multilingual & Long-Form Tracks", "url": "https://huggingface.co/blog/open-asr-leaderboard", "published": "Fri, 21 Nov 2025 00:00:00 GMT", "text_source": "article", "article_fetch_error": null, "text": "Open ASR Leaderboard: Trends and Insights with New Multilingual & Long-Form Tracks\nWhile everyone (and their grandma üëµ) is spinning up new ASR models, picking the right one for your use case can feel more overwhelming than choosing your next Netflix show. As of 21 Nov 2025, there are 150 Audio-Text-to-Text and 27K ASR models on the Hub ü§Ø\nMost benchmarks focus on short-form English transcription (<30s), and overlook other important tasks, such as (1) multilingual performance and (2) model throughput, which can a be deciding factor for long-form audio like meetings and podcasts.\nOver the past two years, the Open ASR Leaderboard has become a standard for comparing open and closed-source models on both accuracy and efficiency. Recently, multilingual and long-form transcription tracks have been added to the leaderboard üéâ\nTL;DR - Open ASR Leaderboard\n- üìù New preprint on ASR trends from the leaderboard: https://hf.co/papers/2510.06961\n- üß† Best accuracy: Conformer encoder + LLM decoders (open-source ftw ü•≥)\n- ‚ö° Fastest: CTC / TDT decoders\n- üåç Multilingual: Comes at the cost of single-language performance\n- ‚åõ Long-form: Closed-source systems still lead (for now üòâ)\n- üßëüíª Fine-tuning guides (Parakeet, Voxtral, Whisper): to continue pushing performance\nTakeaways from 60+ models\nAs of 21 Nov 2025, the Open ASR Leaderboard compares 60+ open and closed-source models from 18 organizations, across 11 datasets.\nIn a recent preprint, we dive into the technical setup and highlight some key trends in modern ASR. Here are the big takeaways üëá\n1. Conformer encoder ü§ù LLM decoder tops the charts üìà\nModels combining Conformer encoders with large language model (LLM) decoders currently lead in English transcription accuracy. For example, NVIDIA‚Äôs Canary-Qwen-2.5B, IBM‚Äôs Granite-Speech-3.3-8B, and Microsoft‚Äôs Phi-4-Multimodal-Instruct achieve the lowest word error rates (WER), showing that integrating LLM reasoning can significantly boost ASR accuracy.\nüí° Pro-tip: NVIDIA introduced Fast Conformer, a 2x faster variant of the Conformer, that is used in their Canary and Parakeet suite of models.\n2. Speed‚Äìaccuracy tradeoffs ‚öñÔ∏è\nWhile highly accurate, these LLM decoders tend to be slower than simpler approaches. On the Open ASR Leaderboard, efficiency is measured using inverse real-time factor (RTFx), where higher is better.\nFor even faster inference, CTC and TDT decoders deliver 10‚Äì100√ó faster throughput, albeit with slightly higher error rates. This makes them ideal for real-time, offline, or batch transcription tasks (such as meetings, lectures, or podcasts).\n3. Multilingual üåç\nOpenAI‚Äôs Whisper Large v3 remains a strong multilingual baseline, supporting 99 languages. However, fine-tuned or distilled variants like Distil-Whisper and CrisperWhisper often outperform the original on English-only tasks, showing how targeted fine-tuning can improve specialization (how to fine-tune? Check out guides for Whisper, Parakeet, and Voxtral).\nThat said, focusing on English tends to reduce multilingual coverage üëâ a classic case of the tradeoff between specialization and generalization. Similarly, while self-supervised systems like Meta‚Äôs Massively Multilingual Speech (MMS) and Omnilingual ASR can support 1K+ languages, they trail behind language-specific encoders in accuracy.\n‚≠ê While just five languages are currently benchmarked, we‚Äôre planning to expand to more languages and are excited for new dataset and models contributions to multilingual ASR through GitHub pull requests.\nüéØ Alongside multilingual benchmarks, several community-driven leaderboards focus on individual languages. For example, the Open Universal Arabic ASR Leaderboard compares models across Modern Standard Arabic and regional dialects, highlighting how speech variation and diglossia challenge current systems. Similarly. the Russian ASR Leaderboard provides a growing hub for evaluating encoder-decoder and CTC models on Russian-specific phonology and morphology. These localized efforts mirror the broader multilingual leaderboard‚Äôs mission to encourage dataset sharing, fine-tuned checkpoints, and transparent model comparisons, especially in languages with fewer established ASR resources.\n4. Long-form transcription is a different game ‚è≥\nFor long-form audio (e.g., podcasts, lectures, meetings), closed-source systems still edge out open ones. It could be due to domain tuning, custom chunking, or production-grade optimization.\nAmong open models, OpenAI‚Äôs Whisper Large v3 performs the best. But for throughput, CTC-based Conformers shine üëâ for example, NVIDIA‚Äôs Parakeet CTC 1.1B achieves an RTFx of 2793.75, compared to 68.56 for Whisper Large v3, with only a moderate WER degradation (6.68 and 6.43 respectively).\nThe tradeoff? Parakeet is English-only, again reminding us of that multilingual and specialization tradeoff ü´†.\n‚≠ê While closed systems still lead, there‚Äôs huge potential for open-source innovation here. Long-form ASR remains one of the most exciting frontiers for the community to tackle next!\nüé§ The Show Must Go On\nGiven how fast ASR is evolving, we‚Äôre excited to see what new architectures push performance and efficiency, and how the Open ASR Leaderboard continues to serve as a transparent, community-driven benchmark for the field, and as a reference for other leaderboards (Russian, Arabic, and Speech DeepFake Detection).\nWe‚Äôll keep expanding the Open ASR LeaderBoard with more models, more languages, and more datasets so stay tuned üëÄ\nüëâ Want to contribute? Head on over to the GitHub repo to open a pull request üöÄ"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/anylanguagemodel", "title": "Introducing AnyLanguageModel: One API for Local and Remote LLMs on Apple Platforms", "url": "https://huggingface.co/blog/anylanguagemodel", "published": "Thu, 20 Nov 2025 00:00:00 GMT", "text_source": "article", "article_fetch_error": null, "text": "Introducing AnyLanguageModel: One API for Local and Remote LLMs on Apple Platforms\nLLMs have become essential tools for building software. But for Apple developers, integrating them remains unnecessarily painful.\nDevelopers building AI-powered apps typically take a hybrid approach, adopting some combination of:\n- Local models using Core ML or MLX for privacy and offline capability\n- Cloud providers like OpenAI or Anthropic for frontier capabilities\n- Apple's Foundation Models as a system-level fallback\nEach comes with different APIs, different requirements, different integration patterns. It's a lot, and it adds up quickly. When I interviewed developers about building AI-powered apps, friction with model integration came up immediately. One developer put it bluntly:\nI thought I'd quickly use the demo for a test and maybe a quick and dirty build but instead wasted so much time. Drove me nuts.\nThe cost to experiment is high, which discourages developers from discovering that local, open-source models might actually work great for their use case.\nToday we're announcing AnyLanguageModel, a Swift package that provides a drop-in replacement for Apple's Foundation Models framework with support for multiple model providers. Our goal is to reduce the friction of working with LLMs on Apple platforms and make it easier to adopt open-source models that run locally.\nThe Solution\nThe core idea is simple:\nSwap your import\nstatement, keep the same API.\n- import FoundationModels\n+ import AnyLanguageModel\nHere's what that looks like in practice. Start with Apple's built-in model:\nlet model = SystemLanguageModel.default\nlet session = LanguageModelSession(model: model)\nlet response = try await session.respond(to: \"Explain quantum computing in one sentence\")\nprint(response.content)\nNow try an open-source model running locally via MLX:\nlet model = MLXLanguageModel(modelId: \"mlx-community/Qwen3-4B-4bit\")\nlet session = LanguageModelSession(model: model)\nlet response = try await session.respond(to: \"Explain quantum computing in one sentence\")\nprint(response.content)\nAnyLanguageModel supports a range of providers:\n- Apple Foundation Models: Native integration with Apple's system model (macOS 26+ / iOS 26+)\n- Core ML: Run converted models with Neural Engine acceleration\n- MLX: Run quantized models efficiently on Apple Silicon\n- llama.cpp: Load GGUF models via the llama.cpp backend\n- Ollama: Connect to locally-served models via Ollama's HTTP API\n- OpenAI, Anthropic, Google Gemini: Cloud providers for comparison and fallback\n- Hugging Face Inference Providers: Hundreds of cloud models powered by world-class inference providers.\nThe focus is on local models that you can download from the Hugging Face Hub. Cloud providers are included to lower the barrier to getting started and to provide a migration path. Make it work, then make it right.\nWhy Foundation Models as the Base API\nWhen designing AnyLanguageModel, we faced a choice: create a new abstraction that tries to capture everything, or build on an existing API. We chose the latter, using Apple's Foundation Models framework as the template.\nThis might seem counterintuitive. Why tie ourselves to Apple's choices? A few reasons:\nFoundation Models is genuinely well-designed. It leverages Swift features like macros for an ergonomic developer experience, and its abstractions around sessions, tools, and generation map well to how LLMs actually work.\nIt's intentionally limited. Foundation Models represents something like a lowest common denominator for language model capabilities. Rather than seeing this as a weakness, we treat it as a stable foundation (hyuk hyuk). Every Swift developer targeting Apple platforms will encounter this API, so building on it directly means less conceptual overhead.\nIt keeps us grounded. Each additional layer of abstraction takes you further from the problem you're actually solving. Abstractions are powerful, but stack too many and they become a problem in themselves.\nThe result is that switching between providers requires minimal code changes, and the core abstractions remain clean and predictable.\nPackage Traits: Include Only What You Need\nOne challenge with multi-backend libraries is dependency bloat. If you only want to run MLX models, you shouldn't have to pull in llama.cpp and all its dependencies.\nAnyLanguageModel uses Swift 6.1 package traits to solve this. You opt in to only the backends you need:\ndependencies: [\n.package(\nurl: \"https://github.com/mattt/AnyLanguageModel.git\",\nfrom: \"0.4.0\",\ntraits: [\"MLX\"] // Pull in MLX dependencies only\n)\n]\nAvailable traits include CoreML\n, MLX\n, and Llama\n(for llama.cpp / llama.swift).\nBy default, no heavy dependencies are included.\nYou get the base API plus cloud providers,\nwhich only require standard URLSession\nnetworking.\nFor Xcode projects (which don't yet support trait declarations directly), you can create a small internal Swift package that depends on AnyLanguageModel with the traits you need, then add that package as a local dependency. The README has detailed instructions.\nImage Support (and API Design Trade-offs)\nVision-language models are incredibly capable and widely used. They can describe images, extract text from screenshots, analyze charts, and answer questions about visual content. Unfortunately, Apple's Foundation Models framework doesn't currently support sending images with prompts.\nBuilding on an existing API means accepting its constraints. Apple will likely add image support in a future release (iOS 27, perhaps?), but vision-language models are too useful to wait for. So we've extended beyond what Foundation Models offers today.\nHere's an example sending an image to Claude:\nlet model = AnthropicLanguageModel(\napiKey: ProcessInfo.processInfo.environment[\"ANTHROPIC_API_KEY\"]!,\nmodel: \"claude-sonnet-4-5-20250929\"\n)\nlet session = LanguageModelSession(model: model)\nlet response = try await session.respond(\nto: \"What's in this image?\",\nimage: .init(url: URL(fileURLWithPath: \"/path/to/image.png\"))\n)\nWe're taking a calculated risk here; we might design something that conflicts with Apple's eventual implementation. But that's what deprecation warnings are for. Sometimes you have to write the API for the framework that doesn't exist yet.\nTry It Out: chat-ui-swift\nTo see AnyLanguageModel in action, check out chat-ui-swift, a SwiftUI chat application that demonstrates the library's capabilities.\nThe app includes:\n- Apple Intelligence integration via Foundation Models (macOS 26+)\n- Hugging Face OAuth authentication for accessing gated models\n- Streaming responses\n- Chat persistence\nIt's meant as a starting point: Fork it, extend it, swap in different models. See how the pieces fit together and adapt it to your needs.\nWhat's Next\nAnyLanguageModel is currently pre-1.0. The core API is stable, but we're actively working on bringing the full feature set of Foundation Models to all adapters, namely:\n- Tool calling across all providers\n- MCP integration for tools and elicitations\n- Guided generation for structured outputs\n- Performance optimizations for local inference\nThis library is the first step toward something larger. A unified inference API provides the scaffolding needed to build seamless agentic workflows on Apple platforms ‚Äî applications where models can use tools, access system resources, and accomplish complex tasks. More on that soon. ü§´\nGet Involved\nWe'd love your help making this better:\n- Try it out ‚Äî Build something, kick the tires\n- Share your experiences ‚Äî What works? What's frustrating? We want to hear about the challenges you face integrating AI into your apps\n- Open issues ‚Äî Feature requests, bug reports, questions\n- Contribute ‚Äî PRs are welcome\nLinks\nWe're excited to see what you build ü¶æ"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/ServiceNow-AI/apriel-h1", "title": "Apriel-H1: The Surprising Key to Distilling Efficient Reasoning Models", "url": "https://huggingface.co/blog/ServiceNow-AI/apriel-h1", "published": "Wed, 19 Nov 2025 05:19:07 GMT", "text_source": "article", "article_fetch_error": null, "text": "Apriel-H1: The Surprising Key to Distilling Efficient Reasoning Models\nWe converted our 15B reasoning model to a Mamba hybrid achieving 2.1x throughput with minimal quality loss. The key? A non-obvious insight about what data to distill on, and why intuition fails here.\nWhen MiniMax published their M2 post-mortem in October explaining why they abandoned efficient attention at 230B scale, the narrative briefly became \"efficient attention is dead.\" Within days, Kimi Linear proved otherwise. The real lesson: it depends on your constraints.\nOur constraint was simple: we had a strong 15B reasoning model and needed to make it efficient without starting over. No infinite compute for 20T-token pretraining. No luxury of architectural co-design from day one. Just a practical question: can you retrofit efficiency into an existing model through distillation?\nSpoilers: yes, but only if you ignore your intuition about what data to use.\nWhat We Built\nThe Apriel-H1 family: seven checkpoints spanning 25-40 Mamba layers (out of 50 total), showing the complete efficiency-quality frontier. Our flagship Apriel-H1-15b-Thinker-SFT achieves 2.1x throughput with minimal quality loss: MATH500 and MTBench improve a few points (0.90 ‚Üí 0.92 and 8.30 ‚Üí 8.58, respectively), while GSM8k (0.97 ‚Üí 0.95), GPQA (0.59 ‚Üí 0.55), and AIME24 (0.70 ‚Üí 0.65) regress slightly. Total training: 76.8B tokens.\nApriel-H1-15b-Thinker-SFT (green) vs full-attention teacher (blue). Reasoning quality stays nearly flat across benchmarks while throughput increases 1.89-2.09x depending on context length.\nThe full details are in our Apriel-H1 paper. Here, we focus on the key insight that made it work.\nThe Non-Obvious Insight\nHere's what we initially thought would work: just distill on pretraining data and round it out with some SFT.\nThe reasoning seemed solid. We're inserting completely new Mamba layers that have never seen data. These linear SSMs need to learn general-purpose token mixing from scratch. How can they become effective mixers unless they get exposure to the same broad distribution the original attention layers saw?\nSo we tried it. Then we tried mixing pretraining and SFT data. It didn't work. The distilled hybrids lost reasoning quality, sometimes dramatically.\nWhat actually worked: high-quality reasoning traces from the teacher's SFT dataset.\nDistilling a reasoning model isn't about transferring general next-token prediction. The base model already has that, and we started from a strong 15B foundation. What we're preserving is specific and fragile: the teacher's multi-step reasoning patterns.\nThose patterns emerge from intricate attention mechanisms. Retrieval heads pulling context from thousands of tokens back. Induction heads recognizing and continuing logical chains. Long-range dependencies connecting premises to conclusions many steps later. When you replace attention wholesale with Mamba's linear recurrence, these computational mechanisms are disrupted. The hybrid must discover new paths to the same reasoning outcomes.\nThat discovery requires explicit examples where reasoning structure is visible and correct:\n- Multi-step math proofs where each thought follows from the previous\n- Coding tasks with clear logical dependencies\n- Scientific analysis with detailed explanatory chains\nPretraining data, on the other hand, is too noisy and too diffuse. The reasoning signal gets lost. You need concentrated examples of the specific capability you're trying to preserve.\nOnce we understood the data choice, our distillation method became clear too. We used reverse KL divergence (temperature 1) rather than forward KL. Reverse won consistently. Why? We're training on problems where the teacher has high confidence and clear structure. Reverse KL's mode-seeking behavior encourages the student to commit to those high-confidence predictions. When your teacher is confident and correct, you want your student to be confident too.\nThis insight is the key to the whole approach: match your distillation data to the capability you're preserving, not the capability you're building.\nHow to Apply It: Staged Distillation\nYou can't just swap 40 attention layers for Mamba and hope. We learned this the hard way, and eventually developed a staged distillation procedure to get there reliably.\nStage 1: Identify least-important layers. We used a Leave-One-Out (LOO) analysis on MMLU: remove each layer, replace with identity, then measure the drop. Sort by importance, replace the bottom 25 with Mamba-in-Llama (MIL) initialized mixers. Distill end-to-end. This worked for our H-25 checkpoint.\nStage 2: Progressive conversion beyond 25 layers. LOO broke down past 25 layers because layers unimportant in isolation became critical in combination. To address this, we developed a dynamic heuristic we call MIL-Mamba-Replacement (MMR). For each remaining attention layer, we initialize a Mamba mixer with MIL, run 100 training steps, and record the distillation loss. Layers converging to lower loss are \"easier\" to replace. This captures training dynamics rather than static importance.\nWe progressed incrementally: 25 ‚Üí 27 ‚Üí 30 ‚Üí 34 ‚Üí 37 ‚Üí 40 Mamba layers, grouping replacements by MMR scores. Each checkpoint distills from the previous.\nStage 3: End-to-end training on SFT data. After reaching the target Mamba layer count, we did a final SFT pass until reasoning performance stabilized. After 55.9B distillation tokens and 20.9B SFT tokens, this produced our final Apriel-H1-15b-Thinker-SFT model.\nThe complete efficiency frontier. Each checkpoint shows cumulative training tokens. Our flagship H-30-SFT (released as Apriel-H1-15b-Thinker-SFT) used 76.8B total for 2.1x throughput at 0.76 average score. The aggressively converted H-40 variant used 136.5B tokens for 3.4x throughput. For reference: NVIDIA's Nemotron-Nano-9B-v2 achieves 4.6x at 0.77 score but required training from scratch with orders of magnitude more compute.\nMaking It Reproducible: Fast-LLM\nWe built all this on Fast-LLM, our open-source training framework. The core architectural principle: large language model transformers should be modular. Attention and Mamba are different implementations of the same \"mixing\" interface, and can be swapped freely.\nHere's a hybrid architecture in Fast-LLM's config format:\ndecoder:\ntype: \"pattern\"\nblocks:\nattention_block:\nmixer:\ntype: \"attention\"\nheads: 32\nhead_groups: 8\nhead_size: 128\nmlp:\ntype: \"gated\"\nactivation: \"silu\"\nmamba_block:\nmixer:\ntype: \"mamba\"\nd_inner: 4096\nstate_size: 16\ndt_rank: 16\nmlp:\ntype: \"gated\"\nactivation: \"silu\"\nnum_blocks: 50\npattern: [\"attention_block\", \"attention_block\", \"mamba_block\", ...]\nThe pattern\nfield specifies layer order. For Apriel-H1-15b-Thinker-SFT: 30 mamba_block\n, 20 attention_block\n, placed by importance. That's it.\nDistillation is configuration too:\nmodel:\nbase_model:\nhead:\ndistillation_model: teacher\ndistillation_loss_implementation: reverse_kl\nreference_models:\nteacher:\npretrained:\nformat: mistral\npath: path/to/Apriel-Nemotron-15b-Thinker\nFast-LLM handles gradient accumulation, distributed training, tensor parallelism, checkpointing, everything you need for large-scale experimentation. It's open source, and licensed under Apache 2.0. You can reproduce this work because we designed the infrastructure to make it reproducible.\nFAQs\nWhy release all checkpoints? Because optimal depends on your constraints. H-30 offers the best balance. H-40 maximizes throughput for latency-critical workloads. The intermediate checkpoints let you choose your exact trade-off.\nWhy do you get different speedups at different context lengths? Mamba's linear complexity advantage grows with sequence length, and attention degrades quadratically.\nWhy did you only try Mamba? We used Mamba-1 for three reasons: it has a proven distillation track record, has shown strong empirical performance, and was simple to implement in our framework. It let us focus on the data question first.\nWhat were the Mamba hyperparameters? State size 16, DT rank 16, inner dimension 4096. For our GQA setup in Apriel we expanded B (input projection) and x (state) to match total attention heads following M1.\nWhy didn't you try more advanced conversion methods? We used Mamba-in-Llama initialization and knowledge distillation rather than MOHAWK's multi-stage procedure because the latter didn't show significant advantages in preliminary experiments.\nWhy did you only SFT the H-30 model? We only applied SFT to H-30 to validate that distilled hybrids can be improved through standard post-training. The other checkpoints are pure distillation but can be fine-tuned similarly.\nWhy didn't you explore RL? This was a scoping decision to isolate the distillation question: can you transfer reasoning via knowledge distillation alone? Answer: yes. But RL should close remaining quality gaps further. We are exploring RL for future iterations.\nDid you really show that Apriel-H1 matches full-attention reasoning at similar compute budgets? We didn't do an apples-to-apples comparison between full-attention Apriel and a hybrid trained identically from pretraining forward. That would require repeating all mid-training and post-training of the teacher with the Apriel-H1 architecture, which was beyond our compute budget. What we can claim though is that retrofitting efficiency via distillation is practical and effective, and that the resulting hybrids can be fine-tuned to match or exceed the teacher's reasoning quality.\nThe Production Reality\nWe've implemented Apriel-H1 in Hugging Face Transformers and vLLM. Transformers integration is straightforward. We ship a new model class with interchangeable attention and Mamba layers. vLLM integration uses their recent Mamba cache operations for continuous batching, prefix caching, and chunked prefill. The vLLM plugin is ready. We are currently waiting for final legal approval to open-source it.\nHonest assessment: Deploying hybrids today means rough edges. The tooling is maturing fast but isn't turnkey. You will write custom code, validate numerical behavior carefully, and work around framework limitations. For teams that can absorb that cost, throughput gains are worth it. For those that can't, waiting might be the right call.\nTakeaway\nMost teams don't have infinite compute for 20T-token pretraining. If you've invested in a strong base model and need efficiency gains, this work shows a practical path: distill into hybrids using high-quality task-specific data that matches the capability you're preserving.\nThe surprising finding, use reasoning data to distill reasoning, seems obvious in retrospect but contradicts initial intuition. We validated it, explained why it works, and built the infrastructure to make it reproducible.\nTry It\nModels: Apriel-H1 Collection on HuggingFace\nTraining framework: Fast-LLM on GitHub\nTeacher model: Apriel-Nemotron-15B-Thinker\nPaper: Apriel-H1: Towards Efficient Enterprise Reasoning Models\nFound something broken? File an issue. Discovered a better layer placement heuristic? Tell us. Built something interesting on Apriel-H1? We'd love to see it.\nCitation:\n@article{apriel-h1-2025,\ntitle={Apriel-H1: Towards Efficient Enterprise Reasoning Models},\nauthor={SLAM Lab, ServiceNow},\njournal={arXiv preprint arXiv:2511.02651},\nyear={2025}\n}\nCore contributors: Oleksiy Ostapenko, Luke Kumar, Raymond Li, Denis Kocetkov, Joel Lamy-Poirier, Torsten Scholak\nContributors: Shruthan Radhakrishna, Soham Parikh, Shambhavi Mishra\nTechnical co-leads: Torsten Scholak, Sathwik Tejaswi Madhusudhan"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/build-rocm-kernels", "title": "Easily Build and Share ROCm Kernels with Hugging Face", "url": "https://huggingface.co/blog/build-rocm-kernels", "published": "Mon, 17 Nov 2025 00:00:00 GMT", "text_source": "article", "article_fetch_error": null, "text": "Easily Build and Share ROCm Kernels with Hugging Face\nIntoduction\nCustom kernels are the backbone of high-performance deep learning, enabling GPU operations tailored precisely to your workload; whether that‚Äôs image processing, tensor transformations, or other compute-heavy tasks. But compiling these kernels for the right architectures, wiring all the build flags, and integrating them cleanly into PyTorch extensions can quickly become a mess of CMake/Nix, compiler errors, and ABI issues, which is not fun. Hugging Face‚Äôs kernel-builder and kernels libraries make it easy to share these kernels with the kernels-community, with support for multiple GPU and accelerator backends, including CUDA, ROCm, Metal, and XPU. This ensures your kernels are fast, portable, and seamlessly integrated with PyTorch.\nIn this guide, we focus exclusively on ROCm-compatible kernels and show how to build, test, and share them using kernel-builder. You‚Äôll learn how to create kernels that run efficiently on AMD GPUs, along with best practices for reproducibility, packaging, and deployment.\nThis ROCm-specific walkthrough is a streamlined version of the original kernel-builder guide. If you‚Äôre looking for the broader CUDA-focused version, you can find it here: A Guide to Building and Scaling Production-Ready CUDA Kernels.\nBuild Steps\nWe will use the GEMM kernel from RadeonFlow_Kernels as an example. If you want to go straight to the guide, click here.\nAbout the kernel\nThis section was written by the RadeonFlow GEMM kernel authors to introduce the kernel.\nAuthors: ColorsWind, Zesen Liu, and Andy\nThe RadeonFlow GEMM kernel is a high-performance, FP8 block-wise matrix multiplication implementation optimized for the AMD Instinct MI300X GPU. GEMM (General Matrix Multiplication) is the core building block behind most deep learning workloads: given two matrices A and B, you compute their product C = A √ó B. Here it‚Äôs implemented in FP8, a low-precision floating-point format that trades a bit of accuracy for much higher throughput and lower memory bandwidth. This kernel was developed for the AMD Developer Challenge 2025, it was awarded the üèÜ Grand Prize in June 2025, recognizing its excellence in performance and innovation on AMD hardware.\nThe kernel operates on quantized inputs using the e4m3fnuz\nfloating-point format and applies per-block scaling to preserve accuracy during low-precision computation. The e4m3fnuz\nformat is an FP8 variant with 4 exponent bits and 3 mantissa bits, designed to be efficient for neural network workloads. Because FP8 has a much smaller dynamic range than FP16/FP32, we apply per-block scaling factors (a_scale and b_scale) so that each block of values is rescaled into a numerically ‚Äúcomfortable‚Äù range before and after computation, which helps preserve accuracy despite the low precision. It takes the following arguments:\n(a, b, a_scale, b_scale, c)\nwhere a\nand b\nare the input matrices, a_scale\nand b_scale\nare the scaling factors for a\nand b\nrespectively,\nand c\nis the output matrix:\na\nis K √ó M in e4m3fnuzb\nis K √ó N in e4m3fnuza_scale\nis (K // 128) √ó M in fp32b_scale\nis (K // 128) √ó (N // 128) in fp32c\nis M √ó N in bf16\nThe kernel is precompiled for specific matrix shapes and assumes a transposed memory layout (as required by the competition). To support additional shapes or alternative memory layouts, you must modify the kernel launcher.\nSo now that we have a high-performance ROCm kernel, the natural question is: how do we integrate it into a real PyTorch workflow and share it with others? That‚Äôs exactly what we‚Äôll cover next, using kernel-builder\nand kernels\nto structure, build, and publish the ROCm kernel.\nThis is a fairly technical guide, but you can still follow it step by step without understanding every detail and everything will work fine. If you‚Äôre curious, you can always come back later to dig deeper into the concepts.\nStep 1: Project Structure\nThe Hugging Face Kernel Builder expects your files to be organized like this:\ngemm/\n‚îú‚îÄ‚îÄ build.toml\n‚îú‚îÄ‚îÄ gemm\n‚îÇ ‚îî‚îÄ‚îÄ gemm_kernel.h\n‚îú‚îÄ‚îÄ flake.nix\n‚îî‚îÄ‚îÄ torch-ext\n‚îú‚îÄ‚îÄ torch_binding.cpp\n‚îú‚îÄ‚îÄ torch_binding.h\n‚îî‚îÄ‚îÄ gemm\n‚îî‚îÄ‚îÄ __init__.py\n- build.toml: The project manifest; it‚Äôs the brain of the build process.\n- gemm/: Your raw CUDA source code where the GPU magic happens.\n- flake.nix: The key to a perfectly reproducible build environment.\n- torch-ext/gemm/: The Python wrapper for the raw PyTorch operators\nSometimes your project might depend on other files, like tests or helper scripts, and you can add them without any issues. In our case, our project will be structured like this:\ngemm/\n‚îú‚îÄ‚îÄ build.toml\n‚îú‚îÄ‚îÄ gemm\n‚îÇ ‚îú‚îÄ‚îÄ gemm_kernel.h\n‚îÇ ‚îú‚îÄ‚îÄ gemm_kernel_legacy.h\n‚îÇ ‚îú‚îÄ‚îÄ transpose_kernel.h\n‚îÇ ‚îî‚îÄ‚îÄ gemm_launcher.hip\n‚îú‚îÄ‚îÄ include\n‚îÇ ‚îú‚îÄ‚îÄ clangd_workaround.h\n‚îÇ ‚îú‚îÄ‚îÄ gpu_libs.h\n‚îÇ ‚îú‚îÄ‚îÄ gpu_types.h\n‚îÇ ‚îî‚îÄ‚îÄ timer.h\n‚îú‚îÄ‚îÄ src/utils\n‚îÇ ‚îú‚îÄ‚îÄ arithmetic.h\n‚îÇ ‚îî‚îÄ‚îÄ timer.hip\n‚îú‚îÄ‚îÄ tests/checker\n‚îÇ ‚îú‚îÄ‚îÄ checker.cpp\n‚îÇ ‚îú‚îÄ‚îÄ metrics.h\n‚îÇ ‚îî‚îÄ‚îÄ checker.h\n‚îú‚îÄ‚îÄ flake.nix\n‚îî‚îÄ‚îÄ torch-ext\n‚îú‚îÄ‚îÄ torch_binding.cpp\n‚îú‚îÄ‚îÄ torch_binding.h\n‚îî‚îÄ‚îÄ gemm\n‚îî‚îÄ‚îÄ __init__.py\nIf you look at the original files of the gemm kernel in the RadeonFlow Kernels, they are HIP source files with .cpp\nextensions. As a first step, you need to change these extensions to either .h or .hip depending on their content and usage:\n- Use\n.h\nfor header files containing kernel declarations, inline functions, or template code that will be included in other files - Use\n.hip\nfor implementation files containing HIP/GPU code that needs to be compiled separately (e.g., kernel launchers, device functions with complex implementations)\nIn our example, gemm_kernel.h\n, gemm_kernel_legacy.h\n, and transpose_kernel.h\nare header files, while gemm_launcher.hip\nis a HIP implementation file. This naming convention helps the kernel-builder correctly identify and compile each file type.\nStep 2: Configuration Files Setup\nThe build.toml\nManifest\nThis file orchestrates the entire build. It tells the kernel-builder what to compile and how everything connects.\n[general]\nname = \"gemm\"\nuniversal = false\n[torch]\nsrc = [\n\"torch-ext/torch_binding.cpp\",\n\"torch-ext/torch_binding.h\",\n]\n[kernel.gemm]\nbackend = \"rocm\"\nrocm-archs = [\n\"gfx942\",\n]\ndepends = [\"torch\"]\nsrc = [\n\"include/clangd_workaround.h\",\n\"include/gpu_libs.h\",\n\"include/gpu_types.h\",\n\"include/timer.h\",\n\"gemm/gemm_kernel.h\",\n\"gemm/gemm_kernel_legacy.h\",\n\"gemm/gemm_launcher.hip\",\n\"gemm/transpose_kernel.h\",\n\"src/utils/arithmetic.h\",\n\"src/utils/timer.hip\",\n\"tests/checker/metrics.h\",\n]\ninclude = [\"include\"]\ngeneral\nThis section contains general project configuration settings.\n- name (required): The name of your project. This should match your kernel name and will be used for the Python package.\n- universal (optional): the kernel is a universal kernel when set to\ntrue\n. A universal kernel is a pure Python package (no compiled files). Universal kernels do not use the other sections described below. A good example of a universal kernel is a Triton kernel. Default:false\ntorch\nThis section describes the Torch extension configuration. It defines the Python bindings that will expose your kernel to PyTorch.\n- src (required): A list of source files and headers for the PyTorch extension. In our case, this includes the C++ binding files that create the Python interface.\nkernel.gemm\nSpecification of a kernel named \"gemm\". You can define multiple kernel sections in the same build.toml file if you have multiple kernels.\n- backend (required): The compute backend for the kernel. We use \"rocm\" for AMD GPU support.\n- rocm-archs (required for ROCm): A list of ROCm architectures that the kernel should be compiled for. \"gfx942\" targets the MI300 series GPUs.\n- depends (required): A list of dependencies. We depend on \"torch\" to use PyTorch's tensor operations.\n- include (optional): Include directories relative to the project root. This helps the compiler find header files.\nThe flake.nix\nReproducibility File\nTo ensure anyone can build your kernel on any machine, we use a flake.nix file. It locks the exact version of the kernel-builder and its dependencies. (You can just copy and paste this example and change the description)\n{\ndescription = \"Flake for GEMM kernel\";\ninputs = {\nkernel-builder.url = \"github:huggingface/kernel-builder\";\n};\noutputs =\n{\nself,\nkernel-builder,\n}:\nkernel-builder.lib.genFlakeOutputs {\ninherit self;\npath = ./.;\n};\n}\nWriting the Kernel\nNow for the GPU code. Inside gemm/gemm_launcher.hip\n, we define how the GEMM kernel is launched.\nDepending on the configuration, we either call the new optimized gemm/gemm_kernel\nor fall back to the legacy implementation (gemm/gemm_kernel_legacy\n).\n// ... previous includes and definitions\nextern \"C\" void run(\nvoid *a, void *b, void *as, void *bs, void *c,\nint m, int n, int k,\nPerfMetrics *metrics, hipStream_t job_stream0\n) {\nconst __FP8_TYPE *a_ptr = static_cast<const __FP8_TYPE *>(a);\nconst __FP8_TYPE *b_ptr = static_cast<const __FP8_TYPE *>(b);\n__BF16_TYPE *c_ptr = static_cast<__BF16_TYPE *>(c);\nconst float *as_ptr = static_cast<const float *>(as);\nconst float *bs_ptr = static_cast<const float *>(bs);\nKernelTimerScoped timer(timers, 2LL * m * n * k,\nmetrics ? &metrics->entries[0].time : nullptr,\nmetrics ? &metrics->entries[0].gflops : nullptr, job_stream0);\n// Dispatch GEMM to the fastest available implementation\nswitch (pack_shape(m, n, k)) {\nDISPATCH_GEMM(1024, 1536, 7168, 256, 128, 128, 4, 2, 512, 4, 16);\nDISPATCH_GEMM(6144, 7168, 2304, 256, 128, 128, 4, 2, 512, 1, 16);\ndefault: {\nprintf(\"Error: Unsupported shape M=%d, K=%d, N=%d\\n\", m, k, n);\nabort();\n}\n}\n}\n// ...\nRegistering a Native PyTorch Operator\nThis step is key. We‚Äôre not just making the function available in Python; we‚Äôre turning it into a native PyTorch operator. That means it becomes a first-class part of PyTorch itself, accessible through torch.ops\n.\nThe file torch-ext/torch_binding.cpp\nhandles this registration.\n#include <torch/all.h>\n#include <torch/library.h>\n#include <hip/hip_runtime.h>\n#include \"registration.h\"\n#include \"torch_binding.h\"\n// Forward declaration of the C function from gemm_launcher.hip\nextern \"C\" {\nstruct PerfMetrics;\nvoid run(void *a, void *b, void *as, void *bs, void *c, int m, int n, int k, PerfMetrics *metrics, hipStream_t job_stream0);\n}\nvoid gemm(torch::Tensor &out, torch::Tensor const &a, torch::Tensor const &b,\ntorch::Tensor const &as, torch::Tensor const &bs) {\n// Validate tensor properties\nTORCH_CHECK(a.device().is_cuda(), \"Input tensor a must be on GPU device\");\nTORCH_CHECK(b.device().is_cuda(), \"Input tensor b must be on GPU device\");\nTORCH_CHECK(as.device().is_cuda(), \"Scale tensor as must be on GPU device\");\nTORCH_CHECK(bs.device().is_cuda(), \"Scale tensor bs must be on GPU device\");\nTORCH_CHECK(out.device().is_cuda(), \"Output tensor out must be on GPU device\");\nTORCH_CHECK(a.is_contiguous(), \"Input tensor a must be contiguous\");\nTORCH_CHECK(b.is_contiguous(), \"Input tensor b must be contiguous\");\nTORCH_CHECK(as.is_contiguous(), \"Scale tensor as must be contiguous\");\nTORCH_CHECK(bs.is_contiguous(), \"Scale tensor bs must be contiguous\");\nTORCH_CHECK(out.is_contiguous(), \"Output tensor out must be contiguous\");\n// Get matrix dimensions from tensor shapes\n// Assuming a is [M, K], b is [K, N], out is [M, N]\nint M = a.size(0);\nint K = a.size(1);\nint N = b.size(1);\nTORCH_CHECK(b.size(0) == K, \"Matrix dimensions mismatch: a.size(1) != b.size(0)\");\nTORCH_CHECK(out.size(0) == M, \"Output tensor dimension mismatch: out.size(0) != M\");\nTORCH_CHECK(out.size(1) == N, \"Output tensor dimension mismatch: out.size(1) != N\");\n// Use default HIP stream (stream 0)\nconst hipStream_t stream = 0;\n// Call the C function\nrun(a.data_ptr(), b.data_ptr(), as.data_ptr(), bs.data_ptr(), out.data_ptr(),\nM, N, K, nullptr, stream);\n}\nTORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {\nops.def(\"gemm(Tensor! out, Tensor a, Tensor b, Tensor a_scale, Tensor b_scale) -> ()\");\nops.impl(\"gemm\", torch::kCUDA, &gemm);\n}\nREGISTER_EXTENSION(TORCH_EXTENSION_NAME)\nThe torch_binding.h\nfile contains function declarations. For instance, the gemm\nkernel has the following declaration in torch_binding.h\n:\n#pragma once\n#include <torch/torch.h>\nvoid gemm(torch::Tensor &out, torch::Tensor const &a, torch::Tensor const &b,\ntorch::Tensor const &as, torch::Tensor const &bs);\nSetting up the __init__.py\nwrapper\nIn torch-ext/gemm/\nwe need an __init__.py\nfile to make this directory a Python package and to expose our custom operator in a user-friendly way.\nfrom typing import Optional\nimport torch\nfrom ._ops import ops\ndef gemm(a: torch.Tensor, b: torch.Tensor, as_: torch.Tensor, bs: torch.Tensor,\nout: Optional[torch.Tensor] = None) -> torch.Tensor:\nif out is None:\n# Create output tensor with appropriate shape and dtype\nM, K = a.shape\nK_b, N = b.shape\nassert K == K_b, f\"Matrix dimension mismatch: A has {K} cols, B has {K_b} rows\"\n# Output should be BF16 type on the same device as inputs\nout = torch.empty((M, N), dtype=torch.bfloat16, device=a.device)\nops.gemm(out, a, b, as_, bs)\nreturn out\nStep 3: Building the Kernel\nThe kernel builder uses Nix for building kernels. You can build or run the kernels directly if you have Nix installed on your system. We recommend installing Nix in the following way:\n- Linux: use the official Nix installer.\n- macOS: use the Determinate Nix installer. In addition, Xcode 16.x is currently required to build kernels.\nGetting Started with Nix\nFirst of all, run this:\nnix flake update\nThis generates a flake.lock\nfile that pins the kernel builder and all its transitive dependencies. Commit both flake.nix\nand flake.lock\nto your repository to ensure that kernel builds are reproducible.\nSince the kernel builder depends on many packages (e.g., every supported PyTorch version), it is recommended to enable the Hugging Face cache to avoid expensive rebuilds:\n# Install cachix and configure the cache\ncachix use huggingface\nOr run it once without installing cachix permanently:\n# Use cachix without installing it\nnix run nixpkgs#cachix -- use huggingface\nBuilding Kernels with Nix\nA kernel that has a flake.nix\nfile can be built with the build-and-copy command:\ncd Build_RadeonFlow_Kernels/gemm\nnix build . -L\nThe compiled kernel will then be in the local build/\ndirectory.\nDevelopment Shell for Local Development\nThe kernel-builder provides shells for developing kernels. In such a shell, all required dependencies are available, as well as build2cmake\nfor generating project files:\n$ nix develop\n$ build2cmake generate-torch build.toml\n$ cmake -B build-ext\n$ cmake --build build-ext\nIf you want to test the kernel as a Python package, you can do so. nix develop\nwill automatically create a virtual environment in .venv\nand activate it:\n$ nix develop\n$ build2cmake generate-torch build.toml\n$ pip install --no-build-isolation -e .\nDevelopment shells are available for every build configuration. For instance, you can get a Torch 2.7 development shell with ROCm 6.3 using:\n$ rm -rf .venv # Remove existing venv if any\n$ nix develop .#devShells.torch27-cxx11-rocm63-x86_64-linux\nStep 4: Uploading the kernel to the Hub\nNow that we built our kernel, we can test it and upload it to the Hub.\nBuilding the Kernel for All PyTorch and ROCm Versions\nOne small thing we'll want to do before we share is clean up all of the development artifacts that were generated during the build process to avoid uploading unnecessary files.\nbuild2cmake clean build.toml\nTo build the kernel for all supported versions of PyTorch and ROCm, the kernel-builder tool automates the process:\n# Outside of the dev shell, run the following command\n# if you are inside of the sandbox you can leave with `exit`\nnix build . -L\nNote:\nThis process may take a while, as it will build the kernel for all supported versions of PyTorch and ROCm.\nThe output will be in theresult\ndirectory.\nThe last step is to move the results into the expected build directory (this is where the kernels library will look for them).\nmkdir -p build\nrsync -av --delete --chmod=Du+w,Fu+w result/ build/\nPushing to the Hugging Face Hub\nPushing the build artifacts to the Hub will make it straightforward for other developers to use your kernel.\nFirst, create a new repo:\nhf repo create gemm\nMake sure you are logged in to the Hugging Face Hub using huggingface-cli login.\nNow, in your project directory, connect your project to the new repository and push your code:\n# Initialize git and connect to the Hugging Face Hub\ngit init\ngit remote add origin https://huggingface.co/<your-username>/gemm\n# Pull the changes (just the default .gitattributes file)\ngit pull origin main\ngit xet install\ngit checkout -b main\n# Update to use Xet for the binary files\ngit xet track \"*.so\"\n# Add and commit your changes (being careful to only include the necessary files\n# since our build2cmake command generated a lot of dev-specific files)\ngit add \\\nbuild/ gemm/ include/ src/utils tests/checker \\\ntorch-ext/torch_binding.cpp torch-ext/torch_binding.h torch-ext/gemm \\\nflake.nix flake.lock build.toml\ngit commit -m \"feat: Created a compliant gemm kernel\"\ngit push -u origin main\nFantastic! Your kernel is now on the Hugging Face Hub, ready for others to use and fully compliant with the kernels library.\nStep 5: Let's use it :)\nWith the kernels library, you don't \"install\" the kernel in the traditional sense. You load it directly from its Hub repository, which automatically registers the new operator.\nimport torch\nfrom kernels import get_kernel\n# Load the kernel from the Hub\ngemm = get_kernel(\"kernels-community/gemm\")\n# Matrix dimensions (must be supported - see gemm_launcher.cpp)\nM, N, K = 1024, 1536, 7168\nQUANT_SIZE = 128\n# Setup device\ndevice = torch.device(\"cuda\")\n# Create inputs - kernel expects A:(K,M), B:(K,N)\nA_fp32 = torch.randn(M, K, device=device)\nB_fp32 = torch.randn(K, N, device=device)\n# Convert to FP8\nA_fp8 = A_fp32.to(torch.float8_e4m3fnuz)\nB_fp8 = B_fp32.to(torch.float8_e4m3fnuz)\n# Create scale factors (uniform scaling)\nA_scale = torch.ones(K // QUANT_SIZE, M, device=device, dtype=torch.float32)\nB_scale = torch.ones(K // QUANT_SIZE, N // QUANT_SIZE, device=device, dtype=torch.float32)\nC = torch.zeros(M, N, device=device, dtype=torch.bfloat16)\n# Use the kernel\nresult = gemm.gemm(A_fp8, B_fp8, A_scale, B_scale, C)\nThat's it! Your ROCm kernel is now ready to use from the Hugging Face Hub.\nConclusion\nBuilding and sharing ROCm kernels with the Hugging Face is now easier than ever. With a clean, reproducible workflow powered by Nix and seamless integration into PyTorch, developers can focus on optimizing performance rather than setup. Once built, your custom kernel can be shared on the Hugging Face Hub; making it instantly accessible to the community and usable across projects with just a few lines of code. üöÄ\nRelated Libraries & Hub\n- kernel-builder ‚Äì Build and compile custom kernels.\n- kernels ‚Äì Library to manage and load kernels from the Hub.\n- Kernels Community Hub ‚Äì Share and discover kernels from the community."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/amd/openroboticshackathon", "title": "Join the AMD Open Robotics Hackathon", "url": "https://huggingface.co/blog/amd/openroboticshackathon", "published": "Thu, 13 Nov 2025 21:37:26 GMT", "text_source": "article", "article_fetch_error": null, "text": "Join the AMD Open Robotics Hackathon\nLooking to show off your robotics aptitude? The AMD Open Robotics Hackathon hosted by AMD, Hugging Face, and Data Monsters is the place to do it. Whether you‚Äôre a student, hobbyist, startup founder, or seasoned engineer, this event brings together makers, coders, and roboticists for a fast-paced, hands-on competition that turns bold ideas into functioning demos.\nThe first of two in-person hackathons will take place from December 5-7, 2025 in Tokyo Japan. Our next stop will be in Paris France from December 12-14, 2025.\nPreparing for the Hackathon:\nForm a team of up to four roboticists (ages 18+) to take on two missions over the course of 3 days.\nMission 1 ‚Äî An instructor-led exploration and preparation session. Learn how to set up the LeRobot development environment using AMD AI solutions\nMission 2 ‚Äî Build your own creative solution to a real-world problem. Your team has two days to develop an innovative freestyle project using LeRobot\nRecommended technical proficiency:\n‚Ä¢ Strong Linux development skills and experience with Python and related tooling and containerization\n‚Ä¢ Machine learning skills, familiarity with PyTorch, and hands-on experience with model training and inference\n‚Ä¢ Bonus if your team has experience with ROCm, LeRobot, and embedded development.\nHardware will be provided to contestants in the form of SO-101 robotics kits, AMD Ryzen‚Ñ¢ AI processor equipped laptops, and access to AMD Instinct‚Ñ¢ MI300X GPUs on the AMD Developer Cloud. User guides and related information will be provided at the start of the contest.\nPrizes will be awarded to the top 7 teams in each city, with the first-place team receiving $10,000! To qualify, teams must complete both Missions, with judges assessing the creativity, difficulty, ease-of-use, and practicality of Mission 2 projects on a 100-point scale.\nIf you care about robotics and edge AI and want to see how your ideas behave in the wild, the AMD Open Robotics Hackathon is a can‚Äôt-miss opportunity.\nTokyo T&Cs: NO PURCHASE NECESSARY. Sponsor: Advanced Micro Devices, Inc. Contest is open to participants 18 years or older. Contest registration begins on 11/12/2025 at 12:00PM JST and ends on 12/4/2025 at 10:00PM JST. The Contest concludes on 12/7/2025 at 8:00PM JST. To enter and for Official Rules and Terms, including prize descriptions and judging criteria, see https://amdroboticshackathon.datamonsters.com/. Void where prohibited.\nParis T&Cs: NO PURCHASE NECESSARY. Sponsor: Advanced Micro Devices, Inc. Contest is open to participants 18 years or older. Contest registration begins on 11/12/2025 at 12:00PM CET and ends on 12/11/2025 at 10:00PM CET. The Contest concludes on 12/14/2025 at 8:00PM CET. To enter and for Official Rules and Terms, including prize descriptions and judging criteria, see https://amdroboticshackathon.datamonsters.com/. Void where prohibited."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/google-cloud", "title": "Building for an Open Future - our new partnership with Google Cloud", "url": "https://huggingface.co/blog/google-cloud", "published": "Thu, 13 Nov 2025 00:00:00 GMT", "text_source": "article", "article_fetch_error": null, "text": "Building for an Open Future - our new partnership with Google Cloud\nToday, we are happy to announce a new and deeper partnership with Google Cloud, to enable companies to build their own AI with open models.\n‚ÄúGoogle has made some of the most impactful contributions to open AI, from the OG transformer to the Gemma models. I believe in a future where all companies will build and customize their own AI. With this new strategic partnership, we‚Äôre making it easy to do on Google Cloud.‚Äù says Jeff Boudier, at Hugging Face.\n‚ÄúHugging Face has been the driving force enabling companies large and small all over the world to access, use and customize now more than 2 million open models, and we‚Äôve been proud to contribute over 1,000 of our models to the community‚Äù, says Ryan J. Salva, Senior Director of Product Management at Google Cloud. ‚ÄúTogether we will make Google Cloud the best place to build with open models.‚Äù\nA Partnership for Google Cloud customers\nGoogle Cloud customers use open models from Hugging Face in many of its leading AI services. In Vertex AI, the most popular open models are ready to deploy in a couple clicks within Model Garden. Customers who want greater control over their AI infrastructure can find a similar model library available in GKE AI/ML, or use pre-configured environments maintained by Hugging Face. Customers also run AI inference workloads with Cloud Run GPUs, enabling serverless open model deployments.\nThe common thread: we work with Google Cloud to build seamless experiences fully leveraging the unique capabilities of each service to offer choice to the customers.\nThe Gateway to Open Models - A Fast Lane for Google Cloud Customers\nUsage of Hugging Face by Google Cloud customers has grown 10x over the last 3 years, and today, this translates into tens of petabytes of model downloads every month, in billions of requests.\nTo make sure Google Cloud customers have the best experience building with models and datasets from Hugging Face, we are working together to create a CDN Gateway for Hugging Face repositories built on top of both Hugging Face Xet optimized storage and data transfer technologies, and Google Cloud advanced storage and networking capabilities.\nThis CDN Gateway will cache Hugging Face models and datasets directly on Google Cloud to significantly reduce downloading times, and strengthen model supply chain robustness for Google Cloud customers. Whether you‚Äôre using Vertex, GKE, Cloud Run or just building your own stack in VMs in Compute Engine, you will benefit from faster time-to-first-token and simplified model governance.\nA partnership for Hugging Face customers\nHugging Face Inference Endpoints is the easiest way to go from model to deployment in just a couple clicks. Through this deepened partnership we will bring the unique capabilities and cost performance of Google Cloud to Hugging Face customers, starting with Inference Endpoints. Expect more and newer instances available as well as price drops!\nWe will ensure all the fruits of our product and engineering collaboration become easily available to the 10 million AI Builders on Hugging Face. Going from a model page to deploying on Vertex Model Garden or GKE should only take a couple steps. Taking a private model securely hosted in an Enterprise organization on Hugging Face should be as easy as working with public models.\nTPUs, Google custom AI accelerator chips now in their seventh generation, have been steadily improving in performance and software stack maturity. We want to make sure Hugging Face users can fully benefit from the current and the next generations of TPUs when they build AI with open models. We are excited to make TPUs as easy to use as GPUs for Hugging Face models, thanks to native support in our libraries.\nAdditionally, this new partnership will enable Hugging Face to leverage Google industry-leading security technology to make the millions of open models on Hugging Face more secure. Powered by VirusTotal, Google Threat Intelligence and Mandiant, this joint effort aims to secure models, datasets and Spaces as you use the Hugging Face Hub daily.\nBuilding the open future of AI together\nWe want to see a future where every company can build their own AI with open models and host it within their own secure infrastructure, with full control. We are excited to make this future happen with Google Cloud. Our deep collaboration will accelerate this vision, whether you are using Vertex AI Model Garden, Google Kubernetes Engine, Cloud Run or Hugging Face Inference Endpoints.\nIs there something you want us to create or improve thanks to our partnership with Google? Let us know in the comments!"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/huggingface/shifting-compute-landscape", "title": "On the Shifting Global Compute Landscape", "url": "https://huggingface.co/blog/huggingface/shifting-compute-landscape", "published": "Wed, 29 Oct 2025 13:56:45 GMT", "text_source": "article", "article_fetch_error": null, "text": "On the Shifting Global Compute Landscape\nSummary\nThe status quo of AI chip usage, that was once almost entirely U.S.-based, is changing. China‚Äôs immense progress in open-weight AI development is now being met with rapid domestic AI chip development. In the past few months, highly performant open-weight AI models‚Äô inference in China has started to be powered by chips such as Huawei‚Äôs Ascend and Cambricon, with some models starting to be trained using domestic chips.\nThere are two large implications for policymakers and AI researchers and developers respectively: U.S. export controls correlates with expedited Chinese chip production, and chip scarcity in China likely incentivized many of the innovations that are open-sourced and shaping global AI development.\nChina‚Äôs chip development correlates highly with stronger export controls from the U.S. Under uncertainty of chip access, Chinese companies have innovated with both chip production and algorithmic advances for compute efficiency in models. Out of necessity, decreased reliance on NVIDIA has led to domestic full stack AI deployments, as seen with Alibaba.\nCompute limitations likely incentivized advancements architecturally, infrastructurally, and in training. Innovations in compute efficiency from open-weight leaders include DeepSeek‚Äôs introduction of Multi-head Latent Attention (MLA) and Group Relative Policy Optimization (GRPO). A culture of openness encouraged knowledge sharing and improvements in compute efficiency contributed to lower inference costs, evolving the AI economy.\nDomestic silicon‚Äôs proven sufficiency has sparked demand and models are beginning to be optimized for domestic chips. In parallel, software platforms are shifting as alternatives to NVIDIA‚Äôs CUDA emerge and challenge NVIDIA at every layer; synergy between AI developers and chip vendors are creating a new, fast-evolving software ecosystem.\nThe shifting global compute landscape will continue to shape open source, training, deployment, and the overall AI ecosystem.\nThe State of Global Compute\nUtility of and demand for advanced AI chips has followed an upward trajectory and is predicted to continue to increase. Over the past few years all NVIDIA chips maintained dominance. Recently, new players are garnering attention. China has had long-term plans for domestic production, with plans for self-sufficiency and large monetary and infrastructural investments. Now, the next generation of Chinese open-weight AI models are starting to be powered by Chinese chips.\nBroader trends worldwide are intensifying, with both the U.S. and China citing national security in chip and rare earth resource restrictions. As U.S. export controls tightened, the rollout of Chinese-produced chips seemingly accelerated. The rise of China‚Äôs domestic chip industry is fundamentally changing norms and expectations for global AI training and deployment, with more models being optimized for Chinese hardware and compute-efficient open-weight models picking up in adoption. In the last few months, Chinese-produced chips have already started to power inference for popular models and are beginning to power training runs.\nThe changes can affect everything from techniques used in training, to optimizing for both compute efficiency and specific hardware, to lower inference costs, to the recent open source boom. This could shift both U.S. trade policy and China's approach to global deployment, leading to a future of AI Advancements from an American-focused global ecosystem to one where China is at the center.\nThe Beginning of a Rewiring\nChina‚Äôs domestic chip production has been in progress for years before the modern AI boom. One of the most notable advanced chips, Huawei‚Äôs Ascend, initially launched in 2018 but expanded in deployment starting in 2024 and increasingly throughout 2025. Other notable chips include Cambricon Technologies and Baidu‚Äôs Kunlun.\nIn 2022, the Biden administration established export controls on advanced AI chips, a move targeting China's access to high-end GPUs. The strategy was intended to curb the supply of high-end NVIDIA GPUs, stalling China‚Äôs AI progress. Yet, what began as a blockade has paradoxically become a catalyst. The intent to build a wall instead laid the foundation for a burgeoning industry.\nChinese AI labs, initially spurred by a fear of being cut off, have responded with a surge of innovation, producing both world-class open-weight models like Qwen, DeepSeek, GLM, and Kimi, and domestic chips that are increasingly powering both training and inference for those models. There is a growing relationship between chip makers and open source, as the ability to locally run open-weight models also leads to mutually beneficial feedback. This is leading to e.g. more Ascend-optimized models.\nChina‚Äôs advancements in both open source and compute are shifting the global landscape. Martin Casado, partner at a16z, noted that a significant portion of U.S. startups are now building on open-weight Chinese models, and a recent analysis shows Chinese open-weight models leading in popularity on LMArena.\nThe vacuum created by the restrictions has ignited a full-stack domestic effort in China, transforming once-sidelined local chipmakers into critical national assets and fostering intense collaboration between chipmakers and researchers to build a viable non-NVIDIA ecosystem. This is no longer a hypothetical scenario; with giants like Baidu and Ant Group successfully training foundation models on domestic hardware, a parallel AI infrastructure is rapidly materializing, directly challenging NVIDIA‚Äôs greatest advantage: its developer-centric software ecosystem.\nSee the Appendix for a detailed timeline of chip controls and effects on hardware development and deployment.\nThe Reaction: Powering Chinese AI\nThe 2022 ban, coinciding with the global shockwave of ChatGPT, triggered a panic across China's tech landscape. The safe default of abundant NVIDIA compute was gone. Claims of smuggling NVIDIA chips arose. Still, the ban had destroyed the trust from the research community, who, faced with the prospect of being left permanently behind, started to innovate out of necessity. What emerged was a new, pragmatic philosophy where a ‚Äúnon-NVIDIA first‚Äù approach became rational, not merely ideological.\nHow China‚Äôs Compute Landscape Catalyzed the Cambrian Explosion of Open Models\nChinese labs took a different path, focusing on architectural efficiency and open collaboration. Open source, once a niche interest, became the new norm, a pragmatic choice for rapidly accelerating progress through shared knowledge. This paradigm allows organizations to leverage existing, high-quality pre-trained models as a foundation for specialised applications through post-training, dramatically reducing the compute burden. A primary example is the DeepSeek R1 model, which required less than $300,000 for post-training on its V3 architecture, thereby lowering the barrier for companies to develop sophisticated models. While not the full base model, the cost reduction for the reasoning model is substantial. Algorithmic advances that improve memory such as Multi-head Latent Attention (MLA) with DeepSeek‚Äôs V3 model, likely incentivized by compute limitations, are a large part of January 2025‚Äôs ‚ÄúDeepSeek moment‚Äù.\nThat moment also catalyzed a larger movement for Chinese companies, including those that were closed-source, to upend strategies and invest in compute-efficient open-weight models. These models‚Äô lower costs could result from many variables and also are influenced by efficiency; as Chinese companies lowered compute and inference costs, they passed those lower costs to users, further evolving the overall AI economy.\nDeepSeek's (Open) Weight: In addition to high performance and low cost that created waves in early 2025, DeepSeek‚Äôs pioneering as an openly compute-efficient frontier lab is a large part of what has made the company and its models mainstays. These advances can likely be attributed to innovating in a compute-scarce environment. Funded by investor Wenfeng Liang with a \"pure pursuit of open source and AGI,\" DeepSeek became the most-followed organization on Hugging Face. Its highly detailed technical papers, including a groundbreaking _Nature_-published study on its R1 model, set a new standard for scientific communication. While a large draw is its open-weights over its API, in 2024, DeepSeek slashed its API prices to 1/30th of OpenAI's, triggering a price war. In 2025, DeepSeek-OCR further proved their prowess in compute efficiency and with the release of DeepSeek-V3.2-Exp, they passed on a further 50%+ discount to the users. Notably, DeepSeek‚Äôs-V3.2-Exp model was also released with day zero support for deploying on Chinese chips (Huawei‚Äôs Ascend and Cambricon). This release also marks emphasis on CUDA alternatives and exemplifies a full-stack hardware-software AI infrastructure in deployment.\nQwen's Ecosystem Dominance: Alibaba is on a path to control a full stack of high performance models and in-house designed chips, reducing reliance on NVIDIA. The company‚Äôs Qwen family became a primary resource for global open-source research. Its permissive Apache 2.0 license enabled commercial use, which was a barrier to comparable models that often used more restrictive customs licenses, leading to over 100,000 derivative models on Hugging Face. Alibaba recently unveiled improved chips for better inference, with its PPU being integrated into domestic infrastructure projects.\nAn Industry-Wide Tidal Wave of Low-Cost, High Efficiency: More open-weight models released boasting SotA performance with significantly lower pricing. Zhipu AI returned with its GLM-4.5 and 4.6 open-weight releases, with both quickly reaching top trending on Hugging Face and 4.6 becoming the top performing open-weight model on LMArena. GLM‚Äôs API pricing continually lowered, boasting cost-effectiveness that even offered a $3/month plan as an alternative to Claude Code at 1/5 of the price. While full transparency on the pricing decisions is unclear, efficiency likely plays a strong role.\nSeeds of Training Fully on Domestic Chips: While many upcoming chips are designed primarily for inference, more models are hinting at being trained on domestic chips. Ant Group pioneered training its Ling model on complex heterogeneous clusters of NVIDIA, Ascend, and Cambricon chips. Baidu successfully conducted continuous pre-training on a cluster of over 5,000 domestic Kunlun P800 accelerators, producing its Qianfan VL model.\nAdvances in Compute-Constrained Environments Pushing the Technical Frontier\nThe innovation was not confined to model weights alone; it went deep into the software and hardware stack.\nArchitectural Exploration: Grassroots independent researchers such as Peng Bo, have championed Linear Attention as a potential successor to the Transformer. This approach, sometimes dubbed the \"revenge of the RNN\" and seen in models like RWKV, has been scaled into commercial grade models like MiniMax M1 and Qwen-Next by Chinese labs who willingly bet on high-risk, high-reward research. Meanwhile, DeepSeek has taken a different path by iterating on the original Transformer architecture. Their work introduces innovations like Multi-head Latent Attention (MLA) and DeepSeek Sparse Attention (DSA) introduced with its v3.2 model, which are designed to significantly reduce computational costs during inference without sacrificing performance, while also accelerating Reinforcement Learning (RL) exploration through faster rollouts. Highly performant proprietary models architectures are not public and are therefore difficult to compare.\nOpen Infrastructure: In a radical departure from corporate secrecy, labs shared their deepest engineering secrets. The Kimi team's work on the Mooncake serving system formalized prefill/decoding disaggregation. StepFun's Step3 enhanced this with Attention-FFN Disaggregation (AFD). Baidu published detailed technical reports on overcoming engineering challenges in its Ernie 4 training, while ByteDance's Volcengine contributed verl, an open-source library that puts production-grade RL training tools into the community's hands. What was once proprietary know-how became community knowledge, fueling a self-iterating flywheel of progress.\nTraining breakthroughs: DeepSeek‚Äôs DeepSeekMath paper introduced a novel reinforcement learning (RL) methodology, Group Relative Policy Optimization (GRPO), that significantly reduces compute costs compared to prior similar methods Proximal Policy Optimization (PPO) while stabilizing training and even higher accuracy. GRPO has since been featured in a DeepLearning.AI course, built on by Meta‚Äôs researchers in their Code World Model, and lauded as having \"in a large way accelerated RL research program of most US research labs\" by OpenAI research lead Jerry Tworek.\nWith all the work aggregated, on public leaderboards like LMSYS's Chatbot Arena, models like DeepSeek R1, Kimi K2, Qwen and GLM-4.6 now frequently appear near the top alongside U.S. models. Innovation under constraints resulted in leaps.\nThe Aftermath: Hardware, Software and Soft Power\nWhen AI models are trained and deployed, they are often optimized for certain types of chips. More than the hardware itself, NVIDIA‚Äôs software universe has been a reliable friend to the global AI ecosystem.\nThe deep-learning revolution, sparked by AlexNet's 2012 victory on NVIDIA GPUs, created a symbiotic relationship. NVIDIA‚Äôs Compute Unified Device Architecture (CUDA), cuDNN, and Collective Communications Library (NCCL) has long formed the bedrock of AI research. An entire ecosystem, including popular frameworks like PyTorch and Hugging Face transformers were heavily optimized on CUDA. An entire generation of developers grew up inside this ecosystem which created enormous switching costs.\nA software ecosystem reluctant to switch from existing platforms are now exploring elsewhere, which could be the first step away from U.S. reliance. The software side has evolved with the rise of new chips; developers are optimizing for and deploying their latest models on new parallel platforms.\nFrom Sufficient to Demanded\nPrior to 2022, domestic chips from companies like Cambricon and Huawei (Ascend) were rarely treated seriously. They were catapulted to the center of the domestic AI ecosystem in 2025 when SiliconFlow first demonstrated DeepSeek's R1 model running seamlessly on Huawei's Ascend cloud a couple weeks after the R1 release. This created a domino effect, sparking a market-wide race to serve domestic models faster and better on domestic chips.Fueled by the entire ecosystem and not just DeepSeek alone, the Ascend's support matrix quickly expanded. This proved domestic silicon was sufficient and ignited massive demand. Notably, Huawei's Ascend had zero-day integration with the release of DeepSeek v3.2‚Äìa level of collaboration previously unimaginable.\nDomestic Synergy\nResearchers began co-developing with domestic chip vendors, providing direct input and solving problems collaboratively. This synergy creates a development ecosystem tailored for Large Language Models (LLMs) that evolves much faster than NVIDIA‚Äôs CUDA.\nA new generation of younger researchers, trained in this multi-vendor world, emerged without the old biases that domestic hardware is inferior to Nvidia's chips. This collaborative approach has already resulted in adoption. The documentation for the DeepSeek-V3.1 model noting that its new FP8 precision format explicitly aims ‚Äúfor next-gen domestic chips,‚Äù a clear example of hardware-aware model co-design. Its successor, DeepSeek-V3.2, took this principle further by baking in TileLang-based kernels designed for portability across multiple hardware vendors.\nA New Software Landscape\nThe CUDA ecosystem is now being challenged at every layer. Open-source projects like FlagGems from BAAI and TileLang are creating backend-neutral alternatives to CUDA and cuDNN. Communication stacks like Huawei Collective Communication Library (HCCL) and others are providing robust substitutes for NCCL. The ecosystem is substantially different from three years ago, which will have future reverberations globally.\nLooking Ahead\nAdaptations to geopolitical negotiations, resource limitations, and cultural preferences have led to leaps in both China‚Äôs development of highly performant AI and now competitive domestic chips. U.S. policy has changed throughout administrations, from prohibition to a revenue-sharing model, while China responds with a combination of industrial policy and international trade law. Researchers and developers have innovated and adjusted. The effects on open source, training, and deployment point to shifts in software dependencies, compute efficiency innovations that shape development globally, and a self-sufficient Chinese AI ecosystem.\nChina‚Äôs domestic AI ecosystem is accelerating, with companies like Moore Threads, MetaX, and Biren racing toward IPOs. Cambricon, once struggling, has seen its valuation soar. This new chip ecosystem‚Äôs expansion globally is yet to be decided.\nThe future of the global chip ecosystem, and therefore the future of AI progress, has become a key item for upcoming leadership talks. The question is no longer if China can build its own ecosystem, but how far it will go.\nAcknowledgements\nThank you to Adina Yakefu, Nathan Lambert, Matt Sheehan, and Scott Singer for their feedback on earlier drafts. Any errors remain the authors‚Äô responsibility.\nAppendix: A Timeline of Chip Usage and Controls\nBefore 2022, U.S. restrictions were targeted toward specific supercomputing entities. Policy then evolved as regulators and industry adapted.\nThe Initial Moves (October 2022):\nChips such as Ascend are nascent while NVIDIA dominates the global and Chinese market.\nThe Commerce Department‚Äôs Bureau of Industry and Security (BIS) released its \"advanced computing\" controls in order to address U.S. national security and foreign policy concerns. The rule established a compute threshold with an interconnect-bandwidth trigger, immediately cutting off China's access to NVIDIA‚Äôs flagship A100 and H100 GPUs. China promptly filed a WTO dispute (DS615), arguing the measures were discriminatory trade barriers.\nThe Adjustment Era (Late 2022‚Äì2023):\nNVIDIA‚Äôs 95% share of the market in China began to quickly drop.\nNVIDIA started to develop compliant variants for the Chinese market. The A800 (November 2022) and H800 (March 2023) were created with reduced chip-to-chip bandwidth to meet regulatory requirements and serve as alternatives to the A100 and H100s. The immensely popular consumer-grade RTX 4090 was also restricted, prompting the creation of a China-specific RTX 4090D.\nClosing Gaps (Late 2023‚Äì2024):\nPerformance in Chinese domestic chips slowly improves.\nBIS comprehensively upgraded the framework. It removed interconnect bandwidth as a key test and introduced new metrics: Total Processing Performance (TPP) and performance density. This was a direct, successful strike against the A800/H800s. Debates expanded on export controls for the H20 and even model weights.\nShifting the Narrative (2025):\nAdoption of Ascend, Cambricon, and Kunlun sharply increases following January‚Äôs ‚ÄúDeepSeek moment‚Äù.\nAlso in January, the Biden Administration established its AI Diffusion Rule, imposing further restrictions for both chips and select model weights amid security and smuggling concerns. In response, NVIDIA designed a new compliant chip, the H20. Leveraging NVIDIA‚Äôs increasing presence in political spheres, NVIDIA CEO Jensen Huang began publicly explaining the strategic importance of selling U.S. chips worldwide. The U.S. then issued a licensing requirement in April 2025, charging NVIDIA $5.5 billion and effectively halting sales, before rescinding the AI Diffusion Rule in May 2025.\nThe Compromise (August 2025):\nAlibaba announces a new chip for inference.\nAfter intense negotiations, the Commerce Department began issuing licenses for the H20 with an unprecedented 15% revenue-sharing arrangement. But by the time the H20 was unbanned, the market had already started to change.\nChina‚Äôs Response (Late 2025):\nDay zero deployment begins for Ascend and Cambricon among new DeepSeek models.\nAs the U.S. shifted to a revenue-sharing model, Beijing responded. Chinese regulators reportedly instructed firms to cancel NVIDIA orders, steering demand toward domestic accelerators under a \"secure supply at home\" narrative. This was followed by an anti-discrimination investigation into U.S. measures and an anti-dumping probe into U.S. analog ICs, centering chips in future leadership talks."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/lerobotxnvidia-healthcare", "title": "Building a Healthcare Robot from Simulation to Deployment with NVIDIA Isaac", "url": "https://huggingface.co/blog/lerobotxnvidia-healthcare", "published": "Wed, 29 Oct 2025 00:00:00 GMT", "text_source": "article", "article_fetch_error": null, "text": "Building a Healthcare Robot from Simulation to Deployment with NVIDIA Isaac\nTL;DR\nA hands-on guide to collecting data, training policies, and deploying autonomous medical robotics workflows on real hardware\nTable-of-Contents\n- Building a Healthcare Robot from Simulation to Deployment with NVIDIA Isaac\nIntroduction\nSimulation has been a cornerstone in medical imaging to address the data gap. However, in healthcare robotics until now, it's often been too slow, siloed, or difficult to translate into real-world systems.\nNVIDIA Isaac for Healthcare, a developer framework for AI healthcare robotics, enables healthcare robotics developers in solving these challenges via offering integrated data collection, training, and evaluation pipelines that work across both simulation and hardware. Specifically, the Isaac for Healthcare v0.4 release provides healthcare developers with an end-to-end SO - ARM based starter workflow and the bring your own operating room tutorial. The SO-ARM starter workflow lowers the barrier for MedTech developers to experience the full workflow from simulation to train to deployment and start building and validating autonomous on real hardware right away.\nIn this post, we'll walk through the starter workflow and its technical implementation details to help you build a surgical assistant robot in less time than ever imaginable before.\nSO-ARM Starter Workflow; Building an Embodied Surgical Assistant\nThe SO-ARM starter workflow introduces a new way to explore surgical assistance tasks, and providing developers with a complete end-to-end pipeline for autonomous surgical assistance:\n- Collect real-world and synthetic data with SO-ARM using the LeRobot\n- Fine-tune GR00t N1.5, evaluate in IsaacLab, then deploy to hardware\nThis workflow gives developers a safe, repeatable environment to train and refine assistive skills before moving into the Operating Room.\nTechnical Implementation\nThe workflow implements a three-stage pipeline that integrates simulation and real hardware:\n- Data Collection: Mixed simulation and real-world teleoperation demonstrations using using SO101 and LeRobot\n- Model Training: Fine-tuning GR00T N1.5 on combined datasets with dual-camera vision\n- Policy Deployment: Real-time inference on physical hardware with RTI DDS communication\nNotably, over 93% of the data used for policy training was generated synthetically in simulation, underscoring the strength of simulation in bridging the robotic data gap.\nSim2Real Mixed Training Approach\nThe workflow combines simulation and real-world data to address the fundamental challenge that training robots in the real world is expensive and limited, while pure simulation often fails to capture real-world complexities. The approach uses approximately 70 simulation episodes for diverse scenarios and environmental variations, combined with 10-20 real-world episodes for authenticity and grounding. This mixed training creates policies that generalize beyond either domain alone.\nHardware Requirements\nThe workflow requires:\n- GPU: RT Core-enabled architecture (Ampere or later) with ‚â•30GB VRAM for GR00TN1.5 inference\n- SO-ARM101 Follower: 6-DOF precision manipulator with dual-camera vision (wrist and room). The SO-ARM101 features WOWROBO vision components, including a wrist-mounted camera with a 3D-printed adapter\n- SO-ARM101 Leader: 6-DOF Teleoperation interface for expert demonstration collection\nNotably, developers could run all the simulation, training and deployment (3 computers needed for physical AI) on one DGX Spark.\nData Collection Implementation\nFor real-world data collection with SO-ARM101 hardware or any other version supported in LeRobot:\npython lerobot-record \\\n--robot.type=so101_follower \\\n--robot.port=<follower_port_id> \\\n--robot.cameras=\"{wrist: {type: opencv, index_or_path: 0, width: 640, height: 480, fps: 30}, room: {type: opencv, index_or_path: 2, width: 640, height: 480, fps: 30}}\" \\\n--robot.id=so101_follower_arm \\\n--teleop.type=so101_leader \\\n--teleop.port=<leader_port_id> \\\n--teleop.id=so101_leader_arm \\\n--dataset.repo_id=<user>/surgical_assistance/surgical_assistance \\\n--dataset.num_episodes=15 \\\n--dataset.single_task=\"Prepare and hand surgical instruments to surgeon\"\nFor simulation-based data collection:\n# With keyboard teleoperation\npython -m simulation.environments.teleoperation_record \\\n--enable_cameras \\\n--record \\\n--dataset_path=/path/to/save/dataset.hdf5 \\\n--teleop_device=keyboard\n# With SO-ARM101 leader arm\npython -m simulation.environments.teleoperation_record \\\n--port=<your_leader_arm_port_id> \\\n--enable_cameras \\\n--record \\\n--dataset_path=/path/to/save/dataset.hdf5\nSimulation Teleoperation Controls\nFor users without physical SO-ARM101 hardware, the workflow provides keyboard-based teleoperation with the following joint controls:\n- Joint 1 (shoulder_pan): Q (+) / U (-)\n- Joint 2 (shoulder_lift): W (+) / I (-)\n- Joint 3 (elbow_flex): E (+) / O (-)\n- Joint 4 (wrist_flex): A (+) / J (-)\n- Joint 5 (wrist_roll): S (+) / K (-)\n- Joint 6 (gripper): D (+) / L (-)\n- R Key: Reset recording environment\n- N Key: Mark episode as successful\nModel Training Pipeline\nAfter collecting both simulation and real-world data, convert and combine datasets for training:\n# Convert simulation data to LeRobot format\npython -m training.hdf5_to_lerobot \\\n--repo_id=surgical_assistance_dataset \\\n--hdf5_path=/path/to/your/sim_dataset.hdf5 \\\n--task_description=\"Autonomous surgical instrument handling and preparation\"\n# Fine-tune GR00T N1.5 on mixed dataset\npython -m training.gr00t_n1_5.train \\\n--dataset_path /path/to/your/surgical_assistance_dataset \\\n--output_dir /path/to/surgical_checkpoints \\\n--data_config so100_dualcam\nThe trained model processes natural language instructions such as \"Prepare the scalpel for the surgeon\" or \"Hand me the forceps\" and executes the corresponding robotic actions. With LeRobot latest release (0.4.0) you will be able to fine-tune Gr00t N1.5 natively in LeRobot!\nEnd-to-End Sim Collect‚ÄìTrain‚ÄìEval Pipelines\nSimulation is most powerful when it's part of a loop: collect ‚Üí train ‚Üí evaluate ‚Üí deploy.\nWith v0.3, IsaacLab supports this full pipeline:\nGenerate Synthetic Data in Simulation\n- Teleoperate robots using keyboard or hardware controllers\n- Capture multi-camera observations, robot states, and actions\n- Create diverse datasets with edge cases impossible to collect safely in real environments\nTrain and Evaluate Policies\n- Deep integration with Isaac Lab's RL framework for PPO training\n- Parallel environments (thousands of simulations simultaneously)\n- Built-in trajectory analysis and success metrics\n- Statistical validation across varied scenarios\nConvert Models to TensorRT\n- Automatic optimization for production deployment\n- Support for dynamic shapes and multi-camera inference\n- Benchmarking tools to verify real-time performance\nThis reduces time from experiment to deployment and makes sim2real a practical part of daily development.\nGetting Started\nIsaac for Healthcare SO-ARM Starter Workflow is available now. To get started:\n- Clone the repository:\ngit clone https://github.com/isaac-for-healthcare/i4h-workflows.git\n- Choose a workflow: Start with the SO-ARM Starter Workflow for surgical assistance or explore other workflows\n- Run the setup: Each workflow includes an automated setup script (e.g.,\ntools/env_setup_so_arm_starter.sh\n)\nResources\n- GitHub Repository: Complete workflow implementations\n- Documentation: Setup and usage guides\n- GR00T Models: Pre-trained foundation models\n- Hardware Guides: SO-ARM101 setup instructions\n- LeRobot Repository: End-to-end robotics learning"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/nvidia/nvidia-isaac-for-healthcare", "title": "How to Build a Healthcare Robot from Simulation to Deployment with NVIDIA Isaac for Healthcare", "url": "https://huggingface.co/blog/nvidia/nvidia-isaac-for-healthcare", "published": "Tue, 28 Oct 2025 20:42:35 GMT", "text_source": "article", "article_fetch_error": null, "text": "How to Build a Healthcare Robot from Simulation to Deployment with NVIDIA Isaac for Healthcare\nA hands-on guide to collecting data, training policies, and deploying autonomous medical robotics workflows on real hardware\nSimulation has been a cornerstone in medical imaging to address the data gap. However, in healthcare robotics until now, it's often been too slow, siloed, or difficult to translate into real-world systems. That‚Äôs now changing. With new advances in GPU-accelerated simulation and digital twins, developers can design, test, and validate robotic workflows entirely in virtual environments - reducing prototyping time from months to days, improving model accuracy, and enabling safer, faster innovation before a single device reaches the operating room.\nThat's why NVIDIA introduced Isaac for Healthcare earlier this year, a developer framework for AI healthcare robotics, that enables developers in solving these challenges via integrated data collection, training, and evaluation pipelines that work across both simulation and hardware. Specifically, the Isaac for Healthcare v0.4 release provides users with an end-to-end SO-ARM based starter workflow and the bring your own operating room tutorial. The SO-ARM starter workflow lowers the barrier for MedTech developers to experience the full workflow from simulation to training to deployment and start building and validating autonomously on real hardware right away.\nIn this post, we'll walk through the starter workflow and its technical implementation details to help you build a surgical assistant robot in less time than ever imaginable before.\nSO-ARM Starter Workflow; Building an Embodied Surgical Assistant\nThe SO-ARM starter workflow introduces a new way to explore surgical assistance tasks, and provides developers with a complete end-to-end pipeline for autonomous surgical assistance:\n- Collect real-world and synthetic data with SO-ARM using LeRobot\n- Post-train GR00T N1.5, evaluate in Isaac Lab, then deploy to hardware\nThis workflow gives developers a safe, repeatable environment to train and refine assistive skills before moving into the Operating Room.\nTechnical Implementation\nThe workflow implements a three-stage pipeline that integrates simulation and real hardware:\n- Data Collection: Mixed simulation and real-world teleoperation demonstrations using SO-101 and LeRobot\n- Model Training: Post-training GR00T N1.5 on combined datasets with dual-camera vision\n- Policy Deployment: Real-time inference on physical hardware with RTI DDS communication\nNotably, over 93% of the data used for policy training was generated synthetically in simulation, underscoring the strength of simulation in bridging the robotic data gap.\nSim-to-Real Mixed Training Approach\nThe workflow combines simulation and real-world data to address the fundamental challenge that training robots in the real world is expensive and limited, while pure simulation often fails to capture real-world complexities. The approach uses approximately 70 simulation episodes for diverse scenarios and environmental variations, combined with 10-20 real-world episodes for authenticity and grounding. This mixed training creates policies that generalize beyond either domain alone.\nHardware Requirements\nThe workflow requires:\n- GPU: RT Core-enabled architecture (Ampere or later) with ‚â•30GB VRAM for GR00T N1.5 inference\n- SO-ARM101 Follower: 6-DOF precision manipulator with dual-camera vision (wrist and room). The SO-ARM101 features WOWROBO vision components, including a wrist-mounted camera with a 3D-printed adapter.\n- SO-ARM101 Leader: 6-DOF Teleoperation interface for expert demonstration collection\nNotably, developers could run all the simulation, training and deployment (3 computers needed for physical AI) on one DGX Spark.\nData Collection Implementation\nFor real-world data collection with SO-ARM101 hardware or any other version supported in LeRobot:\npython /path/to/lerobot-record \\\n--robot.type=so101_follower \\\n--robot.port=<follower_port_id> \\\n--robot.cameras=\"{wrist: {type: opencv, index_or_path: 0, width: 640, height: 480, fps: 30}, room: {type: opencv, index_or_path: 2, width: 640, height: 480, fps: 30}}\" \\\n--robot.id=so101_follower_arm \\\n--teleop.type=so101_leader \\\n--teleop.port=<leader_port_id> \\\n--teleop.id=so101_leader_arm \\\n--dataset.repo_id=<user>/surgical_assistance/surgical_assistance \\\n--dataset.num_episodes=15 \\\n--dataset.single_task=\"Prepare and hand surgical instruments to surgeon\"\nFor simulation-based data collection:\n# With keyboard teleoperation\npython -m simulation.environments.teleoperation_record \\\n--enable_cameras \\\n--record \\\n--dataset_path=/path/to/save/dataset.hdf5 \\\n--teleop_device=keyboard\n# With SO-ARM101 leader arm\npython -m simulation.environments.teleoperation_record \\\n--port=<your_leader_arm_port_id> \\\n--enable_cameras \\\n--record \\\n--dataset_path=/path/to/save/dataset.hdf5\nSimulation Teleoperation Controls\nFor users without physical SO-ARM101 hardware, the workflow provides keyboard-based teleoperation with the following joint controls:\n- Joint 1 (shoulder_pan): Q (+) / U (-)\n- Joint 2 (shoulder_lift): W (+) / I (-)\n- Joint 3 (elbow_flex): E (+) / O (-)\n- Joint 4 (wrist_flex): A (+) / J (-)\n- Joint 5 (wrist_roll): S (+) / K (-)\n- Joint 6 (gripper): D (+) / L (-)\n- R Key: Reset recording environment\n- N Key: Mark episode as successful\nModel Training Pipeline\nAfter collecting both simulation and real-world data, convert and combine datasets for training:\n# Convert simulation data to LeRobot format\npython -m training.hdf5_to_lerobot \\\n--repo_id=surgical_assistance_dataset \\\n--hdf5_path=/path/to/your/sim_dataset.hdf5 \\\n--task_description=\"Autonomous surgical instrument handling and preparation\"\n# Post-train GR00T N1.5 on mixed dataset\npython -m training.gr00t_n1_5.train \\\n--dataset_path /path/to/your/surgical_assistance_dataset \\\n--output_dir /path/to/surgical_checkpoints \\\n--data_config so100_dualcam\nThe trained model processes natural language instructions such as \"Prepare the scalpel for the surgeon\" or \"Hand me the forceps\" and executes the corresponding robotic actions. With the latest LeRobot release (v0.4.0) you will be able to post-train GR00T N1.5 natively in LeRobot!\nEnd-to-End Sim Collect‚ÄìTrain‚ÄìEval Pipelines\nSimulation is most powerful when it's part of a loop: collect data ‚Üí train ‚Üí evaluate ‚Üí deploy. Isaac Lab supports this full pipeline:\nGenerate Synthetic Data in Simulation\n- Teleoperate robots using keyboard or hardware controllers\n- Capture multi-camera observations, robot states, and actions\n- Create diverse datasets with edge cases impossible to collect safely in real environments\nTrain and Evaluate Policies\n- Deep integration with Isaac Lab's RL framework for PPO training\n- Parallel environments (thousands of simulations simultaneously)\n- Built-in trajectory analysis and success metrics\n- Statistical validation across varied scenarios\nConvert Models to TensorRT\n- Automatic optimization for production deployment\n- Support for dynamic shapes and multi-camera inference\n- Benchmarking tools to verify real-time performance\nThis reduces time from experiment to deployment and makes sim-to-real a practical part of daily development.\nGetting Started\nIsaac for Healthcare SO-ARM Starter Workflow is available now. To get started:\n- Clone the repository:\ngit clone https://github.com/isaac-for-healthcare/i4h-workflows.git\n- Choose a workflow: Start with the SO-ARM Starter Workflow for surgical assistance or explore other workflows\n- Run the setup: Each workflow includes an automated setup script (for example,\ntools/env_setup_so_arm_starter.sh\n)\nResources\n- GitHub Repository: Complete workflow implementations\n- Documentation: Setup and usage guides\n- GR00T Models: Pre-trained foundation models\n- Hardware Guides: SO-ARM101 setup instructions\n- LeRobot Repository: End-to-end robotics learning"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://huggingface.co/blog/feed.xml", "feed_title": "Hugging Face - Blog", "id": "https://huggingface.co/blog/ibm-granite/granite-4-nano", "title": "Granite 4.0 Nano: Just how small can you go?", "url": "https://huggingface.co/blog/ibm-granite/granite-4-nano", "published": "Tue, 28 Oct 2025 14:59:38 GMT", "text_source": "article", "article_fetch_error": null, "text": "Granite 4.0 Nano: Just how small can you go?\nToday we are excited to share Granite 4.0 Nano, our smallest models yet, released as part of IBM's Granite 4.0 model family. Designed for the edge and on-device applications, these models demonstrate excellent performance for their size and represent IBM's continued commitment to develop powerful, useful, models that don't require hundreds of billions of parameters to get the job done.\nLike all Granite 4.0 models, the Nano models are released under an Apache 2.0 license with native architecture support on popular runtimes like vLLM, llama.cpp, and MLX. The models were trained with the same improved training methodologies, pipelines, and over 15T tokens of training data developed for the original Granite 4.0 models. This release includes variants benefiting from the Granite 4.0‚Äôs new, efficient hybrid architecture, and like all Granite language models, the Granite 4.0 Nano models also carry with them IBM's ISO 42001 certification for responsible model development, giving users added confidence that models are built and governed to global standards.\nSpecifically, Granite 4.0 Nano comprises of 4 instruct models and their base model counterparts:\n- Granite 4.0 H 1B ‚Äì A ~1.5B parameter, dense LLM featuring a hybrid-SSM based architecture.\n- Granite 4.0 H 350M ‚Äì A ~350M parameter, dense LLM featuring a hybrid-SSM based architecture.\n- Granite 4.0 1B and Granite 4.0 350M ‚Äì Alternative traditional transformer versions of our 1B and 350M Nano models, designed to enable workloads where hybrid architectures may not yet have optimized support (e.g. Llama.cpp).\nBuilding sub-billion to ~1 billion parameter models is an active and competitive space, with advancements in performance and architectures recently made by a number of model developers such as Alibaba (Qwen), LiquidAI (LFM), Google (Gemma) and others. When compared to these other models, Granite 4.0 Nano models demonstrate a significant increase in capabilities that can be achieved with a minimal parameter footprint, as measured by a series of general benchmarks across General Knowledge, Math, Code, and Safety domains.\nChart 1. Average accuracy of 0.2B‚Äì2B parameter models across Knowledge, Math, Code, and Safety benchmarks. See Appendix I for full details.\nIn addition to more general benchmarks, Granite Nano models outperformed several similarly sized models on tasks critical for agentic workflows, including instruction following and tool calling, as measured by IFEval and Berkley's Function Calling Leaderboard v3 (BFCLv3) benchmarks.\nChart 2. Accuracy on IFEval and BFCLv3 benchmarks.\nFull details of the Granite 4.0 Nano can be found on the Hugging Face model cards. Moving forward, expect to see more releases from IBM as we continue to grow the Granite 4.0 family and work to make AI a more efficient and effective tool for developers."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://github.com/karol-broda/snitch", "title": "Snitch ‚Äì A friendlier ss/netstat", "url": "https://github.com/karol-broda/snitch", "published": "Tue, 23 Dec 2025 01:03:57 +0000", "text_source": "article", "article_fetch_error": null, "text": "a friendlier ss\n/ netstat\nfor humans. inspect network connections with a clean tui or styled tables.\ngo install github.com/karol-broda/snitch@latest\n# try it\nnix run github:karol-broda/snitch\n# install to profile\nnix profile install github:karol-broda/snitch\n# or add to flake inputs\n{\ninputs.snitch.url = \"github:karol-broda/snitch\";\n}\n# then use: inputs.snitch.packages.${system}.default\n# with yay\nyay -S snitch-bin\n# with paru\nparu -S snitch-bin\ncurl -sSL https://raw.githubusercontent.com/karol-broda/snitch/master/install.sh | sh\ninstalls to ~/.local/bin\nif available, otherwise /usr/local/bin\n. override with:\ncurl -sSL https://raw.githubusercontent.com/karol-broda/snitch/master/install.sh | INSTALL_DIR=~/bin sh\nmacos: the install script automatically removes the quarantine attribute (\ncom.apple.quarantine\n) from the binary to allow it to run without gatekeeper warnings. to disable this, setKEEP_QUARANTINE=1\n.\ndownload from releases:\n- linux:\nsnitch_<version>_linux_<arch>.tar.gz\nor.deb\n/.rpm\n/.apk\n- macos:\nsnitch_<version>_darwin_<arch>.tar.gz\ntar xzf snitch_*.tar.gz\nsudo mv snitch /usr/local/bin/\nmacos: if blocked with \"cannot be opened because the developer cannot be verified\", run:\nxattr -d com.apple.quarantine /usr/local/bin/snitch\nsnitch # launch interactive tui\nsnitch -l # tui showing only listening sockets\nsnitch ls # print styled table and exit\nsnitch ls -l # listening sockets only\nsnitch ls -t -e # tcp established connections\nsnitch ls -p # plain output (parsable)\ninteractive tui with live-updating connection list.\nsnitch # all connections\nsnitch -l # listening only\nsnitch -t # tcp only\nsnitch -e # established only\nsnitch -i 2s # 2 second refresh interval\nkeybindings:\nj/k, ‚Üë/‚Üì navigate\ng/G top/bottom\nt/u toggle tcp/udp\nl/e/o toggle listen/established/other\ns/S cycle sort / reverse\nw watch/monitor process (highlight)\nW clear all watched\nK kill process (with confirmation)\n/ search\nenter connection details\n? help\nq quit\none-shot table output. uses a pager automatically if output exceeds terminal height.\nsnitch ls # styled table (default)\nsnitch ls -l # listening only\nsnitch ls -t -l # tcp listeners\nsnitch ls -e # established only\nsnitch ls -p # plain/parsable output\nsnitch ls -o json # json output\nsnitch ls -o csv # csv output\nsnitch ls -n # numeric (no dns resolution)\nsnitch ls --no-headers # omit headers\njson output for scripting.\nsnitch json\nsnitch json -l\nstream json frames at an interval.\nsnitch watch -i 1s | jq '.count'\nsnitch watch -l -i 500ms\ncheck for updates and upgrade in-place.\nsnitch upgrade # check for updates\nsnitch upgrade --yes # upgrade automatically\nsnitch upgrade -v 0.1.7 # install specific version\nshortcut flags work on all commands:\n-t, --tcp tcp only\n-u, --udp udp only\n-l, --listen listening sockets\n-e, --established established connections\n-4, --ipv4 ipv4 only\n-6, --ipv6 ipv6 only\n-n, --numeric no dns resolution\nfor more specific filtering, use key=value\nsyntax with ls\n:\nsnitch ls proto=tcp state=listen\nsnitch ls pid=1234\nsnitch ls proc=nginx\nsnitch ls lport=443\nsnitch ls contains=google\nstyled table (default):\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ PROCESS ‚îÇ PID ‚îÇ PROTO ‚îÇ STATE ‚îÇ LADDR ‚îÇ LPORT ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ nginx ‚îÇ 1234 ‚îÇ tcp ‚îÇ LISTEN ‚îÇ * ‚îÇ 80 ‚îÇ\n‚îÇ postgres ‚îÇ 5678 ‚îÇ tcp ‚îÇ LISTEN ‚îÇ 127.0.0.1 ‚îÇ 5432 ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n2 connections\nplain output (-p\n):\nPROCESS PID PROTO STATE LADDR LPORT\nnginx 1234 tcp LISTEN * 80\npostgres 5678 tcp LISTEN 127.0.0.1 5432\noptional config file at ~/.config/snitch/snitch.toml\n:\n[defaults]\nnumeric = false\ntheme = \"auto\"\n- linux or macos\n- linux: reads from\n/proc/net/*\n, root orCAP_NET_ADMIN\nfor full process info - macos: uses system APIs, may require sudo for full process info"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://jalammar.github.io/illustrated-transformer/", "title": "The Illustrated Transformer", "url": "https://jalammar.github.io/illustrated-transformer/", "published": "Mon, 22 Dec 2025 19:15:56 +0000", "text_source": "article", "article_fetch_error": null, "text": "The Illustrated Transformer\nDiscussions:\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\nWatch: MIT‚Äôs Deep Learning State of the Art lecture referencing this post\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\n| Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings). |\nIn the previous post, we looked at Attention ‚Äì a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer ‚Äì a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud‚Äôs recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let‚Äôs try to break the model apart and look at how it functions.\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard‚Äôs NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\n2025 Update: We‚Äôve built a free short course that brings the contents of this post up-to-date with animations:\nA High-Level Look\nLet‚Äôs begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\nPopping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them.\nThe encoding component is a stack of encoders (the paper stacks six of them on top of each other ‚Äì there‚Äôs nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number.\nThe encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:\nThe encoder‚Äôs inputs first flow through a self-attention layer ‚Äì a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. We‚Äôll look closer at self-attention later in the post.\nThe outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position.\nThe decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence (similar what attention does in seq2seq models).\nBringing The Tensors Into The Picture\nNow that we‚Äôve seen the major components of the model, let‚Äôs start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output.\nAs is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm.\nEach word is embedded into a vector of size 512. We'll represent those vectors with these simple boxes.\nThe embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 ‚Äì In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that‚Äôs directly below. The size of this list is hyperparameter we can set ‚Äì basically it would be the length of the longest sentence in our training dataset.\nAfter embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.\nHere we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.\nNext, we‚Äôll switch up the example to a shorter sentence and we‚Äôll look at what happens in each sub-layer of the encoder.\nNow We‚Äôre Encoding!\nAs we‚Äôve mentioned already, an encoder receives a list of vectors as input. It processes this list by passing these vectors into a ‚Äòself-attention‚Äô layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder.\nThe word at each position passes through a self-attention process. Then, they each pass through a feed-forward neural network -- the exact same network with each vector flowing through it separately.\nSelf-Attention at a High Level\nDon‚Äôt be fooled by me throwing around the word ‚Äúself-attention‚Äù like it‚Äôs a concept everyone should be familiar with. I had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it works.\nSay the following sentence is an input sentence we want to translate:\n‚ÄùThe animal didn't cross the street because it was too tired\n‚Äù\nWhat does ‚Äúit‚Äù in this sentence refer to? Is it referring to the street or to the animal? It‚Äôs a simple question to a human, but not as simple to an algorithm.\nWhen the model is processing the word ‚Äúit‚Äù, self-attention allows it to associate ‚Äúit‚Äù with ‚Äúanimal‚Äù.\nAs the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\nIf you‚Äôre familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it‚Äôs processing. Self-attention is the method the Transformer uses to bake the ‚Äúunderstanding‚Äù of other relevant words into the one we‚Äôre currently processing.\nAs we are encoding the word \"it\" in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on \"The Animal\", and baked a part of its representation into the encoding of \"it\".\nBe sure to check out the Tensor2Tensor notebook where you can load a Transformer model, and examine it using this interactive visualization.\nSelf-Attention in Detail\nLet‚Äôs first look at how to calculate self-attention using vectors, then proceed to look at how it‚Äôs actually implemented ‚Äì using matrices.\nThe first step in calculating self-attention is to create three vectors from each of the encoder‚Äôs input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.\nNotice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don‚Äôt HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.\nMultiplying x1 by the WQ weight matrix produces q1, the \"query\" vector associated with that word. We end up creating a \"query\", a \"key\", and a \"value\" projection of each word in the input sentence.\nWhat are the ‚Äúquery‚Äù, ‚Äúkey‚Äù, and ‚Äúvalue‚Äù vectors?\nThey‚Äôre abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you‚Äôll know pretty much all you need to know about the role each of these vectors plays.\nThe second step in calculating self-attention is to calculate a score. Say we‚Äôre calculating the self-attention for the first word in this example, ‚ÄúThinking‚Äù. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.\nThe score is calculated by taking the dot product of the query vector with the key vector of the respective word we‚Äôre scoring. So if we‚Äôre processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.\nThe third and fourth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper ‚Äì 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they‚Äôre all positive and add up to 1.\nThis softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it‚Äôs useful to attend to another word that is relevant to the current word.\nThe fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).\nThe sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).\nThat concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let‚Äôs look at that now that we‚Äôve seen the intuition of the calculation on the word level.\nMatrix Calculation of Self-Attention\nThe first step is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X, and multiplying it by the weight matrices we‚Äôve trained (WQ, WK, WV).\nEvery row in the X matrix corresponds to a word in the input sentence. We again see the difference in size of the embedding vector (512, or 4 boxes in the figure), and the q/k/v vectors (64, or 3 boxes in the figure)\nFinally, since we‚Äôre dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.\nThe self-attention calculation in matrix form\nThe Beast With Many Heads\nThe paper further refined the self-attention layer by adding a mechanism called ‚Äúmulti-headed‚Äù attention. This improves the performance of the attention layer in two ways:\n-\nIt expands the model‚Äôs ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. If we‚Äôre translating a sentence like ‚ÄúThe animal didn‚Äôt cross the street because it was too tired‚Äù, it would be useful to know which word ‚Äúit‚Äù refers to.\n-\nIt gives the attention layer multiple ‚Äúrepresentation subspaces‚Äù. As we‚Äôll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.\nWith multi-headed attention, we maintain separate Q/K/V weight matrices for each head resulting in different Q/K/V matrices. As we did before, we multiply X by the WQ/WK/WV matrices to produce Q/K/V matrices.\nIf we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices\nThis leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices ‚Äì it‚Äôs expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.\nHow do we do that? We concat the matrices then multiply them by an additional weights matrix WO.\nThat‚Äôs pretty much all there is to multi-headed self-attention. It‚Äôs quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place\nNow that we have touched upon attention heads, let‚Äôs revisit our example from before to see where the different attention heads are focusing as we encode the word ‚Äúit‚Äù in our example sentence:\nAs we encode the word \"it\", one attention head is focusing most on \"the animal\", while another is focusing on \"tired\" -- in a sense, the model's representation of the word \"it\" bakes in some of the representation of both \"animal\" and \"tired\".\nIf we add all the attention heads to the picture, however, things can be harder to interpret:\nRepresenting The Order of The Sequence Using Positional Encoding\nOne thing that‚Äôs missing from the model as we have described it so far is a way to account for the order of the words in the input sequence.\nTo address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they‚Äôre projected into Q/K/V vectors and during dot-product attention.\nTo give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern.\nIf we assumed the embedding has a dimensionality of 4, the actual positional encodings would look like this:\nA real example of positional encoding with a toy embedding size of 4\nWhat might this pattern look like?\nIn the following figure, each row corresponds to a positional encoding of a vector. So the first row would be the vector we‚Äôd add to the embedding of the first word in an input sequence. Each row contains 512 values ‚Äì each with a value between 1 and -1. We‚Äôve color-coded them so the pattern is visible.\nA real example of positional encoding for 20 words (rows) with an embedding size of 512 (columns). You can see that it appears split in half down the center. That's because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They're then concatenated to form each of the positional encoding vectors.\nThe formula for positional encoding is described in the paper (section 3.5). You can see the code for generating positional encodings in get_timing_signal_1d()\n. This is not the only possible method for positional encoding. It, however, gives the advantage of being able to scale to unseen lengths of sequences (e.g. if our trained model is asked to translate a sentence longer than any of those in our training set).\nJuly 2020 Update: The positional encoding shown above is from the Tensor2Tensor implementation of the Transformer. The method shown in the paper is slightly different in that it doesn‚Äôt directly concatenate, but interweaves the two signals. The following figure shows what that looks like. Here‚Äôs the code to generate it:\nThe Residuals\nOne detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a layer-normalization step.\nIf we‚Äôre to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:\nThis goes for the sub-layers of the decoder as well. If we‚Äôre to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:\nThe Decoder Side\nNow that we‚Äôve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let‚Äôs take a look at how they work together.\nThe encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its ‚Äúencoder-decoder attention‚Äù layer which helps the decoder focus on appropriate places in the input sequence:\nAfter finishing the encoding phase, we begin the decoding phase. Each step in the decoding phase outputs an element from the output sequence (the English translation sentence in this case).\nThe following steps repeat the process until a special\nThe self attention layers in the decoder operate in a slightly different way than the one in the encoder:\nIn the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf\n) before the softmax step in the self-attention calculation.\nThe ‚ÄúEncoder-Decoder Attention‚Äù layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.\nThe Final Linear and Softmax Layer\nThe decoder stack outputs a vector of floats. How do we turn that into a word? That‚Äôs the job of the final Linear layer which is followed by a Softmax Layer.\nThe Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.\nLet‚Äôs assume that our model knows 10,000 unique English words (our model‚Äôs ‚Äúoutput vocabulary‚Äù) that it‚Äôs learned from its training dataset. This would make the logits vector 10,000 cells wide ‚Äì each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.\nThe softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\nThis figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word.\nRecap Of Training\nNow that we‚Äôve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.\nDuring training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.\nTo visualize this, let‚Äôs assume our output vocabulary only contains six words(‚Äúa‚Äù, ‚Äúam‚Äù, ‚Äúi‚Äù, ‚Äúthanks‚Äù, ‚Äústudent‚Äù, and ‚Äú<eos>‚Äù (short for ‚Äòend of sentence‚Äô)).\nThe output vocabulary of our model is created in the preprocessing phase before we even begin training.\nOnce we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word ‚Äúam‚Äù using the following vector:\nExample: one-hot encoding of our output vocabulary\nFollowing this recap, let‚Äôs discuss the model‚Äôs loss function ‚Äì the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model.\nThe Loss Function\nSay we are training our model. Say it‚Äôs our first step in the training phase, and we‚Äôre training it on a simple example ‚Äì translating ‚Äúmerci‚Äù into ‚Äúthanks‚Äù.\nWhat this means, is that we want the output to be a probability distribution indicating the word ‚Äúthanks‚Äù. But since this model is not yet trained, that‚Äôs unlikely to happen just yet.\nSince the model's parameters (weights) are all initialized randomly, the (untrained) model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output, then tweak all the model's weights using backpropagation to make the output closer to the desired output.\nHow do you compare two probability distributions? We simply subtract one from the other. For more details, look at cross-entropy and Kullback‚ÄìLeibler divergence.\nBut note that this is an oversimplified example. More realistically, we‚Äôll use a sentence longer than one word. For example ‚Äì input: ‚Äúje suis √©tudiant‚Äù and expected output: ‚Äúi am a student‚Äù. What this really means, is that we want our model to successively output probability distributions where:\n- Each probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 30,000 or 50,000)\n- The first probability distribution has the highest probability at the cell associated with the word ‚Äúi‚Äù\n- The second probability distribution has the highest probability at the cell associated with the word ‚Äúam‚Äù\n- And so on, until the fifth output distribution indicates ‚Äò\n<end of sentence>\n‚Äô symbol, which also has a cell associated with it from the 10,000 element vocabulary.\nThe targeted probability distributions we'll train our model against in the training example for one sample sentence.\nAfter training the model for enough time on a large enough dataset, we would hope the produced probability distributions would look like this:\nHopefully upon training, the model would output the right translation we expect. Of course it's no real indication if this phrase was part of the training dataset (see: cross validation). Notice that every position gets a little bit of probability even if it's unlikely to be the output of that time step -- that's a very useful property of softmax which helps the training process.\nNow, because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That‚Äôs one way to do it (called greedy decoding). Another way to do it would be to hold on to, say, the top two words (say, ‚ÄòI‚Äô and ‚Äòa‚Äô for example), then in the next step, run the model twice: once assuming the first output position was the word ‚ÄòI‚Äô, and another time assuming the first output position was the word ‚Äòa‚Äô, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3‚Ä¶etc. This method is called ‚Äúbeam search‚Äù, where in our example, beam_size was two (meaning that at all times, two partial hypotheses (unfinished translations) are kept in memory), and top_beams is also two (meaning we‚Äôll return two translations). These are both hyperparameters that you can experiment with.\nGo Forth And Transform\nI hope you‚Äôve found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, I‚Äôd suggest these next steps:\n- Read the Attention Is All You Need paper, the Transformer blog post (Transformer: A Novel Neural Network Architecture for Language Understanding), and the Tensor2Tensor announcement.\n- Watch ≈Åukasz Kaiser‚Äôs talk walking through the model and its details\n- Play with the Jupyter Notebook provided as part of the Tensor2Tensor repo\n- Explore the Tensor2Tensor repo.\nFollow-up works:\n- Depthwise Separable Convolutions for Neural Machine Translation\n- One Model To Learn Them All\n- Discrete Autoencoders for Sequence Models\n- Generating Wikipedia by Summarizing Long Sequences\n- Image Transformer\n- Training Tips for the Transformer Model\n- Self-Attention with Relative Position Representations\n- Fast Decoding in Sequence Models using Discrete Latent Variables\n- Adafactor: Adaptive Learning Rates with Sublinear Memory Cost\nAcknowledgements\nThanks to Illia Polosukhin, Jakob Uszkoreit, Llion Jones , Lukasz Kaiser, Niki Parmar, and Noam Shazeer for providing feedback on earlier versions of this post.\nPlease hit me up on Twitter for any corrections or feedback."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://brooker.co.za/blog/2024/05/09/nagle.html", "title": "It's Always TCP_NODELAY", "url": "https://brooker.co.za/blog/2024/05/09/nagle.html", "published": "Mon, 22 Dec 2025 21:09:59 +0000", "text_source": "article", "article_fetch_error": null, "text": "I'm currently an engineer at Amazon Web Services (AWS) in Seattle, where I work on databases, serverless, and serverless databases. Before that, I worked on EC2 and EBS.\nAll opinions are my own.\n@marcbrooker on Mastodon @MarcJBrooker on Twitter\nThe first thing I check when debugging latency issues in distributed systems is whether TCP_NODELAY is enabled. And it√¢s not just me. Every distributed system builder I know has lost hours to latency issues quickly fixed by enabling this simple socket option, suggesting that the default behavior is wrong, and perhaps that the whole concept is outmoded.\nFirst, let√¢s be clear about what we√¢re talking about. There√¢s no better source than John Nagle√¢s RFC896 from 19841. First, the problem statement:\nThere is a special problem associated with small packets. When TCP is used for the transmission of single-character messages originating at a keyboard, the typical result is that 41 byte packets (one byte of data, 40 bytes of header) are transmitted for each byte of useful data. This 4000% overhead is annoying but tolerable on lightly loaded networks.\nIn short, Nagle was interested in better amortizing the cost of TCP headers, to get better throughput out of the network. Up to 40x better throughput! These tiny packets had two main causes: human-interactive applications like shells, where folks were typing a byte at a time, and poorly implemented programs that dribbled messages out to the kernel through many write\ncalls. Nagle√¢s proposal for fixing this was simple and smart:\nA simple and elegant solution has been discovered.\nThe solution is to inhibit the sending of new TCP segments when new outgoing data arrives from the user if any previously transmitted data on the connection remains unacknowledged.\nWhen many people talk about Nagle√¢s algorithm, they talk about timers, but RFC896 doesn√¢t use any kind of timer other than the round-trip time on the network.\nNagle√¢s Algorithm and Delayed Acks\nNagle√¢s nice, clean, proposal interacted poorly with another TCP feature: delayed ACK\n. The idea behind delayed ACK\nis to delay sending the acknowledgement of a packet at least until there√¢s some data to send back (e.g. a telnet\nsession echoing back the user√¢s typing), or until a timer expires. RFC813 from 1982 is that first that seems to propose delaying ACKs\n:\nThe receiver of data will refrain from sending an acknowledgement under certain circumstances, in which case it must set a timer which will cause the acknowledgement to be sent later. However, the receiver should do this only where it is a reasonable guess that some other event will intervene and prevent the necessity of the timer interrupt.\nwhich is then formalized further in RFC1122 from 1989. The interaction between these two features causes a problem: Nagle√¢s algorithm is blocking sending more data until an ACK\nis received, but delayed ack is delaying that ack\nuntil a response is ready. Great for keeping packets full, not so great for latency-sensitive pipelined applications.\nThis is a point Nagle has made himself several times. For example in this Hacker News comment:\nThat still irks me. The real problem is not tinygram prevention. It√¢s ACK delays, and that stupid fixed timer. They both went into TCP around the same time, but independently. I did tinygram prevention (the Nagle algorithm) and Berkeley did delayed ACKs, both in the early 1980s. The combination of the two is awful.\nAs systems builders this is should be a familiar situation: two reasonable features of the system that interact to create an undesirable behavior. This kind of interaction is one of the things that makes protocol design so hard.\nIs Nagle blameless?\nUnfortunately, it√¢s not just delayed ACK2. Even without delayed ack and that stupid fixed timer, the behavior of Nagle√¢s algorithm probably isn√¢t what we want in distributed systems. A single in-datacenter RTT is typically around 500√é¬ºs, then a couple of milliseconds between datacenters in the same region, and up to hundreds of milliseconds going around the globe. Given the vast amount of work a modern server can do in even a few hundred microseconds, delaying sending data for even one RTT isn√¢t clearly a win.\nTo make a clearer case, let√¢s turn back to the justification behind Nagle√¢s algorithm: amortizing the cost of headers and avoiding that 40x overhead on single-byte packets. But does anybody send single byte packets anymore? Most distributed databases and systems don√¢t. Partially that√¢s because they simply have more to say, partially its because of additional overhead of protocols like TLS, and partially its because of encoding and serialization overhead. But mostly, they have more to say.\nThe core concern of not sending tiny messages is still a very real one, but we√¢ve very effectively pushed that into the application layer. Sending a byte at a time wrapped in JSON isn√¢t going to be very efficient, no matter what Nagle√¢s algorithm does.\nIs Nagle needed?\nFirst, the uncontroversial take: if you√¢re building a latency-sensitive distributed system running on modern datacenter-class hardware, enable TCP_NODELAY\n(disable Nagle√¢s algorithm) without worries. You don√¢t need to feel bad. It√¢s not a sin. It√¢s OK. Just go ahead.\nMore controversially, I suspect that Nagle√¢s algorithm just isn√¢t needed on modern systems, given the traffic and application mix, and the capabilities of the hardware we have today. In other words, TCP_NODELAY\nshould be the default. That√¢s going to make some √¢write\nevery byte√¢ code slower than it would otherwise be, but those applications should be fixed anyway if we care about efficiency.\nFootnotes\n- I won√¢t got into it here, but RFC896 is also one of the earliest statements I can find of metastable behavior in computer networks. In it, Nagle says: √¢This condition is stable. Once the saturation point has been reached, if the algorithm for selecting packets to be dropped is fair, the network will continue to operate in a degraded condition.√¢\n- As this has gone around the internet, a number of folks have asked about\nTCP_QUICKACK\n. I don√¢t tend to reach for it for a few reasons, including lack of portability, and weird semantics (seriously, read the man page). The bigger problem is thatTCP_QUICKACK\ndoesn√¢t fix the fundamental problem of the kernel hanging on to data longer than my program wants it to. When I saywrite()\n, I meanwrite()\n."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://spectrum.ieee.org/ultrasound-cancer-treatment", "title": "Ultrasound Cancer Treatment: Sound Waves Fight Tumors", "url": "https://spectrum.ieee.org/ultrasound-cancer-treatment", "published": "Mon, 22 Dec 2025 19:37:34 +0000", "text_source": "article", "article_fetch_error": null, "text": "Ultrasound Treatment Takes on Cancer‚Äôs Toughest Tumors\nHistoSonics turns its tumor-liquifying tech against pancreatic cancer\nFor many years, doctors and technicians who performed medical ultrasound procedures viewed bubbles with wary concern. The phenomenon of cavitation‚Äîthe formation and collapse of tiny gas bubbles due to changes in pressure‚Äîwas considered an undesirable and largely uncontrollable side effect. But in 2001, researchers at the University of Michigan began exploring ways to harness the phenomenon for the destruction of cancerous tumors and other problematic tissue.\nThe trouble was, creating and controlling cavitation generated heat, which harmed healthy tissue beyond the target area. Zhen Xu, who was working on a Ph.D. in biomedical engineering at the time, was bombarding pig heart tissue in a tank of water with ultrasound when she made a breakthrough.\nThe key was using extremely powerful ultrasound to produce negative pressure of more than 20 megapascals, delivered in short bursts measured in microseconds‚Äîbut separated by relatively long gaps, between a millisecond and a full second long. These parameters created bubbles that quickly formed and collapsed, tearing apart nearby cells and turning the tissue into a kind of slurry, while avoiding heat buildup. The result was a form of incisionless surgery, a way to wipe out tumors without scalpels, radiation, or heat.\n‚ÄúThe experiments worked,‚Äù says Xu, now a professor at Michigan, ‚Äúbut I also destroyed the ultrasound equipment that I used,‚Äù which was the most powerful available at the time. In 2009, she cofounded a company, HistoSonics, to commercialize more powerful ultrasound machines, test treatment of a variety of diseases, and make the procedure, called histotripsy, widely available.\nSo far, the killer app is fighting cancer. In 2023, HistoSonics‚Äô Edison system received FDA approval for treatment of liver tumors. In 2026, clinicians will conclude a pivotal kidney cancer study and apply for regulatory approval. They‚Äôll also launch a large-scale pivotal trial for pancreatic cancer, considered one of the deadliest forms of the disease with a five-year survival rate of just 13 percent. An effective treatment for pancreatic cancer would represent a major advance against one of the most lethal malignancies.\nHistotripsy‚Äôs Benefits for Cancer Treatment\nHistoSonics is not the only developer of histotripsy devices or techniques, but it is first to market with a purpose-built device. ‚ÄúWhat HistoSonics has developed is a symphony of technologies, which combines physics, biology, and biomedical engineering,‚Äù says Bradford Wood, an interventional radiologist at the National Institutes of Health, who is not affiliated with the company. Its engineering effort has spanned multiple disciplines to produce robotic, computer-guided systems that turn physical forces into therapeutic effects.\nOver the past decade, research has confirmed or found other benefits of histotripsy. With precise calibration, fibrous tissue‚Äîsuch as blood vessels‚Äîcan be spared from damage even in the target zone. And while other noninvasive techniques may leave scar tissue, the liquefied debris created by histotripsy is cleared away by the body‚Äôs natural processes.\nIn HistoSonics‚Äô early trials for pancreatic cancer, doctors used focused ultrasound pulses to ablate, or destroy, tumors deep within the pancreas. ‚ÄúIt‚Äôs a great achievement for the entire field to show that it is possible to ablate pancreatic tumors and that it‚Äôs well tolerated,‚Äù says Tatiana Khokhlova, a medical ultrasound researcher at the University of Washington, in Seattle, who has worked on alternative histotripsy techniques.\nKhokhlova says the key to harnessing histotripsy‚Äôs benefits ‚Äúwill be combining ablation of the primary tumor in the pancreas with some other therapy.‚Äù Combination treatment could fight recurrent cancer and tiny tumors that ultrasound might miss, while also tapping into a surprising benefit.\nHistotripsy generally seems to stimulate an immune response, helping the body attack cancer cells that weren‚Äôt targeted directly by ultrasound. The mechanical destruction of tumors likely leaves behind recognizable traces of cancer proteins that help the immune system learn to identify and destroy similar cells elsewhere in the body, explains Wood. Researchers are now exploring ways to pair histotripsy with immunotherapy to amplify that effect.\nThe company‚Äôs capacity to explore the treatment‚Äòs potential for different conditions will only improve with time, says HistoSonics CEO Mike Blue. The company has fresh resources to accelerate R&D: A new ownership group, which includes billionaire Jeff Bezos, acquired HistoSonics in August 2025 at a valuation of US $2.25 billion.\nEngineers are already testing a new guidance system that uses a form of X-rays rather than ultrasound imaging, which should expand use cases. The R&D team is also developing a feedback system that analyzes echoes from the therapeutic ultrasound to detect tissue destruction and integrates that information into the live display, says Blue.\nIf those advances pan out, histotripsy could move well beyond the liver, kidney, and pancreas in the fight against cancer. What started as a curiosity about bubbles might soon become a new pillar of noninvasive medicine‚Äîa future in which surgeons wield not scalpels, but sound waves."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://z.ai/blog/glm-4.7", "title": "GLM-4.7: Advancing the Coding Capability", "url": "https://z.ai/blog/glm-4.7", "published": "Mon, 22 Dec 2025 18:46:32 +0000", "text_source": "article", "article_fetch_error": null, "text": "2025-12-22 ¬∑ Research\nGLM-4.7, your new coding partner, is coming with the following features:\n- Core Coding: GLM-4.7 brings clear gains, compared to its predecessor GLM-4.6, in multilingual agentic coding and terminal-based tasks, including (73.8%, +5.8%) on SWE-bench, (66.7%, +12.9%) on SWE-bench Multilingual, and (41%, +16.5%) on Terminal Bench 2.0. GLM-4.7 also supports thinking before acting, with significant improvements on complex tasks in mainstream agent frameworks such as Claude Code, Kilo Code, Cline, and Roo Code.\n- Vibe Coding: GLM-4.7 takes a major step forward in UI quality. It produces cleaner, more modern webpages and generates better-looking slides with more accurate layout and sizing.\n- Tool Using: GLM-4.7 achieves significantly improvements in Tool using. Significant better performances can be seen on benchmarks such as œÑ^2-Bench and on web browsing via BrowseComp.\n- Complex Reasoning: GLM-4.7 delivers a substantial boost in mathematical and reasoning capabilities, achieving (42.8%, +12.4%) on the HLE (Humanity‚Äôs Last Exam) benchmark compared to GLM-4.6.\nYou can also see significant improvements in many other scenarios such as chat, creative writing, and role-play scenario.\nBenchmark Performance. More detailed comparisons of GLM-4.7 with other models GPT-5, GPT-5.1-High, Claude Sonnet 4.5, Gemini 3.0 Pro, DeepSeek-V3.2, Kimi K2 Thinking, on 17 benchmarks (including 8 reasoning, 5 coding, and 3 agents benchmarks) can be seen in the below table.\n| Benchmark | GLM-4.7 | GLM-4.6 | Kimi K2 Thinking | DeepSeek-V3.2 | Gemini 3.0 Pro | Claude Sonnet 4.5 | GPT-5 High | GPT-5.1 High |\n|---|---|---|---|---|---|---|---|---|\n| Reasoning | ||||||||\n| MMLU-Pro | 84.3 | 83.2 | 84.6 | 85.0 | 90.1 | 88.2 | 87.5 | 87.0 |\n| GPQA-Diamond | 85.7 | 81.0 | 84.5 | 82.4 | 91.9 | 83.4 | 85.7 | 88.1 |\n| HLE | 24.8 | 17.2 | 23.9 | 25.1 | 37.5 | 13.7 | 26.3 | 25.7 |\n| HLE (w/ Tools) | 42.8 | 30.4 | 44.9 | 40.8 | 45.8 | 32.0 | 35.2 | 42.7 |\n| AIME 2025 | 95.7 | 93.9 | 94.5 | 93.1 | 95.0 | 87.0 | 94.6 | 94.0 |\n| HMMT Feb. 2025 | 97.1 | 89.2 | 89.4 | 92.5 | 97.5 | 79.2 | 88.3 | 96.3 |\n| HMMT Nov. 2025 | 93.5 | 87.7 | 89.2 | 90.2 | 93.3 | 81.7 | 89.2 | - |\n| IMOAnswerBench | 82.0 | 73.5 | 78.6 | 78.3 | 83.3 | 65.8 | 76.0 | - |\n| LiveCodeBench-v6 | 84.9 | 82.8 | 83.1 | 83.3 | 90.7 | 64.0 | 87.0 | 87.0 |\n| Code Agent | ||||||||\n| SWE-bench Verified | 73.8 | 68.0 | 73.4 | 73.1 | 76.2 | 77.2 | 74.9 | 76.3 |\n| SWE-bench Multilingual | 66.7 | 53.8 | 61.1 | 70.2 | - | 68.0 | 55.3 | - |\n| Terminal Bench Hard | 33.3 | 23.6 | 30.6 | 35.4 | 39.0 | 33.3 | 30.5 | 43.0 |\n| Terminal Bench 2.0 | 41.0 | 24.5 | 35.7 | 46.4 | 54.2 | 42.8 | 35.2 | 47.6 |\n| General Agent | ||||||||\n| BrowseComp | 52.0 | 45.1 | - | 51.4 | - | 24.1 | 54.9 | 50.8 |\n| BrowseComp (w/ Context Manage) | 67.5 | 57.5 | 60.2 | 67.6 | 59.2 | - | - | - |\n| BrowseComp-ZH | 66.6 | 49.5 | 62.3 | 65.0 | - | 42.4 | 63.0 | - |\n| œÑ¬≤-Bench | 87.4 | 75.2 | 74.3 | 85.3 | 90.7 | 87.2 | 82.4 | 82.7 |\nCoding: AGI is a long journey, and benchmarks are only one way to evaluate performance. While the metrics provide necessary checkpoints, the most important thing is still how it *feels*. True intelligence isn't just about acing a test or processing data faster; ultimately, the success of AGI will be measured by how seamlessly it integrates into our lives-\"coding\" this time.\nFrontend Development Showcases\nPromptbuild a html website, High-contrast dark mode + bold condensed headings + animated ticker + chunky category chips + magnetic CTA. View full trajectory at Z.ai\nScroll down to see more\nArtifacts Showcases\nPromptDesign a richly crafted voxel-art environment featuring an ornate pagoda set within a vibrant garden.\nInclude diverse vegetation‚Äîespecially cherry blossom trees‚Äîand ensure the composition feels lively, colorful, and visually striking.\nUse any voxel or WebGL libraries you prefer, but deliver the entire project as a single, self-contained HTML file that I can paste and open directly in Chrome View full trajectory at Z.ai\nPoster Showcases\nPromptDesign a poster introducing Paris, with a romantic and fashionable aesthetic. The overall style should feel elegant, visually refined, and design-driven. View full trajectory at Z.ai\nSlides Creation Showcases\nPromptintroduce Zootopia, using bright color, 6 pages View full trajectory at Z.ai\nGLM-4.7 enhances Interleaved Thinking, a feature introduced since GLM-4.5, and further introduces Preserved Thinking and Turn-level Thinking. By thinking between actions and staying consistent across turns, it makes complex tasks more stable and more controllable:\n- Interleaved Thinking: GLM-4.7 thinks before every response and tool calling, improving instruction following and the quality of generation.\n- Preserved Thinking: In coding agent scenarios, GLM-4.7 automatically retains all thinking blocks across multi-turn conversations, reusing the existing reasoning instead of re-deriving from scratch. This reduces information loss and inconsistencies, and is well-suited for long-horizon, complex tasks.\n- Turn-level Thinking: GLM-4.7 supports per-turn control over reasoning within a session‚Äîdisable thinking for lightweight requests to reduce latency/cost, enable it for complex tasks to improve accuracy and stability.\nMore details: https://docs.z.ai/guides/capabilities/thinking-mode\nThe Z.ai API platform offers the GLM-4.7 model. For comprehensive API documentation and integration guidelines, please refer to https://docs.z.ai/guides/llm/glm-4.7. At the same time, the model is also available worldwide through OpenRouter (https://openrouter.ai/).\nGLM-4.7 is now available to use within coding agents (Claude Code, Kilo Code, Roo Code, Cline and more).\nFor GLM Coding Plan subscribers: You'll be automatically upgraded to GLM-4.7. If you've previously customized the app configs (like ~/.claude/settings.json\nin Claude Code), simply update the model name to \"glm-4.7\" to complete the upgrade.\nFor New users: Subscribing GLM Coding Plan means having access to a Claude-level coding model at a fraction of the cost ‚Äî just 1/7th the price with 3x the usage quota. Start building today: https://z.ai/subscribe.\nGLM-4.7 is accessible through Z.ai. Try to change the model option to GLM-4.7, if the system does not automatically do that (not like an AGI in that case :))\nModel weights for GLM-4.7 are publicly available on HuggingFace and ModelScope. For local deployment, GLM-4.7 supports inference frameworks including vLLM and SGLang. Comprehensive deployment instructions are available in the official GitHub repository.\n1: Default settings (most tasks): temperature 1.0, top-p 0.95, max new tokens 131072. For multi-turn agentic tasks (œÑ¬≤-Bench and Terminal Bench 2), enable Preserved Thinking mode.\n2: Terminal Bench and SWE-bench Verified settings: temperature 0.7, top-p 1.0, max new tokens 16384.\n3: œÑ¬≤-Bench settings: temperature 0, max new tokens 16384. For œÑ¬≤-Bench, we added an extra prompt in the Retail and Telecom interactions to avoid failures caused by users ending the interaction incorrectly; for the Airline domain, we applied the domain fixes proposed in the Claude Opus 4.5 release report."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://github.com/anthropics/claude-code/blob/main/CHANGELOG.md", "title": "Claude Code gets native LSP support", "url": "https://github.com/anthropics/claude-code/blob/main/CHANGELOG.md", "published": "Mon, 22 Dec 2025 15:59:01 +0000", "text_source": "article", "article_fetch_error": null, "text": "We read every piece of feedback, and take your input very seriously.\nTo see all available qualifiers, see our documentation.\nThere was an error while loading. Please reload this page."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://about.fb.com/news/2025/12/our-new-sam-audio-model-transforms-audio-editing/", "title": "Our New Sam Audio Model Transforms Audio Editing", "url": "https://about.fb.com/news/2025/12/our-new-sam-audio-model-transforms-audio-editing/", "published": "Tue, 16 Dec 2025 20:25:04 +0000", "text_source": "article", "article_fetch_error": null, "text": "Today, we‚Äôre introducing SAM Audio, a state-of-the-art AI model that enables you to segment sound. Imagine recording a video of your favorite band and isolating the guitar or vocals with a single click, using text prompts to filter traffic noise from a video filmed outside, or removing the sound of a dog barking from your entire podcast recording. SAM Audio, the latest addition to our Segment Anything collection, transforms audio processing by making it easy to isolate any sound from complex audio mixtures using text, visual, and time span prompts.\nThis intuitive approach mirrors how people naturally engage with sound, making professional-grade audio separation more accessible and easier than ever before. SAM Audio has the potential to transform audio and video editing and drive innovation in areas like music, podcasting, television, film, scientific research, accessibility, and more.\nUntil now, audio segmentation and editing has been a fragmented space, with a variety of tools designed for single-purpose use cases. As a unified model, SAM Audio is the first to support use cases that match how people naturally think about audio, and achieves cutting-edge performance across diverse, real-world scenarios. SAM Audio supports three kinds of prompts:\n- Text prompting: Type ‚Äúdog barking‚Äù or ‚Äúsinging voice‚Äù to extract specific sounds.\n- Visual prompting: Click on the person or object in the video that‚Äôs making a sound to isolate their audio.\n- Span prompting: An industry first, this method lets you mark time segments where target audio occurs.\nThese prompting methods can be used alone or in any combination, giving you precise and intuitive control over how audio is separated. We see so many potential use cases, including sound isolation, noise filtering, and more to help people bring their creative visions to life, and we‚Äôre already using SAM Audio to help build the next generation of creative media tools.\nYou can try SAM Audio in the Segment Anything Playground, our new platform that enables anyone to try our latest models. Starting today, people can select from our collection of audio and video assets or upload their own to explore the capabilities of SAM Audio. The model is also available for download.\nWe‚Äôre excited to bring audio to the Segment Anything collection of models and we believe SAM Audio is the all-around best audio separation model available. Learn more about SAM Audio and try it on the Segment Anything Playground today."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://www.jeffgeerling.com/blog/2025/nist-was-5-Œºs-utc-after-last-weeks-power-cut", "title": "NIST was 5 Œºs off UTC after last week's power cut", "url": "https://www.jeffgeerling.com/blog/2025/nist-was-5-Œºs-utc-after-last-weeks-power-cut", "published": "Mon, 22 Dec 2025 17:01:28 +0000", "text_source": "article", "article_fetch_error": null, "text": "If you were 5 microseconds late today, blame it on NIST.\nTheir facility in Boulder Colorado just had its power cut for multiple days. After a backup generator failed, their main ensemble clock lost track of UTC, or Universal Time Coordinated.\nBut even if you used the NTP timing servers they run, they were never off by more than 5 microseconds.\n5 Œºs might seem insignificant. But it is significant for scientists and universities who rely on NIST's more specialized timing signals.\nBut no, you don't need to panic. And yes, they have it under control now.\nBut I thought I'd go over what happened, what it means, and what we can learn from NIST's near-outage.\nVideo\nThis blog post is a lightly-edited transcript of my most recent YouTube video:\nWhat happened\nThe NIST campus, which distributes Internet time on six of the most popular NTP servers, lost power last Wednesday.\nThe power company was forced to cut power because of wind gusts over 100 mph (160 km/h). Power lines were coming down and they didn't want to risk starting a wildfire.\nThe whole campus was locked down for safety, so nobody could enter or exit.\nThey have backup generators. And those were working... but apparently one of the generators failed after a couple days. Specifically, the generator that powered the main ensemble clock that's used by the NTP servers.\nThings were dicey last Friday, and they couldn't get any more staff in to fix it.\nIt got to the point Jeff Sherman, the Group Leader for NIST's Time Realization and Distribution Group, considered shutting down the backup generator that powered the time servers. That would've prevented them from sending out inaccurate time, which would be worse than no time at all for a lot of applications.\nNTP's designed so you have multiple servers you look at, and if one fails, it won't cause you to lose time.\nAnd luckily for NIST, they have another building in their Boulder campus with more clocks, and that building could transfer time back to the one that had the power failure, if they needed to.\nBut yesterday Jeff posted another update: power was restored, and apparently there were still some staff on-site who saved the clocks.\nThey were able to re-route emergency power after the main backup generator went down.\nBattery backups, I'm assuming some big UPSes, were able to bridge the gap, until they got the backup backup power going.\nWhen all was said and done, their monitoring showed deviation from UTC was less than 5 Œºs.\nSeeing all that, Jeff and the team at NIST decided to keep their time servers online.\nBut why would they do that, if they were off? Well, time scales are important here. If you're on a Mac like I am, go in the Terminal and run sntp time-a-b.nist.gov\n.\nThis or a command like ntpdate\non Linux gives back an error bound, that shows latency between your computer and the NTP time servers.\n$ sntp time-a-b.nist.gov\n+0.005771 +/- 0.035081 time-a-b.nist.gov 132.163.96.1\nIn my case, it's showing 0.035 seconds. That's 35 milliseconds, or 35 thousand microseconds. 5 microseconds isn't even a blip there.\nSo instead of taking down the servers, which could cause more problems, NIST kept them online.\nBut Jeff said NIST's time is usually about 5,000x more accurate. And if you're one of the universities or aerospace companies that relies on NIST for timing, a 5 Œºs difference probably does matter.\nSo they'll be working with those groups directly. But for most people, they'll never even notice.\nJeff finished off the email mentioning the US GPS system failed over successfully to the WWV-Ft. Collins campus. So again, for almost everyone, there was zero issue, and the redundancy designed into the system worked like it's supposed to.\nTime is fragile\nI was following this closely over the weekend. I have two Raspberry Pi GPS clocks in the studio. One runs my main Stratum 0 NTP server, and the other one I have running as a backup for testing. (Yes I know I should have 4+ going for good quorum.)\nThey both run off my outdoor GPS antenna, which is distributed in my rack room and in my studio for time research.\nLike my studio, most places that need precise time rely on GPS. And that could be a problem!\nI'm glad redundancies kept GPS from drifting‚ÄîI don't know what would happen if GPS time goes away, but it wouldn't be good! But the main takeaway I think is this: timing infrastructure is fragile.\nCISA identified a lot of risk in the US's over-dependence on GPS.\nBecause of that, the US announced it's trying to find good alternatives for PNT (Position, Navigation, and Timing) earlier this year.\nI was actually at a meeting at the NAB where Jeff Sherman, the scientist who wrote the two NIST updates, was talking about BPS. The Broadcast Positioning System would give us redundancy even if GPS was down.\nBut even with multiple time sources, some places need more. I have two Rubidium atomic clocks in my studio, including the one inside a fancy GPS Disciplined Oscillator (GPSDO). That's good for holdover. Even if someone were jamming my signal, or my GPS antenna broke, I could keep my time accurate to nanoseconds for a while, and milliseconds for months. That'd be good enough for me.\n(If I'm being truthful, it's actually overkill, but I'm in the time-nut rabbit hole now‚Äîif you know, you know.)\nBut some places do need nanoseconds, for science experiments, RF, media, or finance. And they might run their own even more precise clocks. But they still trace things back to NIST, at least most do here in the US.\nSo when NIST's disaster response is tested, everyone's watching.\nLast week, when we were microseconds from disaster, the team at NIST fixed it so almost nobody noticed.\nComments\nJeff Sherman just posted a final update on the situation:\nBy 22:24 UTC (3:24 pm MST) on Sunday, Dec 21, NIST staff brought the offset of UTC(NIST) from UTC to the level of a few nanoseconds (billionths of a second), which is well within the normal range of precision for this system. This re-alignment was achieved using a backup time scale system at NIST Boulder. Assessment and other service repairs continue, but UTC(NIST) as furnished to the Boulder Internet Time Service is now providing accurate time.\nWe have determined that during the backup-power outage, the UTC(NIST) signal provided to the Boulder Internet Time Service did not deviate by more than 5 microseconds (five millionths of one second). As the typical uncertainty of time transfer over the public Internet is on the order of one millisecond (1/1000th of a second), we can say in retrospect that the accuracy of the Internet Time Service was not compromised and that users were not impacted by the time deviation."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://gchandbook.org/index.html", "title": "The Garbage Collection Handbook", "url": "https://gchandbook.org/index.html", "published": "Mon, 22 Dec 2025 19:30:18 +0000", "text_source": "article", "article_fetch_error": null, "text": "Second edition\nRichard Jones√¢s Garbage Collection (Wiley, 1996) was a milestone book in the area of automatic memory management. Its widely acclaimed successor, The Garbage Collection Handbook: The Art of Automatic Memory Management captured the state of the field in 2012. However, technology developments have made memory management more challenging, interesting and important than ever. This second edition updates the handbook, bringing together a wealth of knowledge gathered by automatic memory management researchers and developers over the past sixty years. The authors compare the most important approaches and state-of-the-art techniques in a single, accessible framework.\nThe book addresses new challenges to garbage collection made by recent advances in hardware and software, and the environments in which programs are executed. It explores the consequences of these changes for designers and implementers of high performance garbage collectors. Along with simple and traditional algorithms, the book covers state-of-the-art parallel, incremental, concurrent and real-time garbage collection. Algorithms and concepts are often described with pseudocode and illustrations.\nThe nearly universal adoption of garbage collection by modern programming languages makes a thorough understanding of this topic essential for any programmer. This authoritative handbook gives expert insight on how different collectors work as well as the various issues currently facing garbage collectors. Armed with this knowledge, programmers can confidently select and configure the many choices of garbage collectors.\nFeatures of the book\n- Provides a complete, up-to-date, and authoritative sequel to the 1996 and 2012 books\n- Offers thorough coverage of parallel, concurrent and real-time garbage collection algorithms\n- Discusses in detail modern, high-performance commercial collectors\n- Explains some of the tricky aspects of garbage collection, including the interface to the run-time system\n- Over 90 more pages, including new chapters on persistence and energy-aware garbage collection\n- Backed by a comprehensive online database of nearly 3,400 garbage collection-related publications\ne-book and translations\nThe e-book enhances the print versions with a rich collection of over 37,000 hyperlinks to chapters, sections, algorithms, figures, glossary entries, index items, original research papers and much more.\nChinese and Japanese translations of the first edition were published in 2016. We thank the translators for their work in bringing our book to a wider audience.\nWeb Resources\nThe online bibliographic database includes nearly 3,400 garbage collection-related publications. It contains abstracts for some entries and URLs or DOIs for most of the electronically available ones, and is continually being updated. The database can be searched online, or downloaded as BibTeX, PostScript or PDF."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://docs.fcc.gov/public/attachments/DOC-416839A1.pdf", "title": "FCC Updates Covered List to Include Foreign UAS and UAS Critical Components [pdf]", "url": "https://docs.fcc.gov/public/attachments/DOC-416839A1.pdf", "published": "Tue, 23 Dec 2025 03:57:02 +0000", "text_source": "article", "article_fetch_error": null, "text": "%PDF-1.7\n%ÔøΩÔøΩÔøΩÔøΩ\n5 0 obj\n<</Type/Page/Parent 3 0 R/Contents 6 0 R/MediaBox[0 0 612 792]/Resources<</Font<</FAAAAI 8 0 R/FAAABC 12 0 R/FAAABG 16 0 R>>/XObject<</X1 19 0 R>>>>/Group<</Type/Group/S/Transparency/CS/DeviceRGB>>>>\nendobj\n6 0 obj\n<</Length 20 0 R/Filter/FlateDecode>>stream\nxÔøΩÔøΩY]oÔøΩÔøΩ\u0012ÔøΩ+ÔøΩhKÔøΩ\u0004ÔøΩÔøΩ+oc{ÔøΩÔøΩhs\u0013ÔøΩÔøΩ6ÔøΩ\u0001CÔøΩ\n2–§\u0001;ÔøΩÔøΩÔøΩÔøΩtÔøΩÔøΩ=U\nxÔøΩw,{ÔøΩ\u00154ÔøΩTÔøΩ:uÔøΩ4„ëã\u001f<ÔøΩ\u0017%>e\u0015ÔøΩAÔøΩÔøΩyÔøΩÔøΩ\u0014FN`\u0007yÔøΩÔøΩÔøΩ<tÔøΩ$IÔøΩ(J0P>\n\b#ÔøΩ\tÔøΩyÔøΩx\u0011.ÔøΩÔøΩÔøΩ7tCÔøΩÔøΩÔøΩpÔøΩÔøΩÔøΩÔøΩ_^ÔøΩ\u0014ÔøΩ_ÔøΩ\u0016%ÔøΩÔøΩÔøΩxÔøΩ+ÔøΩÔøΩ\\ÔøΩ\u000fÔøΩKÔøΩ]ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0004ÔøΩÔøΩNÔøΩXÔøΩ!oÓ∏ë'ÔøΩW\u0015.ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ.?ÔøΩxÔøΩ#ÔøΩÔøΩÔøΩÔøΩusÔøΩÔøΩ;ÔøΩÔøΩNÔøΩtÔøΩÔøΩQÔøΩ8ÔøΩ1ÔøΩÔøΩÔøΩaÀá\nuÔøΩ.ÔøΩÔøΩsEÔøΩÔøΩ`\\\u0015ÎöÆÔøΩ|ÔøΩÔøΩÔøΩÔøΩKWÔøΩzÔøΩa\u00119ÔøΩ?ÔøΩk=ÔøΩ\u0013ÔøΩÔøΩhÔøΩ$ÔøΩ\u0018ÔøΩ\nÔøΩu>=vjÔøΩÔøΩﬂëÔøΩ%ÔøΩ]5ÔøΩÔøΩq\u000e@\u000eÔøΩÔøΩtMÔøΩtQÔøΩÔøΩÔøΩ>ÔøΩpjÔøΩ\bO“∂hÔøΩÔøΩ?^ÔøΩÔøΩÔøΩÔøΩEÔøΩÔøΩÔøΩI\u0012ÔøΩÔøΩ.t'ÔøΩÔøΩÔøΩOÔøΩÔøΩÔøΩ8ÚÅ±óÔøΩ)ÔøΩwÔøΩ<CÔøΩ›ô äÔøΩ@LÔøΩUŸ¥ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0001\u0002\nÔøΩÔøΩÔøΩÔøΩÔøΩ7ÔøΩÔøΩÔøΩ\u0019ÔøΩÔøΩI3ÔøΩt)\u0010\u0003ÔøΩnK\u0002E]ÔøΩx*7mhÔøΩÔøΩjÔøΩG&ÔøΩÔøΩÔøΩt&gÔøΩÔøΩYÔøΩÔøΩÔøΩœù0ÔøΩÔøΩÔøΩÔøΩ\u0007\u000eÔøΩ\nÔøΩXÔøΩÔøΩtUÔøΩiÔøΩ)ÔøΩ)ÔøΩÔøΩÔøΩ9T:\u001bgne÷æÔøΩQÔøΩY\"ÔøΩ◊öÔøΩÔøΩÔøΩÔøΩO[EzÔøΩOÔøΩ€®ÔøΩBÔøΩmÔøΩÔøΩÔøΩÃ≥—¶ÔøΩKÔøΩv:ÔøΩÔøΩ\u000eÔøΩ\"pÔøΩ8\t\\wÔøΩGÔøΩsÔøΩ\n\u0017NÔøΩÔøΩxÔøΩÔøΩOHÀìÔøΩÔøΩLw_ÿæ\b\u001fÔøΩtÔøΩÔøΩÔøΩ|ÔøΩÔøΩtÔøΩÔøΩ\u0019!oÔøΩBÔøΩÔøΩ êÔøΩÔøΩÔøΩwÔøΩÔøΩ6ÔøΩibÔøΩÔøΩÔøΩ\u0011⁄ßÔøΩÔøΩzÔøΩÔøΩÔøΩ0\u0002mÔøΩ\nA;ÔøΩ2}ÔøΩP€•\n\u0005P(jÔøΩ6\nÔøΩh;ÔøΩb\u0001\u0014ÔøΩÔøΩ)8ÔøΩÔøΩ\u0012ÔøΩ\"\n\u0007tCÔøΩ(~ÔøΩ◊≤0-ÔøΩed\nÔøΩ\u0015ÔøΩ|ÔøΩSeÔøΩTÀΩÔøΩi2\u0012ÔøΩÔøΩQ_Wi]#ÔøΩÔøΩ0\u0019\u000fÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩLUÔøΩÔøΩÔøΩÔøΩPtÔøΩ\u001fÔøΩH#NÔøΩÔøΩaÔøΩÔøΩD›ªZÔøΩhMÔøΩ8ÔøΩ\u001fÔøΩ<pÔøΩ0ÔøΩÔøΩ\u0018HÔøΩKÔøΩ1ÔøΩj\u0005ÔøΩ\u0019ÔøΩFÀõÔøΩ@mKyÔøΩ⁄®ÔøΩ\u0003wTÔøΩ·πôÔøΩM[\u0016\u0019ÔøΩÔøΩ_>ÔøΩDQÔøΩÔøΩ>ÔøΩPg\u000e—≤ÔøΩÔøΩ1cÔøΩVÔøΩÔøΩBÔøΩÔøΩÔøΩ«¥ÔøΩ\nÔøΩ\nY \u0003\u001bSfÔøΩJÔøΩÔøΩÔøΩÔøΩlÔøΩ€Æ(\u0015ÔøΩÔøΩ)ÔøΩ\u0019ÔøΩKÔøΩÔøΩN\u0019ÔøΩÔøΩÔøΩÔøΩÔøΩLÔøΩÔøΩÔøΩ\u0011\u0013ÔøΩÔøΩb\u0001lÔøΩ}ÔøΩÔøΩÔøΩ\u0006\u0005%\u0001\tÔøΩOÔøΩ\u001aÔøΩ\u000f<ÔøΩKmÔøΩÔøΩÔøΩ8CT@\u0018ÔøΩ6\u000e\tÔøΩ\u0010ÔøΩÔøΩÔøΩTiÔøΩÔøΩ5K’ëMÔøΩÔøΩÔøΩÔøΩcÔøΩÔøΩÔøΩNÔøΩÔøΩ\u0011ÔøΩnRDÔøΩoÔøΩÔøΩQÔøΩ@\u0018ÔøΩ]MÔøΩk?ÔøΩSÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ√ÇÔøΩÔøΩY\u0002-ÔøΩÔøΩÔøΩ\u0000ÔøΩÔøΩ\u0013x\n\u00009\u001fÔøΩÔøΩÔøΩD≈Ñt~qÔøΩÔøΩ\u001bm úNÔøΩfFÀäYÔøΩÔøΩÔøΩ\u0007hkÔøΩTÔøΩÔøΩÔøΩÕ©\u001bqÔøΩÔøΩÔøΩÔøΩ”í.ÔøΩÔøΩ\u0012ÔøΩ,}K\nnÔøΩÔøΩÔøΩp+ﬂ∂\u0017H\n:P\u0002 eKÔøΩÔøΩÔøΩJÔøΩ@\u0006eÔøΩÔøΩQWÔøΩÔøΩ\u0002\bÔøΩ[ÔøΩx5iÔøΩ\u0017ÔøΩz\n(9ÔøΩ\n#4Pl›ß&^ÔøΩÔøΩÔøΩjÔøΩn\u0005ÔøΩUÔøΩÔøΩSxÔøΩ\u000eÔøΩ`1tÔøΩÔøΩjÔøΩiÔøΩ\nÔøΩÔøΩRÔøΩÔøΩ#ÔøΩ\u0007ÔøΩÔøΩ\u000f\u0003'ÔøΩÔøΩ$\u000eÔøΩÔøΩ1\u0001~\u0014;ÔøΩ\nÔøΩ9\u000fÔøΩw\tÔøΩrÔøΩÔøΩA,ÔøΩÔøΩQ;jÔøΩ hÃ™oÔøΩSkÔøΩ\u0014/(ÔøΩ\u0019ÔøΩ<ÔøΩÔøΩ2TÔøΩÔøΩ(ÔøΩ$,QÔøΩ'ÔøΩ\u0006ÔøΩcËø§\"l\n\u0017]b\u000eÿÇ-ÔøΩnÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩU\u0016PÔøΩ\u0019ÔøΩÔøΩÔøΩÔøΩÔøΩ\u000e\u001aÔøΩBÔøΩ\u0011ÔøΩzÔøΩbQ\u0016ÔøΩBÔøΩ,ÔøΩÔøΩyÔøΩ;\nÀúÔøΩ\u0014ÔøΩ\nÔøΩ9,jXÔøΩÔøΩtÔøΩyÔøΩ\u0010)ÔøΩÔøΩÔøΩÔøΩ\u0004@ÔøΩÔøΩ\u0010h#ÔøΩh>FÔøΩUÔøΩÔøΩKx‘ö\u000fU\nÔøΩtÔøΩÔøΩ\nÔøΩAÔøΩsÔøΩs\nVÔøΩÔøΩmÔøΩ\u0001\"ÔøΩÔøΩpÔøΩ=ÔøΩK\u0012ÔøΩNÔøΩÔøΩÔøΩs\"keÔøΩw\u0011ÔøΩ7\u0014ÔøΩ2y\nÔøΩtÔøΩÔøΩÍ©ôHJ`5ÔøΩÔøΩÔøΩ\u001bbœ†\nËñµÔøΩ>ÔøΩÔøΩc`»âÔøΩÔøΩÔøΩ CÔøΩ,]+—•-ÔøΩÔøΩ|KÔøΩtÔøΩÔøΩ-,j\u001aÔøΩÔøΩ\bÔøΩ\u0018ÔøΩvdÔøΩÔøΩ6\u000fÔøΩR6ÔøΩ\bÔøΩ BÔøΩI;z@\u0015 ÔøΩPG\b,\u0002\n\u001aÔøΩÕ¶ÔøΩ\u0014}ÔøΩÔøΩÔøΩÔøΩ(ÔøΩÔøΩ<\u0019\\ÔøΩÔøΩDÔøΩÔøΩÔøΩÔøΩ-LÔøΩÔøΩÔøΩu\\ÔøΩÔøΩÔøΩ\\aÔøΩCkÔøΩ0%ÔøΩIÔøΩÔøΩ\u0011kÔøΩ/)c{,ÔøΩZ\">ÔøΩ«®RÔøΩ\u0011ÔøΩÔøΩ›ÖcG\u0007\u0000\u0001uÔøΩgÔøΩ\t9L&!&:`ÔøΩ{ÔøΩÔøΩ\u0014ÔøΩR4ÔøΩÔøΩÔøΩÔøΩÔøΩ,SÔøΩ\u0007ÔøΩ[ÔøΩKÔøΩÔøΩ;\u0011PÔøΩÔøΩzTÔøΩÔøΩÔøΩÔøΩkÔøΩ\\ÔøΩÔøΩÔøΩÔøΩEIÔøΩÔøΩÔøΩj\u001f}ÔøΩÔøΩÔøΩÔøΩ\nYÔøΩ\u0007\u0013ÔøΩÈè•ÔøΩÔøΩÔøΩÔøΩŒ•CÔøΩ[ÔøΩmÔøΩÔøΩÔøΩ\u0013ÔøΩ}ÔøΩÔøΩÔøΩGÔøΩ\\ÔøΩ=ÔøΩ\u0001ik≈∞ÔøΩÔøΩ6ÔøΩjrlg⁄éÔøΩR9ÔøΩÔøΩ\u0017TÔøΩÔøΩÔøΩwÔøΩÔøΩtÔøΩ„ååÔøΩ\nDuÔøΩ\u001aÔøΩfÔøΩ\nÔøΩeÔøΩjÔøΩ\u0011.ÔøΩF ÔøΩYrÔøΩÔøΩÔøΩ\t«´ÔøΩ\u0001\u001aÔøΩÔøΩÔøΩ\nÔøΩ1ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩL\tÔøΩÔøΩAkÔøΩÔøΩÔøΩhÔøΩ3ÔøΩ ¥m\u0005G>CÔøΩ#ÔøΩ2\u001bOÔøΩYÔøΩÔøΩÔøΩ\nÔøΩ(ÔøΩ\bÔøΩ÷ùÔøΩEÔøΩg/TÔøΩ^\u0017\u0014ÔøΩ@\u0003ÔøΩÔøΩ\u0006/ÔøΩ1ÔøΩÔøΩÔøΩbÔøΩÔøΩ\"ÔøΩ6ÔøΩÔøΩÔøΩyÔøΩ\u00110mÔøΩÔøΩ}ÔøΩM…°RjÔøΩÔøΩÔøΩ\u0006vCÔøΩ ÔøΩ'ÔøΩÔøΩrÔøΩÔøΩÔøΩ~`\nvoÔøΩÔøΩwÔøΩ ÔøΩmd_v\u0004ÔøΩl\u0014ÔøΩ\u0015dXÔøΩVFÔøΩ1c\u000fÔøΩÔøΩÔøΩ\u0013ÔøΩ\n\u0012ÔøΩE\u00108ÔøΩÔøΩ\nÔøΩÔøÆÔøΩÔøΩ\u0010ÔøΩ\u0017ÔøΩ\n'ÔøΩÔøΩZÔøΩiÔøΩÔøΩZÔøΩ\u0019\nUÀ®\u0000ÔøΩ\u0007lÔøΩÔøΩd‡πèN-eÔøΩl”ä\u0007~\n+ÔøΩÔøΩEB}ÔøΩ\u001f\u0000ÔøΩKcÔøΩ\nÔøΩ]ÔøΩÔøΩÔøΩÔøΩ\u000fÔøΩ'ÔøΩ5(ÔøΩ\u0004%;\u0012\u00162ÔøΩÔøΩ$dÔøΩÔøΩ[ÔøΩoÔøΩ5ÔøΩIÔøΩ\u001f6JÔøΩ\u0004\u0005\u001aÔøΩuÕΩÔøΩÔøΩÔøΩÔøΩ\n!yÔøΩÔøΩÔøΩÔøΩ|RkÔøΩF#2ÔøΩÔøΩÔøΩ∆†\nÔøΩÔøΩÃÆ\u0019ÔøΩ\u000fÔøΩÔøΩÔøΩ_\u0005ÔøΩÔøΩÔøΩM\nÔøΩ W!Á´∑ÔøΩÔøΩ[;ÔøΩ\u0019naRÔøΩl(ÔøΩLRÔøΩ\u0015uÔøΩÔøΩÔøΩx—ëÔøΩnÔøΩ[ÔøΩÔøΩÔøΩÔøΩ\u0013\u0018ÔøΩÔøΩMoÔøΩ\nNÔøΩEÔøΩf(1ÔøΩ=\u0007ÔøΩÔøΩ~\u0018ÔøΩÔøΩr\"ÔøΩKÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0019’•ÔøΩ»•ÔøΩÔøΩA+#\u001aÒÜñóÔøΩ5\n~ÔøΩ\nÔøΩ*ÔøΩŸÄÔøΩ6ÔøΩÔøΩPÔøΩÔøΩ;\u0005ÔøΩÔøΩEÔøΩvJÔøΩÔøΩU\tYÔøΩn~oÔøΩ\\ÔøΩ\u0006~n\u0010YÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ<~ÔøΩ8ÔøΩÔøΩÔøΩƒå<ÔøΩJÔøΩÔøΩÔøΩ\bY%ÔøΩÔøΩ_VÔøΩ\u0010EÔøΩZVÔøΩ\u0013t!ÔøΩÔøΩÔøΩÔøΩ&ÔøΩyÔøΩ`ÔøΩwÔøΩ\u0007ÔøΩÔøΩﬂôÔøΩTÔøΩÔøΩ-x\u0003\u0002omÔøΩ,\u0003PÔøΩyÔøΩÔøΩÔøΩyÔøΩ\u0000ÔøΩlÔøΩ\u0018ÔøΩ\u001a.ÔøΩÔøΩd\u0018%ÔøΩ@ÔøΩ\u0005a\u0002ÔøΩÔøΩÔøΩ(xÔøΩ4\nÔøΩÔøΩ ÔøΩ\nÔøΩ1ÔøΩ1\tÔøΩÔøΩ|ÔøΩ\nN7ÔøΩYUpÔøΩÔøΩÔøΩ1ÔøΩÔøΩmoUJ\\ÔøΩ~ÔøΩrÔøΩ\u000f0ÔøΩÔøΩÔøΩÔøΩÔøΩhÔøΩÔøΩ\u0013\u00017ÔøΩÔøΩÔøΩÔøΩ)t#\nÔøΩI\u0012\u0006ÔøΩÔøΩÔøΩ_–É*m7eÔøΩ\tÔøΩ>ÔøΩ\u0019ÔøΩÔøΩÔøΩÔøΩ}q/ÔøΩÔ™¥ÔøΩl7ÔøΩ\u0017ÔøΩVÔøΩ\n\u001b2ÔøΩ\u0000=ÔøΩ5cÔøΩ'\u0018ÔøΩÔøΩ\u0005ÔøΩÔøΩ\u0003oÔøΩÔøΩÔøΩh\u0001\u0007)ÔøΩ['ÔøΩ\u0014J\u001fÿµ\u0002 ÔøΩhÔøΩÔøΩ\u0019ÔøΩ-ÔøΩ\u0000\u0019ÔøΩNÔøΩoÔøΩÔøΩ)\n÷ÑÔøΩÔøΩÔøΩÔøΩ|Z1{ÔøΩÔøΩÔøΩDÔøΩÔøΩ=\u001bÔøΩÔøΩ|ÔøΩÔøΩÔøΩ\u00178ÔøΩ]ÔøΩ4T)\tÔøΩyÔøΩ¬û404\u000fEY“É6w0JX\n$ÔøΩA:ÔøΩÔøΩÔøΩ$ÔøΩÔøΩv+…®ÔøΩÔøΩ\u0019qÔøΩÔøΩ$ÔøΩ9ÔøΩÔøΩ\n2ÔøΩ\tÔøΩÔøΩÔøΩ\\ÔøΩ\u0000ÔøΩ?tÔøΩÔøΩÔøΩi\u000eÔøΩ\nh\u0014«ñÔøΩÔøΩ<ÔøΩ3\u0019nÔøΩÔøΩÔøΩŒØ\u0014\u001fvTÔøΩÔøΩ+<gÔøΩXÔøΩjÔøΩx<ÔøΩÔøΩÔøΩŒÉ/ÔøΩjÔøΩÔøΩÔøΩkÔøΩ0\b\u0002/ÔøΩ–ÖÔøΩ\u0013DÔøΩÔøΩq\nÔøΩÔøΩÔøΩ!ÔøΩ\u001fÔøΩÔøΩ—ô\nÔøΩÔøΩÔøΩÔøΩÔøΩGÔøΩÔøΩ\u0003ÔøΩÔøΩÔøΩQ\nendstream\nendobj\n20 0 obj\n2520\nendobj\n21 0 obj\n<</Type/Page/Parent 3 0 R/Contents 22 0 R/MediaBox[0 0 612 792]/Resources<</Font<</FAAAAI 8 0 R/FAAABC 12 0 R/FAAABG 16 0 R/FAAACE 24 0 R>>>>/Group<</Type/Group/S/Transparency/CS/DeviceRGB>>/Annots[29 0 R]>>\nendobj\n22 0 obj\n<</Length 30 0 R/Filter/FlateDecode>>stream\nxÔøΩÔøΩXÔøΩnÔøΩÔøΩ\u0011ÔøΩÔøΩBÔøΩ\"\u0001ÔøΩ\u0016ÔóÖÔøΩ4k\u0007\nÔøΩ\u0005ÔøΩ\u001ac\u001fÔøΩ<PdÔøΩ\naÔøΩ{ÔøΩÔøΩÔøΩÔøΩÔøΩ3ÔøΩ|oÔøΩÔøΩjrnÔøΩÔøΩ\u0006yÔøΩdÔøΩ\u00186ÔøΩÔøΩu9ÔøΩTuÔøΩÔøΩÔøΩ+\u001fÔøΩKÔøΩ\u0000ÔøΩ\u0016~\u0003ÔøΩÔøΩ|ÔøΩ=HR\u0011ÔøΩA\u001aj!\n\u0013ÔøΩÔøΩyÔøΩÔøΩiÔøΩ\u0003ÔøΩÔøΩ@ÔøΩe\"\nÔøΩ<OÔøΩ\u0004ÔøΩyÔøΩ\u0003KÔøΩ\u0015ÔøΩÔøΩ\u0006ÔøΩÔøΩ<ÔøΩÔøΩgÔøΩÔøΩv\u0006ÔøΩ\u001fnÔøΩÔøΩ\u000e|\u001ffÔøΩa\n\u001bÔøΩ{\"HinÔøΩÔøΩÔøΩ≈Å\u0005|ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩZ\u0015\nÔøΩ\u0016Â∑Ö—ΩÔøΩ~ÔøΩÔøΩ;ÔøΩÔøΩ\nÔøΩgÔøΩÔøΩ…≠ÔøΩ@$QN?SÔøΩtR\u0010ÔøΩ0ÔøΩÔøΩ$ÔøΩ \u0016AÔøΩÔøΩ0⁄∑mÔøΩÔøΩÔøΩmÔøΩÔøΩbZÔøΩ\nF€ÆÔøΩWÔøΩÔøΩÔøΩoÔøΩoJÔøΩ\u0017ÔøΩLÔøΩn/ÔøΩÔøΩ\u001fÔøΩÔøΩÔøΩwpÔøΩÔøΩ\u0014ÔøΩÔøΩÔøΩÔøΩÔøΩ9K?NÔøΩ⁄∑\u0019ÔøΩÔøΩrÔøΩÔøΩir1[JÔøΩ0ÔøΩÔøΩÔøΩÔøΩÔøΩ~ÔøΩFV∑∫≥PwP@sÔøΩ_–ÉÔøΩCÔøΩ€∂WuÔøΩ#\u0005EÔøΩ\u0003ÔøΩ[_ÔøΩZÔøΩ\u0003ÔøΩBÔøΩ*ÔøΩyÔøΩKŸÅ]\u00168ÔøΩÔøΩFÔøΩÔøΩÔøΩÔøΩ\u000f?#ÔøΩSÔøΩ|ÔøΩÔøΩÔøΩ»óÔøΩJJ^ÔøΩ\u0002ÔøΩaÔøΩ;ÔøΩÔøΩAÔøΩÔøΩÔøΩÔøΩ—ï-\n\u001a\tÔøΩÔøΩÔøΩ{ÔøΩChÔøΩ*\\JÔøΩ\tÔøΩÔøΩ+ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÌÜ¨ÔøΩ8Â≥∏\u0017ÔøΩ\nÔøΩÔøΩ9MÔøΩÔøΩOÔøΩAlÔøΩ1f=\b\u0012gÔøΩÔøΩ\u0017ÔøΩoxÔøΩœ¥\u0005ÔøΩÔøΩÔøΩÔøΩCÔøΩÔøΩÔøΩÔøΩÔøΩg6ÔøΩÔøΩ\u0017ÔøΩS\u0012gÔøΩÔøΩY6ZU‰∏ãÔøΩ=9%9W3\u001aÔøΩ9rÔøΩ$f}:$ÔøΩSÔøΩ\tÔøΩY⁄µ6ÔøΩ:ÔøΩÔøΩiÔøΩy;ÔøΩuhjÔøΩÔøΩÔøΩyÔøΩsÔøΩaÔøΩÔøΩIvÔøΩÔøΩ\n+ÔøΩ4\n@NÔøΩÔøΩÔøΩ]Xﬁ•ÔøΩ-ÔøΩ,ÔøΩ\u0018ÔøΩ\u0001ÔøΩÔøΩÔøΩÔøΩD\n“ÉsqÔøΩÔøΩKÔøΩ\u000fÔøΩÔøΩ\u0004ÔøΩ\nÔøΩ≈ØÔøΩÔøΩ3rÔøΩ\u000eVÔøΩÔøΩ^)maÔøΩzÔøΩÔøΩÔøΩ„ÜÇÔøΩvÔøΩ◊ä\u0003ÔøΩÔøΩ1ÔøΩ\nÔøΩÔøΩÔøΩPÔøΩÔøΩFÔøΩ<yjRÔøΩ\nÔøΩK0Bq0ÔøΩÔøΩJÔøΩÔøΩÔøΩj`\u001aÔøΩb>ÔøΩE%ÔøΩ\u0001AŸõS\u0011q\u0001\\HUn@ÔøΩÔøΩhÔøΩ\u0018ÔøΩ\u0017ÔøΩÔøΩÔøΩÔøΩ\\~F\u0002\u001bÔøΩ\u0016ZÔøΩ;9Q\nÔøΩSÔøΩÔøΩzÔøΩOÔøΩ\u001bÔøΩ\u0017ÔøΩ\u000fÔøΩ\u0003}ÔøΩÔøΩÔøΩÔøΩ2zY?ÔøΩÔøΩiÔøΩÔøΩÔøΩ\u0016\u0016ÔøΩÔøΩZ-HÔøΩÔøΩ\u0001Á°§9\\}\u001f\tÔøΩÔøΩÔøΩÔøΩ—é5Ê∂ë]\u0007ÔøΩ$ÔøΩÔøΩ\u0002ÔøΩÔøΩOÔøΩ<o'ÔøΩÔøΩÔøΩ0ÔøΩ\n\u0013JÔøΩ8≈î'ÔøΩ\\ÔøΩÔøΩÔøΩ\n\u0001ÔøΩÔøΩÔøΩ+m,ÔøΩa\u0002ÔøΩÔøΩÔøΩ|ÔøΩÔøΩŸÑ\u0014ÔøΩ+P iÔøΩÔøΩ\tÔøΩ6v\u0007>aÔøΩ\n\u0010ÔøΩr\bÔøΩ\u0019hD\u0002JZÔøΩÔøΩ¬®ÔøΩzÔøΩ\u0017#rÔøΩ\u0003\"RieÔøΩ#ÔøΩ\nÔøΩSÔøΩ(mX\u0016xK|%\n€∫ÔøΩ\u0019ÔøΩ‘éÔøΩ\u0017I\n\u0011 ÔøΩ‘¥ÔøΩ1~ÔøΩÔøΩÔøΩÔøΩi.ÔøΩ8ÔøΩPÔøΩÔøΩkVÔøΩÔøΩÔøΩ\u0001ÔøΩÔøΩ>/Q\u001fÔøΩÔøΩÔøΩ\u0014ÔøΩ5=.nXÔøΩ\nÔøΩe7\u0003mUÔøΩÔøΩF\u000eoÔøΩÔøΩÕÇ0ÔøΩ\u0018?,\b\u001aÔøΩÔøΩÔøΩj\u0007\u0016ÔøΩÔøΩ\u000eDÔøΩ#dÔøΩÔøΩÔøΩÔøΩrÔøΩ8ÔøΩÔøΩrÔøΩÔøΩ=w:ÔøΩÔøΩUÔøΩ_ÔøΩEÔøΩ0\u001fÔøΩÔøΩPKÔøΩjjÔøΩÔøΩÔøΩ Á∏Æ=ÔøΩ2\u0006^,bz\nÔøΩÔøΩÔøΩÔøΩs7zDbKÔøΩi\u0007ÔøΩÔøΩÔøΩÔøΩ\u000e|?%ÔøΩXc0\bs\u0017NÔøΩÔøΩÔøΩ@ÔøΩjÔøΩÔøΩvÔøΩEu\u0016ÔøΩÔøΩ48#7ÔøΩÔøΩ\u0015ÔøΩÔøΩÔøΩKÔøΩÔøΩ\nY/ÔøΩ\u0015ÔøΩ\nÔøΩÔøΩÔøΩÔøΩ=ÔøΩpÔøΩÔøΩÔøΩÔøΩ\u0007I…Ø8ÔøΩÔøΩÔøΩÔøΩiÔøΩEÔøΩÔøΩU’ùÔøΩWÔøΩ\u0011ÔøΩ\nÔøΩÔøΩ&k ÔøΩÔøΩ\u0001ÔøΩÔøΩ< ∫i\nEÔøΩÔøΩ\t\u0006@uXY\u001fÔøΩ\u001a3ÔøΩ\n,*ÔøΩf$ÔøΩÔøΩÔøΩÔøΩÔøΩkÔøΩRtÔøΩÔøΩÔøΩv{ÔøΩÔøΩ\u0019\u001aÔøΩ‘≠dXÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩKCÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ»êÔøΩt\u0001\u0016ÔøΩÔøΩRÔøΩPÔøΩÔøΩxƒó]_.\u0007ÔøΩÔøΩÔøΩ—ò√ûsÔøΩ=h5ÔøΩÔøΩ]ÔøΩÔøΩVÔøΩÔøΩ\nÔøΩVU=ÔøΩFÔøΩ<\u0014#[ÔøΩÔøΩ?ÔøΩÔøΩÔøΩÔøΩ\u000e\u0007~\nBÔøΩÔøΩ\"NÔøΩ1ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\n\u0011dÔøΩ\bÔøΩ ÔøΩÔøΩ<ÔøΩÔøΩN.ÔøΩRL*Õ∂\u001fÔøΩÔøΩÔøΩAÔøΩÔøΩÔøΩÔøΩ_8ÔøΩ\u0004y$ÔøΩ-ÔøΩ<»û; Ñ\nÔøΩÔøΩ\u0002\u000f{ÔøΩ7Zo#1ÔøΩÔøΩÔøΩ1\u0007kÔøΩÔøΩ\u0013ÔøΩ_\u0000ÔøΩÔøΩ\u000e\u0011ZÔøΩÔøΩ6\u0019ÔøΩ3ÔøΩ<N\nlÔøΩnÔøΩDJ/2ÔøΩÔøΩƒ¢6ÔøΩDÔøΩÔøΩÔøΩÔøΩ\u0007ÔøΩÔøΩÔøΩ4LBÔøΩŸöÔøΩÔøΩÔøΩÔøΩ4ÔøΩ0ÔøΩ#j\na&ÔøΩ1[ÔøΩo\u001aÔøΩÔøΩÔøΩ=ÔøΩ\u001a\u0007\u0005ÔøΩ\u001aÔøΩÔøΩ\u001aÃå$\u001bÔøΩ6xÔøΩ\tÔøΩÔøΩÔøΩÔøΩ\u0019M5ÔøΩAÔøΩJ5w\u0018N\u0006HYÔøΩ-ÔøΩÔøΩÔøΩ8CK\\\u0013ÔøΩm›ê)|f\u0002>j,ÔøΩÔøΩ\u0018{h=\u0018vÔøΩÔøΩ1LÔøΩÔøΩf\nÔøΩ5\u001aÔøΩÔøΩÔøΩHÔøΩmÔøΩ}ÔøΩÔøΩ9O4ÔøΩÔøΩ#\n\u001a\nE}ÔøΩ\u0016ÔøΩ\n\"ÔøΩ÷â\u0002ÔøΩÔøΩÔøΩÔøΩ8rS0ÔøΩ{ÔøΩuÔøΩ!ÔøΩ1ÔøΩdÔøΩ6#uÔøΩÔøΩÔøΩÔøΩÔøΩ\u0016∆ôÔøΩ…í8K^ÔøΩ\u0006ÔøΩ#ÔøΩ\u0017Â†•P—≥ÔøΩu\u0016\u0014ÔøΩb>ÔøΩ&ÔøΩ:\u0000\u001aÔøΩP\u0010\u0018`⁄ÅÔøΩÔøΩÔøΩÔøΩ\u0015ÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\nÔøΩALÔøΩQÔøΩÔøΩÔøΩF\tV]ÔøΩb\u000f\u0004ÔøΩyÔøΩCÔøΩÔøΩÔøΩ8ÔøΩÔøΩÔøΩÔøΩ«°\u0004:ÔøΩuÔøΩÔøΩÔøΩÔøΩÔøΩrT4XgÔøΩ\nÔøΩ¬äÔøΩÔøΩÔøΩ\"\u0001+ÔøΩnÔøΩfQe,ÔøΩÔøΩ\u0000\u0001vÔøΩ ÔøΩY$ÔøΩ!ÔøΩ|?zr_ÔøΩÔøΩÔøΩ»£k(?ÔøΩ\u0003ÔøΩ@ÔøΩ>oÔøΩ\nÔøΩ@ÔøΩ\u0011!\"ÔøΩOkÔøΩ7o^\u0010ÔøΩÔøΩÔøΩ66ÔøΩQxÔøΩÔøΩyÔøΩOcÔøΩ\nÔøΩ]/ÔøΩ!ÔøΩÔøΩ}ÔøΩT\u0012ÔøΩ6ÔøΩ(ÔøΩÔøΩÔøΩtÿ¢ÔøΩ?\u0000=RÔøΩ>1≈õÔøΩŒèÔøΩ</)ÔøΩbÔøΩ\u001fÔøΩ\u001aÔøΩ\\ÔøΩ\tÔøΩÔøΩÔøΩpxÔøΩ≈ûÔøΩÔøΩÔøΩQ$ÔøΩ0ÔøΩÔøΩ0IÔøΩ7ÔøΩO<ÔøΩ*h3ÔøΩÔøΩÔøΩœèÔøΩÔøΩÔøΩ]ÔøΩÔøΩeÔøΩÔøΩ\u001b$\bÔøΩ8LÔøΩ~M&\u0019ZbÔøΩ^ÔøΩ\u0004ÔøΩ…°ÔøΩ<⁄ìhÔøΩÔøΩÔøΩHÔøΩ\u000fÔøΩÔøΩ\u0011\u0011~\u0014ÔøΩ^\u0007ÔøΩ*ƒ¥KÔøΩ%{/HÔøΩÔøΩ \u0010q*¬åÔøΩ\nÔøΩÔøΩÔøΩ)ÔøΩÔøΩ\u0017&ÿÄ\nÔøΩSnÔøΩÔøΩ\n*ÔøΩ\bÔøΩcÔøΩ?HEÔøΩdt\u0013ÔøΩ\u0017ÔøΩ\u0019ÔøΩ+ÔøΩÔøΩ—≠ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0012ÔøΩÔøΩÔøΩ\u0017rÔøΩÔøΩÔøΩ‹ñbÔøΩÔøΩ\nÔøΩp\n\\ÔøΩ@ÔøΩÔøΩÔøΩÔøΩÔøΩ¬©ÔøΩÔøΩD+ÔøΩÔøΩ*ÔøΩbÔøΩÔøΩH‹ßÔøΩ`sÔøΩg\u0000ÔøΩÔøΩÔøΩÔøΩvÔøΩB\u001bÔøΩ\u0011lÔøΩÔøΩ<ÔøΩÔøΩ1ÔøΩC,|\tÔøΩ)ÔøΩ\b\u000e\"IÔøΩ7HÔøΩ\u0012ÔøΩK\t\u001fÔøΩwÔøΩ(ÔøΩ\u0003.1ÔøΩN ÔøΩcÔøΩ ÔøΩ\nÔøΩ\u001a#0ÔøΩÔøΩN\u0005LÔøΩ\u0003#ÔøΩÔøΩÔøΩÔøΩÔøΩ%ÔøΩÔøΩÔøΩt?ÔøΩGWÔøΩgÔøΩ8ÔøΩ\nVÔøΩÔøΩÔøΩ◊ãÔøΩÔøΩÔøΩhÔøΩÔøΩ\u001bÔøΩ_ÔøΩÔøΩ\u0011ÔøΩÔøΩ, ≥8ÔøΩÔøΩ\u0019{uaÔøΩ#ÔøΩ/SK,ÔøΩ\nendstream\nendobj\n30 0 obj\n2088\nendobj\n29 0 obj\n<</Type/Annot/Subtype/Link/Rect[297.66799927 267.11199951 356.92700195 279.76098633]/BS<</Type/Border/S/S/W 0>>/A<</Type/Action/S/URI/URI(http://www.fcc.gov)>>>>\nendobj\n1 0 obj\n<</Producer(ÔøΩÔøΩ\u0000A\u0000s\u0000p\u0000o\u0000s\u0000e\u0000.\u0000W\u0000o\u0000r\u0000d\u0000s\u0000 \u0000f\u0000o\u0000r\u0000 \u0000J\u0000a\u0000v\u0000a\u0000 \u00002\u00004\u0000.\u00001\u00001\u0000.\u00000)>>\nendobj\n2 0 obj\n<</Type/Catalog/Pages 3 0 R/Lang(en-US)/Metadata 4 0 R>>\nendobj\n3 0 obj\n<</Type/Pages/Count 2/Kids[5 0 R 21 0 R]>>\nendobj\n4 0 obj\n<</Type/Metadata/Subtype/XML/Length 31 0 R>>stream\n<?xpacket begin=\"Ôªø\" id=\"W5M0MpCehiHzreSzNTczkc9d\"?>\n<x:xmpmeta xmlns:x=\"adobe:ns:meta/\" x:xmptk=\"PDFNet\">\n<rdf:RDF xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n<rdf:Description rdf:about=\"\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\">\n<dc:format>application/pdf</dc:format>\n</rdf:Description>\n<rdf:Description rdf:about=\"\" xmlns:pdf=\"http://ns.adobe.com/pdf/1.3/\">\n<pdf:Producer>Aspose.Words for Java 24.11.0</pdf:Producer>\n</rdf:Description>\n</rdf:RDF>\n</x:xmpmeta>\n<?xpacket end=\"w\"?>\nendstream\nendobj\n31 0 obj\n501\nendobj\n24 0 obj\n<</Type/Font/Subtype/Type0/Encoding/Identity-H/BaseFont/FAAACE+SymbolMT/DescendantFonts[25 0 R]/ToUnicode 28 0 R>>\nendobj\n25 0 obj\n<</Type/Font/Subtype/CIDFontType2/BaseFont/FAAACE+SymbolMT/CIDToGIDMap/Identity/FontDescriptor 27 0 R/DW 1000/W 26 0 R/CIDSystemInfo<</Ordering(Identity)/Registry(Adobe)/Supplement 0>>>>\nendobj\n26 0 obj\n[0[600 722 250 460]]\nendobj\n27 0 obj\n<</Type/FontDescriptor/FontName/FAAACE+SymbolMT/StemV 80/Descent -220/Ascent 1005/CapHeight 0/Flags 32/ItalicAngle 0/FontBBox[0 -220 1113 1005]/FontFile2 23 0 R>>\nendobj\n28 0 obj\n<</Length 32 0 R/Filter/FlateDecode>>stream\nxÔøΩ]PÔøΩnÔøΩ\nÔøΩ#ÔøΩ\u000f>vÔøΩ*ÔøΩÔøΩ\u0018!MÔøΩ&ÔøΩÔøΩnZÔøΩ\u000f ÔøΩdH\nB\u000eÔøΩÔøΩ\u0001IÔøΩÔøΩ>\u0018=ÔøΩÔøΩÔøΩÔøΩ‹æÔøΩÔøΩDÔøΩ>ÔøΩS\nF\u0018ÔøΩÔøΩ\u0001gÔøΩ\u0004ÔøΩÔøΩÔøΩh,%ÔøΩÔøΩ6*ÔøΩ`yÔøΩ$=%UÔøΩwÔøΩ\nqjÔøΩÔøΩ(i\u001aÔøΩ>ÔøΩÔøΩ\nÔøΩ\nÔøΩ\u0017ÔøΩz|JÔøΩÔøΩÔøΩ1\u0018;ÔøΩÔøΩÔøΩÔøΩÔøΩBÔøΩxÔøΩÔøΩ\u0013ÔøΩ\b5%BÔøΩÔøΩ!wÔøΩHÔøΩ\u0013BUÔøΩÔøΩV'ÔøΩÔøΩÔøΩ1ÔøΩÔøΩQÔøΩVÔøΩÔøΩÔøΩ\u0002ÔøΩ\\)ÔøΩqÔøΩRaÔøΩvÔøΩdÔøΩN!ÔøΩÔøΩ\u0014ÔøΩ\u0012ÔøΩÔøΩÔøΩpÔøΩtÔøΩÔøΩ~dxÔøΩgÔøΩDÔøΩ'ÔøΩ#ÔøΩ\u0011ÔøΩwÔøΩ,\u001a^sÔøΩuÔøΩÔøΩÔøΩCÔøΩQÔøΩ[ÔøΩ%ÔøΩÔøΩ@ÔøΩ]1ÔøΩ-\u001bÔøΩÔøΩÔøΩzÔøΩnOw\u000ewÔøΩ\nendstream\nendobj\n32 0 obj\n244\nendobj\n8 0 obj\n<</Type/Font/Subtype/TrueType/BaseFont/FAAAAI+TimesNewRomanPS-BoldMT/Encoding/WinAnsiEncoding/FirstChar 32/LastChar 121/Widths 9 0 R/FontDescriptor 10 0 R>>\nendobj\n9 0 obj\n[250 0 0 0 0 0 0 0 333 333 0 0 0 333 250 278 500 500 500 0 500 500 0 0 500 0 333 0 0 0 0 500 930 722 667 722 0 667 611 778 778 389 0 0 667 944 722 0 0 0 722 556 667 722 0 1000 0 0 0 0 0 0 0 0 0 500 0 444 556 444 333 500 556 278 0 556 278 833 556 500 556 0 444 389 333 556 500 722 0 500]\nendobj\n10 0 obj\n<</Type/FontDescriptor/FontName/FAAAAI+TimesNewRomanPS-BoldMT/StemV 80/Descent -216/Ascent 891/CapHeight 0/Flags 262176/ItalicAngle 0/FontBBox[-558 -307 2034 1026]/FontFile2 7 0 R>>\nendobj\n12 0 obj\n<</Type/Font/Subtype/TrueType/BaseFont/FAAABC+TimesNewRomanPS-ItalicMT/Encoding/WinAnsiEncoding/FirstChar 32/LastChar 122/Widths 13 0 R/FontDescriptor 14 0 R>>\nendobj\n13 0 obj\n[250 0 0 0 0 0 0 0 333 333 0 0 250 0 250 0 0 500 500 500 500 500 0 500 500 500 0 0 0 0 0 0 0 611 0 667 722 0 611 0 722 333 0 0 0 833 0 722 611 0 611 500 556 722 0 833 0 0 0 0 0 0 0 0 0 500 0 444 500 444 278 500 500 278 0 0 278 722 500 500 500 0 389 389 278 500 444 0 444 444 389]\nendobj\n14 0 obj\n<</Type/FontDescriptor/FontName/FAAABC+TimesNewRomanPS-ItalicMT/StemV 80/Descent -216/Ascent 891/CapHeight 0/Flags 96/ItalicAngle -16.26020432/FontBBox[-498 -307 1120 1023]/FontFile2 11 0 R>>\nendobj\n16 0 obj\n<</Type/Font/Subtype/TrueType/BaseFont/FAAABG+TimesNewRomanPSMT/Encoding/WinAnsiEncoding/FirstChar 32/LastChar 151/Widths 17 0 R/FontDescriptor 18 0 R>>\nendobj\n17 0 obj\n[250 0 0 500 0 0 0 0 333 333 0 0 250 333 250 0 500 500 500 0 0 500 500 500 500 500 0 0 0 0 0 0 0 722 667 667 722 611 556 722 722 333 0 0 611 889 722 722 556 0 667 556 611 722 0 944 0 722 0 0 0 0 0 0 0 444 500 444 500 444 333 500 500 278 0 500 278 778 500 500 500 500 333 389 278 500 500 722 500 500 444 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 333 444 444 0 0 1000]\nendobj\n18 0 obj\n<</Type/FontDescriptor/FontName/FAAABG+TimesNewRomanPSMT/StemV 80/Descent -216/Ascent 891/CapHeight 0/Flags 32/ItalicAngle 0/FontBBox[-568 -307 2029 1006]/FontFile2 15 0 R>>\nendobj\n19 0 obj\n<</Type/XObject/Subtype/Image/Width 1302/Height 191/ColorSpace/DeviceRGB/BitsPerComponent 8/Length 33 0 R/Filter/FlateDecode>>stream\nxÔøΩÔøΩ}ÔøΩ{\neÔøΩÔøΩ\u001fÔøΩ#q--,,\nk4MÔøΩÔøΩÔøΩ\u0014ÔøΩlÔøΩÔøΩ.,ÔøΩÔøΩÔøΩÔøΩHÔøΩÔøΩÔøΩ-5ÔøΩ\u0017ÔøΩÔøΩRwÔøΩÔøΩRÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ;ÔøΩÔøΩÔøΩ9ÔøΩÔøΩÔøΩ^ÔøΩrAÔøΩÔøΩ3ÔøΩÔøΩ;ÔøΩÔøΩÔøΩ}ÔøΩIwuKÔøΩa7ÔøΩ›∞\u001bvÔøΩnÔøΩ\nÔøΩa7ÔøΩ›∞\u001bvÔøΩnÔøΩ\nÔøΩa7ÔøΩ›∞\u001bvÔøΩnÔøΩ\nÔøΩa7ÔøΩ›∞\u001bvÔøΩnÔøΩ\nÔøΩa7ÔøΩ›∞\u001bvÔøΩnÔøΩ\nÔøΩa7ÔøΩ›∞\u001bÔøΩ$s5ÔøΩÔøΩŒØpÔøΩ}\u0019ÔøΩWÔøΩÔøΩV\u0018ÔøΩÔøΩÔøΩOvÔøΩ_7K\u0001W\u0002wmŸÇ√ΩÔøΩ{ÔøΩ\nOÔøΩÔøΩÔøΩÔøΩÔøΩ#ÔøΩÔøΩÔøΩÔøΩÔøΩDﬂìÔøΩ\u0019o6ÔøΩÔøΩÔøΩ\u0006ÔøΩÔøΩ\u001f6ÔøΩÔøΩLs\b\u001a.#ÔøΩ\u000fD{gcx\nÔøΩ\u0003ÔøΩÔøΩÔøΩ;rÔøΩÔøΩÔøΩ\nÔøΩÔøΩ9ÔøΩ5ÔøΩ:–õÔøΩK'ÔøΩ\u0012%\u0019ÔøΩÔøΩÔøΩ ≠ÔøΩ9ÔøΩ[ÔøΩ-ÔøΩÔøΩÔøΩÔøΩ\u0018\u000eÔøΩÔøΩuÔøΩÔøΩcÔøΩ\u0014ÔøΩÔøΩÔøΩ'ÔøΩu-ÔøΩÔøΩÔøΩ+cÔøΩwÔøΩÔøΩÔøΩfÔøΩy\u0001ÔøΩ!nÔøΩÔøΩ&ÔøΩÔøΩÔøΩÔøΩ,ÔøΩÔøΩÔøΩ[ÔøΩÔøΩÔøΩzh}ÔøΩv\n(ÔøΩ'ÔøΩÔøΩÔøΩÔøΩzÔøΩÔøΩ(15ÔøΩ7ÔøΩ\u0011ÔøΩÔøΩI/ÔøΩÔøΩUÔøΩÔøΩÔøΩÔøΩ\u0018€∞ÔøΩ\u0000ÔøΩÔøΩÔøΩ\u0016TÔøΩ%eÔøΩ\u0004ÔøΩrJÔøΩÔøΩKÔøΩÔøΩÔøΩ\u0005ÔøΩÔøΩDtÔøΩ!)-+ÔøΩ3ÔøΩ\u0011ÔøΩ€¢2ÔøΩU\u0000ÔøΩÔøΩÔøΩ—Æ\u0005Q\\O\u0005\\OVIÔøΩÔøΩÔøΩÔøΩ$\u000e6Xfq\u0000ÔøΩÔøΩ\u00107ÔøΩÔøΩ\u0007ÔøΩ–§uu^ÔøΩjÔøΩ\u0000iE~ÔøΩ\u001fArK_ny\u0015<\bSÔøΩ\u0005n6ÔøΩ\u00017\n~\u0000^ÔøΩÔøΩÔøΩ ÔøΩ\bÔøΩ\u0012ﬁélÔøΩ\nÔøΩ|∆ëC\u0000\u001fÔøΩÔøΩbÔøΩPF\u000eÔøΩÔøΩa\u000e!!ÔøΩsÔøΩ\nÔøΩoÔøΩÔøΩciÔøΩQ#ÔøΩÔøΩÔøΩ-+ÔøΩ.ÔøΩ)ÔøΩxÔøΩyÔøΩÔøΩ\u0006=&ÔøΩÔøΩ\u0005ÔøΩ@ÔøΩE\u0005\\|Vq ÔøΩÔøΩ*ÔøΩÔøΩ\u001a^∆îÔøΩ>\u000f=\bÔøΩa\u0018ÔøΩ\nÔøΩÔøΩÔøΩ6Yy_ÔøΩ.BZÔøΩ\u000fÔøΩD\nÔøΩ\u000f\u001fÔøΩY\u0012ÔøΩl\u0015PÔøΩÔøΩÕªeÔøΩÔøΩÔøΩÔøΩÔøΩutÔøΩÔøΩÔøΩÔøΩÔøΩ-ÔøΩop/iÔøΩ\u0018ÔøΩÔøΩÔøΩsÔøΩwK)ÔøΩ5i]#ÔøΩTÔøΩ÷∞CÔøΩGÔøΩZ(\nB.\u0006\tbÔøΩÔøΩGÔøΩÔøΩ\u0019,5\nÔøΩ,8ÔøΩÔøΩI'#,lzQÔøΩÔøΩA\u0003ÔøΩ\u001fX\npÔøΩÔøΩhÔøΩ\n%F\u0019EnA\u0003?\u0010ÔøΩ\nÔøΩÔøΩ\nÔøΩÔøΩfÔøΩ_ÔøΩ8_\nÔøΩÔøΩ6ÔøΩUSÔøΩ\u0013.>ÔøΩÔøΩÔøΩ\u000fq\bÔøΩtlÔøΩÔøΩÔøΩÔøΩ:ÔøΩÔøΩÔøΩD\u0003ÔøΩ%ÔøΩÔøΩ%ÔøΩÔøΩ\u001aÔøΩ\n~H)~HHÔøΩÔøΩ\u0002\u0012ÔøΩ]ÔøΩÔøΩÔøΩRÔøΩÔøΩ@ÔøΩ\u0004\u0002oÔøΩÔøΩÔøΩÔøΩQÔøΩZÔøΩ{4;ÔøΩ\nÔøΩdÔøΩDÔøΩf|5?;ÔøΩ$\n|LnJÔøΩÔøΩÔøΩJÔøΩ5[ÔøΩs\u0013ÔøΩFÔøΩÔøΩÔøΩÔøΩjx\u0005pÔøΩ\u0017ÔøΩ<RÔøΩÔøΩÔøΩkÔøΩÔøΩÔøΩ ÔøΩ\u0018ÔøΩ\u0019aÔøΩJ1dÔøΩ,ÔøΩÔøΩ≈ÅÔøΩÔøΩ+ÔøΩÔøΩÔøΩ\u0006MÔøΩ\nf{∆äÔøΩÔøΩÔøΩ◊¥mMÔøΩvÔøΩpÔøΩÔøΩÓîº\u0018TÔøΩÔøΩ6;ÔøΩÔøΩ.≈∏ÔøΩÔøΩÔøΩMÔøΩD\u000f.\u0006ÔøΩÔøΩÔøΩXÔøΩÃï(pÔøΩÔøΩ_ÔøΩÔøΩ\u0014ÔøΩ;ÔøΩ◊∫\u001aÔøΩ\u0002ÔøΩ$ÔøΩE\nÔøΩ`√î\b|)\nÔøΩÔøΩ\u0013bÔøΩ\"XÔøΩÔøΩÔøΩÔøΩ+,MÔøΩÔøΩÔøΩÔøΩ4\u0018ÔøΩ]ÔøΩK|ÔøΩÔøΩÔøΩ1ÔøΩÔøΩkÔøΩ\\ÔøΩ3D\u0014\u0010$ÔøΩ\u0005ÔøΩ[\u0006\u0017ÔøΩÔøΩLÕß\u001bÔøΩ\u001b<ÔøΩÔøΩ\u0002ÔøΩfÔøΩÔøΩÔøΩCÔøΩ/ÔøΩÔøΩ:|f\"\u00191ÔøΩÔøΩYXÔøΩÔøΩ\u0018ÔøΩÔøΩnÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩ0ÔøΩ â>8ÔøΩ`ÔøΩp\u0000\u0000ÔøΩ\u0007ÔøΩ\u0003v\bÔøΩ\u0015ÔøΩp+V68\u0004k='|5\u0006{ÔøΩWÔøΩÔøΩ»çÔøΩVÔøΩlhÔøΩ\u0014ÔøΩzOa7rÔøΩÔøΩkaÔøΩ\u0006aÔøΩÔøΩ\u0019t\u0012eÔøΩÔøΩ-ÔøΩ\u0005,ÔøΩRÔøΩÔøΩj\\ÔøΩ\t.\u0017[\u0000ÔøΩ\nÔøΩZKÔøΩÔøΩ2ÔøΩÔøΩÔøΩÔøΩ\u000fÔøΩ\u0018ÔøΩ!ÔøΩÔøΩÃ°ÔøΩ\u0002ÔøΩ88ÔøΩ)ÔøΩÔøΩW8ÔøΩ6<ÔøΩÔøΩtÔøΩ\u001aÔøΩÔøΩKÔøΩhq8ÔøΩGÔøΩL3@i\nÔøΩÔøΩ\u0019x\u0001\u0004'ÔøΩeÔøΩ“†-mhÔøΩÔøΩ\u001bÔøΩdÔøΩÔøΩ*\u0005fX\u00190ÔøΩÔøΩPÔøΩ\b[ÔøΩÔøΩWUÔøΩAÔøΩ\u0000/58ÔøΩÔøΩ\u0012ÔøΩH9ﬁÄÔøΩLÔøΩ*\u0010lrÔøΩ6ÔøΩ\u000eDt&eÔøΩÔøΩq\nÔøΩÔøΩ\nÔøΩÔøΩÔøΩcÔøΩ\u0014ÔøΩU{ÔøΩ,ÔøΩ[ÔøΩÔøΩƒéx ÔøΩ\u0001a-1ÔøΩ◊∫\nÔøΩRV\u0014?fÔøΩÔøΩQÔøΩ2ŸàÔøΩ|lÔøΩÔøΩÔøΩ;)ÔøΩﬁØÔøΩ\u00140>\u0004ÔøΩv=ÔøΩÔøΩÔøΩ1\u0019ÔøΩÔøΩADÔøΩÔøΩÔøΩ\u00100ÔøΩÔøΩ\b\nB*:\u0004\u001fÔøΩ ∂ÔøΩ\u00160`ÔøΩÔøΩ0ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0018ÔøΩÔøΩÔøΩ>lÔøΩ`ÔøΩÔøΩÔøΩÔøΩuvÔøΩ\u0007\u0017yBÔøΩex\u0004ÔøΩ,ÔøΩ\u0001\u0005ÔøΩÔøΩg\u0004\n\u0012ÔøΩÔøΩÔøΩ<\u0006ÔøΩÔøΩÔøΩ\nÔøΩR\u0016HÏÆ±ÔøΩWÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0015\u0016JÔøΩÔøΩrÔøΩ\u0014\u0016SCÔøΩÔøΩÔøΩg5ÔøΩÔøΩ:2Z\u0013\nuÔøΩÔøΩÔøΩn⁄∂&ÔøΩÔøΩ[qPÔøΩ\u0010ÔøΩ1TÔøΩÔøΩ\"qÔøΩÀ¢zHÔøΩÔøΩÔøΩ.lÔøΩ”ëdÔøΩÔøΩzÔøΩqÔøΩ\u0018ÔøΩ\u0001mÔøΩ\u000fÔøΩÔøΩÔøΩtÔøΩÔøΩÔøΩÔøΩ\u0001ÔøΩÔøΩJT4ÔøΩ]\u00079ÔøΩew\u0001ÔøΩÔøΩ.\u000f\n\"\u0018|ÔøΩÔøΩ4‹®ÔøΩÔøΩÔøΩÔøΩ>ÔøΩÔøΩÔøΩ\u0013rÔøΩÔøΩ\u0010 B\bÔøΩÔøΩv\bÔøΩ8ﬂÖE`\u001fÔøΩ\u0017nÔøΩa ÔøΩ\u000e@h\u0001O9CÔøΩxÔøΩs\u0011#ÔøΩÔøΩÔøΩÔøΩOnAÔøΩcK,ÔøΩq>ÔøΩ)\nÔøΩiÔøΩ\u0011ÔøΩÔøΩ◊ÖÔøΩAÔøΩ]ZeW7ÔøΩ;ÔøΩ\u0014|\u001aÔøΩcÔøΩXÔøΩ-ÔøΩ6dPÔøΩÔøΩ\u00119CÔøΩfYÔøΩÔøΩ#GbÔøΩÔøΩ0w.\u0011ÔøΩÔøΩ5ÔøΩÔøΩÔøΩBÔøΩFUÔøΩ!◊∏ÔøΩ \u0000ÔøΩcogÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ4ÔøΩ2`SAÔøΩj{\u0003ÔøΩÔøΩjÔøΩÔøΩÔøΩKÔøΩ<RÔøΩdÔøΩ\u001aKÔøΩÔøΩ*\n6\u0000ÔøΩ4-\n\u0018ÔøΩkÔøΩDdÔøΩÔøΩGÔøΩ3ÔøΩÔøΩ\u0013ŒÆhÔøΩ|ÔøΩrpTaH\\ÔøΩO+ÔøΩszÔøΩÔøΩ\tÔøΩ8\u0019\nÔøΩSsÔøΩp\n][ÔøΩ<ÔøΩ&rÔøΩÔøΩ\nmÔøΩ@ttSÔøΩj5ÔøΩ\u000eÔøΩn\u0002q[ÔøΩV\u001auÔøΩ+ÔøΩÔøΩÔøΩO\n+ÔøΩÔøΩp<ÔøΩÔøΩÔøΩ\u0010ÔøΩD\u001bPÔøΩ(ÔøΩÔøΩzÔøΩ$\u0004\u0017DwÔøΩ%ÔøΩÊÅ±4\u0015ÔøΩ\u0019ÔøΩÔøΩ\u0010nÔøΩÔøΩMÔøΩ[ÔøΩY\u0005,ÔøΩ\\?ÔøΩ\u001ak)ÔøΩÔøΩCÔøΩ\b\u0001ÔøΩÔøΩÔøΩIÔøΩ[:to÷ÆÔøΩg2ÔøΩ\u0010ÔøΩÔøΩ\bÔøΩÔøΩÔøΩ`ÔøΩÔøΩf\nÔøΩÔøΩ0ÔøΩ'<ÔøΩhj%ÔøΩÔøΩ\u0013ÔøΩpÔøΩÔøΩ\u0017>K\u0013ÔøΩÔøΩÔøΩ!ÔøΩÔøΩ,vÔøΩ\n0\u0018^rÔøΩ\u0012ÔøΩeÔøΩÔøΩlRÔøΩ\u0019$lÔøΩÔøΩeÔøΩ+OÔøΩÔøΩÔøΩÔøΩ\u0010J\u0019ÔøΩ\u0006\u001b\u000f\u000e\u0014v\b`pÔøΩpÔøΩ wÔøΩÔøΩœ∫KÔøΩÔøΩÔøΩÔøΩÔøΩh√á”ÑUÔøΩ\u001bÔøΩÔøΩÔøΩYÔøΩÔøΩÔøΩÔøΩÔøΩf\u0016ÔøΩÔøΩJ#\u0007?—æ\"ÔøΩßåÖdÔøΩÔøΩLÔøΩ\u000eG\nÔøΩÔøΩ}UÔøΩ\u001aÔøΩÔøΩÔøΩ:€ô\nÔøΩ\u0016|j6p\u000eyÿãÔøΩÔøΩ\u0014ÔøΩv68\u0013s\u0013ÔøΩ\nRhQÔøΩ\n&NÔøΩ \u0019ÔøΩR\"\u0019ÔøΩMNi\"ZU\u0006ÔøΩÔøΩ\u0013ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0005\u0019ÔøΩ2x`ÔøΩÔøΩÔøΩ\n<ÔøΩ<»†ÔøΩ*ÔøΩDÔøΩ\u0006ÔøΩÔøΩÔøΩ>ÔøΩ$ÔøΩ\u001fÔøΩ`A6ÔøΩ?ÔøΩTYÔøΩ¬ªÔøΩ[=0\u0016ÔøΩRÔøΩNÔøΩ7lÔøΩÔøΩ4ÔøΩÔøΩPYÔøΩÔøΩDXICÔøΩP\u0001gb&\u000eÔøΩUÂî°[ÔøΩÔøΩ!ÔøΩeÔøΩI\u0004VzÔøΩÔøΩÔøΩHÀ†& DlxiUFt\n,ÔøΩh)ÔøΩaÔøΩu2ÔøΩÔøΩY'/3ÔøΩ\u0011hÔøΩpaeAÔøΩ!ÔøΩbÔøΩ paÔøΩÔøΩ\u000fÀøeU\tEÔøΩÔøΩc*eÔøΩ\u001aÔøΩ[\u001aÔøΩÔøΩIÔøΩÔøΩ?\t_ÔøΩÔøΩÔøΩÔøΩÔøΩ<ÔøΩ\u0000ÔøΩ\"rÔøΩsÔøΩ3ÔøΩD\nÔøΩÔøΩÔøΩG ÔøΩ*ÔøΩ;ÔøΩÔøΩMMÔøΩv5MZWC\u0014jÔøΩLÔøΩ\nÔøΩK ±<ÔøΩ\u0006ÔøΩÀï(ÔøΩÔøΩ$ÔøΩa_N\u0019UÔøΩ’ΩÔøΩÔøΩDÔøΩ\u001fÔøΩ|(\u0002ÔøΩ\u0007ÔøΩ)ixÔøΩÔøΩAHÔøΩÔøΩ2NÔøΩÔøΩ9ÔøΩkÔøΩ0@8ÔøΩÔøΩ\bVÔøΩÔøΩdÔøΩ\u0003ÔøΩÔøΩÔøΩ\n` êMÔøΩz\nÔøΩN4BÔøΩ.<G2rÔøΩÔøΩ\u0016ÔøΩ(1uI\u00077ÔøΩ_ÔøΩo%\u0017ÔøΩÔøΩ\n\u0005ÔøΩÔøΩÔøΩÔøΩKÔøΩ}ÔøΩÔøΩÔøΩM^ÔøΩÔøΩq»¶ÔøΩpÔøΩÔøΩÔøΩÔøΩÔøΩ'ÔøΩpNÔøΩ'ÔøΩCÔøΩ=\tn]ÔøΩx\u0017\u0004ÔøΩ]«áÔøΩÔøΩ\u0005|ÔøΩTÔøΩ/ÔøΩ\b<ÔøΩ\u0006\nÔøΩIiÔøΩÔøΩÔøΩÔøΩHcÔøΩCÔøΩÔøΩ/ÔøΩpÔøΩÔøΩkA\n]ÔøΩ\u0006ÔøΩÔøΩÔøΩ\u0018ÔøΩ\u0007gÔøΩyÔøΩ[ÔøΩ-*s,\u001fHkÔøΩÔøΩT%ÔøΩ\tÔøΩÔøΩx[ÔøΩÔøΩÔøΩÔøΩ2ÔøΩ_\u0015x9nÔøΩYÔøΩuÔøΩÔøΩÔøΩ\u0011D48Gr\u0018ÔøΩÔøΩxÔøΩÔøΩÔøΩD}eÔøΩÔøΩ0ÔøΩCÔøΩÔøΩ\n\u0002ÔøΩ_pÔøΩ\u0018ÔøΩu`ÔøΩ1ÔøΩÔøΩjÔøΩÔøΩ<\u0006xÔøΩ\n¬≥ÔøΩX\u0007ÔøΩÔøΩQ\u0019aÔøΩÔøΩZ\u000f'TorÔøΩÔøΩÔøΩiÔøΩwÔøΩ,d\u0004ÔøΩ\u0016ÔøΩ_\u001a\u000eP]O\u0019tÔøΩÔøΩP\u0003«®!2gÔøΩ\n\u0015\u0019tÔøΩ\u0006ÔøΩB„à∂ÔøΩÔøΩ\u0018ÔøΩÔøΩ\nmFÔøΩP.\"ÔøΩt\u0005ÔøΩ9ÔøΩÔøΩ)ÔøΩ\u0010u\u0015ÔøΩ|ÔøΩÔøΩ[ÔøΩÔøΩÔøΩÔøΩ\u0012\u000eÔøΩÔøΩÔøΩNÔøΩÔøΩFÔøΩÔøΩ>ÔøΩd\u0017ÔøΩÔøΩ!ÔøΩ…∞ÔøΩ:ÔøΩÔøΩ0ÔøΩÔøΩÔøΩM,=ÔøΩ\tm\u0001ÔøΩÔøΩ\nyiPÔøΩ4K4ÔøΩ÷ïÔøΩYÔøΩÔøΩ\t ú39ÔøΩ\tÔøΩAÔøΩ6=\u0011\u0002ÔøΩÔøΩ4ÔøΩÔøΩÔøΩ=ÔøΩÔøΩ\buÔøΩ<<ÔøΩÔøΩ]ÔøΩ\nJ~ÔøΩs\u0002ÔøΩeÔøΩÔøΩ\u0016ÃÜ\u001aÔøΩÔøΩPÔøΩÃûlwÔøΩ\u001a—∏{ÔøΩ\nÔøΩ\\\n\u0006ÔøΩsÔøΩSÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩCÔøΩ\nY]ÔøΩ€π|ÔøΩLw<\u0002ÔøΩ\u0003\u0018\u0004\u0013`ÔøΩÔøΩÔøΩOÔøΩ\u0007eÔøΩ\u0012dcÔøΩÔøΩOÔøΩ#:ÔøΩ5ÔøΩ\u0018ÔøΩÔøΩTÔøΩÔøΩÔøΩ3ÔøΩI\nÔøΩÔøΩyœÖÔøΩdÔøΩ-\u0011\u0006ÔøΩ!ÔøΩÔøΩqVÔøΩÔøΩ_‘üÔøΩ\u001anÔøΩZÔøΩÔøΩÔøΩIÔøΩ\"\bLnÔøΩ\u001fcÔøΩÔøΩÏòåXI\nÔøΩÔøΩÔøΩ\u0010ÔøΩÔøΩÔøΩ/ÔøΩCÔøΩÔøΩu8$ÔøΩ#\nx@ÔøΩÔøΩd\bOÔøΩY\u000eÔøΩ3ÔøΩ7ÔøΩ<ÔøΩkÔøΩÔøΩÔøΩÔøΩ\u0018\u0002U&ÔøΩ√ÖÔøΩ.ÔøΩÔøΩJxﬁõX\"ÔøΩ\u0018ÔøΩÔøΩÔøΩ8\u0000ÔøΩL\n\u0013ÔøΩÔøΩNÔøΩÔøΩy.QiÔøΩJÔøΩ4*OÔøΩ\\wÔøΩgÔøΩ+)ÔøΩÔøΩ\u0002ÔøΩÔøΩt\brÔøΩÔøΩÔøΩ!(ÔøΩ\u0014 ÔøΩÔøΩÔøΩÔøΩ$sÔøΩÔøΩ\b>iÔøΩaI«™\n:ÔøΩc\u0003¬≤ÔøΩÔøΩW<%ÔøΩaSÔøΩÔøΩ≈•KsNjXs$ÔøΩÔøΩÔøΩw\u0011ÔøΩÔøΩ_\u000fÔøΩ2ÔøΩ\bÔøΩÔøΩN{wÿ§F3\u0003ÔøΩ\u0004!\u0012ÔøΩ=ÔøΩC@B$]ÔøΩBÔøΩ\u0001'mÔøΩÔøΩÔøΩ\u0007ÔøΩÔøΩ[ÔøΩÔøΩÔøΩÃ±ÔøΩEÔøΩuÔøΩÔøΩLÔøΩ\u0006ÔøΩ\n\"\u001fc_ÔøΩ8V1ÔøΩ\ndBBÔøΩ|ÔøΩ\u0004~ÔøΩ\u0004\u001b\nÔøΩÔøΩbÔøΩ\nÔøΩ$ÔøΩupÔøΩ\u0001ÔøΩbÔøΩÔøΩAÔøΩÔøΩ\u0004m\u0016\u0018x\u0011ÔøΩ\u0005ÔøΩ?ÔøΩÔøΩR\n5ÔøΩgÔøΩ\u0006ÔøΩÔøΩÔøΩ\u0006ÔøΩ7\n\u0003_ÔøΩk⁄î5\nÔøΩ\u0005SmÔøΩ{3ÔøΩ9sÔøΩFÔøΩ\u0000zÔøΩ\u0002YÔøΩÔøΩÔøΩ?N,ÔøΩ\n0ÔøΩÔøΩ5ÔøΩÔøΩ]ZxÔøΩN;ÔøΩ\u0018◊ô)M2ÔøΩ`Xx% ÖHF<\u0013S\n\u00159T1ÔøΩ\u0011\u0002ÔøΩÔøΩ\tÔøΩ\u0018ÔøΩCtÔøΩ\u0010ÔøΩÔøΩ_ÔøΩ\nÔøΩWÔøΩVMÔøΩÔøΩ\u0004\u000e)ÔøΩW8pÔøΩ;ÔøΩj\bÔøΩ4\nyÔøΩ8ÔøΩ\n,ÔøΩÔøΩ\"\u0011\u0017VÔøΩ%ÔøΩT€ØSÔøΩÔøΩ2&:ÔøΩqn5\u0003cn\u001apÔøΩT-ÔøΩ!2ÔøΩ\u0014ÔøΩ2t\bÔøΩ\n\u000eAÔøΩ\u001bEÔøΩ\u0000ÔøΩ}Dx\u0014ÔøΩfÔøΩTerÔøΩ>\u001a:\u0010ÔøΩuÔøΩÔøΩ!!ÔøΩwYÔøΩÔøΩÔøΩsÔøΩÔøΩÔøΩ\u0003ÔøΩ#\u0011ÔøΩÔøΩÔøΩDÔøΩ\u0016JU\nÔøΩÔøΩ|ÔøΩt3È•ÄÔøΩÔøΩD\nyÔøΩ9ÔøΩV\u0001.ÔøΩŸπ\u0013==]ÔøΩ\u0005CÔøΩCFÔøΩXy\u0019ÔøΩÔøΩTÔøΩ!ÔøΩ\u0011\u0002ÔøΩ\u0013\u000eÔøΩÔøΩBÔøΩ(CÔøΩÔøΩÔøΩpÔøΩÔøΩCÔøΩÔøΩgÔøΩ)LxÔøΩÔøΩ\"\u0011ÔøΩ\u0018\u0018|ÔøΩ\u001b@ÔøΩ\n‘©ÔøΩ\nÔøΩÔøΩOÔøΩ›≤ÔøΩ\u00011ÔøΩÔøΩÔøΩ_\nÔøΩ8ÔøΩd.\u0002ÔøΩkÔøΩÔøΩÔøΩr-ÔøΩHÔøΩÔøΩDÔøΩÔøΩÔøΩÔøΩ)ÔøΩÔøΩV\"ÔøΩÔøΩÔøΩÔøΩ,“äH∆ÇJÔøΩÛêå≤eÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ=ÔøΩ\\,ÔøΩÔøΩÔøΩÔøΩ\u0019_ÔøΩ\\ÔøΩÔøΩÔøΩÔøΩ\\ÔøΩQWÔøΩÔøΩÔøΩ}ÔøΩW\nÔøΩ\n\u0013-ÔøΩ\u001aÔøΩ4ÔøΩjZÔøΩJ!%ÔøΩjÔøΩÔøΩR\u0004A\"ÔøΩ\u0018\u0018ÔøΩƒÉUÔøΩCÔøΩ\u0010-\u0018ÔøΩÔøΩbNÔøΩ5\u0019ÔøΩÔøΩ\u0010ÔøΩ?\bÔøΩÔøΩ n\"ÔøΩÔøΩFÔøΩÔøΩÔøΩ{\u0018*ÔøΩÔøΩ%ÔøΩRFF-\nÔøΩÔøΩ\u0018ÔøΩ\u0012ÔøΩÔøΩARÔøΩ\n\u0010RÔøΩgÔøΩÔøΩ@1ÔøΩJ\t4mÔøΩÔøΩÔøΩ^ÔøΩÔøΩ$ÔøΩhr&ÔøΩÔøΩ778ÔøΩÔøΩÔøΩ\"1tIÔøΩ 9*ÔøΩ$l4IÔøΩ\u0015\n\u0002vÔøΩ%$cNÔøΩHe\nÔøΩÔøΩbj\\\u001aÔøΩÔøΩÔøΩÔøΩÔøΩqÔøΩÔøΩOÔøΩÔøΩFÔøΩ]ÔøΩÊªòÔøΩÔøΩÔøΩZÔøΩ,qÔøΩÔøΩ-ÔøΩaq2\u0002ÔøΩÔøΩ]ÔøΩ\u0012^\u001fÔøΩ0fÔøΩÔøΩÔøΩÔøΩ\"mÔøΩ}ÔøΩÔøΩÔøΩeÔøΩIÔøΩÔøΩ jÔøΩÔøΩƒçÔøΩÔøΩÔøΩÔøΩ\u0010ÔøΩ+YÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ{\u0019ÔøΩlhÔøΩ aÔøΩ#ÔøΩœºÔøΩ3\u001bsÔøΩÔøΩ%\u0019ÔøΩÔøΩOKfÔøΩÔøΩ\u0012ÔøΩÔøΩ\u0019ÔøΩ8|ÔøΩ<ÔøΩ'+MsÔøΩEfÔøΩÔøΩhÔøΩÔøΩÔøΩ\u0012ÔøΩ[f\"rÔøΩÔøΩ\u0010ÔøΩÔøΩHÔøΩÔøΩ}\u0014s&kÔøΩ( ÔøΩT\u00151ÔøΩ%ÔøΩÔøΩÔøΩÔøΩY\nÔøΩ\u0005ÔøΩÔøΩ\u0003ÔøΩ5\u0019ÔøΩÔøΩp~\bÔøΩÔøΩ44ÔøΩ``ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nu\u001bnÔøΩX\u001fÔøΩxÔøΩÔøΩ9U]ÔøΩÔøΩ+\u000f{[ÔøΩÔøΩn\"ÔøΩÔøΩlÔøΩ[z\u0016ÔøΩy&+ÔøΩi6ÔøΩ8ÔøΩ\"ÔøΩtÔøΩ$ÔøΩ:ÔøΩÔøΩ!\b\u0005\u0013\nÔøΩ&ÔøΩo9OÔøΩ!ÔøΩAZNfÔøΩNlÔøΩ!lJ]v#HA\u0013$ÔøΩÔøΩ\"ÔøΩÔøΩOÔøΩÔøΩ{ÔøΩRÔøΩÔøΩ)ÔøΩÔøΩÔøΩ\\ÔøΩ9A\u001aQÔøΩt'N\u0014cÔøΩÔøΩÔøΩ'ÔøΩDÔøΩÔøΩÔøΩÔøΩGÔøΩÔøΩÔøΩBÔøΩÔøΩ*Sr\nÔøΩ&ÔøΩ„ûæK\n\u0012BÔøΩÔøΩ\\'IÔøΩ\nÔøΩ:0#ÔøΩÔøΩ9DÔøΩÔøΩ\u0006,FÔøΩ:»éÔøΩDgÔøΩpÔøΩ5ÔøΩ≈ºÔøΩ÷®ÔøΩÔøΩXDB\u0018ÔøΩÔøΩÔøΩÔøΩ^ÔøΩ\u0004ÔøΩÔøΩ\u0018TÔøΩÔøΩVG3ÔøΩ{\u0019≈îÔøΩÔøΩw.{%ÔøΩÔøΩ\u0006\u0011y@ÔøΩÔøΩ…•\u0006ÔøΩ\u0000€¶\u0010\u0007\u0018r\u001b\n\u0002ÔøΩ\nÔøΩH&D\bb[ÔøΩbÔøΩ9bXKmÔøΩ\bÔøΩ(ÔøΩ_ÔøΩGV.kÔøΩ\u0012ÔøΩ\u0013|ÔøΩ\u001aw'ÔøΩuÔøΩlÔøΩxÔøΩÔøΩfÔøΩ\n+QÔøΩlÔøΩÔøΩFÔøΩ\u0015a\tÔøΩÔøΩ\u0019\u0017ÔøΩÔøΩ\u0016%\\ÔøΩÔøΩYÔøΩDÔøΩÔøΩGÔøΩÔøΩÔøΩ)\nÔøΩ,TÔøΩÔøΩ\u0000mNÔøΩ\u0011ÔøΩÔøΩÔøΩÔøΩ\u0010ÔøΩÔøΩÔøΩ\nÔøΩZ^ÔøΩÔøΩÔøΩÔøΩ$ÔøΩatB«≥.ÔøΩÔøΩoeÔøΩsBz\nL,ÔøΩ\n~ÔøΩÔøΩƒñaÔøΩ\u0016ÔøΩÔøΩÔøΩÔøΩ3\u0011ÔøΩ.ÔøΩ$ÔøΩW5kW€¥mÔøΩÔøΩdDhsJÔøΩ&ÔøΩuÔøΩ\u0010\"\u001aÔøΩZÔøΩÔøΩÔøΩRÔøΩÔøΩÔøΩÔøΩ\u00129ÔøΩÔøΩ>\b+_'z\u0016ÔøΩ\u0002FjtÔøΩ\ne,ÔøΩ\nr/ÔøΩ\u0016ÔøΩÔøΩ4\nÔøΩ.ÔøΩ2\u0004ÔøΩÔøΩ(ÔøΩ1rfÔøΩkN-ÔøΩŒ≤:UkÔøΩÔøΩÔøΩÔøΩÔøΩ\u0018ÔøΩÔøΩTÔøΩ\u0019ÔøΩ<ÔøΩÔøΩÔøΩÔøΩ`ÔøΩE\u0004ÔøΩJÔøΩÔøΩlÔøΩ.\u0000cÔøΩÔøΩDÔøΩ#ÔøΩ\u0012\u0015ÔøΩÔøΩÔøΩÔøΩÔøΩ–∞#ÔøΩnÔøΩÔøΩDŒ≤ÔøΩ“çr(ÔøΩtZÔøΩp`/WÔøΩ9r ÔøΩ*;\u0015'YZÔøΩÔøΩW\u0003IÔøΩLÀûÔøΩ\u0018;LÍÑ™%\u0016ÔøΩ'B\u0013ÔøΩÔøΩÔøΩp78ÔøΩbÔøΩÃπÔøΩÔøΩYÔøΩÔøΩ6ÔøΩy=ÔøΩ\tÔøΩ\u001aCÔøΩ\nÔøΩ\u001aÔøΩ\u0001oX)\u0012ÔøΩ3ÔøΩÔøΩ:ÔøΩNÔøΩÔøΩÔøΩ\u001aB$7Õ∫ÔøΩ–∏;8ÔøΩ\n!\u001f)\u0000qÔøΩ\u0012ÔøΩÔøΩÔøΩ_ÔøΩs>fﬂù6SÔøΩDk_Ws&}\n$ÔøΩNÔøΩrÔøΩ\u001bÔøΩÔøΩÔøΩ\u0006ÔøΩ ÔøΩF\u000f\u000eÔøΩ\u001agÔøΩÔøΩj◊çÔøΩ=ÔøΩÔøΩ#ÔøΩ≈ósLÔøΩ\u001anÔøΩÔøΩÔøΩÔøΩ\u000fÔøΩiÔøΩÔøΩÔøΩÔøΩ'\u0006\u0017t\u0019pWÔøΩ~ÔøΩ|ÔøΩÏ°æwÔøΩÔøΩÔøΩ÷é=ÔøΩÔøΩÔøΩ6ÔøΩ\nKÔøΩ\u0003\u0001cÔøΩ*\"DE¶ïòDÔøΩ\bS~ÔøΩÔøΩ;ÔøΩ∆πÔøΩÔøΩ&ÔøΩgÔøΩ\nÔøΩCVXÔøΩÔøΩÔøΩ\bÔøΩrvÔøΩ+ÔøΩ\nÔøΩU\u0000vÔøΩÔøΩÔøΩk!ÔøΩÔøΩk]%kÔøΩÔøΩ~ÔøΩÔøΩ,ÔøΩ4ÔøΩÔøΩÔøΩh\u0011\u0007M)ÔøΩQÔøΩ[U0<MÔøΩE\u0012ÔøΩKDÔøΩÔøΩÔøΩÔøΩ\u001aÔøΩÔøΩ~0ÔøΩ,ÔøΩÔøΩ9ÔøΩÔøΩÔøΩÔøΩL3%\u0014?[ÔøΩÔøΩ\u0004ÔøΩ\nÔøΩi:`ÔøΩÔøΩÔøΩU&Q`ÔøΩ=RQÔøΩÔøΩuu4ÔøΩ*)\u001fÔøΩSÔøΩ$P0ÔøΩÔøΩ\t\u001fÔøΩ&ÔøΩ\u0000!\u0019ÔøΩCHÔøΩÔøΩOÔøΩÔøΩÔøΩ\n&ÔøΩe4ÔøΩ|bÔøΩÔøΩÔøΩn|ÔøΩÔøΩvrÔøΩ~HÔøΩ\u0006eÔøΩ-\u0019ÔøΩÔøΩ\n;'rÔøΩ\u0010ÔøΩÔøΩÔøΩÔøΩLÔøΩÔøΩj\nÔøΩ∆ºDÔøΩ!ÔøΩYNÔøΩÔøΩ\u0003i·ÜÅSÔøΩ@6ÔøΩ\nqÔøΩÔøΩ+ÔøΩ0ÔøΩ!ÔøΩ\u0012ÔøΩ/Er\bÔøΩk0ÔøΩ!$p1ÔøΩÔøΩGÔøΩÔøΩmÔøΩ(SÔøΩ0uÔøΩSCÔøΩÔøΩ\buY>ÔøΩ\n*ÔøΩ\u0018&ÔøΩÔøΩ\u0005\u0017MÔøΩÔøΩÔøΩYVÔøΩ6ÔøΩbÔøΩDkÔøΩ&VÔøΩÔøΩvÔøΩÔøΩ|ÔøΩ\u000fÔøΩ\n!ÔøΩ8AÔøΩqÔøΩ\u0015\u0012NAh2ÔøΩHkÔøΩ\u00075ÔøΩ#!ÔøΩÔøΩÔøΩ\"2jD1ÔøΩÔøΩÔøΩ{zvxaÔøΩ{ÔøΩY√ßÔøΩ[ÔøΩÔøΩÔøΩ{O\n=ÔøΩ€âÔøΩ\u0017~=}ÔøΩÔøΩŸãgÔøΩtÔøΩÔøΩÔøΩ`gÔøΩ]ÔøΩ\nÔøΩ\nÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩ›¥ÔøΩÿºeÔøΩÔøΩÔøΩ^ÔøΩÔøΩœßÔøΩ<1ÔøΩÔøΩvÔøΩÔøΩÔøΩdÔøΩ9ÔøΩwÔøΩm\u001acs+nÔøΩÔøΩ\nÔøΩÔøΩgL)ÔøΩb7ÔøΩ>ÔøΩÔøΩQÔøΩjÔøΩ!-\u0017ÔøΩÔøΩVÔøΩ0ÔøΩ6U\u0019KÔøΩƒøÔøΩ5ÔøΩ\u0010ÔøΩAÔøΩ\u001fHÔøΩÔøΩÔøΩ\nÔøΩXÔøΩ\bÔøΩÔøΩÔøΩjHÔøΩÔøΩ\u0002ÔøΩnÔøΩE\u001aÔøΩÔøΩHÔøΩ\u0013ÔøΩÔøΩ\nÔøΩÔøΩÔøΩ[=c0ÔøΩDjÔøΩTyrÔøΩi]ÔøΩpÔøΩoBuc\u0017u|ÔøΩ\u001aÔøΩÔøΩ\u001aÔøΩÔøΩ%%MÔøΩÔøΩ\u0001<ÔøΩ\nW'Z*ÔøΩGÔøΩÔøΩ8\u0018ÔøΩvjÔøΩ\u000fa_GÔøΩÔøΩvÔøΩÔøΩÔøΩ\u000eÔøΩÔøΩLÔøΩ&]0!ÔøΩ55ÔøΩÔøΩÔøΩÔøΩhÔøΩ\u0002\u0017ÔøΩi\u0011ÔøΩÔøΩ\u0011E\u0015ÔøΩTZ0ÔøΩÔøΩÔøΩ\u001704{].wBÔøΩÔøΩÔøΩ\u0001ÔøΩvÔøΩ\nÔøΩKBCÔøΩÔøΩ\nÔøΩÔøΩA'IÔøΩ‹ÑÔøΩÔøΩÔøΩ\u0010ÃÉÔøΩÔøΩÔøΩjp\bÃàE>*(\n\u00025mqÔøΩ<ÔøΩ\n[ÔøΩÔøΩÔøΩ\t\u0011ÔøΩ\u0018ÔøΩ\nPw\u0006\u0013ÔøΩ\u0018i\u0002ÔøΩ\n\u0006l,\nÔøΩÔøΩ43ÔøΩÔøΩJpZ<\n[ÔøΩ\nÔøΩÔøΩ\nÔøΩÔøΩÔøΩ3ÔøΩ$\u0016#ÔøΩ0—ö?ÔøΩKÔøΩz.CÔøΩ6ÔøΩÔøΩSvIÔøΩÔøΩc∆£1ÔøΩÔøΩ<ÔøΩÔøΩÔøΩÔøΩ&ÔøΩÔøΩÔøΩÔøΩ\u0004&-ÔøΩÔøΩÔøΩ}ÔøΩeÔøΩÔøΩÔøΩfÔøΩ\u0018ÔøΩ|÷¢ÔøΩ[v\n?qÔøΩÔøΩÔøΩkÔøΩ_ÔøΩ]\u0018ÔøΩÔøΩÔøΩÔøΩ54ÔøΩ?ÔøΩUÔøΩ’ÉÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩ\u0007~ÔøΩJ›°cÔøΩÔøΩn><iÔøΩ\u0016ﬂÄÔøΩ/1ÔøΩ›≥ÔøΩnÔøΩÔøΩÔøΩÔøΩ\nÔøΩD\u0007ÔøΩ*YX\\ÔøΩVbÔøΩÔøΩ|)ƒì<ÔøΩm\nÔøΩÔøΩ-ÔøΩ=BUSÔøΩÔøΩhÔøΩÔøΩ*ÔøΩÔøΩƒº\ng4ÔøΩÔøΩÔøΩ\u0005ÔøΩQ8\u0004ÔøΩcÔøΩHÔøΩÔøΩÔøΩ…çhÔøΩh«∑ÔøΩ-@ÔøΩCM=ÔøΩM\u001aÔøΩTj|XÔøΩÔøΩÔøΩtÔøΩÔøΩs\u0014\u00177ŸåxÔøΩ\\◊• ÔøΩ«ÆÔøΩX»Ç2ÔøΩÔøΩ\nÔøΩÔøΩÔøΩ$ÔøΩÔøΩÔøΩqÔøΩÿòh\"¬çÔøΩÔøΩÕ≥dÔøΩ\u001f\u001fÔøΩÔøΩr[ÔøΩÔøΩÔøΩT:\u0004ÔøΩÔøΩÔøΩ\u0011,7A\nÔøΩÔøΩ6\u0004lFÔøΩGLÔøΩLYyÔøΩÔøΩOÔøΩÔøΩ\u0007.ÔøΩ\tIt\nÔøΩ@ÔøΩMUÔøΩbRÔøΩ\nrÔøΩÔøΩÔøΩ,\u0013ÔøΩ7kÔøΩcH\u0019ÔøΩÔøΩ?\u0016\u0018WÔøΩ,ÔøΩÔøΩÔøΩÔøΩ%ÔøΩL2OÔøΩ\u0018ÔøΩ1NÔøΩÔøΩÔøΩÔøΩÔøΩ\n⁄´|A{%;\u0004ÔøΩ\u0007dÔøΩ+ÔøΩ\u0014\u00168&ÔøΩ(ÔøΩÔøΩ8ÔøΩ…àÔøΩOÔøΩÔøΩWÔøΩy\u0006\nÔøΩ4PÔøΩX√ÉÔøΩeÔøΩÔøΩTÔøΩÔøΩ&ÔøΩÔøΩ\nROÔøΩ\u0006ÔøΩ\u0016NÔøΩ,ÔøΩ\u0018∆û{ÔøΩnEÔøΩ>ÔøΩB\u0016ÔøΩÔøΩÔøΩÔøΩ\nbÔøΩÔøΩb&ÔøΩÔøΩ2HyJÔøΩ\u0014HÔøΩÔøΩ\u000fÔøΩWÔøΩÔøΩ\"ÔøΩÔøΩ\nÔøΩÔøΩÔøΩz}ÔøΩkÔøΩÔøΩÔøΩÔøΩ\u000f\n;{ÔøΩÔøΩ’´uÔøΩÔøΩBÔøΩcLÔøΩÔøΩÔøΩ#ÔøΩÔøΩÔøΩÔøΩiuÔøΩÔøΩÔøΩw≈üÔøΩ)6ÔøΩ!ÔøΩÔøΩhÔøΩWÔøΩÔøΩÔøΩÔøΩ]ÔøΩÔøΩÔøΩÔøΩ…ôÔøΩvÔøΩÔøΩÔøΩ‘ÑÔøΩh\u001a D0ÔøΩÔøΩ\t/ÔøΩbk2hÔøΩÔøΩ>WÔøΩÔøΩÔøΩÔøΩaÔøΩÔøΩÔøΩF3`)\u0010!Áñô>\u0018\u0016ÔøΩ8KUÔøΩu2ÔøΩÔøΩ\u0018ÔøΩ$\nBÔøΩ\n\u0002\u0003ÔøΩR\u0019ÔøΩÔøΩ8zGFhÏ¥äÔøΩÔøΩ,ÔøΩ;<ÔøΩÔøΩk<\u0014ÔøΩaÔøΩÔøΩ\u0017ÔøΩÔøΩÔøΩÔøΩÔøΩ+nÔøΩiÔøΩÔøΩ\\ÔøΩ\nÔøΩÔøΩ:—≤xÔøΩ\n\u000f#ÔøΩ\u0004ÔøΩr\"O\u000esÔøΩA\u0006ÔøΩYÔøΩIÔøΩ‘âÔøΩÔøΩadÔøΩoÔøΩqÔøΩ\u0011ÔøΩ\u0013ÔøΩ}ÔøΩzÔøΩ3ÔøΩI1 ÄGÔøΩ2$ÔøΩ\u0015r\bÔøΩ*ÔøΩÔøΩ\u0014 ]XÔøΩ(ÔøΩÔøΩ8ÔøΩÔøΩDcÔøΩm2ÔøΩÔøΩÔøΩ\bpÔøΩSÔøΩÔøΩgÏ∏àÔøΩ!ÔøΩ\u0004ÔøΩÔøΩ4\u001aÔøΩ`ÔøΩÔøΩÔøΩ-ÔøΩÔøΩÔøΩ5ÔøΩÔøΩ\nÔøΩGÔøΩÔøΩÔøΩ\u0010p[CÔøΩ2ÔøΩIÔøΩKÔøΩCÔøΩÔøΩ\u0019ÔøΩ\u0010 wÔøΩ|ÔøΩÔøΩ!ÔøΩ@ÔøΩ4ÔøΩ\"ÔøΩgÔøΩÔøΩTaaoRÔøΩ\\f%&:ÔøΩy\u001aÔøΩÔøΩÔøΩt8\u0006E,G45(ÔøΩb<#ÔøΩ\u0000ÔøΩ 5ÔøΩ$ÔøΩÔøΩ%ÔøΩfiÔøΩ ÔøΩ<_qbwÔøΩ|ÔøΩE~ÔøΩdÔøΩÔøΩÔøΩPÔøΩz\u000fÔøΩÔøΩ/8F;ÔøΩ\\<ÔøΩSÔøΩÔøΩ\u000fÔøΩgÔøΩoÔøΩÔøΩ\u0015ÔøΩ\u000f\\ÔøΩtÔøΩÔøΩcNoEÔøΩÔøΩÔøΩÔøΩyÔøΩÔøΩÔøΩ'NÔøΩ?pÔøΩÔøΩ\n«ñÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ√ßÔøΩÔøΩ;fe’êÔøΩÔøΩÔøΩ-ÔøΩÔøΩÔøΩÔøΩÔøΩVÔøΩzÔøΩ7ÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩ%ÔøΩG,ÔøΩ?~ÔøΩÔøΩÔøΩ\u001bg-ﬁπrÔøΩÔøΩm{N\n>~ÔøΩÔøΩÔøΩ\n\u0017.^·úöÔøΩÔøΩ\nkÔøΩÔøΩOÔøΩÔøΩ8}ÔøΩÔøΩÔøΩÔøΩsÀü\u001aÔøΩÔøΩÔøΩ%1\u001fÔøΩ\u00146\u0018ÔøΩ9\t\u001f»ò>\nÔøΩ\u001fÔøΩÔøΩÔøΩÔøΩpjÔøΩÔøΩ1&:K\u0013p5ŒÜ!ÔøΩz[r\u0013\u0017\u000fÔøΩÔøΩ\u0006_ÔøΩÔøΩWÔøΩq\u0017ÔøΩSÔøΩ“ã\u0018ÔøΩ\nyÔøΩaKQ1ÔøΩ&cÔøΩÔøΩ_\u001fÔøΩ\u0015»¶ÔøΩN@7ÔøΩÔøΩÔøΩÔÉêﬁ¥^ÔøΩ@ÔøΩ\u0018ZÔøΩÔøΩÔøΩ'ÔøΩ!ÔøΩÔøΩ—ÉaXÔøΩiYÔøΩ.#pÔøΩ}ÔøΩZd0.,ÔøΩÔøΩ3&:&ÔøΩ.ÔøΩ$ÔøΩr\u0019!ÔøΩ\"CrÔøΩÔøΩ\u0010\u000fÔøΩEÔøΩg\b\u000fÔøΩ%ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0013ÔøΩi~ÔøΩ2ÔøΩkÔøΩÔøΩ&ÔøΩUÔøΩs]RÔøΩÔøΩzÔøΩMÔøΩÔøΩ@ÔøΩÔøΩÔøΩH\u0016aÔøΩÔøΩ.Yf\u0012ÔøΩ\nÔøΩG<ƒª\u000e{ÔøΩzu-ÔøΩÔøΩ\u0018ÔøΩ\u0005ÔøΩ;\u0006KÔøΩÔøΩ\\ÔøΩ5ÔøΩÔøΩÔøΩs?\"|ÔøΩÔøΩ\\\u0003«´ÔøΩ\\ÔøΩÔøΩÔøΩT%ÔøΩYÔøΩ\u001a )kÈ≥ΩÔøΩ#\u001bÔøΩ\u0000\u0018 ÔøΩi\u0011ÔøΩ]ÔøΩÔøΩHIÔøΩ}ÔøΩ‘¶FÔøΩÔøΩe\u0006ÔøΩlk1ÔøΩqÔøΩÔøΩ”òÔøΩGÔøΩÔøΩ\u0002irÔøΩ4ÔøΩÔøΩRË≥´gÔøΩÔøΩp@ÔøΩÔøΩÔøΩ\u001a\"7ÔøΩ\u0002ÔøΩÔøΩ\u0015ÔøΩÔøΩ\u0002ÔøΩ\u0007\n\u0018ÔøΩÔøΩ\u0014ÔøΩ0p&ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩwÔøΩÔøΩ5dÔøΩÔøΩs[ÔøΩÔøΩWÔøΩNÔøΩÔøΩÔøΩÔøΩV\nÔøΩÔøΩÔøΩÔøΩ¬ß>ÔøΩÔøΩÔøΩÔøΩÔøΩtÔøΩÔøΩMyÔøΩB>ÔøΩMsÔøΩyÔøΩU)ÔøΩ\u0007ÔøΩ\n\u0016ÔøΩÔøΩ{{ÔøΩ~ÔøΩÀüOÔøΩÔøΩÔøΩÔøΩ=ÔøΩ\nÔøΩÔøΩ\nÔøΩlEÔøΩÔøΩÔøΩÔøΩ⁄≤ÔøΩÔøΩÔøΩ?ÔøΩo\u000fÔøΩ\u0001ÔøΩÔøΩW\u0000\nÔøΩ!ÔøΩÔøΩ%ÔøΩUÔøΩ\nÔøΩ\u0017ÔøΩ\n◊ÜÔøΩÔøΩÔøΩtÔøΩÔøΩUÔøΩÔøΩ\u0018BÔøΩÔøΩ@œÆÔøΩ,ÔøΩÔøΩÔøΩ4ÔøΩ6!\u0006ÔøΩÔøΩ\u001fÔøΩÔøΩ^\u001aÔøΩ#ÔøΩÔøΩC|nj\"\u0010ÔøΩÔøΩ\t\u0019^\"ÔøΩÔøΩ\u0000ÔøΩÔøΩ€≤VzÔøΩ+ÔøΩ8ÔøΩÔøΩÔøΩÔøΩÔøΩ}dÔøΩLÔøΩÔøΩÔøΩ\u0002ÔøΩÔøΩÔøΩSHÔøΩÔøΩÔøΩ9ÔøΩ|ÔøΩÔøΩ5ojÔøΩ\u0005ÔøΩn\u001aÔøΩÔøΩÔøΩ~ÔøΩP\u0006ÔøΩÔøΩÕû*DÔøΩÔøΩÔøΩ\u0018X`ÔøΩ5ÔøΩ<\u001aÔøΩ'Y\u0014ÔøΩFÔøΩ\u0015ÔøΩ-#HÔøΩYWÔøΩÔøΩ!HB9\n!ÔøΩbÔøΩÔøΩgÔøΩdRÔøΩ\u0014ÔøΩÔøΩt}.ÔøΩÔøΩ\n«πÔøΩ\nÔøΩÔøΩÔøΩD\u001fÔøΩaLÔøΩÔøΩ#ÔøΩ,NÔøΩYÔøΩÔøΩÔøΩÔøΩQÔøΩ‹≤*\u001bÔøΩrÔøΩÔøΩÔøΩÔøΩÔøΩkdÔøΩ#ÔøΩÔøΩ\b<\u000eA=ÔøΩÔøΩÀÅ\u000fÔøΩ+ÔøΩV?ÔøΩK\u0013ÔøΩÔøΩ!ÔøΩÔøΩ\u0012s\nÔøΩWÔøΩGÔøΩI\u001bÔøΩrnÔøΩ2*oÔøΩÔøΩË¢¶NHÔøΩXÔøΩVwÔøΩÔøΩUÔøΩ5%qÔøΩ\u0018ÔøΩÔøΩ\u0014ÔøΩaÔøΩ√èÔøΩÔøΩÔøΩ\u0016ÔøΩÔøΩÔøΩÔøΩW\u000e)6ÔøΩ;ÔøΩ.ÔøΩ\u0019\u001fÔøΩ4ÔøΩÔøΩÔøΩÔøΩ\n\u0019ÔøΩ\u0013ÔøΩﬂØ[ÔøΩ1ÔøΩaÔøΩÔøΩ*\u0017ÔøΩÔøΩÔøΩ{ÔøΩÔøΩÔøΩÔøΩ\u0015ÔøΩ\u000f\nÔøΩÔøΩ|\nNiÔøΩ\u0019ÔøΩz)ÔøΩ>|ÔøΩÔøΩÔøΩ[?\nÔøΩyÔøΩÔøΩÔøΩ-\nÔøΩﬂ¨]MÔøΩRÔøΩ\\ÔøΩeÔøΩ–àÔøΩ/jÔøΩÔøΩ\u0006oÔøΩÔøΩGÔøΩ\u0013ÔøΩ 1ÔøΩÔøΩhÔøΩÔøΩÔøΩ\u0017.^\t\u0007S_ÔøΩ\\ÔøΩÔøΩÔøΩ3ÔøΩ\u0017ÔøΩxÔøΩÔøΩi\u0011ÔøΩCÔøΩÃ†ÔøΩwc–ÑÔøΩÔøΩD\u0002ÔøΩ\u0006ÔøΩcÔøΩÔøΩÔøΩÔøΩÔøΩmTÔøΩÔøΩ>\u0007ÔøΩ\u00023ÔøΩ%ÔøΩÔøΩÔøΩ\u0001!ÔøΩÔøΩÔøΩ—¢ÔøΩG\u0017dyAYÔøΩ1!ÔøΩÔøΩ&ÔøΩÔøΩÔøΩ\n”†2p]uÔøΩÔøΩ#ZÔøΩ_ÔøΩ\u000fÔøΩÔøΩÔøΩ\nJh{{IÔøΩHgvÔøΩ\u0013MÔøΩ\u0013\b\u0016dÔøΩDYÔøΩ\u001bÔøΩÔøΩJÔøΩÔøΩ\nÔøΩ\u0001\u0017&1ÔøΩ0ÔøΩ\nÔøΩÔøΩŸ∏c\u0003^”îÔøΩ]D\u0005ÃºÔøΩ\u0011oÔøΩÔøΩ≈ï\u0003cTÔøΩ\u0012ÕªÔøΩOÔøΩ\n\\^ÔøΩÔøΩgÔøΩÔøΩnvK\"ÔøΩÔøΩ\u0010rÔøΩ)ÔøΩ\\4ÔøΩF\bÔøΩÔøΩGCVÔøΩ7\nÔøΩÔøΩ~ÔøΩ€¶ÔøΩÔøΩÔøΩA5ÔøΩN\u001a?ÔøΩ&r\nÔøΩ%\nÔøΩa({ÔøΩ'ÔøΩ\u0000,ÔøΩÔøΩmÔøΩÔøΩÔøΩ“•eÔøΩjÔøΩH5ÔøΩ\n:IJÔøΩÔøΩ\nÔøΩPÔøΩsÔøΩÔøΩÔøΩÔøΩÔøΩ\";\u0004d\b\u0014\nÔøΩU2C`*\u000fÔøΩpÔøΩÔøΩ»î\u0001ﬂã\u00021\\ÔøΩÔøΩcÔøΩÔøΩ»éÔøΩOÔøΩ(·ÄóÔøΩÔøΩÔøΩv*ÔøΩÔøΩYÔøΩ\u0006ÔøΩi|ÔøΩÔøΩÔøΩÔøΩ,7\u0019\bÔøΩÔøΩ1(ÔøΩÀ†%ÔøΩKÔøΩOÔøΩÔøΩJÔøΩÔøΩÔøΩuÔøΩV\u001aÏìª\nÔøΩ\u0017\u0018ÔøΩÔøΩÔøΩÔøΩKÔøΩ2ÔøΩÔøΩwl7_ÔøΩRwÔøΩÔøΩÀªÔøΩÔøΩÔøΩ8kÔøΩÔøΩÔøΩMÔøΩÔøΩQzÔøΩ0zr<ÔøΩcq;ÔøΩXÔøΩ65ÔøΩÔøΩg/ZÔøΩÔøΩÔøΩ—≥ÔøΩ]ÔøΩ\"ÔøΩ{Eg|ÔøΩÔøΩ”Ø~ÔøΩ=ÔøΩÔøΩ)ÔøΩÔøΩÔøΩÔøΩ.ÔøΩÔøΩ[ÔøΩ\u0013p0ÔøΩdÔøΩ-s\u0014ÔøΩC6qÔøΩRÔøΩsÔøΩD ÔøΩÔøΩ`b0Ã¶xÔøΩ\u0016M\nÔøΩi{ÔøΩJhsÔøΩ$“öÔøΩ\u0014iÔøΩÔøΩu\u0000ÔøΩ\u0013NÔøΩFÔøΩÔøΩ\u0018ÔøΩÔøΩzÔøΩNÔøΩhÔøΩ}/\u0002ÔøΩÔøΩ;ÔøΩGÔøΩÔøΩ\u001aÔøΩÔøΩÔøΩÔøΩÔøΩIÔøΩj\nÔøΩz\u0011÷øeÔøΩ\u0003ÔøΩÔøΩSÔøΩ5\\e=a;ÔøΩ—ºÔøΩqÃ¶ÔøΩ_ÔøΩ*ÔøΩTÔøΩÔøΩ~ÔøΩKÔøΩÔøΩ\u0007#\u0012kÔøΩ“õÔøΩÔøΩ/ÔøΩ\u0012U\u001aÔøΩ,+ÔøΩ':\u0010\u0016aÔøΩ\u0015ÔøΩ\n—öÔøΩXÔøΩBÔøΩ\u0015GÔøΩ*ÔøΩÔøΩÔøΩ\n]+b(Jtp]ÔøΩ\u0005#ÔøΩh{:eBÔøΩÔøΩ\nP$;+\u001biÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩMIG\u0016\u000eÔøΩYÔøΩIÔøΩJÔøΩÔøΩÔøΩd\nÔøΩrÔøΩ\u0002\u0005ÔøΩ\u0011ÔøΩ4ÔøΩ'ÔøΩ\u001f\u000eÔøΩhÔøΩ.ÔøΩÔøΩÔøΩœ§5ÔøΩ\u0007\u0006%ÔøΩ$ÔøΩ\u0010ÔøΩÔøΩÔøΩÔøΩ[ÔøΩa\\VÔøΩ)SÔøΩ}ÔøΩÔøΩ\n\u0005ÔøΩÔøΩ]\u000ffÔøΩAÔøΩjGÔøΩzÔøΩÔøΩ^ÔøΩ/ÔøΩ8\ny>ÔøΩÔøΩÔøΩÔøΩ\\<ÔøΩuSÔøΩjÔøΩvÔøΩÔøΩÔøΩkÃ∑I\nÔøΩ\u0004ÔøΩÔøΩ\nÔøΩEÔøΩÔøΩ\nÔøΩÔøΩÔøΩoÔøΩ\n7c”°cgÔøΩ]\nÔøΩÔøΩ^ÔøΩÔøΩoÔøΩÔøΩ^\u001bs{ÔøΩÔøΩÔøΩG\\\\ÔøΩf\nÔøΩÔøΩÔøΩÔøΩ{ÔøΩ{vxÔøΩÔøΩÔøΩÔøΩÔøΩQÔøΩÔøΩ9ÔøΩÔøΩsÔøΩBÔøΩT4ÔøΩÔøΩoÔøΩÔøΩÔøΩ^\u001aÔøΩÔøΩN=ÔøΩﬂçÔøΩLÔøΩ\tÔøΩÔøΩ\u000fÔøΩ{ÔøΩÔøΩÔøΩC'ÔøΩ\nI}\bÔøΩÔøΩÔøΩÔøΩu[w\nÔøΩ9byÔøΩ\u0013ÔøΩÔøΩÔøΩAÔøΩÔøΩ8ÔøΩ;ÔøΩÔøΩ\u0016\u0019ÔøΩÔøΩÔøΩ\u000fuÔøΩÔøΩÔøΩ}6ÔøΩÔøΩÔøΩÔøΩhÔøΩbÔøΩ<ÔøΩeÔøΩ3ƒçÔøΩÔøΩpÔøΩ\\FV\u0002ÔøΩdZ0F2ÔøΩe%ÔøΩÔøΩÔøΩÔøΩ(em\nÔøΩe?ÔøΩÔøΩsÔøΩÔøΩÔøΩÔøΩJuQÔøΩ)\u0017\u001fvKBÔøΩ\u001a6OÔøΩJÔøΩÔøΩ3fÔøΩ\u0018\u0013ÔøΩÔøΩb\u000e9\u0001\u0013ÔøΩÔøΩ5…àrÔøΩ2\nÔøΩœ∏ÔøΩTÔøΩ@ÔøΩ.)ÔøΩ\nOs ÔøΩ\"ÔøΩÔøΩÔøΩÔøΩ\u0002z$ÔøΩÔøΩÔøΩ\u000fI.\u0010ÔøΩ!ÔøΩÔøΩ\u0010ÔøΩÔøΩFÔøΩÔøΩ\"ÔøΩ6%ÔøΩ1ÔøΩU\u00004ÔøΩ\u000eJLÔøΩ-A\u0011ÔøΩÔøΩÔøΩÔøΩ:LÔøΩ\\6ÔøΩiRTÔøΩ]ÔøΩdÔøΩÔøΩMﬁähN\nÔøΩKÔøΩÔøΩ,QÔøΩÔøΩ:\u0014ÔøΩÔøΩÔøΩ.ÔøΩ\u0019w\u001fnÔøΩÔøΩÔøΩÔøΩ1<|F\\ÔøΩKiÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩFlHpSÔøΩÔøΩ6ÔøΩ5ÔøΩ\nq[ÔøΩÔøΩlB\\\u0006‘∞ÔøΩ·Åû\bÔøΩ=ÔøΩ%ÔøΩt[ÔøΩ\u0013mÔøΩ2ÔøΩÔøΩ\bÔøΩtÔøΩ÷®OJ‹ò\nÔøΩ4\t^ÔøΩ~ÔøΩqMÔøΩDÔøΩ\n~ÔøΩ\u0017ÔøΩ>YÔøΩAaÔøΩ\u0000bÔøΩ-ÔøΩaÔøΩ&ÔøΩ\u001b>ÔøΩÔøΩmÔøΩÔøΩ_V2iÔøΩÔøΩÔøΩ\u001f?yaÔøΩÔøΩ-\u000fÔøΩ::/zÔøΩÔøΩEtÔøΩÔøΩÔøΩ›£eÔøΩ\u0001\u000fÔøΩgÔøΩÔøΩ_}ÔøΩwÔøΩÔøΩ}ÔøΩÔøΩoÔøΩÔøΩPÔøΩÔøΩ\u0013ÔøΩ.YÔøΩÔøΩÔøΩFÔøΩyfÔøΩWÔøΩ\u0016ÔøΩ[ÔøΩÔøΩÔøΩKÂßï?5dÔøΩÔøΩu/|:e∆Ç\nÔøΩbÔøΩ&ÔøΩÔøΩfﬁΩÔøΩ\nuÔøΩ}ÔøΩPÔøΩÔøΩ\u0007ÔøΩ\\<$»∑ÔøΩÔøΩÔøΩÔøΩoÔøΩ/ÔøΩÔøΩÔøΩsÔøΩ(SiHÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ9pÔøΩ2ÔøΩKÔøΩ@\u0017|ÔøΩ!ÔøΩ\u0013ÔøΩÔøΩyPÔøΩ2ÔøΩ\u0013ÔøΩÔøΩÔøΩ’áÔøΩÔøΩm`ÔøΩÔøΩWDc RÔøΩkÔøΩiLÔøΩ^ÔøΩ\u0003'$#ÔøΩ5\u0013⁄•*ÔøΩ\u0004NEXUpÔøΩ1ÔøΩ\u0012\"ÔøΩ*ÔøΩ&ÔøΩ\u00075∆†EÔøΩ$ÔøΩ<b&'ÔøΩÔøΩÔøΩ\u0006a'ÔøΩ|\u0018ÔøΩ8FÔøΩÔøΩRÔøΩÔøΩr\u0006&\n\u000e‚Ñ†F\u0016\u0018ÔøΩ+ÔøΩÔøΩÔøΩÔøΩÀÇÔøΩi\"ÔøΩ\u0010\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0012ÔøΩƒ±~ÔøΩ\nuÔøΩ\u001bÔøΩÔøΩÔøΩ∆éÔøΩÔøΩÔøΩl\nS\u00044ÔøΩÔøΩ-\u0004ÔøΩHÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩdÔøΩÔøΩÔøΩÔøΩ ÕøÔøΩÎúÜIÔøΩÔøΩwÔøΩÔøΩÔøΩRÔøΩƒÄÔøΩÔøΩÔøΩvÔøΩÔøΩÔøΩÔøΩÔøΩ*ÔøΩo]T)ÔøΩ!ÔøΩnÔøΩÔøΩÔøΩw\nÔøΩ\u001bÔøΩWÔøΩß©ºÔøΩ\u001aÔøΩ\u0014%—§ÔøΩi>ÔøΩ\nÔøΩ\u000eAV-ÔøΩ\u0007ÔøΩ|\u0011\u001aÔøΩ=ÔøΩcÔøΩÔøΩ`ÔøΩe√àÔøΩTUp\u0018ÔøΩÔøΩ1ÔøΩÔøΩÔøΩ%]_X6ÔøΩLbÔøΩLÔøΩÔøΩ\u0002ÔøΩÔøΩ\\ÔøΩpDÔøΩÔøΩ¬Üa~G\nÔøΩ!ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0004y\u0005\u0012»®ÔøΩpÔøΩPtÔøΩÀíÔøΩÔøΩ6ƒêI\u0006ÔøΩFÈìÉÔøΩÔøΩÔøΩxÔøΩÔøΩEEÔøΩyÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nZ\\ÔøΩu`ÔøΩQÔøΩÔøΩ\"ÔøΩ\nÔøΩÔøΩzÔøΩÔøΩ\u0002ÔøΩÔøΩ@ÔøΩÔøΩÔøΩÔøΩq?o=\\=lÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩcÔøΩ◊æÔøΩ!ÔøΩ\b\u000fÔøΩ\u0007^\n3yÔøΩV~ÔøΩÔøΩ1}R=wﬁ≤›©\n\u0007>kÔøΩÔøΩ7ÔøΩÔøΩÔøΩY\nÔøΩÔøΩÔøΩÔøΩ\u0007_\u001bÔøΩÔøΩ«ìÔøΩÔøΩgÔøΩÔøΩ/ÔøΩÔøΩ\u0019ÔøΩlÂÜÉÔøΩÔøΩ\u001f\u0006ÔøΩ?=$ÔøΩÔøΩÔøΩ\nhÔøΩÃ∞aÔøΩÔøΩ\n:~ÔøΩÔøΩNÔøΩÔøΩÔøΩsÔøΩÔøΩÔøΩ-ÔøΩÔøΩ\u0003}ÔøΩÔøΩÔøΩÔøΩ\tÔøΩpKcÔøΩ&ÔøΩ}ÔøΩÔøΩKÔøΩÔøΩÔøΩÔøΩÔøΩ\u0002JÔøΩK5ÔøΩÔøΩ»Ω$8nÔøΩmÔøΩQÔøΩ\u001fÔøΩÔøΩF‹äÔøΩÔøΩÔøΩÔøΩZWKÔøΩI2jÔøΩx_ÔøΩÔøΩÔøΩ^ÔøΩ\u001aÔøΩÔøΩÔøΩ\u00161ÔøΩÔøΩ&n\u0019\u001bW/ÔøΩ]`ÔøΩÔøΩ\nÔøΩ>ÔøΩÔøΩÔøΩ\u001aÔøΩD<JGÔøΩ$ÔøΩUÔøΩ\u0018\u0013\nmÔøΩÔøΩJÔøΩÔøΩÔøΩ?ÔøΩÔøΩÔøΩJTÔøΩeoÔøΩÔøΩÔøΩÔøΩMÔøΩÔøΩÔøΩ DÔøΩÔøΩ\n!hZkxÔøΩ\u000f\u0014ÔøΩ\u001f5A\u0014ÔøΩÔøΩ2ÔøΩFÔøΩVÔøΩÔøΩÔøΩ∆•KÔøΩÔøΩÒæ´π)ÔøΩS+\tÔøΩ—ç`ÔøΩLSÔøΩ\nÔøΩ\u0006f8\u00043ÔøΩÔøΩNÔøΩc7Q\nÊíêÔøΩÔøΩÔøΩÔøΩÔøΩ\u0010ÃÄC⁄ïAÔøΩ3IDtÔøΩMÔøΩhÔøΩ\nÔøΩ`ÔøΩÔøΩœ†yMÔøΩÔøΩÔøΩÔøΩ\n«â!GÔøΩÔøΩÔøΩM\nÔøΩ3\n#ÔøΩ`.E\bÔøΩÔøΩ\"ÔøΩÔøΩÔøΩÔøΩ!D3,\u0014ÔøΩ\u0011ÔøΩ9ÔøΩ[ÔøΩRÔøΩHÔøΩfÔøΩ$HZÔøΩo\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩUJÔøΩ\u0018ÔøΩ$\u0013eÔøΩÔøΩÔøΩ%M√úRÔøΩBÂ∞ÖÔøΩÔøΩ.√±;ÔøΩÔøΩÔøΩÔøΩÔøΩkÔøΩÔøΩAÔøΩ>ÔøΩ{\u0007ÔøΩÔøΩV»≤nÔøΩ\n+\u0011ÔøΩBviÔøΩÔøΩ7xÔøΩÔøΩxÔøΩoÔøΩ‡•áÔøΩÔøΩSÔøΩ4ÔøΩ;yÔøΩÔøΩ}ÔøΩÔøΩfÔøΩÔøΩ~\u0011#\u0013ÔøΩE»éÔøΩ{eÔøΩ3\u001fM\u001a9mÔøΩgÔøΩ?v~cÔøΩoÔøΩÔøΩÔøΩ\u000eÔøΩ\n2ÔøΩÔøΩÔøΩ`ÔøΩ/ÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩaÔøΩN/ÔøΩÔøΩÔøΩ`\u0007ÔøΩÔøΩÔøΩ\t)\u0005ÔøΩÔøΩ7\njÔøΩÔøΩ0ÔøΩ3ÔøΩÔøΩ]ÔøΩ`ÔøΩ^ÔøΩÔøΩaÔøΩÔøΩ“æÔøΩÔøΩÔøΩÔøΩÔøΩnÔøΩMÔøΩ\u0005ÔøΩ\u0017ÔÑìÔøΩÔøΩ\u001bÔøΩ*ÔøΩ/ÔøΩ’Ü?o⁄Æ6ÔøΩ+ÔøΩ?/zlP`ÔøΩÔøΩmÔøΩO\\ÔøΩZÔøΩ\u0014ÔøΩ^ÔøΩÔøΩÔøΩGÔøΩ9ÔøΩuÔøΩg'{ÔøΩ\u0019\u00063fÔøΩÔøΩÔøΩÔøΩmÔøΩ|ÔøΩÔøΩVÔøΩTbÔøΩ~ÔøΩM\nh'ÔøΩkx.ÔøΩB\u0011ÔøΩ[PÔøΩÔøΩÔøΩ\u001fÔøΩÔøΩXj\u0013ZÔøΩÔøΩ-e\u0019ws=\u001bÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩaÔøΩ\u0012gÔøΩÔøΩ]ÔøΩ1ÔøΩ\n3ÔøΩÔøΩC\u0004\b8\bÔøΩ-ÔøΩr›ï\u0018ÔøΩJÔøΩ\u0007ÔøΩ\bÔøΩJÔøΩÔøΩÔøΩd\tÔøΩkÔøΩ5ÔøΩY.\u000fÔøΩ\nÔøΩ4ÔøΩLÔøΩiÔøΩ4M,+ÔøΩ\u0010%ÔøΩÔøΩÔøΩM\u0001\u0007(\u0007`)-y\n\u0017ÔøΩ\u0011DYÔøΩjÔøΩÔøΩq)ÔøΩÔøΩÔøΩsÔøΩdÔøΩÔøΩ\"?\u0013\ndQ9=]ÔøΩ\u0010,ÔøΩÔøΩ2 íÔøΩÔøΩÔøΩW\u0019ÔøΩÔøΩ\u0002XÔøΩÔøΩÔøΩÔøΩÔøΩ\bÔøΩÔøΩÔøΩjGÔøΩ|ÔøΩ\u0002ÔøΩxrÔøΩÔøΩÔøΩL0ÔøΩÔøΩÔøΩÔøΩI4ÔøΩÔøΩÔøΩÔøΩ\u0011\u000fÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ¬ÉXÔøΩÔøΩ'ÔøΩ]\u001b,HÔøΩ657ÔøΩÔøΩ~S€öÔøΩbÔøΩ`ÔøΩ8ÔøΩÔøΩ:ÔøΩ»ça4ÔøΩ]ÔøΩY\u0018ÔøΩUÔøΩ‘ú‘∞hxx%ÔøΩÔøΩDÔøΩYÔøΩÔøΩÔøΩn€™B\u0006\nIÔøΩ)ÔøΩ`€§HÔøΩi\u0006MgQ\bÔøΩm&GMÔøΩÔøΩ\u001aÔøΩœ†ÔøΩL\u001fÔøΩ\u000eg6\u0012|ÔøΩÔøΩ2T\u0000KÔøΩ\b\u0018EÔøΩ\"h)tÔøΩ:\u0004F\u0017ÔøΩ`ÔøΩﬁ©ÔøΩÔøΩÔøΩÔøΩÔøΩW\u0005ÔøΩ6spÔøΩÔøΩrÔøΩÔøΩWGÔøΩH\nlÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩ;ÔøΩÔøΩÔøΩvÔøΩrHÔøΩÔøΩ‘°ÔøΩÔøΩ[ÔøΩ_dÔøΩzÔøΩÔøΩÔøΩÔøΩYÔøΩyÔøΩÔøΩÔøΩÔøΩ\u000fÔøΩ2oÔøΩ=/ÔøΩÔøΩÔøΩÔøΩ\u0011ÔøΩ\u0016ÔøΩÔøΩ+o cÔøΩÔøΩt◊ìÔøΩ'ÔøΩgÔøΩ65ÔøΩÔøΩ{\u000eÔøΩÔøΩÔøΩÔøΩ\b*ÔøΩÿ°ÔøΩcÔøΩÔøΩÔøΩÔøΩ÷∑3\u0018X\ny1ÔøΩÔøΩ-\nÔøΩ\u000fÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ1oÔøΩwÔøΩÔøΩ=>ÔøΩÔøΩwÔøΩÔøΩEÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ.^ÔøΩÔøΩrÔøΩÔøΩV]\u0007i~R),v`BÔøΩÔøΩ-ÔøΩÔøΩ6ÔøΩCÔøΩUÔøΩIÔøΩT\u0006\u0012ÔøΩQÔøΩÔøΩ⁄∂ÔøΩ\u0002ÔøΩÔøΩ%e\u0019ÔøΩlJÔøΩÔøΩ`fÔøΩÔøΩÔøΩXiÔøΩÔøΩÔøΩÔøΩ\u0016VFePÔøΩ\"ÿπ\nÔøΩDÔøΩ\n9\u000f]\u00189\u0014ÔøΩ!ÔøΩÔøΩTur`\u0011-ÔøΩ<ÔøΩ6ÔøΩ$\"ÔøΩhBÔøΩÔøΩ,ÔøΩÔøΩD√°\u0004ÔøΩÔøΩÔøΩAÔøΩx8ÔøΩÔøΩEÔøΩÔøΩ1ÔøΩ\u000f«ñ>pnj6\u000fÕ±4ÔøΩÔøΩ1'9ÔøΩ”ëk\u0012ÔøΩEÔøΩÔøΩ}&FÔøΩÔøΩÔøΩ\n\u0002`ÔøΩ\u0004ÔøΩÔøΩ\nJÔøΩ\u0002$9ÔøΩÔøΩÔøΩ\tÔøΩ\u0018b.dÔøΩÔøΩ;JÔøΩÔøΩ\u0010ÔøΩÔøΩ\u0004ÔøΩ<ÔøΩ\u0006ÔøΩhÔøΩQÔøΩÔøΩ`^h.ÔøΩÔøΩZ{ÔøΩ&3ÔøΩ\u0019ÔøΩÔøΩƒ°.\u0012ÔøΩÔøΩ\\b%\nq1\u0010\u0002ÔøΩ“ºD^ÔøΩÔøΩÔøΩ\u001bÔøΩ\bÔøΩ\naÔøΩÔøΩ\u0011\u0002%8ÔøΩ\u00144mW€¥mÔøΩPÔøΩÔøΩÔøΩ\"ÔøΩ2ÔøΩ>ÔøΩOÔøΩÔøΩ^cmÔøΩÔøΩ\nÔøΩÔøΩ.ÔøΩÔøΩ ÔøΩÔøΩt.#ÔøΩ\u001bIÔøΩ\\/ÔøΩhÔøΩRÔøΩK)ÔøΩjÔøΩÔøΩsDQ[\"ÔøΩÔøΩtÔøΩpÔøΩÔøΩÔøΩ:ÔøΩ}ÔøΩy\u001b\u0004&:ÔøΩÔøΩÔøΩrYÔøΩÔøΩ\u0012\n*1ÔøΩN=ÔøΩÔøΩÔøΩ\u000fAÔøΩ√•ÔøΩ‡°ó<1xÔøΩÔøΩWÔøΩ¬öGÔøΩ/]ÔøΩÔøΩf”°\u000fÔøΩsd,\u0015AÔøΩÔøΩTÔøΩEÔøΩÔøΩSZÔøΩÔøΩ'SÔøΩ.ÔøΩÔøΩÔøΩÔøΩÔøΩ:?ÔøΩÔøΩd\ny}ÔøΩrÔøΩÔøΩ%ÔøΩÔøΩÔøΩﬁ≥\u001fO>wÔøΩÔøΩÔøΩ_|ÔøΩÔøΩGÔøΩÔøΩÔøΩ;ÔøΩ\u0011%ÔøΩO\nÔøΩÔøΩboÔøΩv5ÔøΩ3ÔøΩiÔøΩ6\n,yBdÔøΩi$!\u0007\nÔøΩ{ÔøΩÔøΩ}\u0007OÔøΩÔøΩÔøΩﬁ∞ÔøΩ`IojSÔøΩ|›Å>ÔøΩOyÔøΩÔøΩ8W\nYÔøΩb›Å[;vOÔøΩÔøΩÔøΩ’Ø~xÏùâÔøΩÔøΩÔøΩFLRÓ∏ØWÔøΩ—´ÔøΩ\n:ÔøΩÔøΩÔøΩxÔøΩÔøΩwÔøΩ⁄µs\u0017.ÔøΩwÔøΩÔøΩ;ÔøΩ‘∞oS\u0010ÔøΩWmxÔøΩÔøΩÔøΩ\u0017`\u0010ÔøΩ)ÔøΩÔøΩ\u001a\u0017C\u0004ÔøΩUÔøΩoÔøΩ\u0013ÔøΩ\u000eÔøΩ\nb<?ÔøΩt\u0007`%ÔøΩ\n\u0014ÔøΩ\nÔøΩ`ÔøΩ\u0010ÔøΩ\u0018eÔøΩ%ÔøΩ\u0012\u0010ÔøΩ`ÔøΩ\u0015\u001b\tpÔøΩÔøΩÔøΩ\u0017zgÔøΩ_ÔøΩÔøΩ\u0007ÔøΩt\u0014\n8\u00032<ÔøΩL%ÔøΩRÔøΩu\u0016hÔøΩ]ÔøΩiÔøΩÔøΩÔøΩg\"ÔøΩ\tÔøΩÔøΩbRÔøΩÔøΩ-\u0013ÔøΩc&\u0015ÔøΩ<ÔøΩ4dÔøΩkÀìÔøΩÔøΩ+q\nÔøΩÔøΩ*\u0006ÔøΩe\u0013\u0014ÔøΩÔøΩhÔøΩ-ÔøΩÔøΩEÔøΩ\u000eÔøΩÔøΩ\u0017XÔøΩÔøΩ2\nÔøΩÀëÔøΩkÔøΩ\u0018\u0011ÔøΩÔøΩÔøΩ\u0004ÔøΩÔøΩÔøΩÔøΩtÔøΩÔøΩÔøΩea\u0012ÔøΩ)ÔøΩÔøΩ\n&ÔøΩ>”≤ÔøΩ`ÔøΩ\u0001\nCjONI.\n+ÔøΩÔøΩÔøΩ%\u0011<DGIÔøΩÔøΩE\u0012ÔøΩRCÔøΩ>{T\u000fÔøΩ\t[ÔøΩ\t-*ÔøΩRÔøΩS&ÔøΩwxdÔøΩÔøΩKQÔøΩ(ÔøΩÔøΩÔøΩ\u0010ÔøΩ\u0016'\u00183oclYÔøΩÔøΩÔøΩ6\u001bÔøΩz@ÔøΩ\u0010\n\u001bjÔøΩÔøΩiÔøΩdÔøΩ\u0012!\u000f:|«Ø?ÔøΩÔøΩi(]ÔøΩÔøΩ -)\nÔøΩÔøΩ4ÔøΩcÔøΩpÔøΩyLÔøΩ\u0012ÔøΩ4ÔøΩNtÔøΩxÔøΩ9`ÔøΩ\u0017Z]\u0012ÔøΩ4ÔøΩ\"ÔøΩ\bÔøΩG/√©ÔøΩÔøΩ\u000e\u0017›∞ÔøΩÔøΩ-ÔøΩÔøΩjÓ±ìÔøΩ!ÔøΩÔøΩÔøΩ\u0019ÔøΩÔøΩÔøΩÔøΩOkÔøΩqB*ÔøΩ\u0017ÔøΩÔøΩAw}wbÔøΩ\u0003ÔøΩ)?ÔøΩ⁄ò}ÔøΩÔøΩt|q8\u00070ÔøΩcÔøΩO\u000f9xÔøΩÔøΩ]ÔøΩ\u001bÔøΩÔøΩÔøΩÔøΩ\n9ÔøΩ€•ÔøΩ.\u0003<ÔøΩ1ÔøΩ*{jÔøΩ-\nÔøΩÔøΩz|ÔøΩÔøΩÔøΩ{!ÔøΩeÔøΩÔøΩ\u0011S◊èÔøΩÔøΩQ\u000e~\nyc\\ÔøΩG\u0007ÔøΩ_ÔøΩÔøΩCÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\b/MxÔøΩ{~\u0018dÔøΩJx\u0014|oEÔøΩÔøΩÔøΩÔøΩ<jÔøΩÔøΩ=ÔøΩÔøΩ'ÔøΩÔøΩSCﬁ≠ÔøΩÔøΩÔøΩÔøΩÔøΩ}ÔøΩ„ß∑*fÔøΩ á?#ÔøΩÔøΩÔøΩ\u001fÔøΩ?|ÔøΩz\u0016ÔøΩÊªæ|ÔøΩn√éÔøΩ\u000fÔøΩgtbOÔøΩÔøΩbFgÔøΩ\u0004\u0003\u0011ÔøΩgÔøΩ7IÔøΩÔøΩ2\u0000\u000f]}'ÔøΩÔøΩÔøΩl…†yÔøΩ\u0010^ÔøΩ&ÔøΩÔøΩÔøΩÔøΩÔøΩW/\u0017ÔøΩ\u0005jÔøΩ\tdaÔøΩÔøΩmBFÔøΩDÔøΩIa2,ÔøΩ^\u0019ÔøΩ$)8ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0015ÔøΩ\nuf–åÔøΩ7ÔøΩQHÔøΩZ\nÔøΩÔøΩ12hÔøΩ^8pÔøΩÔøΩQ\u0002M7ÔøΩÔøΩ1ÔøΩÔøΩÔøΩÔøΩÔøΩ|ÔøΩÔøΩÔøΩ\u0005ÔøΩEÔøΩaK\bÔøΩ^\nÔøΩXRÔøΩAsÔøΩXÔøΩdÔøΩ,ÔøΩÔøΩ<ÔøΩÔøΩsÔøΩY!ÔøΩkÔøΩWÔøΩ@ÔøΩÔøΩ\u0014ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩb7\u0016ÔøΩ\u0014ÔøΩ\u0017aÔøΩkl5ÔøΩbZK^4ÔøΩ\u0004)LŸáE$ÔøΩH\n¬ÆÔøΩÔøΩ\nÔøΩ\nÔøΩÔøΩ\b\nd\u0003ÔøΩÔøΩ%\n◊ÅW!ÔøΩPÔøΩE'.ÔøΩÔøΩÔøΩ19(\nÔøΩ>ÔøΩÔøΩÔøΩL\u0015[ÔøΩlÔøΩ\tÔøΩ\\ÔøΩYÔøΩ3ÔøΩ.\u0011\\ÔøΩÔøΩÔøΩÔøΩ\nZ;\n&ÔøΩÔøΩÔøΩÔøΩÔøΩnI\u0018B[%JÔøΩ\naaÔøΩÔøΩN⁄ú6ÔøΩÔøΩÔøΩ\u0013ÔøΩ/\u0006ÔøΩ\u001bÔøΩÔøΩ\nuÔøΩ\u0015\nuÔøΩdÔøΩ$ÔøΩÔøΩLp*Bl\u0005ÔøΩ#\u0014ÔøΩÔøΩÔøΩÔøΩ8V\u0017ÔøΩBnÔøΩo\u000fÔøΩÔøΩ:oÎÖãWÔøΩqÔøΩ|ÔøΩÔøΩoÔøΩ/ÔøΩÔøΩ~C—£\u0003CÔøΩÔøΩ,\u0013\tYg`–í\u001fÔøΩoÔøΩÔøΩ4ÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩBÔøΩÔøΩMÔøΩVoÔøΩÔøΩÔøΩÀüOÔøΩÔøΩÔøΩN=ÔøΩ_ÔøΩÔøΩÔøΩÔøΩÔøΩYÔøΩÔøΩN/ÔøΩ\n5m=|ŒßÔøΩs7l?ÔøΩA`ŒõÔøΩÔøΩQÔøΩÔøΩ◊Ω\u00164ÔøΩÔøΩÔøΩ`ÔøΩÔøΩ3aÔøΩmÔøΩO\u000e~ÔøΩÔøΩÔøΩSÔøΩÔøΩ8 x|O8iÔøΩÔøΩ›≤ÔøΩ\t\u0004€Ø=ÔøΩÔøΩÔøΩÔøΩ2eÔøΩÔøΩ_Ó∏ØW\u0012MÔøΩrÔøΩÔøΩ’Ø~XÔøΩÔøΩ\u0000ÔøΩÔøΩ„™πeOFÔøΩ\nÔøΩÔøΩÔøΩ◊õÔøΩVn8X'ÔøΩuÔøΩÔøΩ_;~ÔøΩ|pÔøΩ\u0012»∂T.ÔøΩ\u0010ehÔøΩÔøΩ\nÔøΩÔøΩ\\{ÔøΩwÔøΩ4ÔøΩ\u0000B*1ÔøΩ2CÔøΩ πÔøΩ\nÔøΩÔøΩd\u0014ÔøΩ\u0012IÔøΩ1E \u0019}ÔøΩ√ù 2HcÔøΩÔøΩD\u0013\"\u000ebÔøΩÔøΩÔøΩ*ÔøΩxÔøΩ √π\u001a\u0013\u00180`ÔøΩÛº¥û\u0014ÔøΩ\u0007ÔøΩ2tÔøΩ\u0006ÔøΩ÷ü$ÔøΩÔøΩÔøΩuÔøΩ`i!+ÔøΩÔøΩls—πÔøΩsÔøΩ<9ÔøΩÔøΩ\n5ÔøΩÔøΩyÔøΩFsÔøΩ4ÔøΩÔøΩcÔøΩ\u0018ÔøΩ\u0015ÔøΩ\u0015p\nÔøΩÔøΩKÔøΩ&ÔøΩÔøΩ!1ÔøΩÔøΩÔøΩÔøΩÔøΩÀ∏ZÔøΩ!0\nÔøΩÔøΩÔøΩ`ÔøΩqÔøΩcÔøΩÔøΩ`ÔøΩ$ÔøΩD[ÔøΩDÔøΩÔøΩuzÔøΩÔøΩ\u0013◊ô\u0016fÔøΩÔøΩÔøΩ+f\u001btÔøΩG ÔøΩMÔøΩDÔøΩ?ÔøΩUÔøΩi5ÔøΩÔøΩ^rÔøΩ,b0ÔøΩ5dLnÔøΩÔøΩÔøΩÔøΩ6ÔøΩÔøΩlX\u0013$\u0007ÔøΩÔøΩÔøΩbÔøΩ[\u0011\u000eÔøΩÔøΩ@ÔøΩÔøΩÔøΩÔøΩl,TÔøΩÔøΩ6ÔøΩFDÔøΩÔøΩg9ÔøΩÔøΩ\u0002ÔøΩ\u0006ÔøΩÔøΩÔøΩ ≥j\u0006ÔøΩm4XRÔøΩoÔøΩÔøΩxeÔøΩ4lK>\n\u0019ÔøΩBc\u0003ÔøΩ\u0004ÔøΩ\u0018“ï\\ÔøΩl€±\u0012d7≈ájÔøΩhÔøΩ4ÔøΩÔøΩn\n\u0011yjÔøΩ\u0019KÔøΩ[1h\u001a?ÔøΩÔøΩJÔøΩ\u001b\u0011tÔøΩwÔøΩÔøΩÔøΩÔøΩ=}ÔøΩÔøΩÔøΩ\u0018\u0011ÔøΩÔøΩÔøΩÔøΩÔøΩ.ÔøΩ5ÔøΩÔøΩÔøΩqÔøΩZXY5diÔøΩÔøΩb‹∫ÔøΩ\n#ÔøΩÔøΩ:\u0006ÔøΩÔøΩ…±ÔøΩ7\u000e\u0018ÔøΩZÔøΩg:ÔøΩ>ÔøΩÔøΩÔøΩg&ÔøΩÔøΩZÔøΩoÔøΩÔøΩ=~ÔøΩÔøΩÔøΩÔøΩÔøΩRTLÔøΩ=\u0010cÔøΩ\n\u0016ÔøΩÔøΩÔøΩ~<9ÔøΩÔøΩ/ŒºÔøΩ{ÔøΩ/ÔøΩqkÔøΩÔøΩ8ÔøΩP\u0001ÔøΩoÔøΩQ+`IÔøΩ}yÔøΩmGÔøΩÔøΩP_ÔøΩÔøΩÔøΩÔøΩ\u0017ÔøΩÔøΩÔøΩÔøΩo6ÔøΩÔøΩ/}>\u0015ÔøΩ|\b*nÔøΩÔøΩÔøΩec\u0019#ÔøΩÔøΩÔøΩÔøΩ/ÔøΩf4ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩvÔøΩÔøΩÔøΩ>jVÔøΩÔøΩ\u0001\n,\u0010y(ÔøΩ¬äÔøΩÔøΩ$ÔøΩqÔøΩÔøΩÔøΩj\bE=-\u0004yÔøΩyoÔøΩÔøΩÔøΩÔøΩdDcf0ÔøΩ\nr2 êÔøΩÔøΩÔøΩ3ÔøΩÔøΩÔøΩ5K'LÔøΩ\u00067+\u000f_!&ÔøΩÔøΩSX7ÔøΩÔøΩÔøΩD ÔøΩ-ÔøΩÔøΩWÔøΩ )∆ôÔøΩAÔøΩ0D\b\n\"R,ÔøΩv\u0011ÔøΩB3ÔøΩ◊öoÔøΩÔøΩ‹íÔøΩ4ÔøΩÔøΩ\\\u0001ÔøΩ\u000fGÔøΩrx8J:≈∞ÔøΩYpÔøΩgÔøΩ9ÔøΩNÔøΩ\u0019cr\bÔøΩÔøΩ!ÔøΩÔøΩuÔøΩaÔøΩitÔøΩÔøΩ~ÔøΩfÔøΩÔøΩÔøΩ\u001aÔøΩapÔøΩÔøΩ∆ïf\tpUÔøΩ'qÔøΩ‘òwÔøΩ+ÔøΩÔøΩYÔøΩ9\u001b4Uud\n!FDÔøΩ~ÔøΩL\u0011ÔøΩ&ÔøΩÔøΩR{TÔøΩÔøΩÔøΩÔøΩHÔøΩMÔøΩpxÔøΩ)\u0005JÔøΩ\u0010`ÔøΩ_v/ÔøΩÔøΩ\u001b!ÔøΩÔøΩx\u0002\nÔøΩÔøΩÔøΩ’çÔøΩÔøΩe@\u0014ÔøΩÔøΩ\bÔøΩ'5ÁøôÔøΩÔøΩÔøΩyf@ÔøΩD€ΩÔøΩÔøΩn3ÔøΩ‹Ä\n:ÔøΩ\\t82gÔøΩtÔøΩa{HdÔøΩ-ÔøΩnÔøΩ|ÔøΩ,ÔøΩ4ƒ≤EÔøΩ\u0019ÔøΩÔøΩrÔøΩm\tÔøΩÔøΩ\u0005aÔøΩÔøΩÔøΩ*ÔøΩ\u0004ÔøΩÔøΩ&~\u0015ÔøΩ\u0002fÔøΩ\nÔøΩÔøΩ\"ÔøΩuuÔøΩEÔøΩ~ÔøΩÔøΩÔøΩÔøΩ1ÔøΩ‚•´}FÔøΩTÔøΩ^ÔøΩÔøΩ\u0003zÔøΩ\t}«ÆzÈ≥©ÔøΩ;ÔøΩchUÔøΩ#ÔøΩ_ÔøΩÔøΩ\u0007ÔøΩ<xÔøΩ+ÔøΩ\u001f|ÔøΩÔøΩLÔøΩjÔøΩYÔøΩ\n[ÔøΩnÔøΩ6ÔøΩ#ÔøΩÔøΩ=ÔøΩ\u0007,ÔøΩÔøΩDÔøΩ!ÔøΩÔøΩ?o÷ÆÔøΩÔøΩ—ÅÔøΩ5ÔøΩ=5ÔøΩœùzÔøΩÔøΩÕå\u001fÔøΩÔøΩÔøΩ«ÉÔøΩ57m[ÔøΩdÔøΩ/ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ{NÔøΩEÔøΩÔøΩ?ÿßÔøΩÔøΩ\u0000ÔøΩÔøΩ\u0001ÔøΩWÔøΩk>ÔøΩÔøΩÔøΩ\n\u0002\u001f;kÔøΩ.ÔøΩÔøΩÔøΩFÔøΩÔøΩÔøΩÔøΩÔøΩ?ÔøΩÔøΩ‰∏Æ\u0006+c\tJÔøΩÔøΩ0ÔøΩW\tÔøΩ@ÔøΩC\u0003fÔøΩÔøΩsÔøΩI\u0000'\u0017ÔøΩTÔøΩ\nÔøΩ7\u0017ÔøΩÔøΩpÔøΩÔøΩÔøΩÔøΩÔøΩ~∆âgS\u0014\n[1\u0006ÔøΩÔøΩyÔøΩs\nÔøΩÔøΩÔøΩƒö=ÔøΩ<ÔøΩj%,HTÔøΩKMdÔøΩJ+ÔøΩÔøΩvÔøΩÔøΩtV.3ÔøΩ(UÔøΩD[ÔøΩIÔøΩ(ÔøΩÔøΩÔøΩÔøΩ\n”àÔøΩÔøΩ\"~ÔøΩsÔøΩÔøΩÔøΩ√ë(ÔøΩ\u00027ÔøΩÔøΩ\u0001ÔøΩ\u000f\u0006ÔøΩÔøΩ\u0004ÔøΩuuz\u0011>\u000eÔøΩBÔøΩ_ÔøΩ)ÔøΩ?lÔøΩ9ÔøΩ1IÔøΩyÔøΩ_ÔøΩ-ŸãZA,fÔøΩÔøΩ\u0014ÔøΩÔøΩ\u00052TÔøΩIÔøΩÔøΩ\"RSÔøΩ2ÔøΩ{ÔøΩMÔøΩÔøΩ∆Æ?\\mFÔøΩYÔøΩÔøΩjLÔøΩl\u001bÔøΩ\\ÔøΩ!ÔøΩ\u0011ÔøΩÔøΩ\"\u0004ÔøΩAÔøΩ\nBUVq@=C |\u00147ÔøΩÔøΩ\u001bÔøΩ#ÔøΩ:ÔøΩÔøΩNsÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩL`\u0005ÔøΩ]NÔøΩÔøΩÔøΩÔøΩ\u0005\u0015b8ÔøΩÔøΩ\u0006aÔøΩÔøΩ&#\u0007ÔøΩ\nQÔøΩV,nlÔøΩ\u0013\u0011ÔøΩ#6ÔøΩ8sla\"ÔøΩVr\n—âÔøΩ\u0016ÔøΩpÔøΩ\u001fÔøΩs\\ÔøΩÔøΩ\u00061$ÔøΩ$\t\u0017F~ÔøΩ\u0012\u0013ÔøΩhœ¶q)3ÔøΩwpÔøΩÔøΩÔøΩaÔøΩ0ÔøΩ[>ÔøΩÔøΩÔøΩ\u0007_\n\u0013r#\u0010ÔøΩÔøΩOÔøΩÔøΩÔøΩÔøΩ[ÔøΩ\u0016ÔøΩÔøΩÔøΩÔøΩKÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0002ÔøΩÔøΩjQÔøΩa`ÔøΩÔøΩ4Q9`ÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ{C~\nCÔøΩÔøΩÔøΩt.-d\u000eÔøΩ\nÔøΩ\"ÔøΩu'ÔøΩÔøΩ\nÔøΩ\u0002ÔøΩÔøΩÔøΩ%|ÔøΩINÔøΩ\u000fÔøΩÔøΩÔøΩÔøΩmÔøΩ\nÔøΩBÎßáÔøΩÔøΩÔøΩpÔøΩ.\u0003ÔøΩÔøΩ?nuÔøΩÔøΩ+ÔøΩÔøΩÔøΩÔøΩÔøΩzmÔøΩqÏì™yÔøΩGÔøΩÔøΩÔøΩﬂ£ÔøΩkÔøΩ$ÔøΩpkÔøΩ√ó]ÔøΩpÔøΩÔøΩÔøΩe:ÔøΩÔøΩnÔøΩ\u0016ÔøΩÔøΩÔøΩÔøΩsÔøΩÔøΩÔøΩ!ÔøΩÔøΩÔøΩÔøΩÔøΩ5ÔøΩ;ÀãmÔøΩœÖKÔøΩQmcÔøΩ\u001f\u0003[uÔøΩ7ÔøΩÔøΩR*ÔøΩ3ÔøΩÔøΩÔøΩZÔøΩÔøΩ\tÔøΩ$ÔøΩnÔøΩÔøΩqÔøΩÔøΩÔøΩ\u0013xÔøΩ◊≤j\u0000ÔøΩ\u001bÔøΩÔøΩL&ÔøΩÔøΩ\u0006w\u0002eÔøΩhÔøΩpWSÔøΩ\u0016yÔøΩÔøΩÔøΩW\u001aloÔøΩÔøΩ5ÔøΩÔøΩ\nh\nÔøΩc\u0001ÔøΩ?ÔøΩ .\nIEÔøΩÔøΩÔøΩÔøΩÔøΩ}ÔøΩ:ÔøΩÔøΩÔøΩCw÷öiB\"ÔøΩÔøΩ\u0006ÔøΩÔøΩ!ÔøΩ!ÔøΩ#UÔøΩ&r\u0005&ÔøΩÔøΩÔøΩÔøΩA,ÔøΩaÔøΩ ≠xÔøΩÔøΩÔøΩÔøΩ*HÔøΩÔøΩÔøΩdsÔøΩT\\ÔøΩNwÔøΩ1ÔøΩt.9ÔøΩ8ÔøΩÔøΩZXÔøΩ=5ÔøΩÔøΩÔøΩÔøΩ].GÔøΩR]\nCÔøΩƒπ\u000eÔøΩ\u0010ÔøΩ\u001aVÔøΩfÔøΩ1'ÔøΩ7ÔøΩDFÔøΩÔøΩ{ÔøΩÔøΩiÔøΩÔøΩÀöÔøΩ\u0006o1*ÔøΩÔøΩ>ÔøΩ\u0014=&gÔøΩÔøΩ`ÔøΩÔøΩZÔøΩ\u0015_'ÔøΩMÔøΩVÔøΩÔøΩÔøΩD\u0016ÔøΩ*ÔøΩ6ÔøΩÔøΩÔøΩÔøΩÔøΩ\"dLÔøΩÔøΩ6ÔøΩÔøΩÔøΩ‘îV$ÔøΩƒÄÔøΩ`XÔøΩ\ni\bM\u0002MKsÔøΩ4ÔøΩÔøΩRÔøΩWp\u0007\u0016ÔøΩÔøΩ3CÔøΩÔøΩÔøΩœì€êA_ÔøΩ|ÔøΩÔøΩÔøΩ€ãÔøΩ\b\u0015rj÷∂ÔøΩ}ÔøΩÔøΩGÔøΩ\u0018ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0013ÔøΩÔøΩhÔøΩ{ÔøΩÔøΩ3\u0017ÔøΩXÔøΩÔøΩÔøΩ\u0019ÔøΩÔøΩÔøΩ,ÔøΩÔøΩÔøΩG~ÔøΩÔøΩ~4yÔøΩÔøΩÔøΩTÔøΩÕéﬁäÔøΩ\u0017\u0007BÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0017>ÔøΩ\u0002?ÔøΩ\u001fÔøΩ\u0004ÔøΩÔøΩ1?lÔøΩÔøΩÔøΩ9KwÔøΩ_ÔøΩÔøΩmÔøΩÔøΩSÔøΩ:ÔøΩÔøΩ\nÔøΩÔøΩVÔøΩ_ÔøΩÔøΩÔøΩ\u001f~\u0007iÔøΩ?\nÔøΩ\u0003)ÔøΩÔøΩÔøΩÔøΩ\u0013Y4\tÔøΩK)ÔøΩÔøΩÔøΩ¬àÔøΩ{ÔøΩÔøΩÔøΩÔøΩO4cPÔøΩÔøΩ\u0007ÔøΩnÔøΩqÔøΩUÔøΩA\nÔøΩ+ÔøΩÔøΩÔøΩÔøΩ¬πKwÔøΩ‘¶\u001a~18dÔøΩÔøΩÔøΩ^ÔøΩ4EÔøΩ\u0003VÔøΩÔøΩ\u000f'mÔøΩsÔøΩÔøΩÔøΩÔøΩ8ÔøΩÔøΩpÔøΩÔøΩ\tkÔøΩ‘°{ÔøΩÔøΩÔøΩgÔøΩÔøΩo\u0000ÔøΩ\u0016\u0014ÔøΩgÔøΩwÔøΩÔøΩ∆ÑÃôQ0ÔøΩ.ÔøΩ5QÔøΩÔøΩ\u0004DÔøΩ+>\n\\ÔøΩ!\nVÔøΩÔøΩwDÔøΩHÔøΩÔøΩÔøΩÔøΩzaÔøΩ\u00193\u0016ÔøΩ}%\n9f”ìhfÔøΩÔøΩpb&j\u0010\u0010RÔøΩXÔøΩÔøΩgÔøΩ2V;2ÔøΩjÔøΩÔøΩÔøΩe1wÔøΩÔøΩÔøΩ\u001fgÔøΩJÔøΩ\tmÔøΩÔøΩ\t6mSsKÔøΩÔøΩ[;ÔøΩÔøΩSÔøΩ\nÔøΩAÔøΩÔøΩfÔøΩÔøΩÔøΩÔøΩ_ÔøΩUÔøΩÔøΩÔøΩÔøΩÔøΩ\u0013ÔøΩ<ÔøΩK\u0007ÔøΩÔøΩ\t)sDS4hÔøΩ5#ÔøΩÔøΩÔøΩY\u000f$ÔøΩÔøΩ*ÔøΩLD\ni\t\u0019ÔøΩÔøΩNoÔøΩÔøΩp\u001bC\u000eÔøΩ_EÔøΩ5–∞J%#dEÔøΩ@x\nÔøΩ;ÔøΩÔøΩ\bmVi^¬¥F<ÔøΩd|AÔøΩ\u0003ÔøΩ:ÔøΩ[LÔøΩkÔøΩ}\u000e1ÔøΩÔøΩÔøΩÔøΩ4\u0002ÔøΩrÔøΩ\u0011\u0019tÔøΩ\u0014ug9lP3ÔøΩÔøΩÔøΩÔøΩÔøΩzÔøΩvg#ÔøΩiÔøΩ<2ÔøΩÔøΩ8ÔøΩÔøΩÔøΩÔøΩdBhFÔøΩ8JÔøΩ\tUÔøΩÔøΩ\nvfzÔøΩÔøΩ~5ÔøΩÔøΩÔøΩ&\n#ÔøΩÔøΩsfÔøΩ@=ÔøΩ\"_\n\u0011ÔøΩ$4D\u0001G*ÔøΩÔøΩÔøΩÔøΩ*\bÔøΩ!7ÔøΩxÔøΩvÔøΩÔøΩÔøΩÔøΩ\bÔøΩÔøΩÔøΩÔøΩ<ÔøΩÔøΩnÔøΩ\n}{<ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ5r%ÔøΩUÔøΩ\u0003ÔøΩO\nÔøΩ\u0019ÔøΩlÔøΩÔøΩ_œúÔøΩ\u0004YÔøΩ'ÔøΩÔøΩ\u0013ÔøΩMÔøΩÔøΩÔøΩ}ÔøΩ\u0017\n.ÔøΩ÷éÔøΩ;ÔøΩ4ÔøΩ\n?ÔøΩÔøΩÔøΩiÕ¶C;ÔøΩÔøΩ<|ÔøΩÔøΩÔøΩÔøΩg7l?2ÔøΩqÔøΩ7ÔøΩ\nÔøΩÔøΩ?`…ó=~ÔøΩÔøΩ?ÔøΩÔøΩÔøΩ^ÔøΩ9ÔøΩÔøΩ4wÀ¢ÔøΩÔøΩÔøΩÔøΩ>qÔøΩÔøΩÔøΩCGÔøΩÔøΩ7ÔøΩX`ÔøΩÔøΩÔøΩÔøΩÔøΩfÔøΩ?=ÔøΩÔøΩvÔøΩ\u0019ÔøΩÔøΩ∆∞,~ÔøΩÔøΩÔøΩ>ÔøΩÔøΩÿπKvÔøΩ[ÔøΩ[ÔøΩ\\ÔøΩÂûûÔøΩÔøΩÔøΩÔøΩﬂöyÔøΩÔøΩwÔøΩ€´ÔøΩ[ÔøΩ\nH\u0018CÔøΩ+M\u0012=S\u0011ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0017ÔøΩzÔøΩAÔøΩkŒí]ÔøΩ[ÔøΩﬁíu\u0003ÔøΩÔøΩÔøΩ|ÔøΩrAÔøΩQ>!ÔøΩ5WfÔøΩÔøΩÔøΩÔøΩÔøΩ\u0011ÔøΩÔøΩ\tÔøΩÔøΩ8NNÔøΩ\nÔøΩÔøΩ◊ñÔøΩFÔøΩFÔøΩÔøΩfÔøΩÔøΩ\u0000ÔøΩ?E}ÔøΩ\u0000ÔøΩÔøΩÔøΩŸïLÔøΩ ÔøΩ8ÔøΩ%TÔøΩÔøΩ*ÔøΩÔøΩ<\u001fMÔøΩÔøΩÔøΩbÔøΩaÔøΩÔøΩq\u001b\u0012*ÔøΩÔøΩ⁄ëÔøΩO9ÔøΩÔøΩ(\"ÔøΩW[ÔøΩ~\u001f\u0011ÔøΩXOÔøΩÔøΩ\u0010cÔøΩnÔøΩ\u0010ÔøΩÔøΩ\u0018ÔøΩ`'ÔøΩYÔøΩÔøΩ[:t\u0007k÷Æ&ÔøΩÔøΩ”Ö“¥\u000f~ÔøΩ\u000fÔøΩ\u0012\u0016ÔøΩPyÔøΩ‘òÔøΩÔøΩ_ÔøΩ`RÔøΩJUÔøΩÔøΩÔøΩÔøΩÔøΩ.ÔøΩF4ÔøΩzÔøΩJc\u0018ÔøΩÔøΩÔøΩÔøΩiÔøΩp<\u0005ÔøΩHÔøΩÔøΩÔøΩÔøΩ:ÔøΩ:ÔøΩ\u00141ÔøΩÔøΩÔøΩÔøΩ<ÔøΩÔøΩ)ÔøΩÔøΩÔøΩÔøΩÔøΩM\nH\u0016ÔøΩ! ÔøΩ9C\nÔøΩ1wÔøΩÔøΩÔøΩ\u0018W\t^ÔøΩÔøΩ6ÔøΩÔøΩ\u000e\u0011iÔøΩÔøΩÔøΩfÔøΩ!&ÔøΩ]RU«ê\u000fÔøΩF~ÔøΩ{F\n\u0004ÔøΩ\u0007ÔøΩr\u0017ÔøΩÔøΩÔøΩgÔøΩ2ÔøΩÔøΩEÔøΩA\u001aÔøΩ◊∫ÔøΩ\tÔøΩÔøΩ…íÔøΩ4ÔøΩMcÔøΩ\u0003\u0019ÔøΩ\n1sLmÔøΩJÔøΩz/RÔøΩy\b\u0019ÔøΩfÔøΩiÔøΩÔøΩÔøΩk3ÔøΩÔøΩ!‹ºÔøΩlÔøΩÔøΩÔøΩY\u0016ÔøΩ…êpa\u0018-\u0010\u001fWÔøΩÔøΩlÔøΩÔøΩ/ÔøΩÔøΩ9wÔøΩ\n\u0005ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ_{ÔøΩÔøΩ)ÔøΩ‰•ê5ÔøΩ\u0011T\u0004ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0015ÔøΩÔøΩ–íÔøΩﬂùÔøΩÔøΩ\u0002ÔøΩskÔøΩ\nÔøΩEÔøΩ\nLÔøΩÔøΩjy5\\ÔøΩÔøΩÀû\u001a⁄≠ÔøΩBÔøΩwÔøΩrÔøΩÔøΩÔøΩÔøΩk6\u001f\u001a<q-|ZÔøΩ\u0017G\u0014ÔøΩÔøΩÔøΩmÔøΩz2ÔøΩ\"<MÔøΩÔøΩ‡£ºÔøΩBÔøΩ\u0013ÔøΩ:[T&ÔøΩÔøΩ,ÔøΩx\tÔøΩÔøΩ_ÔøΩÔøΩUÔøΩÔøΩÔøΩ{_\n\u0005ÔøΩÔøΩÔøΩÔøΩ\u001b7ÔøΩ<vÔøΩÔøΩÔøΩÔøΩ{NÔøΩ]ÔøΩ\u001b2ÔøΩÔøΩÔøΩ\u0007A\u0010E42ÔøΩN.ÔøΩxÔøΩÔøΩ\u0007ÔøΩÔøΩÔøΩÔøΩ\bX4ÔøΩÔøΩÔøΩKvÔøΩÔøΩÔøΩÔøΩÔøΩ3CG}ÔøΩ\u0001~\n<\u0000|\u0017dÔøΩ\u0013fmÔøΩ\u000f\\ÔøΩÔøΩÔøΩq\u0004/j\b<nj[ÔøΩcƒäÔøΩWÍò£ÔøΩJ]ÔøΩÔøΩ[ÔøΩÔøΩe4ÔøΩ-ÔøΩiÔøΩ<QÔøΩOpcIÔøΩKÔøΩÔøΩMÔøΩ,mÔøΩJÔøΩÔøΩ\bÔøΩ\u0006ÔøΩÔøΩJeÔøΩÂ¶áŒìÔøΩÔøΩqnÔøΩÔøΩDÔøΩÔøΩ6HkÔøΩÔøΩC&ÔøΩ\tÔøΩ\u0018\u001ak2\nÔøΩQ)<vBÔøΩÔøΩ“âÔøΩ\u0019ÔøΩ,\b\bÔøΩ\u0007ÔøΩb\b\u0001\u0007ÔøΩÔøΩ+ÔøΩÔøΩÔøΩI\u0012ÔøΩ\tÔøΩDÔøΩÔøΩÔøΩÔøΩ|ÔøΩOÔøΩÔøΩxÔøΩFs)\n\u0002XjÔøΩOqÔøΩUe1{\u0018)ÔøΩA\u0012›¥mmÔøΩ65ÔøΩ$F\bhÔøΩc\u0019ÔøΩÔøΩFÔøΩÔøΩÔøΩÔøΩ&UÔøΩkcÔøΩ?k$ÔøΩ!VL1\u0019\u0013ÔøΩÔøΩAnÔøΩ\u0012~\n\u00025O\nÔøΩ%ÔøΩÔøΩÔøΩ\u0013ÔøΩÔøΩÔøΩ%0ÔøΩ(0ÔøΩ}ÔøΩ=-ÔøΩÔøΩ\n4Ê®åÔøΩÔøΩJp?\u0011!ÔøΩ\u0013Ã†ÔøΩ1ÔøΩÈôâ@ÔøΩ\u00132ÔøΩ0yÔøΩZÔøΩÔøΩiEÔøΩÔøΩ9thÔøΩÔøΩÔøΩ<ÔøΩgÔøΩ\u0012ÔøΩ3ÔøΩvvÔøΩ\u00194dmÔøΩÔøΩjP5ÔøΩÔøΩNŸéY|\u0019ÔøΩÔøΩÂà∂ÔøΩ\u0007ÔøΩJÔøΩÔøΩÔøΩ\u0014ÔøΩÔøΩ\u0013V3hÔøΩ+ÔøΩaBÔøΩY\u0012eÔøΩÔøΩÔøΩf\nÔøΩ\bÔøΩ\u0004ÔøΩ\u0014\u0002MÔøΩÔøΩ;\u0001BÔøΩÔøΩ>?ÔøΩÔøΩ%ÔøΩÿóÔøΩÔøΩÔøΩƒ©\nÔøΩÔøΩsÔøΩ~”éÔøΩÔøΩ_\u001f\u001b\u0012ÔøΩÔøΩÔøΩÔøΩ>\u0003&ÔøΩ\u0019ÔøΩÔøΩFÔøΩ.ÓºøÔøΩ◊ΩÊ≥èÔøΩÔøΩ=ÔøΩ\u0006ÔøΩ_ÔøΩÔøΩN=\"ÔøΩApaMZ◊¥~ÔøΩÔøΩÔøΩÔøΩ/]ÔøΩ\nÔøΩÔøΩk7\n1eÔøΩÔøΩÔøΩLoÔøΩH?\u0003ÔøΩ-*ÔøΩÔøΩKÔøΩ\nÔøΩa`ÔøΩÔøΩYÔøΩ7ÔøΩ<vÔøΩÿπyÔøΩvÔøΩÔøΩÔøΩÔøΩ6O\u000fÔøΩm,9ÔøΩÔøΩﬁºsÔøΩœªÔøΩ\u0004IwÔøΩÔøΩ\u0007AÔøΩRÔøΩQ\u0013RÔøΩÔøΩÔøΩÔøΩyÔøΩ·∑∫ÕÑÔøΩÔøΩqpnÔøΩgÔøΩÔøΩTTÔøΩ?ÔøΩÔøΩ#ÔøΩÔøΩÔøΩÔøΩaÔøΩ—≤'\u0007{ÔøΩ\u0002ÔøΩÔøΩVÔøΩ\u001b\nÔøΩH~ ÔøΩÔøΩjveCLÔøΩÔøΩÔøΩ?ÔøΩÔøΩ\u0019ÔøΩuÔøΩA\nÔøΩÔøΩÔøΩ{ÔøΩ;ÔøΩ\u0004&d\nyÔøΩÔøΩÔøΩÔøΩZ)`ÔøΩ!%ÔøΩÔøΩÔøΩÔøΩ\u0010ÔøΩ#7√ÉCBÔøΩ\u0004ÔøΩ`ÔøΩ\n7QÔøΩ*6\n@ÔøΩ%\bÔøΩ5ÔøΩœÑÔøΩ&&\n\u0013\"g\n+ÔøΩ\u0002mmÔøΩ5ÔøΩÔøΩhT[8ÔøΩ2ÔøΩÔøΩÔøΩEx\"ÔøΩC`\u0004ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ1ÔøΩÔøΩQÔøΩg$UÔøΩ\u0000ÔøΩ-ÔøΩ&ZÔøΩÔøΩÔøΩÔøΩÔøΩ\u000fÔøΩfÔøΩ\u0013ÔøΩ4ÔøΩ\u001a0—¶1›π\t≈†S\u0007\u0016\n/SËõîÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩDskOÔøΩ\u0013ÔøΩ\u0019<RhÔøΩÔøΩt/cÔøΩÔøΩpI4\b^ÔøΩÔøΩÔøΩ/ÔøΩÔøΩ\u0010(;\u0004ÔøΩ9/5)N\u0006ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩF}ÔøΩÔøΩ\u000fÔøΩÔøΩF4tgÃ∂ÔøΩSr5ÔøΩÔøΩÔøΩÔøΩmh\u0003atLH\u0002{ÔøΩÔøΩÔøΩgÔøΩÔøΩl\nÔøΩÔøΩ’õÔøΩ\nÔøΩÔøΩpDÔøΩÔøΩ#d,ÔøΩ⁄ú\\ÔøΩÔøΩ@\u0014ÔøΩ$ÔøΩtÔøΩ$FÔøΩÔøΩcNÔøΩÔøΩ>GBÔøΩ\u0006,√∂ÔøΩhAÔøΩl\u00106ÔøΩÔøΩ“èrÔøΩtRÔøΩ\u001fÔøΩUÔøΩOkÔøΩÔøΩÔøΩ\u0012dÔøΩÔøΩ.ÔøΩA\u0006ÔøΩrÔøΩÔøΩÔøΩ.\u0003¬ÉÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩﬂåm{ÔøΩÀ≠[XÔøΩ6ÔøΩ\u000e}ÔøΩÔøΩÔøΩp.ÔøΩÔøΩÔøΩ?nupÔøΩÔøΩ[ÔøΩnÔøΩ\bÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ^ÔøΩ\u0017ÔøΩÔøΩwÔøΩÔøΩo\nVÔøΩÔøΩÔøΩ\u001aÔøΩÔøΩÔøΩ7ÔøΩÔøΩ5u5ÔøΩZÔøΩÔøΩÔøΩ=ÔøΩeÔøΩoÔøΩÔøΩu[ÔøΩ\n8rfÔøΩÔøΩ]ÔøΩ\u0011sÔøΩ,ÔøΩÔøΩÔøΩka\u0005ÔøΩl>tKÔøΩÔøΩn7KÔøΩ”∞6ÔøΩ8ÔøΩN\u0005fÔøΩpÔøΩ-ÔøΩ’ü\u0007ÔøΩÔøΩUGXÔøΩÔøΩ?ÔøΩÔøΩ+\u0016\u001fÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u001bÔøΩÔøΩ\u0000ÔøΩ{ÔøΩÔøΩ\n\u001agQDÔøΩÔøΩP~~5&KÔøΩÔøΩ~,ÔøΩ`\u0018ÔøΩ\nÔøΩÔøΩ\nuÔøΩ2ÔøΩÔøΩÔøΩ\"ÔøΩ),ÔøΩfÔøΩ\nÔøΩÔøΩsÔøΩ2ÔøΩhÔøΩ*ÔøΩÔøΩÔøΩ\u0014ÔøΩFÔøΩ#ÔøΩ'+ÔøΩÔøΩÔøΩ\b4\u0018\u0016ÔøΩ0ÔøΩ`ÔøΩÔøΩÔøΩÔøΩ\u001aT,ÔøΩ≈¥ÔøΩÔøΩÔøΩÔøΩ!ÔøΩ%LÔøΩ\u001fÔøΩ\u0001ÔøΩÔøΩN\u0004ÔøΩÔøΩ\u0012\"ÔøΩAÔøΩ ÔøΩÔøΩCÔøΩÔøΩ3ÔøΩXÔøΩJÔøΩ\u0001xÔøΩy»øÔøΩCJÔøΩ\nÔøΩrLÔøΩÔøΩÔøΩ\u0015)BLÔøΩ>ÔøΩ\nN)ÔøΩÔøΩYÔøΩ#mÔøΩqÔøΩoÔøΩsÔøΩ@=ÔøΩ\baÔøΩÔøΩÔøΩÔøΩ\nZ_,\u0001;'ÔøΩÔøΩ#LXÔøΩTIÔøΩÔøΩlÔøΩHÔøΩÔøΩO'=M\n*]ÔøΩfÔøΩ&ÔøΩ],‹≤\tÔøΩÔøΩÔøΩ!ÔøΩ1@ÔøΩ\u0002ÔøΩ\u0000\u0017ÔøΩÔøΩ!pÔøΩlÔøΩÔøΩÔøΩMÔøΩÔøΩ <–∏KÔøΩÔøΩNÔøΩÔøΩÔøΩÔøΩ\u001asI}=pÔøΩ\u0006b`ÔøΩÔøΩKÔøΩ$6^ÔøΩ\u0011ÔøΩAÔøΩIÔøΩ\u000eÔøΩÔøΩÔøΩqÔøΩIq2`{\"\u001f◊®L]!ÔøΩÔøΩ\t)ÔøΩÔøΩÔøΩ\u0015ÔøΩ\t4ÔøΩÿÜavt%ÔøΩ\u0018ÔøΩ2'za\nKaXÔøΩÔøΩU\bÔøΩ\u0000MÔøΩ^ÔøΩ0ÔøΩO8z>ÔøΩÔøΩwÔøΩÔøΩUÔøΩÔøΩ._`ÔøΩwkWo<ÔøΩÔøΩ\u0007ÔøΩÔøΩÔøΩÔøΩ\t/ÔøΩÔøΩÔøΩÔøΩ\u001bÔøΩj\u000eÔøΩmÔøΩ4\u0013ÔøΩÔøΩÔøΩÔøΩ9ÔøΩ?ÔøΩ>;ÔøΩÔøΩœß)CDÔøΩﬂÇ.ÔøΩ}\u0003\u0016\u001f8zfÔøΩSÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ,ÔøΩ\u0015ÔøΩÔøΩ~ÔøΩÔøΩÔøΩ.ÔøΩuÔøΩÔøΩÔøΩÔøΩ{ÔøΩÔøΩÔøΩmÔøΩ_ÔøΩÔøΩ-\u0017FÔøΩÔøΩ|OÔøΩnÔøΩ\u0016ÔøΩÔøΩÔøΩ]<ÔøΩÔøΩÔøΩÔøΩTÔøΩ”Æ\nÔøΩ|ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ*bCÔøΩ\nÔøΩ9ÔøΩÔøΩÔøΩ[\u0017ÔøΩÔøΩÔøΩuÔøΩÔøΩÔøΩ._ÔøΩ5ÔøΩÔøΩÔøΩÔøΩuÔøΩÔøΩ\u000eÔøΩzNÔøΩ úÔøΩ|ÔøΩqÔøΩ\nMÔøΩ2}ÔøΩ\u0003+W\tÔøΩKÔøΩÔøΩD{\u001b\u000fÔøΩÔøΩÔøΩ]JÔøΩÔøΩÔøΩ\u0018~ÔøΩ.ÔøΩÔøΩO3ÔøΩ√´1ÔøΩ8ÔøΩ5X2ÔøΩÔøΩ—¨ÔøΩd;ÔøΩ'q^\u0005ÔøΩŸömÔøΩnn\u0000?yÔøΩ\u0007ÔøΩ6ÔøΩ`ÔøΩÔøΩÔøΩs@ÔøΩÔøΩkPvÔøΩ‹ûÔøΩjÔøΩÔøΩÔøΩ—úÔøΩZÔøΩÔøΩ\u0010ÔøΩÔøΩ\u000e!ÔøΩ\nEÔøΩÔøΩÔøΩ\u0014\u0001ÔøΩ≈ñe7ÔøΩÔøΩ\u001bÔøΩÔøΩÔøΩ–üxÔøΩ\u0018ÔøΩÔøΩHÔøΩÔøΩ'KÔøΩt8ÔøΩÿ¥ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\t,,ÔøΩÔøΩXaÔøΩÔøΩÔøΩÔøΩL\u0001IÔøΩ\t\u00123.\u0014\u0002\bÔøΩÔøΩ\u0013ÁÇ≠“§uÔøΩ9ÔøΩÔøΩ\n2ÔøΩsPÔøΩÔøΩIÔøΩ\u0012ÔøΩ\u0004\u0000ÔøΩÔøΩx:ÔøΩÔøΩÔøΩÔøΩ'\u0019H\nÔøΩÔøΩ=ÔøΩ2ÔøΩÔøΩÔøΩ4ÔøΩ4ÔøΩÔøΩhÔøΩNz«™ÔøΩ_ÔøΩD;ÔøΩ\u000f\"\u0013ÔøΩ\u0019ÔøΩ}ÔøΩ\nÔøΩ<ÔøΩÔøΩ’æ’µÔøΩÔøΩ≈π*1ÔøΩ\u0014\n'NÔøΩhÔøΩÔøΩÔøΩS \u001a\nÔøΩÔøΩ\u0006ÔøΩ4\nÔøΩÔøΩ\u0014ÔøΩ#ÔøΩm\u0015ÔøΩSh\n;ÔøΩKaÔøΩOÔøΩÔøΩDÔøΩÔøΩq,\u0004ÔøΩ(ÔøΩÔøΩÿîÔøΩ\\\u001bÔøΩfs\u0001ÔøΩÔøΩ\nÔøΩa`ÔøΩÔøΩÔøΩ.sÔøΩ\nÔøΩ;sÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÁ•ëÔøΩÔøΩﬁ≤dÔøΩÔøΩWÔøΩÔøΩ·¶∂5!ÔøΩ\u000eÔøΩÔøΩÔøΩ='>ÔøΩÔøΩÔøΩÔøΩ\t+ÔøΩeÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0004ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩo<pÔøΩÃä\n\u0007ÔøΩÔøΩvÔøΩmwÔøΩÔøΩ◊ôÔøΩ\u0013 ÔøΩÔøΩ*}jHÔøΩ~ÔøΩ6ÔøΩ<ÔøΩÔøΩÔøΩÔøΩ\n#ÔøΩÔøΩ?ÔøΩ?I\u001a\u0007ÔøΩÔøΩÔøΩ7ÔøΩuykÔøΩÔøΩÔøΩÔøΩyÔøΩÔøΩÔøΩp4\u0017w\nÔøΩnÎëõÔøΩÔøΩÔøΩÔøΩÔøΩKÔøΩ\u000fvÔøΩCÔøΩÔøΩÔøΩ8kÔøΩÔøΩÔøΩ{^ÔøΩlZvIÔøΩÔøΩÔøΩÔøΩ_ÔøΩrMÔøΩÔøΩ_;zÔøΩ\n,ÔøΩ\n!ÔøΩ\u0013ÔøΩÔøΩ4\u0011ÔøΩ?&ÔøΩÃ•ÔøΩ-ZBX1\u0016ÔøΩuÔøΩÔøΩÔøΩÔøΩÔøΩo_RÔøΩÔøΩÔøΩ÷àÔøΩ*ÕÉ\u0004ÔøΩÔøΩÔøΩÔøΩ*o:ÔøΩŸ°uÔøΩ~\u0011-ÔøΩ?ÔøΩÔøΩÔøΩÔøΩÔøΩ\n\u0019\bÔøΩ\nB\u0015ÔøΩR^>IÔøΩ∆∞$ÔøΩF4ÔøΩ`ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0014\n\u0017bÔøΩÔøΩÔøΩp\bÔøΩIW\u0018ÔøΩÔøΩÔøΩXÔøΩÔøΩ@NÔøΩ–†ÔøΩrKÔøΩe%g–åÔøΩRÔøΩ7+ÔøΩÔøΩÔøΩÔøΩPh\u0015lx\t4\nY<ÔøΩz1ÔøΩr\bcÔøΩÔøΩVLÔøΩÔøΩ\u0019q@ÔøΩÔøΩÔøΩ€ï&ÔøΩÔøΩÔøΩw`oÔøΩ\nÔøΩÔøΩ\u0017÷Ä}ÔøΩ\u001fgÔøΩÔøΩÔøΩÔøΩ\u0017ÔøΩ\u001bÔøΩ3h[ÔøΩÔøΩJÔøΩÔøΩÔøΩD\b2ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ=\u0005ÔøΩÔøΩÔøΩh1,ÔøΩU\u0002\u0019t\u0012IÔøΩÔøΩÔøΩ$ÔøΩ~,\nÔøΩ<ÔøΩÔøΩ4<ÔøΩÔøΩ\nZOÔøΩÔøΩa\u000em\b\u0007ÔøΩÔøΩL,fÔøΩÔøΩ3?yÔøΩÔøΩÔøΩu[Ãù/ÔøΩÔøΩs\u0014JÔøΩiEÔøΩÔøΩ»™pQ÷üÔøΩH\u0002ÔøΩqÔøΩ\u0000BmÔøΩSÔøΩÔøΩq\u0000ÔøΩÔøΩÔøΩÔøΩÔøΩOÔøΩÔøΩ(ÔøΩIÔøΩXÔøΩÔøΩÔøΩÔøΩs/|:\u0015ÔøΩÔøΩ[:ÔøΩv}g¬®iÔøΩ\\ÔøΩÔøΩÔøΩÔøΩ\nÔøΩ\u001f\u001fÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ@^ÔøΩQpÔøΩÔøΩÔøΩÔøΩÔøΩlÔøΩÔøΩw&dQÔøΩÔøΩyhŸìÔøΩ\u0007O\\ÔøΩÔøΩÔøΩS\nVÔøΩOk÷ÆÔøΩvÔøΩ!\u001bSwÔøΩ\u0013ÔøΩÔøΩÔøΩ}?ÔøΩÔøΩ^ÔøΩÔøΩÔøΩÔøΩ='ÔøΩ\u0003ÔøΩÔøΩÔøΩÔøΩ> ãÔøΩ<ÔøΩÔøΩÔøΩÔøΩÔøΩ~{ÔøΩÔøΩÔøΩ6\nÔøΩsÔøΩÔøΩÔøΩ6(|tÔøΩÔøΩ\u0015ÔøΩ`ÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩ:ÔøΩ[\u0006«æ`ÔøΩWÔøΩÔøΩ\nÔøΩÔøΩk\u0011uÔøΩÔøΩ\u001bÔøΩ\u001fmÔøΩÔøΩpmÔøΩÔøΩ2vÿÆÔøΩÔøΩ\u0019ÔøΩ&b\u0019ÔøΩÔøΩÔøΩÔøΩTÔøΩQÔøΩ\u001a%ÔøΩ!ÔøΩÔøΩ\nÔøΩ6ÔøΩ\nÔøΩRÔøΩÃü^dÔøΩÔøΩZJÔøΩ/ÔøΩÔøΩ\u0000\u0005ÔøΩ\u0018ÔøΩic)ÔøΩÔøΩ5WuÔøΩÔøΩ<≈úq«àÔøΩÔøΩ/5\u001aGlÔøΩdÔøΩÔøΩoÔøΩ=ÔøΩ\bÔøΩ/ÔøΩÔøΩÔøΩÔøΩ:u\u0011ÔøΩÔøΩCTt}rÔøΩJ\u0012ÔøΩJ$c\nÔøΩ S%ÔøΩ!ÔøΩR246ÔøΩÕØHVhs'ÔøΩ\u001fzÔøΩÔøΩ\nOÔøΩÔøΩƒïi\u0012hZf\u0014ÔøΩ<=\"ÔøΩÔøΩÔøΩ-ÔøΩ\u0016 ÆiR.bPOÔøΩ`f'ÔøΩI2uÔøΩ]ÔøΩ5\u0013\u0003ÔøΩ\u001a.\tÔøΩÔøΩ2NÔøΩ$n*5ÔøΩÔøΩÔøΩÔøΩ\u0015UÔøΩ÷¢ÔøΩ\n”™e—§w*B\u0015ÔøΩ\u0015\n10ÔøΩÔøΩÔøΩ*\u0016ÔøΩ(<\nÔøΩÔøΩo\tÔøΩ\u0006*ÔøΩ⁄µ\u001fHÔøΩÔøΩÔøΩÔøΩÔøΩf\u0014ÔøΩÔøΩÔøΩÔøΩA+`ÔøΩÔøΩÔøΩÔøΩÔøΩ/ÔøΩ7ÔøΩÔøΩ\tÔøΩÔøΩ|ÔøΩ\u0019FaÂóöw/.ÔøΩp$j_?#Õ•ÔøΩT ÔøΩÔøΩIEdÔøΩSÔøΩÔøΩ3ÔøΩD”âlÔøΩ@ÔøΩÔøΩEÔøΩÔøΩ[ÔøΩÔøΩ«û>{ÔøΩÔøΩxÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u000eÔøΩ\u001fzmÔøΩÔøΩ}'\nÔøΩœ®ÔøΩ2ÔøΩ]MÔøΩT\u0017t\u00190tÔøΩCGÔøΩÔøΩÔøΩbÔøΩ\u0003ÔøΩ\u0019ÔøΩNÔøΩ`%ÔøΩ\u000f\u001a6ÔøΩÔøΩÔøΩ‡•∑vÔøΩÔøΩLÔøΩTÔøΩ\nÔøΩ\u0007.>yÔøΩÔøΩ\u0003ÔøΩÔøΩÔøΩ5ÔøΩVÔøΩqÔøΩ\u0011\u000fÔøΩW“êÔøΩÔøΩÔøΩfÔøΩ9wÔøΩÔøΩÔøΩ3/1M&…Ñ#ÔøΩÔøΩÔøΩŸüuÔøΩq¬¨M#ÔøΩÔøΩ'pMÔøΩÔøΩÔøΩÔøΩ=hÔøΩÔøΩÔøΩ'ÔøΩÔøΩ]ÔøΩÔøΩ›≥√à\\\nÔøΩ;ÔøΩÔøΩeÔøΩ\u000eLÔøΩÔøΩ\u0015^ÔøΩÔøΩÔøΩ\u0006ÔøΩ:sÔøΩ\u001aV$ÔøΩÔøΩÔøΩ_ÔøΩÔøΩÔøΩÔøΩ{{'ÔøΩÔøΩ\u0019ÔøΩÔøΩÔøΩÔøΩ\u001fc>\u0013ÔøΩ\u000eÔøΩÔøΩÔøΩB37ÔøΩÔøΩÔøΩÔøΩ0ÔøΩPÔøΩwÔøΩ>ÔøΩY\u0011dÔøΩÔøΩ\u0016ÔøΩ\u0006ÔøΩÔøΩ\u0005ÔøΩÔøΩ^IÔøΩ8Q?ÔøΩ\u000ePhÔøΩYZ’ÅÔøΩÔøΩ1ÔøΩÔøΩQ †-=‘∏=jÔøΩ|6Q&ÔøΩ`ÔøΩÔøΩÔøΩ\nÔøΩÔøΩÏöôÔøΩ2WË¥¶ÔøΩHkr\u0001\nÔøΩÔøΩ\u000eÔøΩ9}ÔøΩRÔøΩÔøΩÔøΩÔøΩÔøΩ6ÔøΩÔøΩÔøΩ-ÔøΩŒúÔøΩÔøΩ◊©\u0004ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0004\ncbÔøΩÔøΩ\u0012qÔøΩ‘ûÔøΩ.4ÔøΩyÔøΩ\nVCÔøΩÔøΩÔøΩ.ÔøΩÔøΩ\u001aÔøΩÔøΩx\u000fhÔøΩ7ÔøΩÔøΩiÔøΩto*iÔøΩeÔøΩÔøΩÔøΩZfÔøΩ>ÔøΩ\u0013@EÔøΩEÔøΩq√òÔøΩ@wÔøΩÔøΩÔøΩÔøΩxÔøΩÔøΩdÔøΩ\u0014\u000eÔøΩÔøΩu\u001fÔøΩ?ÔøΩG\n|ÔøΩÔøΩ\u000eÔøΩÔøΩÔøΩÔøΩ#z\u000f[\u0006ÔøΩeÔøΩÔøΩx%DmF%\u0002\u0004ÔøΩÔøΩ\tÔøΩhÔøΩÔøΩ\u0012ÔøΩ\u0011cxZÔøΩaa53^ÔøΩ\u0018ÔøΩÔøΩÔøΩLÔøΩÔøΩÔøΩÔøΩ.oÔøΩÔøΩÔøΩÔøΩÔøΩ¬≤ÔøΩzZÔøΩbÔøΩÔøΩ…´ÔøΩÔøΩÔøΩD\nÔøΩÔøΩX_LÔøΩ\u0017ÔøΩÔøΩs√∂ÔøΩ:^W_ÔøΩ\n”ôÔøΩvÔøΩ2D\u0002ÔøΩ|4iÔøΩÔøΩÔøΩ«≠ÔøΩÔøΩÔøΩ\u001194s\bo\nÔøΩÀüTÔøΩÔøΩnÔøΩÔøΩ)ÔøΩÔøΩ\n]ÔøΩÔøΩqÔøΩwÔøΩA\nÔøΩÔøΩÔøΩÃíÔøΩ\nÔøΩ?ÔøΩÔøΩ‰Ö´ÔøΩÔøΩÔøΩvÔøΩ”öÔøΩÔøΩÔøΩÔøΩnÔøΩÔøΩÔøΩ7ÔøΩDÔøΩgÔøΩÔøΩ\u0007ÔøΩT\u000fYÔøΩÔøΩÔøΩÔøΩi?mÔøΩÔøΩÔøΩ)ÔøΩrÔøΩÔøΩ(ybÔøΩ\u0013ÔøΩM|‰ç±ÔøΩÔøΩÔøΩ8ÔøΩ\nÔøΩ\u0017ÔøΩÔøΩÔøΩoaÔøΩ\u0017G4m[ÔøΩVÔøΩ=ÔøΩÔøΩÔøΩÔøΩÔøΩEÔøΩÔøΩ`ÔøΩÔøΩÔøΩy\u0012;YÔøΩ\u0017ÔøΩoÔøΩqÔøΩ;ÔøΩuuÔøΩÔøΩhÔøΩÔøΩ÷™ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0014ÔøΩ\u0015kLF2ÔøΩ5F22\u0014ÔøΩÔøΩ657ÔøΩƒåeÔøΩÔøΩDÔøΩÔøΩ\u0007\u0016ÔøΩÔøΩ<\u0018fAÔøΩÔøΩÔøΩC\\ÔøΩÔøΩD:—îÔøΩÔøΩCÔøΩPÔøΩ1\bAÔøΩ ´ÔøΩMÔøΩ*f\u0000ÔøΩÔøΩXÔøΩ\u0010\u0010—ïDÔøΩÔøΩÔøΩ\u0007ÔøΩLÔøΩ/^ÔøΩÔøΩHkKDTÔøΩ\n!\n\nhs@\u001b(@ÔøΩÔøΩÔøΩQ}ÔøΩ{ÔøΩLÔøΩÔøΩÔøΩ\\ÔøΩ?.fvÔøΩÔøΩÔøΩÔøΩÔøΩ\u0006Gd2jOÔøΩ3dhÔøΩ\u0004ÔøΩfÔøΩiCÔøΩ4ÔøΩ0ÔøΩEÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩ'”ëÔøΩ ∞ÔøΩÔøΩCSÔøΩÔøΩ\u0013UÔøΩÔøΩ\u0013ÔøΩÔøΩ\u0016ÔøΩÔøΩÔøΩÔøΩjÔøΩÔøΩÔøΩlÔøΩÔøΩÔøΩÔøΩÔøΩ^ÔøΩ\u0012eR\nÔøΩÔøΩÔøΩ4)HÔøΩÔøΩÔøΩ\u0019ÔøΩ\u0013\bÔøΩ\u0015rÔøΩÔøΩÔøΩÔøΩUe\u0001AmÔøΩ\u0007ÔøΩ=ÔøΩÔøΩsÔøΩ6ÔøΩ>Áõ≤ÔøΩ\u0014W0œøÔøΩWI\u0010—òÔøΩÎ§Å%ÔøΩ\"\u0019ÔøΩX\bXSÔøΩÔøΩÔøΩ\u0018\nÔøΩJÔøΩÔøΩ\nÔøΩQÔøΩÔøΩÔøΩpÔøΩ5ÔøΩ7u~\u0017kÔøΩÔøΩÔøΩÔøΩÔøΩ'ÔøΩÔøΩÔøΩk7ÔøΩ8ZGÔøΩÔøΩ◊Æ][ÔøΩÔøΩÔøΩ-\n\u001a\u0012^ÔøΩ7ÔøΩÔøΩÔøΩO{\u000eÔøΩjÔøΩÔøΩ\u00107”õSlﬂ¥mmÔøΩG\u0007|\u0014ÔøΩÔøΩsﬂØÔøΩFÔøΩÔøΩÔøΩ\u0013v\u0005d”ìÔøΩnÔøΩtÔøΩnÔøΩ\u000f\u001boÔøΩÔøΩÔøΩÔøΩ\u0003ZÔøΩQ\u0012\nÔøΩ)\nÔøΩPÔøΩ\u0003ÔøΩ\tÔøΩ2gÔøΩsÔøΩ]\u000e\u000e^\u0002?\u0000A◊õÔøΩÔøΩÔøΩnÔøΩÔøΩÔøΩ\u0007OÔøΩ\n]\u00067ÔøΩÔøΩ\\ÔøΩ√§ÔøΩ5,⁄ÇU{'ÔøΩÔøΩ|{ÔøΩÔøΩ!/ÔøΩÔøΩÔøΩÔøΩÔøΩpÔøΩÔøΩﬂø6uÔøΩ68LUÔøΩ|ÔøΩ>ÔøΩÔøΩ›°ÔøΩXkFÔøΩ‹∂\u0014nJ\u0016SÔøΩ7ÔøΩÔøΩK\nQDÔøΩ÷íÔøΩ!ÔøΩVf\u0018*ÔøΩÿíAÔøΩZÔøΩA'\u0011e\u0019+ÔøΩ\u0001\nyÔøΩmÔøΩb\u0015uXÔøΩKÔøΩ\nÔøΩ`ÔøΩ/6TÔøΩÔøΩrÔøΩÔøΩM\"\ngÔøΩIcÔøΩ{ÔøΩ\bÔøΩ\nÔøΩAÔøΩ\u0014\u0002ÔøΩÔøΩ\u0012ÔøΩKÔøΩ\u0010ÔøΩÔøΩ\u0004 £/ÔøΩKÔøΩÔøΩ6ÔøΩÔøΩ\u001aÔøΩÔøΩq\u0006-ÔøΩ8#*ÔøΩIÔøΩÔøΩz9ÔøΩeÔøΩ,\u0016∆íi\npÔøΩ<?*(ÔøΩ\u0016#ÔøΩyÔøΩU]ÔøΩ$\nÔøΩ\nÔøΩ\u0018ÔøΩÔøΩhfÔøΩÔøΩÔøΩ6q]ÔøΩÔøΩuÔøΩaÔøΩÔøΩ#ÔøΩÔøΩ$iÔøΩ6i]ÔøΩ\u0018ÔøΩÔøΩ?ÔøΩBÔøΩeÔøΩÔøΩ5\u0018G\bnIQN8ÔøΩrÔøΩÔøΩBg\u0018#ÔøΩrQRoÔøΩÔøΩÔøΩfÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩLÔøΩÔøΩÔøΩÔøΩ\u000e\u00023EÔøΩ*\u0004ÔøΩZHÔøΩ›∫ÔøΩ&\u001aÔøΩ\nÔøΩÔøΩ?ÔøΩ“ê1ÔøΩÔøΩjÔøΩM\u0012]ÔøΩÔøΩÔøΩsbeÔøΩÔøΩ\u001b\u0017\u0012Tt8ÔøΩÔøΩÔøΩ$SÔøΩ\u0010ÔøΩ ÄÔøΩwÔøΩt\u001a\n1X6ÔøΩ\nÔøΩÔøΩÃÖ;ÔøΩÔøΩÔøΩÔøΩ◊ÆÔøΩÔøΩ|ÔøΩ\u000fÔøΩmtUÔøΩ\u0015wÔøΩÔøΩÔøΩÔøΩoÔøΩÔøΩXw`ÔøΩÔøΩÔøΩ\u000f99ÔøΩHÔøΩQÔøΩÔøΩ’èÔøΩ9nÔøΩÔøΩ5+ÔøΩ\u001fÔøΩaÔøΩÔøΩ-ÔøΩÔøΩÔøΩrÔøΩÔøΩKÔøΩMÔøΩ,ÔøΩmÔøΩyÔøΩ\u0005ÔøΩ?\"OÔøΩ\nÔøΩY\u0004\u0006-9ÔøΩÔøΩÔøΩMÔøΩVÔøΩ?8ÔøΩÔøΩ\nO8If\nÔøΩ\nb$pÔøΩ\u001fÔøΩg\u001f<zÔøΩÔøΩ^ÔøΩÀû\u001a\u0012ÔøΩ&¬ß=ÔøΩ…î_OÔøΩ8ÔøΩÔøΩ\u0017/]ÔøΩÔøΩznÔøΩPÔøΩ#q\nX&÷©ÔøΩ$ÔøΩ\u0003\u001fÔøΩÔøΩ\u000f&ÔøΩKÔøΩÔøΩÔøΩÔøΩ+ÔøΩBÔøΩQÔøΩÔøΩÔøΩ\\ÔøΩ\u0019ÔøΩÔøΩIÔøΩÔøΩxÔøΩ4ÔøΩ\u0001ÔøΩzÔøΩ\u0014ÔøΩjÔøΩ\u0016ÔøΩcE&\nÔøΩƒéÔøΩ<\u0004›µ\u0018ÔøΩ,ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩq>ÔøΩyÔøΩÔøΩ\"{ÔøΩÀ®Z[ÔøΩ$\u0000ÔøΩÔøΩÔøΩ\u0019qÔøΩR[chÔøΩ\u0012ÔøΩÔøΩ\u0000mvÔøΩÔøΩ6ÔøΩ]=$\nP`ÔøΩÔøΩ\u0017\u0010¬ÆÔøΩÔøΩÔøΩQÔøΩÔøΩÔøΩÔøΩ[\u000f'1ÔøΩ\nÔøΩ\u0004ÔøΩn|ÔøΩU\u0012hÔøΩÔøΩvÔøΩ4qÔøΩTÔøΩÿù}4<AÔøΩÔøΩI7H)ÔøΩ[ FÔøΩ5ÔøΩLÔøΩ2h\u0002ÔøΩ\"ÀçÔøΩÔøΩÔøΩÔøΩ\u001bÔøΩ\u001fdÔøΩ\u0016cx\u0011&ÔøΩÔøΩ\u0001\u0019ÔøΩ\u0012ÔøΩ,ÔøΩÔøΩÔøΩ\u0006\u000eÔøΩ\u0014ÔøΩ\\\u0007“ßÔøΩÔøΩlÔøΩ4.\u0000ÔøΩ[Qy3\u0016;\u0010ÔøΩ\u00110ÔøΩbF<\u0012EÔøΩÔøΩfÔøΩÔøΩ\t\u0003mL\nÔøΩÔøΩÔøΩ3ÔøΩLÔøΩ\u00196ƒ∑ÔøΩkW‰πÆÔøΩ%\u0015ÔøΩ8y\u0014EÔøΩrÔøΩ\u0004[ÔøΩF2ÔøΩ0ÔøΩ\u0016$ÔøΩ)ÔøΩÔøΩ=GÔøΩÔøΩÔøΩ\u0013ÔøΩÔøΩgÔøΩ]ÔøΩÔøΩÔøΩ!ÔøΩsÔøΩ+ÔøΩ6ÔøΩ<6vÔøΩ∆∑ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0011?ÔøΩÔøΩhÔøΩÔøΩMÔøΩÔøΩ<ÔøΩÔøΩÔøΩÔøΩlŸºÔøΩÔøΩÔøΩÔøΩÔøΩ\nzmL”∂5(ÔøΩi\u001fÔøΩ*!ÔøΩÔøΩ\u000eÔøΩÔøΩ <ÔøΩÔøΩ?ÔøΩÔøΩÔøΩ^ÔøΩ!wÔøΩÔøΩlÔøΩk_ÔøΩ–¨mm4ÔøΩ∆Ø^ÔøΩ.\u0003ÔøΩÔøΩtJÔøΩQ+ÔøΩn>ÔøΩvÔøΩÔøΩ!ÔøΩafqÔøΩÀûÔøΩÔøΩÔøΩs\u001bÔøΩÔøΩŸã\u000fÔøΩ66∆ïÔøΩhÔøΩT;ÔøΩ|\u001aÔøΩ\u0005bÔøΩz/ÔøΩKÔøΩ\u0004p\tÔøΩÔøΩ(ÔøΩ{^\u0012lÔøΩÔøΩL2aÔøΩ\u0019ÔøΩ\u0011zÔøΩÔøΩp\u001fÔøΩ9[ÔøΩÔøΩ\bÔøΩÔøΩ◊èÔøΩMÔøΩ$XJ‘â2/ÔøΩÔøΩÔøΩ’£OÔøΩ«í1QÔøΩ?pÔøΩÔøΩƒ°oSqÔøΩ%ZÔøΩ\u0012ÔøΩf.ÔøΩFÔøΩÁ¢áÔøΩÔøΩXsÔøΩ`\"+o\nae“ùaÔøΩ@ÔøΩfcÔøΩHÔøΩe\u000eÔøΩÔøΩHf\"ÔøΩV\nqÔøΩÔøΩ$JÔøΩIÔøΩ*ÔøΩyT&ÔøΩP,ÔøΩ<ÔøΩCÔøΩiÔøΩ ÔøΩÔøΩ\u0016JÔøΩvÔøΩwsƒûa4\nÔøΩÔøΩÔøΩHÔøΩ\u0002\taÔøΩc\u001bBÔøΩ\u0013\u000fÔøΩxÔøΩ\u001aSÔøΩÔøΩ\nÔøΩ-ÔøΩ\u0010-ÔøΩÔøΩ\\\nÔøΩÔøΩÔøΩDÔøΩ\u0005bZÔøΩÔøΩÔøΩ\"ÔøΩÔøΩtRÔøΩ…íÔøΩdÔøΩ|ÔøΩC@BÔøΩƒØÔøΩÔøΩÔøΩÔøΩ4ÔøΩÔøΩ5ÔøΩÔøΩ0@\\XÔøΩ\u0018”†”ù–ä\u00195o\u0013ÔøΩÔøΩ\u0018\nÔøΩ?ÔøΩÔøΩHTÔøΩÔøΩÔøΩJ÷ä\u0015E\n3ÔøΩÔøΩÔøΩ4m-ÔøΩÔøΩtÔøΩqÔøΩ,ÔøΩ4ÔøΩÔøΩ⁄∞\n:IÔøΩÔøΩx\\ÔøΩÔøΩ$ÔøΩÔøΩ\u000f'ÔøΩ$ÔøΩ+ÔøΩÔøΩÔøΩÔøΩ\u0017\u001f{gBÔøΩ7¬¶ÔøΩÔøΩd◊õÔøΩÔøΩPIkÔøΩ!ÔøΩÔøΩ[ÔøΩwoÔøΩuÔøΩ[ÔøΩÔøΩÔøΩaÔøΩÔøΩÔøΩ+ÔøΩÔøΩÔøΩÔøΩtÔøΩ\u0018ÔøΩ^\u0018Is:9%ÔøΩÔøΩÔøΩÔøΩWÔøΩ|7kÔøΩÔøΩ\n\u0007ÔøΩÔøΩÔøΩSÔøΩgÔøΩÔøΩ\n≈æ`\u0019ÔøΩCÔøΩ“æÔøΩ/ÔøΩÿæÔøΩÔøΩ\u0003ÔøΩ\u0019ÔøΩtÔøΩpXÔøΩÔøΩSÔøΩﬁ£W^DÔøΩÔøΩÔøΩÔøΩ5ÔøΩ}ÔøΩTÔøΩG\u0007DÔøΩ@ÔøΩÔøΩÔøΩQgGÔøΩ\u0005F4\u0001m&.\nD2ÔøΩjÔøΩ63wbrA|ÔøΩ\nOG+_\u001fÔøΩ\u0004KqZÔøΩ[TÔøΩÔøΩÔøΩ\t:ÔøΩÔøΩ\u0000\u001bfÔøΩÔøΩÔøΩÔøΩI\\ahÔøΩ&ÔøΩXÔøΩ\u0018dj◊£ÔøΩ)\u0011ÔøΩ\u001a:ÔøΩAÔøΩHÔøΩE…† ÔøΩRÔøΩÔøΩ\tkcÔøΩ0PBÔøΩ<]ÔøΩdÔøΩ3ÔøΩY\u0016SÔøΩ\nÔøΩÔøΩÔøΩ\nÔøΩÔøΩ\u0006)ÔøΩÔøΩTxx\u0019ÔøΩÔøΩÔøΩÔøΩ\u001a\u0016EÌà±ÔøΩ&ugx8ÔøΩSW\\\u0019FÔøΩÔøΩÔøΩÔøΩ\t–¥%O\u0010KÔøΩHÔøΩdÔøΩ\u0002HÔøΩÔøΩÔøΩÔøΩ&'qÔøΩ◊ü3ÔøΩD?ÔøΩ;ÔøΩÔøΩ+ÔøΩÔøΩ\u0012CÔøΩuÔøΩ>KÔøΩ_ÔøΩÔøΩH\u0016\n>CoGP\"'ÔøΩÔøΩ=ÔøΩÔøΩÔøΩ[ÔøΩ3ÔøΩ9ÔøΩX\tmÔøΩÔøΩÔøΩv\bLÔøΩÔøΩQÏ∑´\u0010ÔøΩ#.TÔøΩ6 !ÔøΩÔøΩ9\nÔøΩÔøΩÔøΩ\u0018ÔøΩ\u0000:ÔøΩVÔøΩÔøΩf\u0017ÓººaPÔøΩ÷òÔøΩÔøΩ@ÔøΩÔøΩN\nÔøΩLÔøΩ\u001a|ÔøΩÔøΩ\u0012x\u0006ÔøΩ.ÔøΩ4ÔøΩkÔøΩ6ÔøΩ:0|~ÔøΩÔøΩxÔøΩ43ÔøΩÔøΩ÷ºs?ÔøΩ~\u0006ÔøΩtÔøΩÔøΩ7`q8N\u0007ÔøΩÔøΩÔøΩ^ÔøΩÔøΩo;ÔøΩÔøΩkcÔøΩ`ÔøΩ ÔøΩÔøΩ@ÔøΩOYP\nÔøΩmÔøΩÔøΩgÔøΩ~ÔøΩrd·™ΩÔøΩzsÔøΩ\n{ÔøΩÔøΩRÔøΩSÔøΩaÔøΩ0ÔøΩ*ÔøΩÔøΩÔøΩÔøΩy\n*ÔøΩÔøΩ@ÔøΩ>cVÔøΩ?tÔøΩ7pÔøΩ-ÔøΩÔøΩÔøΩÔøΩ#ÔøΩÔøΩ~q0ÔøΩÔøΩoÔøΩÔøΩ\u000e\n?sÔøΩÔøΩE;!ÔøΩVÔøΩÔøΩÔøΩÔøΩ-ÔøΩ-XÔøΩ\u0017\u0001—®oumÔøΩÔøΩ\nÔøΩyÔøΩÔøΩPÔøΩÔøΩ:m ÔøΩÔøΩ\u0003M'\nMFÔøΩÔøΩÔøΩ\u0019ÔøΩ\\ÔøΩÔøΩÔøΩKT ^\u00049ÔøΩ\nÔøΩWÔøΩgtN\u0012ÔøΩRÔøΩÈñ®\nÔøΩÔøΩ\u0017⁄Æ\n\u0000ÔøΩ)ÔøΩÔøΩ\u001aU6<ÔøΩÔøΩÔøΩÿåÔøΩcR\u0011ÔøΩv\u0005w3ÔøΩ\u0014ÔøΩ\\ÔøΩJ\nÔøΩALÔøΩÔøΩJÔøΩÔøΩ6ÔøΩÔøΩÔøΩ\u0001BÔøΩ\n\u0002ÔøΩÿÑd\u0004ÔøΩPÔøΩ\u001bÔøΩÔøΩÔøΩAhÔøΩÔøΩÔøΩÔøΩÔøΩEL\u0017\u001fRÔøΩÔøΩ64ÔøΩÔøΩ#ÔøΩ((Z3&ÔøΩÔøΩm2ÔøΩ?ÔøΩ¬™\u00194MJ”∏ÔøΩÔøΩjÔøΩÔøΩÔøΩÔøΩ\\'FqrÔøΩLgvÔøΩzÔøΩ\te\nÔøΩÔøΩ<\u0015ÔøΩsÂë•ÔøΩ4ÔøΩÔøΩÔøΩÔøΩH+ÔøΩ{2ÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩ3ÔøΩq_ÔøΩ\u0006›∑\u0016H…õNjMÔøΩuPÔøΩC,\n^ÔøΩÔøΩ?ÔøΩÔøΩ\u0012ÔøΩÔøΩÔøΩÔøΩƒéÔøΩPMsÔøΩDÔøΩÔøΩÁµÆÔøΩOÔøΩ\u0015nÔøΩ-+ÔøΩÔøΩoÔøΩÔøΩprdTf\u0004ÔøΩ\nÔøΩÔøΩ\u0019ÔøΩÔøΩa_T!ÔøΩtfÔøΩ(ÔøΩÔøΩ\u0007ÔøΩ+TT#<ÔøΩp\b\\=\u0016$!|ÔøΩ\u0011\u000eÔøΩÔøΩ8ÔøΩ$ÔøΩgÔøΩÔøΩ\u001aÔøΩÔøΩ6ÔøΩ9ÔøΩ'ÔøΩ[ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩkÔøΩÔøΩÔøΩ\u0012\u000eÔøΩeÔøΩÔøΩÔøΩÔøΩ∆∏ÔøΩSÔøΩÔøΩYÔøΩÔøΩÔøΩ\u0015OÔøΩÔøΩÔøΩ?\u001fÔøΩ\u001bÔøΩzOS:>&ÔøΩSÔøΩy”∂ÔøΩ/|2ÔøΩÔøΩÔøΩÔøΩGMÔøΩPÔøΩÔøΩ R“§ÔøΩÔøΩÔøΩ\"ÔøΩ\u0006!uÔøΩ=QÔøΩKÔøΩÔøΩÔøΩOÔøΩÔøΩZÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ*ÔøΩsÔøΩ!Za8œü;ÔøΩ|ÏùâÔøΩ\u0003\u0017M_ÔøΩc⁄èÔøΩÔøΩÔøΩvFHÔøΩÔøΩÔøΩÔøΩTÔøΩÔøΩÔøΩ/ÔøΩ:pœÅSÔøΩÔøΩÔøΩtÔøΩÔøΩÔøΩÔøΩÔøΩFyJ>ÔøΩ1ÔøΩpzXo2ÔøΩÔøΩa\nÔøΩÔøΩiÔøΩ\b|ÔøΩ\ncÔøΩpÔøΩƒã\u001bÔøΩA'Q)ÔøΩÔøΩÔøΩ\nAtÔøΩ&eÔøΩV\u0000ÔøΩÔøΩkÔøΩFx\u0017ÔøΩÔøΩÔøΩtÔøΩr\n≈¥ÔøΩYÔøΩ`lsÔøΩmv\u0006ÔøΩÔøΩ\u001f\u0019FkÔøΩ$|\u0019ÔøΩvÔøΩ~ÔøΩÔøΩÔøΩ\nÔøΩ%iÔøΩeqÔøΩÔøΩ\u0007\"ÔøΩ\nÔøΩ\u0010ÔøΩ+ÔøΩÔøΩ\n7ÔøΩÔøΩÔøΩÔøΩÔøΩf2ÔøΩwÔøΩ?ÔøΩ!ÔøΩh.ÔøΩ\u0019ÔøΩL\u0012y\t]ÔøΩÔøΩ\u00027ÔøΩ\u000eÔøΩÔøΩLÔøΩ\u0004\u000e\u0016ÔøΩ4M\n‰úúAÔøΩ-'*bhÔøΩ\n\u0002z'ÔøΩ\nÔøΩÔøΩ'ÔøΩAÔøΩ\u001b4ÔøΩ\n\u0018ÔøΩÔøΩ\\GÔøΩÔøΩÔøΩ\n>yÔøΩ5ÔøΩV$ÔøΩ7ÔøΩÔøΩ#dÔøΩ\u001aÔøΩÔøΩÔøΩ\u0018~ÔøΩ\u0019B\u0010«ÜÔøΩÔøΩÔøΩ@kÔøΩI4\u0017ÔøΩ\n\\%ÔøΩ}\u000eZsÔøΩÔøΩÔøΩÔøΩF+QjZg>‡¨à\u0006ÔøΩÔøΩbÔøΩÔøΩ\nÔøΩ+ÔøΩÔøΩ*3ÔøΩÔøΩÔøΩiÔøΩ\u0012bÔøΩÔøΩ\n…≥C`ÔøΩ9nÔøΩ\u0012 ÔøΩÔøΩzÔøΩ“úÔøΩj6KdbÔøΩÔøΩÔøΩÔøΩN4<ÔøΩ/zÔøΩtÔøΩÔøΩUÔøΩ?ÔøΩÔøΩÔøΩÔøΩÔøΩ\u001fÔøΩ\u0013ÔøΩW ÔøΩ,zl ÔøΩÔøΩÔøΩ\u0015{ÔøΩÔøΩ?ÔøΩ`ÔøΩﬁû#Wt~cÔøΩrKÔøΩ\bDÔøΩÔøΩÔøΩÔøΩÔøΩjÔøΩÔøΩÔøΩÔøΩÔøΩpÔøΩÔøΩÔøΩÔøΩ87\u000f\u0000ÔøΩ0\u001bÔøΩÔøΩQÔøΩÔøΩÔøΩ=ÔøΩÔøΩ>ÔøΩwÔøΩ}ÔøΩÔøΩniﬁπÔøΩÔøΩ^\u0018)ÔøΩz\u001f\u0011\nhÔøΩÔøΩÔøΩÔøΩ\u0006ÔøΩsÔøΩÔøΩE;ÔøΩÔøΩ;ÔøΩÔøΩÔøΩ_ÔøΩÔøΩ[ÔøΩÔøΩÔøΩcÔøΩÔøΩÔøΩÔøΩ0\u0003ÔøΩÔøΩdÔøΩÔøΩÔøΩÔøΩ9qÔøΩÔøΩ5ÔøΩD\u001f>vÔøΩÔøΩÔøΩAÔøΩ\u0002\nÔøΩ\nÔøΩuN\u0010ÔøΩ\"$c%=ÔøΩ\u0000= ÔøΩ6ÔøΩÔøΩ\u0015ÔøΩ2VÔøΩÔøΩÔøΩÔøΩÔøΩFÔøΩÔøΩc\u0012GÔøΩƒ∂ÔøΩÔøΩ\u0019dÔøΩÔøΩ\u0013WÔøΩ ÔøΩÔøΩ\nk,ÔøΩ?ÔøΩyÔøΩIÔøΩM\u001aﬁ∂ÔøΩ\u0011ÔøΩzÔøΩÔøΩ0ÔøΩTÔøΩ4ÔøΩ\n\u0018ÔøΩk\nÔøΩÔøΩxÔøΩ\u0019FGÔøΩÔøΩ}.ÔøΩB\nÔøΩG|=%ÔøΩÔøΩ'ÔøΩe;D\bW≈úÔøΩÔøΩ^sƒ™\u0017ÔøΩnT~WAMF\nÔøΩ ë~&\u001fÔøΩ≈Ñ%M-lXg\u000eG9ÔøΩÔøΩÔøΩ-.ÔøΩÔøΩÔøΩ\u0013ÔøΩ\u0010ÔøΩ\nelÔøΩ\u000eB\u00187\u0001{\u0013jÔøΩAÔøΩ≈≥ÔøΩÔøΩ\\LÔøΩ:RÔøΩ*ÔøΩÔøΩKÔøΩ0—úÔøΩÔøΩtkH\nGÔøΩ7ÔøΩ!\u0010KÔøΩÔøΩ!ÔøΩ\u001fÔøΩPvo\u000fy\u0005ÔøΩÔøΩM\nEqÔøΩÔøΩÔøΩÃÇ…æÔøΩÔøΩ,ÔøΩ%AKÔøΩÔøΩÔøΩÔøΩÔøΩ\n\u0014ÔøΩÔøΩ&\u0014M\u001aXÔøΩ\u001aÔøΩ\nÔøΩÔøΩ\u001bÔøΩÔøΩÔøΩ#ÔøΩW;\n;cÔøΩ“±ÔøΩÔøΩRd\nÔøΩÔøΩ\u0010ÔøΩ\u0007ÔøΩ|Ôåîg∆§ÔøΩ\u001fÔøΩ-ÔøΩaLÔøΩÔøΩSlÔøΩ\nÔøΩÔøΩÔøΩÈ•ëÔøΩÔøΩ_fÔøΩÔøΩ3ÔøΩ]zÔøΩXÔøΩVÔøΩ\u0016'ÔøΩÔøΩÔøΩzÔøΩÔøΩmÔøΩÔøΩÔøΩ'ÔøΩrÔøΩÔøΩÔøΩNUÔøΩ\u000e\nÔøΩÔøΩ]3ÔøΩÔøΩÔøΩu[\u000f?ÔøΩÔøΩdÔøΩÔøΩHÔøΩ\u0012<\u000f/ÔøΩiRÔøΩVÔøΩÔøΩ5ÔøΩÔøΩ\u0003C\u0002;ÔøΩÔøΩOkÔøΩÔøΩzÔøΩ\u0005ÔøΩA\u0004ÔøΩ>X\n»†ÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩ√ñ\u0017?>ÔøΩYÔøΩ⁄àÔøΩÔøΩ.ÔøΩ÷îÔøΩ>lÔøΩﬁ£W^ÔøΩ\u0013ÔøΩÔøΩ[v\nk⁄∂FÔøΩ1ÔøΩX\u00160ÔøΩ8~K+bÔøΩÔøΩ\u0006\u0018c&=ÔøΩ\u0014CazÔøΩ4\u0007ÔøΩ\u0012}bÔøΩxÔøΩÔøΩÔøΩÔøΩÔøΩ3ÔøΩÔøΩXSÔøΩÔøΩÕáÔøΩÔøΩCÔøΩq-\u0005s…™ÔøΩÔøΩT7i]ÔøΩf>Ge\\KÔøΩJÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ|\u001b◊üÔøΩÔøΩÿ£ÔøΩ‹ª4i√àÔøΩ\u000e\nr#ÔøΩÔøΩÔøΩ-ÔøΩZh\u0011n7ÔøΩ\u0019ÔøΩÔøΩ>ÔøΩÔøΩÔøΩ\u0011ÔøΩX“ãÔøΩ3ÔøΩÔøΩÔøΩNÔøΩÔøΩvÔøΩ}ÔøΩÔøΩco>ÔøΩÔøΩ?@kNÔøΩcÔøΩ\u0000Mk{%EÔøΩÔøΩÔøΩAÔøΩÿ¶ ÔøΩÔøΩ^ÔøΩÔøΩY\u0002ÔøΩÔøΩ+ÔøΩÔøΩG\u0018ÔøΩSo)\nUÔøΩÔøΩÔøΩ3ÔøΩÔøΩgJÔøΩ&ÔøΩÔøΩÔøΩ<R\nÔøΩd$ÔøΩ \u0010ÔøΩ\u0002ÔøΩOÔøΩÔøΩÔøΩ\nÔøΩtÔøΩÔøΩ+ÔøΩÔøΩ!◊úY\u000e\u000e>ÔøΩcÔøΩ1}ÔøΩÔøΩ»äÔøΩl\n\u001aÔøΩ3p`ÔøΩÔøΩrÔøΩÔøΩfÔøΩ-ÔøΩ2\nÔøΩÔøΩ\u0006&_\u0010GÔøΩ\u001bÔøΩOÔøΩ\nÔøΩÔøΩÕ©E\nmÔøΩÔøΩTNr\b\u0011ÔøΩ\u000eÔøΩÔøΩÔøΩEÔøΩÔøΩÔøΩÔøΩÔøΩxÔøΩ%ÔøΩ ™ÔøΩ\u0012ÔøΩÔøΩÔøΩaÊ¢ùÔøΩ>_ÔøΩ\\ÔøΩ\u001fÔøΩXÔøΩÔøΩ{ÔøΩÔøΩ\u0015ÔøΩ\u0016ÔøΩXÔøΩcÔøΩÔøΩMÔøΩÔøΩÔøΩ&aSÔøΩb\"7ÔøΩﬁÄÔøΩ.}rÔøΩOÔøΩÔøΩÔøΩÔøΩÔøΩ)ÔøΩkÔøΩ\u0011mﬂÄ$\u0012*ÔøΩi\u0012(ÔøΩ\nmÔøΩV]\u0007.^ÔøΩÔøΩÔøΩn3ÔøΩ\nÔøΩ‹âÔøΩ+ÔøΩNÔøΩ4V\n1ÔøΩÔøΩ\n?`ÔøΩÔøΩÔøΩ+ÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩC6CAÔøΩÔøΩ7ÔøΩÔøΩﬁ§uu~ÔøΩ~ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ=ÔøΩÔøΩ?pM(DÔøΩ^3l\u0019|ÔøΩ-ÔøΩWYÔøΩ\n,ÔøΩÔøΩÔøΩ9“úvvÔøΩÔøΩHÔøΩt*kÔøΩv&L,ÔøΩÔøΩ,\u0007pÔøΩÔøΩÔøΩ\u0011√õ\u0016ÔøΩ\u0014ÔøΩÔøΩ\n.ÔøΩ~ÔøΩÔøΩ\b}\u0010ÔøΩ\n:ÔøΩ\u0019ÔøΩ\n-4;R\u0002h\nÔøΩ8ÔøΩÔøΩSÕäÔøΩÔøΩJ\u0011ÔøΩj\u0006ÔøΩAÔøΩÔøΩÔøΩ\u0010ÔøΩÔøΩ,ÔøΩ6K\"ÔøΩÔøΩÔøΩ\u0016ÔøΩ\n\u001aÔøΩj1~\n2PÔøΩ`\u0011ÔøΩÔøΩdÔøΩOÔøΩÔøΩ\u0013ÔøΩ ò\\ÔøΩ'r›çfÔøΩIiZÔøΩ\u0000V&z8ÔøΩQwÔøΩ\n&ÔøΩÔøΩ–£vÔøΩ`7)ÔøΩÔøΩÔøΩbÔøΩGÔøΩÔøΩxmÔøΩÔøΩKÔøΩÃôÔøΩ\u0016ÔøΩÔøΩZ`ÔøΩDÁµÜ»ÑÔøΩ}?ÔøΩtÔøΩ\nÔøΩÔøΩÔøΩgÔøΩ\nÔøΩÔøΩÔøΩrÔøΩÔøΩ÷∂ÔøΩ1…¢“ñIG\u0012ÔøΩsCÔøΩÔøΩÔøΩÔøΩÔøΩ\"aÔøΩ-\nÔøΩhÔøΩ+jÔøΩRÔøΩjpVn]ÔøΩÔøΩe,!ÔøΩ‘å\t>QÔøΩVÔøΩ!ÔøΩÔøΩ\nÔøΩÔøΩ}ÔøΩuÔøΩÔøΩÔøΩx.%ÔøΩÔøΩxo\"ÔøΩC*YÔøΩOeÔøΩTÔøΩSu\u0011{[\ncÔøΩuÔøΩÔøΩ◊Ω\u0017\\ÔøΩZ\u000f\t4ÔøΩÀÆÔøΩwÔøΩÔøΩSÔøΩo%ÔøΩÔøΩÔøΩ<ÔøΩÔøΩÔøΩÔøΩmÔøΩ\nÔøΩ\u0011I\u0007$I]\u0006Õ´◊™ÔøΩ\n{O@ÔøΩ\u0019ÔøΩ\u0007\u0004ÔøΩXÔøΩ\u0002ÔøΩs\u0001\u001f\u0002FÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩmk\u0016ÔøΩÔøΩÔøΩ óÔøΩ«âÔøΩHÔøΩ\tŒëhÔøΩGx:ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ9qÔøΩÔøΩÔøΩÔøΩ7RÔøΩÔøΩÔøΩÔøΩGGÔøΩÔøΩ√ß\u001fwÔøΩÔøΩÔøΩ<3ÔøΩÔøΩÔøΩ\u0010}ÔøΩÔøΩN/ÔøΩÔøΩ8.nÔøΩ1g]6ÔøΩÔøΩ7i]\u0003{LÔøΩÔøΩ[ÔøΩUÔøΩŸ®\ngÔøΩ\u0003<ÔøΩ\nÔøΩÔøΩÔøΩ∆±ÔøΩÔøΩÔøΩ√üÔøΩ\u0018fÔøΩFCÔøΩ4\u0018ÔøΩÔøΩeQ%ÔøΩÔøΩÔøΩRÌéîÔøΩ\u00005n0,ÔøΩÔøΩS ÔøΩfPÔøΩ“àÎ©§ÔøΩ5*?0ÔøΩ\u00179e\u0006ÔøΩÔøΩ$I\u000e\u0001\nz^9ÔøΩ\\p\nB\u001f\\RqÔøΩ}5xÔøΩÔøΩÔøΩ\u0007ÔøΩÔøΩ/,\bV>ÔøΩ8\nqÔøΩ*ÔøΩV\u0018ÔøΩnÔøΩ8wÔøΩ\u0007$\nÔøΩ\u0014ÔøΩc;\u0003ÔøΩQÔøΩÔøΩ ¢ÔøΩÔøΩ*ÔøΩÔøΩTf\nÔøΩ4ÔøΩÔøΩos1\u0013\u0002◊¨ÔøΩ\u0013À†\nÔøΩcZÔøΩÔøΩYÔøΩÔøΩ(vÔøΩÔøΩ\n6k[ssÔøΩZÔøΩÔøΩ0ÔøΩÔøΩÔøΩ\bvÔøΩC\nBÔøΩ4ÔøΩÔøΩiÔøΩL\u0005 •\tÔøΩIÔøΩÔøΩÔøΩ&]\u0003ÔøΩ-ÔøΩÔøΩsÔøΩ$z\n+_vÔøΩ\nÔøΩ0ÔøΩDBLÔøΩ[ÔøΩA'Q\u00196ÔøΩ\bTÔøΩc\t]w\tÔøΩv'LTÔøΩ1\"CÔøΩ.'ÔøΩ\nX^#2+ÔøΩÔøΩDMÔøΩÔøΩ0ÔøΩ\u000eÔøΩOÔøΩB0ÔøΩVÔøΩ}ÔøΩWÔøΩÔøΩ\nÔøΩÔøΩkﬂñeO\n9uÔøΩ\"ÔøΩ=œúÔøΩÔøΩY1ÔøΩ\nOÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩWÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩGÔøΩ`}B~\u0017\u000eÔøΩÔøΩÔøΩ^ÔøΩzÔøΩÔøΩ*gÔøΩy_ÔøΩÔøΩtFn\u001fÔøΩqbÔøΩ:ÔøΩÔøΩÔøΩ\u000fÔøΩFÔøΩÔøΩÔøΩÔøΩxd\bÔøΩ\b\n|LÌÖìl%(ÔøΩÔøΩ:\u0011zÔøΩ\u0010`\u0019ÔøΩÔøΩÕö8{ÔøΩÔøΩÔøΩÔøΩ,ÔøΩ\u0018RƒÄoo÷∂ÔøΩÔøΩw&,YÔøΩÔøΩ\u000f\u001bJÔøΩ\u0018\u0014ÔøΩ\u0005\u0018ÔøΩS_ÔøΩÔøΩ\u001aÔøΩÔæóG+ÔøΩ\u0017~ÔøΩjÔøΩRÔøΩfÔøΩ„ò±pgn$\nÔøΩÔøΩÔøΩr*BÔøΩÔøΩÔøΩÔøΩA÷æDÔøΩB\u0001ÔøΩ\u0005ÔøΩJ\u0000\u001bÔøΩp\u0019\u0002ÔøΩR%\nÔøΩ\nÔøΩÔøΩN'\u0001;ScÔøΩd\u001a\nÔøΩ7ÔøΩFF íÔøΩÔøΩ\u0000…ØÔøΩ\u0016TPÔøΩmÔøΩ\nUÔøΩÔøΩ\u0001jA\tZ\u0012(ÔøΩ\u0019ÔøΩÔøΩp=ÔøΩIÔøΩqÓàª\u0002ÔøΩ\u00166ÔøΩÔøΩÔøΩÔøΩÔøΩ0ÔøΩ$ÔøΩÔøΩ-;ÔøΩBt\bÔøΩuÔøΩ$ÔøΩA>ÔøΩ3’©\u0018√ÇÔøΩ\u0011\u001fcÔøΩ*\u000fÔøΩ÷ç\u0001JvÔøΩsG~pÔøΩ4]PÔøΩ,\u001fÔøΩÔøΩEÔøΩ\u0016JÔøΩHX\u0011M8ÔøΩy-ÔøΩﬂÄ\u0016wÔøΩ\\ÔøΩÔøΩ4ÔøΩH$\nÔøΩÔøΩÔøΩÔøΩÔøΩÿ§MÔøΩÔøΩIWIÔøΩÔøΩ,ÔøΩ^ÔøΩW\u001bÔøΩ\nÔøΩÔøΩ\u0010 }hJ\u00194\u0017ÔøΩ\nÔøΩFÔøΩKÔøΩWXL–ÑÔøΩ\u00008\u0010ÔøΩW\u0003’ê[\tÔøΩÔøΩÔøΩÔøΩ#ÔøΩÔøΩÔøΩ\u000eÔøΩg&ÔøΩ~5ÔøΩÔøΩYÔøΩÔøΩ%5ÔøΩ@\u0016ÔøΩ\u0019t]?\u0013ÔøΩXZÔøΩ0dŸâ2ÔøΩÔøΩ\u000eJÔøΩÔøΩ5dPÔøΩE\u0011ÔøΩOMÔøΩÔøΩÔøΩ4ÔøΩÔøΩ\u0019\nl\u000eÔøΩÔøΩ\nÔøΩÔøΩ3:ÔøΩÔøΩÔøΩ\"\u001b0—ö\b*=ÔøΩÔøΩ\u0006O\\[ÔøΩ\u0019ÔøΩÔøΩuuÔøΩ\u0002ÔøΩÔøΩ(ÔøΩPÔøΩÿµ[\u000e\n<vv«æ_ÔøΩÔøΩ9\u0011ÔøΩÔøΩ\u0007ÔøΩpÔøΩ+ÔøΩŒûÔøΩÔøΩcﬂâ\u0011SÔøΩZ=ÔøΩÔøΩ/ÔøΩÔøΩbÔøΩMm PÔøΩ\u0001`ÔøΩÔøΩ·∑¥ÔøΩÔøΩ”äÔøΩ9ayzÔøΩ\u0015ÔøΩ\u000f\u0007MÔøΩ\u0018\nÔøΩÔøΩS\u0013TÔøΩÔøΩ_ÔøΩÔøΩ`;ÔøΩ6ÔøΩÔøΩ\u0015ÔøΩÔøΩÔøΩÔøΩZWÔøΩÔøΩÔøΩÔøΩ3ﬁ≠ÔøΩÔøΩgÔøΩÔøΩ\u0015ÔøΩ\u000e\n=ÔøΩ€ØÔøΩ/ÔøΩÔøΩ:(\u000fÔøΩ\u0018ÔøΩ[ÔøΩÔøΩ\u0007ÔøΩÔøΩÔøΩtÔøΩ\nÔøΩgXÔøΩÔøΩ\t37ÔøΩ\u0007PÔøΩÔøΩ;ÔøΩXÔøΩÔøΩ ÔøΩÔøΩ(nÔøΩÔøΩœ¶ÔøΩq61CfZ!ÔøΩÔøΩYÔøΩRh2\"ÔøΩ[ÔøΩ\nÔøΩV5∆å+ÔøΩ7ÔøΩÔøΩÔøΩc6\n9ÔøΩÔøΩÔøΩÔøΩyskÔøΩÔøΩÔøΩÔøΩh2ÔøΩdH~ÔøΩ\u0002\u0001ÔøΩm@ÔøΩÔøΩsaÔøΩÔøΩXÔøΩOÔøΩÔøΩIÔøΩÔøΩ5JK\u0015Qzh\u0012ÔøΩn.4ÔøΩÔøΩ'\u0017ÔøΩÔøΩ\u0010\u000eÔøΩÔøΩK\u0010ÔøΩKÔøΩÔøΩ\u0002ÔøΩÔøΩ 9IﬁÅÔøΩÔøΩ…±wLËõ®KZÔøΩÔøΩ^pÔøΩ–âvD':ÔøΩ8ÔøΩN&ÔøΩ\nÔøΩp\nrÔøΩ\nÔøΩ\u0017\n4ÔøΩÔøΩÔøΩÔøΩb WBP\u0014ÔøΩ/;ÔøΩ\u001b$ÔøΩÔøΩ ÔøΩÔøΩ÷üÔøΩ+ÔøΩ*ÔøΩ+`ÔøΩ4iÔøΩp\u0006ÔøΩ\u0007ÔøΩ,ÔøΩÓº∑FÔøΩ1ÔøΩÔøΩ\n\u0002ÔøΩÔøΩTO\nJÔøΩÔøΩÔøΩÔøΩ&4ÔøΩÔøΩJNmÔøΩ\t<ÔøΩÔøΩegnÔøΩYÔøΩ\n1ÔøΩ&8ÔøΩÔøΩ\"UÔøΩQÔøΩFÔøΩÔøΩVÔøΩÔøΩÔøΩO^mFÔøΩDGÔøΩ\n0\"\n◊ß1<ÔøΩÔøΩ\u001fÔøΩ\u0018ÔøΩ<ÔøΩ\u0010sÔøΩinÔøΩÔøΩÔøΩ ÔøΩf\u0011$ÔøΩ\u0011\u0002ÔøΩTiÔøΩuÔøΩOÔøΩÔøΩ\u0003E\u0001eÔøΩ\nÔøΩ0ÔøΩ3%ÔøΩ\nÔøΩÔøΩq\u000eÔøΩPeÔøΩÔøΩh\u0012ÔøΩÔøΩÔøΩ\n?yÔøΩÔøΩÔøΩÔøΩ\u000f\u0012ÔøΩ=\u0007NÔøΩÔøΩ øÔøΩÔøΩYÔøΩÔøΩ\u0017NÔøΩ ÔøΩn÷Æ&ÔøΩ3ÔøΩÔøΩlÔøΩ~ÔøΩÔøΩÔøΩ[:ÔøΩ0ÔøΩÔøΩogÔøΩ\n[ÔøΩÔøΩÔøΩÔøΩ)ÔøΩ2s\n2ﬁ®>}ÔøΩ@\u0002[ÔøΩÔøΩ\u0010Õ´!\u0017F`Ar$ÔøΩ4ÔøΩ\n^ÔøΩMÔøΩ|ÔøΩ7ÔøΩÔøΩÔøΩÔøΩxÔøΩÔøΩÔøΩJÔøΩ\u001bÔøΩsÀ´>ÔøΩÔøΩ{ÔøΩÔøΩÔøΩ^ÔøΩV~R=ÔøΩÔøΩÔøΩ,}bÔøΩÔøΩÔøΩ«ßÔøΩÔøΩÔøΩÔøΩ{{ÔøΩQÔøΩÔøΩÔøΩÔøΩO&ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩJ\u000fÔøΩÔøΩsÔøΩÔøΩÔøΩ\u001bÔøΩ%#ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nUÔøΩÔøΩ[ÔøΩÔøΩ…•gÔøΩÔøΩÔøΩ∆ñ,“äÔøΩsÔøΩ`\u0013ÔøΩÔøΩ~ÔøΩÔøΩ\u00112FqÔøΩ3ÔøΩ–ëÔøΩNÔøΩÔøΩÔøΩh\u000eÔøΩÔøΩÔøΩ7ÔøΩÔøΩ\"0ÔøΩYfÔøΩ.ÔøΩ€î\u0012vXÔøΩÔøΩa6ÔøΩC[\n~3ÔøΩZÔøΩÔøΩÔøΩ\u0001_GÔøΩÔøΩ\u001aÔøΩH+ÔøΩiÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÁçÖÕó\n\u0002ÔøΩ\u0000ÔøΩ+\nÔøΩ\u0004mvf`,\u001b\n=ÔøΩÔøΩ{{\u0004\tdÔøΩ#ÔøΩk2ÔøΩÔøΩÔøΩ\nSJÔøΩÔøΩ5ÔøΩ\u0017$ÔøΩ1ÔøΩ⁄àU|>ÔøΩÔøΩ+'\nÔøΩ]G\u00194\u001bcÔøΩÔøΩ\u0012Ï©±ÔøΩK^ÔøΩCt9\t\u0006iTjÔøΩb\u0005yÔøΩÔøΩ'!s”Ä\u001f#\u0019iÔøΩ\u0004CÔøΩ(ÔøΩ+aK\u001a≈πÔøΩD\u001bÔøΩ◊ôDÔøΩgÔøΩ fenÔøΩn. ]ÔøΩ|\n%ÔøΩ~\n\u0003ÔøΩ\nÔøΩÔøΩ<ÔøΩj *f\u0018¬éÔøΩ\u0019ÔøΩÔøΩ6J\\ÔøΩ\nÔøΩnÔøΩÔøΩhÔøΩÔøΩÔøΩ\b*ÔøΩÔøΩAÔøΩ\u001bCÔøΩÔøΩ!ÔøΩgÔøΩÔøΩ!ÔøΩÔøΩ|.Yg6NÔøΩÔøΩÔøΩ!6ÔøΩn\u0001\u0007ÔøΩÔøΩÔøΩ\n√∏ÔøΩ\u0004ÔøΩ\u0011ÔøΩÔøΩ\u0004>\nÔøΩﬁöÕáÔøΩ@ÔøΩ‚•´ÔøΩTÔøΩTn\u0003ÔøΩÔøΩÔøΩÔøΩ%ÔøΩ/ÔøΩÔøΩSÔøΩÔøΩj]ÔøΩÔøΩÔøΩ{oiÔøΩ]ÔøΩ\tMÔøΩÔøΩÔøΩÔøΩbÔøΩ]ÔøΩon_ÔøΩÔøΩÔøΩ]ﬂùÔøΩÔøΩÔøΩÔøΩÔøΩGÔøΩ%ÔøΩ%ÔøΩÔøΩ’ìÔøΩ\u0014ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0011ÔøΩÔøΩÔøΩ.ÔøΩqÔøΩS\u0016ÔøΩ@ÔøΩAÔøΩÔøΩ#~ÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩCÔøΩƒæÔøΩÔøΩ5ÔøΩÔøΩÔøΩÔøΩÔøΩTÔøΩÔøΩÔøΩ/ÔøΩÔøΩCÔøΩÔøΩCÔøΩ\u0007ÔøΩ_\u0003ÔøΩÔøΩÔøΩ`ÔøΩ<ÔøΩÔøΩÔøΩÔøΩ{N4m€àx\nÔøΩ\u0010ÔøΩÔøΩÀ™ÔøΩÔøΩÔøΩ^ÔøΩÔøΩ\u0017ÔøΩS_ÔøΩZÔøΩUÔøΩÔøΩ:7ÔøΩ,ÔøΩEÔøΩÔøΩj\u0002g5\nÔøΩÔøΩ`,\u001aÔøΩa*ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩn[ÔøΩbÔøΩÔøΩÔøΩ(kÔøΩD4ÔøΩ!'ÔøΩÔøΩÔøΩÔøΩ?ÔøΩÔøΩ$Nz%S\u00135Q÷∂*LÔøΩÔøΩO\u001b\u0006ÔøΩ‘¨\u0003ÔøΩ*rkÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\\ÔøΩ0\n;ÔøΩ<r\bÔøΩÔøΩNOÔøΩÔøΩÔøΩCÔøΩŸâÔøΩ'ÔøΩ%ÔøΩÔøΩÔøΩhÔøΩ\nÔøΩLÔøΩe\u0019\u000fÔøΩjÔøΩc\u0000ÔøΩ\u001bÔøΩMÔøΩpÔøΩÔøΩÔøΩÔøΩ65ÔøΩ%ÔøΩ&LÔøΩ\u0017#LzÔøΩÔøΩÔøΩÔøΩ;\u0017{ÔøΩc5ÔøΩ$1!ÔøΩ\u0014\u0002h_ÔøΩÔøΩÔøΩTv\nÔøΩ1ÔøΩ\nÔøΩÕÅhÔøΩeÔøΩÔøΩ‹ÜÔøΩ{2ÔøΩDÔøΩ\u0011ÔøΩp\"IV;je›ã)\u0013ÔøΩ[vÔøΩÔøΩAÔøΩÔøΩ)ÔøΩÔøΩ#Ka\u0012oÔøΩ=%2OÔøΩJFÔøΩÔøΩ4ÔøΩCÔøΩÔøΩÔøΩT\u0006‹íÔøΩÔøΩÔøΩÔøΩNeÔøΩÔøΩÔøΩÔøΩk\ncm\nÔøΩ6ÔøΩBÔøΩÔøΩÔøΩK\u001bÔøΩ\u0018ÔøΩOÔøΩÔøΩÔøΩ$ÔøΩA\nq]ÔøΩ»†ÔøΩÔøΩ\u0015ÔøΩÔøΩÔøΩ\t$ÔøΩÔøΩ2ÔøΩÔøΩ:HÔøΩ Y[ÔøΩj/,ÔøΩÔøΩoÔøΩ\u0003\u001f|uÔøΩÔøΩÔøΩLÔøΩyÔøΩÔøΩÔøΩ\u0017ÔøΩ|ÔøΩÔøΩ>ÔøΩÔøΩYÔøΩÔøΩÔøΩ\u000fnÔøΩrÔøΩÔøΩm{ÔøΩÔøΩ}v\u0018ÔøΩ≈ªÔøΩ3aÊ¢ùmÔøΩ\u0013l`j ºJKÔøΩ\nÔøΩr\n=ÔøΩpÔøΩÔøΩ4YÔøΩI!ÔøΩ\u0019ÔøΩ\u0001$eÔøΩpsÔøΩÔøΩYÔøΩwﬁëH\u000fZÔøΩ?\u0012$ÔøΩZu\n4gÔøΩ.ﬂÄE|ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ#ÔøΩœΩÔøΩmÔøΩrÔøΩ3ÔøΩÔøΩwÔøΩÔøΩ=\n}{ÔøΩ“õa?\u0005)ÔøΩÔøΩÔøΩ\nÔøΩV–•ÔøΩÔøΩ„ø±<ÔøΩÔøΩÔøΩg[v\u0019ÔøΩÔøΩ%ÔøΩÔøΩ0ÔøΩSÔøΩÔøΩÔøΩ-ÔøΩÕÑ/ÔøΩÔøΩLÔøΩ\u0016ÔøΩ\u0010ÔøΩ\nÔøΩ\\K\u0006MKÔøΩMÔøΩsÔøΩ4dÔøΩÔøΩIÔøΩPÔøΩ3ÔøΩd7»†U\u001aÔøΩKÔøΩÔøΩ\u0012cƒµe%+&ÔøΩnZÔøΩA3ÔøΩ!ÔøΩy|ÔøΩ(ÔøΩLÔøΩ9f@ÔøΩw\n>\u0018ÔøΩJ\n8ÔøΩÔøΩÔøΩÔøΩ\u0012ÔøΩ€ßÔøΩ2ÔøΩ-ÔøΩÔøΩÔøΩ8ÔøΩ‡°≥[KÔøΩÔøΩ\u0001ÔøΩ\u0017<ÔøΩ'^\nÔøΩÔøΩÔøΩ\nLÔøΩuÔøΩD3ÔøΩÔøΩÔøΩ\neÔøΩÔøΩÔøΩ\n0ÔøΩÔøΩÔøΩ\u001a0—™ÔøΩ6SÔøΩÔøΩ\u0003N8`ÔøΩNÔøΩg\"ÔøΩB\u001a\bÔøΩ}Mb\u001bÔøΩ\u0017x`ÔøΩÔøΩ!Y#ÔøΩf\u0012p,\u0001ÔøΩÔøΩQn\nv\u0005ÔøΩÔøΩxopÔøΩ<QÔøΩ\nR$|ÔøΩÔøΩ\u0005ÔøΩÔøΩ\u001aÔøΩ\u000e∆¥fXÔøΩ\u000fÔøΩFÔøΩÔøΩ_ÔøΩÈéº\bÔøΩÔøΩÔøΩ\u00056ÔøΩÔøΩÔøΩy\u0015ÔøΩD!K∆ö<⁄îhÔøΩÔøΩ\u0011\u0005ÔøΩ/ÔøΩ#{4ÔøΩƒêÔøΩÔøΩ!d\u0010u\u0006\u0011\"UÔøΩjÔøΩÔøΩ_ÔøΩuÔøΩÔøΩHuÔøΩ\u0014ÔøΩÔøΩÔøΩ\u000f‘ÉÔøΩÔøΩ\bÔøΩ6ÔøΩ1ÔøΩ6R\u0016ÔøΩÔøΩPÔøΩ\"ÔøΩÔøΩÔøΩS[ÔøΩz\u001f7:ÔøΩÔøΩÔøΩÔøΩhÔøΩÔøΩÔøΩgS\u0007NX\u0013RÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩg~ÔøΩÔøΩÔøΩ«ìY\u0016ÔøΩÔøΩÔøΩÔøΩÔøΩO?ÔøΩÔøΩXÔøΩÔøΩxiÔøΩ3=\u0011ÔøΩÔøΩÔøΩÔøΩkÔøΩÔøΩÔøΩ\u0019ÔøΩCÔøΩÔøΩÔøΩ;C'\u0004hÔøΩÔøΩA”∏OÔøΩ\u0002ÔøΩ.ÔøΩÔøΩÔøΩÔøΩ\u001a\"dÔøΩ\u0002ÔøΩPdÔøΩÔøΩ\u0003o|3Cƒ´%ÔøΩÔøΩaÔøΩNÔøΩ<ÔøΩÔøΩ≈ëÔøΩqCÔøΩ{ÔøΩÔøΩ\tÔøΩ#\u0012ÔøΩÔøΩ!8ÔøΩ\nX\\WÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ9?ÔøΩ\u0005ÔøΩ–å\u000fnÔøΩÔøΩÔøΩÔøΩ\u001bÔøΩrq+jÔøΩJ\u0007ÔøΩÔøΩÔøΩÔøΩÔøΩrÔøΩvnm\nÔøΩK'ÔøΩ\u0007ÔøΩpÔøΩÔøΩÔøΩOFÔøΩGÔøΩ\u001bÔøΩÔøΩ\u0014≈ê\u00015ÔøΩ%cÔøΩ\nÔøΩJÔøΩÔøΩÔøΩÔøΩÔøΩV[ÔøΩÔøΩÔøΩ:ÔøΩŒ©ÔøΩÔøΩ%sÔøΩMÔøΩF1ÔøΩ\u00072eÔøΩÔøΩd\u0014\u000eAfÔøΩ`ÔøΩ\u0012AÔøΩ\nw\u0017qÔøΩXÔøΩ8}N\u0018}ÔøΩÔøΩ\tÔøΩÔøΩ+Q\n\u001a8L4ÔøΩÔøΩÔøΩ\u0010:ÔøΩNœûÔøΩÓ∑Ç≈π\u0018ÔøΩ*sb\bZ\n\u0004MÔøΩÔøΩÔøΩ\u0002ÔøΩÔøΩiÔøΩ#ÔøΩR\tz\nM\u0013$ÔøΩÔøΩ7ÔøΩ\nEÔøΩ\u0019xÔøΩBÔøΩ\nÔøΩL1%ÔøΩÔøΩÔøΩÔøΩ9ÔøΩeÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩPÔøΩpÔøΩ*>'ÔøΩÔøΩÔøΩÔøΩ}.+ÔøΩiÔøΩ\u00183…ºÔøΩ\u00159VÔøΩÔøΩ›ÇÔøΩ“ÜÔøΩ\\ÔøΩÔøΩD\u0013!RÔøΩÔøΩÀ≥FÔøΩr5ÔøΩÔøΩ“•ÔøΩÔøΩÔøΩÔøΩÔøΩm07ÔøΩD\u0007\u0013\u001alvÔøΩ\u0014ÔøΩÔøΩÔøΩÔøΩnj/\u0012ÔøΩÔøΩ/AÔøΩ\u0015\u000eÔøΩÔøΩg\u0012ÔøΩÔøΩSuÔøΩÔøΩhÔøΩÔøΩÔøΩ⁄°ÔøΩsX}ﬁ®3ÔøΩ∆âÔøΩLÔøΩ\u0014;+ÔøΩ8ÔøΩÔøΩO&ÔøΩÔøΩÔøΩ23V-[w FÔøΩ\u000fÔøΩ¬øÔøΩ\u001b⁄ñÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ:ÔøΩÔøΩ=ÔøΩÔøΩlÔøΩ3z%ÔøΩLÔøΩqÔøΩ\u0017ÔøΩÔøΩWÔøΩÔøΩ\u0010\nMÔøΩÔøΩMUÔøΩ-\u001bÔøΩÔøΩcÔøΩoÔøΩÔøΩMwQÔøΩÔøΩ\u0004MÔøΩlÔøΩÔøΩÔøΩÔøΩÔøΩ{ÔøΩZÔøΩMÔøΩ\u0005pŸûDÔøΩÔøΩÔøΩÔøΩ\u0013ÔøΩ\n\u001bÔøΩ{ÔøΩÔøΩÔøΩ{NÔøΩ{nÔøΩÔøΩiÔøΩNZÔøΩÔøΩÔøΩ\nÔøΩl>4oÔøΩnnÔøΩ'ÔøΩÔøΩ‰ïë?ÔøΩ#0YÔøΩ~ÔøΩÔøΩÔøΩÔøΩ\u0003GÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ√õJÔøΩÔøΩÔøΩ%fÔøΩÔøΩBÔøΩ1[0ÔøΩaÔøΩÔøΩÔøΩHFŸ∏ÔøΩÔøΩÔøΩ@:ZÔøΩÔøΩÔøΩÔøΩ8wÔøΩÔøΩÔøΩ<ÔøΩ\u001fÔøΩÔøΩÔøΩÔøΩ…îÔøΩZ_ﬁó+\u0012ÔøΩ_a\nÔøΩÔøΩ\bÔøΩ6ÔøΩ86@\nt\n\u0013\u0016ÔøΩ\u0018Ÿ≤ÔøΩÔøΩ\nÔøΩRÔøΩ«ö+ÔøΩ\\)\nB\u0015\u0007Q8ÔøΩÔøΩÔøΩ?!ÔøΩÔøΩYlÔøΩ\u0004ÔøΩ\u0014>\nsTHÔøΩzÔøΩ1AÔøΩPÔøΩÔøΩNÔøΩÔøΩÿπÔøΩ\u0018ÔøΩÔøΩÔøΩz*ÔøΩ\u000fÔøΩT\"\u0002ÔøΩÔøΩ\u0014\u0006ÔøΩ%SwÔøΩÔøΩVÔøΩvÔøΩÔøΩÔøΩjPÔøΩN4WÔøΩ3ÔøΩw:\\ÔøΩAgÔøΩ√¥&\u0013ÔøΩÔøΩ\u0016FÔøΩ\u0018F2ÔøΩ\nÔøΩ'D|ÔøΩp\bÔøΩ\u0000ÔøΩÔøΩÔøΩq\u0006\nÔøΩxÔøΩÔøΩ@hÔøΩÔøΩ$ÔøΩ^ÔøΩ+ÔøΩÔøΩ%jG\"ÔøΩ.ÔøΩ\u0007ÔøΩJH=ÔøΩJÔøΩpu1ÔøΩQbu[<ƒ∏GlÔøΩÔøΩHÔøΩÔøΩ\nÔøΩ\u0016YÔøΩÔøΩÔøΩ4VOÔøΩ\u0011\n\u0004UÔøΩÔøΩÔøΩg~rK=ÔøΩ$f$rg&\u0012ÔøΩ3{xÔøΩÔøΩ/\u0006ÔøΩ\nDmÔøΩmÔøΩÔøΩ.ÔøΩNÔøΩ\nÔøΩÔøΩU\bÔøΩŸ®ÔøΩs$ÔøΩsÔøΩ-ÔøΩZÔøΩr/gg'ÔøΩ\\HÔøΩÔøΩ\n\u0016|ÔøΩÔøΩÕáÔøΩÔøΩÔøΩ\u001aÔøΩÔøΩrÔøΩÔøΩ?wÔøΩ\tÔøΩÔøΩÔøΩ3ÔøΩÔøΩÔøΩYÔøΩrÔøΩÔøΩÔøΩÔøΩvV\u000f]ÔøΩÔøΩÔøΩÔøΩÔøΩ2ÔøΩIkÔøΩÔøΩPÔøΩÔøΩ_ÔøΩÔøΩÈ£™ÔøΩÔøΩ_4OÔøΩÔøΩÔøΩÔøΩxÔøΩÔøΩ=ÔøΩ\nÔøΩÔøΩÔøΩÔøΩ3\u0001ÔøΩÔøΩÔøΩ@\u001a{SÔøΩ\u001aÔøΩÔøΩÔøΩÔøΩ:ÔøΩY\u0004\u00042YÔøΩS\u001fNZÔøΩvÔøΩÔøΩÔøΩÔøΩÔøΩ5DruL\u001fÔøΩe\u0010,rÔøΩÔøΩN=ÔøΩ}yÔøΩgÔøΩ?NÔøΩÔøΩyÔøΩ∆ÉSÔøΩm}ÔøΩÔøΩiMÔøΩÔøΩÔøΩﬁ©WEÔøΩE\u001f\u0006ÔøΩ<ÔøΩÔøΩÔøΩÔøΩNÔøΩÔøΩÔøΩÔøΩl\u0014ÔøΩÔøΩvÔøΩÔøΩ\u0014ÔøΩÔøΩÔøΩ\u00036ÔøΩ\u0015ÔøΩGÔøΩÔøΩZWÔøΩC\u0002ÔøΩÔøΩ](ÔøΩÔøΩ\\<:ÔøΩRÔøΩÔøΩÔøΩWÔøΩÔøΩ\u001aÔøΩÔøΩ+ÔøΩÔøΩ2ÔøΩÔøΩd&:ÔøΩƒÜÔøΩhKP9ÔøΩ\u0002`ÔøΩ%\u0002ÔøΩÔØß\u0002\u0010n\n\u0015…∞ÔøΩÔøΩ\b.ÔøΩ4ÔøΩÔøΩPÔøΩhÔøΩuÔøΩÔøΩÔøΩÔøΩzÔøΩ\u0010ÔøΩÔøΩ:'DdsI\u0011,ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ“æ;ÔøΩZ\u0011ÔøΩ~ÔøΩ:ÔøΩÔøΩÊï∞ÔøΩ\u001aRTÔøΩÔøΩPÔøΩŸÇ4S%\u0001mÔøΩ02,\u0011&ÔøΩzÔøΩDÔøΩÔøΩ{\u0013;ÔøΩÔøΩ.ÔøΩÔøΩÔøΩÔøΩÔøΩ\"ÔøΩ\u001fÔøΩhÔøΩ\u0018S…¢ÔøΩÔøΩÔøΩJsÔøΩ5ÔøΩ<\u0018#/V\u001aSÔøΩF{ àLQÔøΩ\u0016DSÔøΩÔøΩJÔøΩ9h\u0011CDÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ0^CÔøΩCÔøΩÔøΩ\u000e\u0001ÔøΩÔøΩ67ÔøΩÔøΩ[\nÔøΩm0ÔøΩ-ÔøΩÔøΩÔøΩ\u00116RvÔøΩÔøΩÔøΩ \u0010;ÔøΩÔøΩ\u0011ÔøΩE$\u0016ÔøΩÔøΩwÔøΩÔøΩGÔøΩ1\nÔøΩrÔøΩÔøΩ\u0012ÔøΩÔøΩÁ£∏ÔøΩÔøΩ\u0017–çSÔøΩÔøΩÔøΩ^XÔøΩÔøΩÔøΩ“•ELw.Q\u0017ÔøΩÔøΩ89\u0004ÔøΩÔøΩ\u0012^ÔøΩ¬ö\bÔøΩ%ÔøΩÔøΩ1[ÔøΩ1ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ54ÔøΩÔøΩÔ∫´ar#ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0011gÔøΩƒ¢√ìÔøΩnMhÔøΩÔøΩ>yÔøΩiÔøΩÔøΩ]ÔøΩ\u000f\\\\ÔøΩÔøΩ@ÔøΩyÔøΩÔøΩ#ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩtH'7n?ÔøΩvÔøΩ)ÔøΩ~R57ÔøΩÔøΩ,ÔøΩrÀÇÔøΩ\n)jÔøΩÔøΩÀÄ‹πÔøΩÔøΩ\u0015?o>ÔøΩ~ÔøΩ\u0011HZ\u0007MXÔøΩ{ÔøΩ Å\u0013÷åÔøΩÔøΩqÔøΩÔøΩ\u0003ÔøΩÔøΩÔøΩÔøΩfÔøΩ$ÔøΩ9=ÔøΩ\u0016ZÔøΩxÔøΩÔøΩÔøΩ^\nÔøΩÔøΩl\u0012h:ÔøΩDÔøΩgd_ÔøΩÔøΩÔøΩÔøΩÔøΩ:| ∫\u0015ÔøΩ\u000flÔøΩuÔøΩÔøΩÔøΩÔøΩ>\nÃÅDÔøΩÔøΩ{{eÔøΩ>uÔøΩÔøΩÔøΩÔøΩpKÔøΩ\u0018ÔøΩeÔøΩÔøΩÔøΩ\n=yÔøΩbÔøΩÔøΩÔøΩgÔøΩs·†éeÔøΩ\"ÔøΩ; 7?|ÔøΩ\ng–ßÔøΩ^lﬁπ\u001fÔøΩÔøΩÔøΩBÔøΩ9KÔøΩ6'ÔøΩÔøΩÔøΩÔøΩuÔøΩq\u000eÔøΩd\"`1\u0004GLÔøΩ\u000f\tÔøΩ1CÔøΩ%ÔøΩ√å=@aW0<ÔøΩÔøΩ0\u0015#\u0001ÔøΩ}÷≤ÔøΩÔøΩyÔøΩ\"Sf\u00161h\nÔøΩ\\\nÔøΩÔøΩ\u0019ÔøΩCÔøΩ#\tÔøΩ\nÔøΩ'OÔøΩÔøΩÔøΩ;C\u0012MB3fÔøΩÔøΩÔøΩsÔøΩBÔøΩ:ÔøΩÔøΩ3[:ÔøΩ#ÔøΩIÔøΩ9\u0011@ÔøΩ\nM\u0004/\\\nÔøΩrR>\u0015kU1OÔøΩœ™ÔøΩ‰çíÔøΩ\u0011ÔøΩÔøΩt2fÀ†i\u0007‹ØÔøΩw\ndÔøΩH\n$ÔøΩÔøΩÔøΩ7ÔøΩÔøΩN%ÔøΩÔøΩJÔøΩ~ÔøΩÔøΩFÔøΩ\u0003JÔøΩ@\"ÔøΩÔøΩ>9HÔøΩÔøΩ\u00108hÔøΩÔøΩh\u0016ÔøΩ\u000e}ÔøΩ\u0013x\u0015\bjjÔøΩ⁄ëÔøΩJÔøΩÔøΩ◊∫CÔøΩÔøΩ\u001b\\fÔøΩÔøΩÔøΩP\n.„§õI4GÔøΩ3ÔøΩzQ\u0006\u0013ÔøΩ_∆§ÔøΩ\n>ÔøΩÔøΩavÔøΩOÔøΩÔøΩÔøΩFÔøΩÔøΩ6ÔøΩÿçsÔøΩlÔøΩÔøΩ0ÔøΩ8ÔøΩÔøΩÔøΩsÔøΩ!ÔøΩÔøΩÔøΩ\u0005dÔøΩÔøΩ$|ÔøΩÔøΩ~ÔøΩÔøΩ\u001aÔøΩDÔøΩ\u0016ÔøΩDÔøΩ#ÔøΩÔøΩÔøΩyÔøΩÔøΩ+uÔøΩ~<9ÔøΩÔøΩ\nwÔøΩjÔøΩÔøΩÔøΩ37ÔøΩzwÔøΩ[:tÔøΩÔøΩ}ÔøΩ?uÔøΩqkÔøΩ\nÔøΩÔøΩÔøΩ|fÔøΩÔøΩfUÔøΩv5ÔøΩÔøΩfoÔøΩ{ÔøΩÔøΩUJ\nÔøΩÔøΩÔøΩÔøΩ\u0007ÔøΩ~ZÔøΩ\u0017\u0002ÔøΩÔøΩ\u0017\u0000[Ó∂é=ÔøΩNÔøΩ4|ÔøΩÔøΩ6ÔøΩ\ncÔøΩÔøΩÔøΩgÔøΩ~qƒºeÔøΩÔøΩÔøΩ]\n7s\u000f7ÔøΩÔøΩÔøΩÔøΩ]%ÔøΩ>ÔøΩÔøΩ(M\u0017ÔøΩÔøΩﬁ≠ÔøΩÔøΩuÔøΩÔøΩIsÔøΩÔøΩÂûûÔøΩ>5ÔøΩÔøΩ>ÔøΩÔøΩ\u001f\u000fÔøΩÔøΩÔøΩÔøΩwÔøΩÔøΩÎ∂ª{ÔøΩÔøΩSÔøΩÔøΩB\u0012=ÔøΩÔøΩÔøΩÔøΩÔøΩŒÉa<ÔøΩŒâÔøΩMmÔøΩÔøΩÔøΩ`\u001fÔøΩ∆áﬂïw\u0017ÔøΩLÔøΩ1ÔøΩÔøΩZwÔøΩÔøΩUs96ÔøΩEZÔøΩpj\u001bbpÔøΩ\u0018ÔøΩiÔøΩ\u001aÔøΩ\u0017ÔøΩ\u000e\u0013-\u0006ÔøΩ\nMÔøΩÔøΩÔøΩÃ∏K'ÔøΩ>k>ÔøΩqÔøΩ80C=∆ôÔøΩeÔøΩ\"\u001bÔøΩg⁄•,F(ÔøΩ\u0006ÔøΩÔøΩÔøΩÔøΩ!ÔøΩÔøΩ~ÔøΩ\u0013ÔøΩ=ÔøΩÔøΩZÔøΩD[NÔøΩdDI\u00044ÔøΩÔøΩIDkÔøΩ^v”ìÔøΩ-\"\u001bÔøΩJÔøΩuÔøΩÔøΩmrxU3v\u001f6ÔøΩqÔøΩ\u0017ÔøΩ[ÔøΩÔøΩQ\u0006MW\u0012ÔøΩÔøΩF‹•ÔøΩhÔøΩÔøΩsÔøΩ\u0004`Dnv\"ÔøΩ'\u0016ÔøΩÔøΩÔøΩ\u000f\u0019ÔøΩÔøΩ|ÔøΩPXJF2\u0016ÔøΩCÔøΩP:\u0004ÔøΩ\"ÔøΩ1ÔøΩY\\bÔøΩÔøΩ8\u0006IÔøΩx8 ÔøΩ\n^ÔøΩÔøΩvdÔøΩHÔøΩFXsÔøΩ\n&√ªÔøΩ\u0019ÔøΩÔøΩ€àÔøΩÔøΩMTÔøΩL.,ÔøΩ ÔøΩÔøΩÔøΩ\nÔøΩJ*ÔøΩÔøΩ\u001b[bÔøΩc$\u001a\u00156ÔøΩ\u0002ÔøΩ6ÔøΩÔøΩÕ¥ÔøΩ\nW…†ÔøΩKÔøΩIm\u0001ÔøΩ\u0003ÔøΩÿç)?ÔøΩ@9ÔøΩÔøΩ(ÔøΩ%TÔøΩÔøΩIÔøΩX*4-ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ/\\ÔøΩ\nzÔøΩŒ£ÔøΩÔøΩ%ÔøΩ\u000eÔøΩÔøΩÔøΩ]ÔøΩÔøΩÔøΩÔøΩÔøΩaÀæÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩgÔøΩ\u0017=~ÔøΩÔøΩxÔøΩ◊ΩÔøΩÔøΩÔøΩ\b\u0011+\u0005\u001f~mÔøΩÔøΩ='ÔøΩÔøΩ5.ÔøΩjÔøΩ\u0015ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u001fÔøΩ\u0006ÌÖè\u000eÔøΩ?nÔøΩÔøΩ5ÔøΩÔøΩÔøΩÔøΩ\u000eÔøΩc\n”êÔøΩÔøΩ\u0007/YÔøΩfÔøΩc\u0003\u0013ZÔøΩÔøΩÔøΩ\u000eXÔøΩÔøΩÔøΩÔøΩÔøΩkÔøΩÔøΩÔøΩsÔøΩ\u0001WÔøΩ%ÔøΩV]\u0007ÔøΩ\\ÔøΩ\u00032nÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩ\n&ÔøΩÔøΩ\u0002ÔøΩ\u0012\u0018ÔøΩÔøΩÔøΩÔøΩÔøΩoÔøΩ.ÔøΩÔøΩhÔøΩÔøΩÔøΩÔøΩ~ÔøΩÔøΩÔøΩÔøΩÔøΩ)DeÔøΩÔøΩo\u000fÔøΩŸ∂ÔøΩDÔøΩ–•ÔøΩgn\u001aÔøΩÔøΩ\u0006ÔøΩÔøΩÔøΩÔøΩÔøΩ\\ÔøΩÔøΩG\\ÔøΩRÔøΩ|b\nVÔøΩm“¶ÔøΩ\tÔøΩ]cÔøΩ%ÔøΩz\u001aX/e)O=R}ÔøΩ\u001aÔøΩB\u0018ÔøΩÔøΩzLÔøΩÔøΩ\u0018ÔøΩ- ÔøΩÔøΩ(QeBÔøΩÔøΩDÔøΩÔøΩcÔøΩÔøΩZÔøΩ,ÔøΩÔøΩH\nA^aYp\bÔøΩ—ÉrÔøΩÔøΩ\nÔøΩR\u0015<ÔøΩlÔøΩdl\u0014ÔøΩÔøΩ,GrÔøΩÔøΩŸ°\n5YÔøΩÔøΩ‰Ω§ÔøΩ^XÔøΩKÔøΩ\u000fÔøΩrÔøΩÔøΩ\n:IÔøΩÔøΩvÔøΩÔøΩÔøΩÔøΩÔøΩ~ÔøΩD&RÔøΩÔøΩÔøΩÔøΩgG%ÔøΩ4ÔøΩ>ÔøΩÔøΩ65ÔøΩdÔøΩÔøΩÔøΩ:ÔøΩÔøΩOÔøΩÔøΩw!ÔøΩÔøΩ*ÔøΩ\u0014\u0013\u000f\u001bd\u0016[ÔøΩÔøΩÔøΩ\u0016HÔøΩJ\u0016\u0019ÔøΩÔøΩ'ÔøΩ6ÔøΩÔøΩÔøΩÔøΩHF\u0016ÔøΩÔøΩ]\nÔøΩ∆πÔøΩÔøΩ@,ÔøΩBb\",ÔøΩÔøΩ\u001a8y[0ÔøΩÔøΩÊåï\u0007Pa'ÔøΩ\u0011ÔøΩ\"ÕÑ\u0004-QÔøΩcÔøΩÔøΩ\u0018ÔøΩ#2hGÔøΩdpwÔøΩÔøΩÔøΩÔøΩ÷ü\u000f\bCN\u0007ÔøΩÔøΩqÔøΩdB$v\b2ÔøΩSÔøΩÔøΩÔøΩÔøΩÔøΩV2ÔøΩÔøΩlÔøΩÔøΩcÔøΩ!«´>ÔøΩb\t93`ÔøΩÔøΩÔøΩBÔøΩÔøΩ H\nÔøΩÔøΩZ\"\u0010ÔøΩvÔøΩÔøΩÔøΩÔøΩÔøΩNUÔøΩÔøΩÔøΩÔøΩÔøΩs«ΩÔøΩ ÔøΩÔøΩÔøΩ]ÔøΩ?\nÔøΩ\u001bÔøΩÔøΩ\n_\nqÔøΩjÔøΩÔøΩÔøΩÔøΩ‹¢ÔøΩWÔøΩÔøΩKÔøΩ7ÔøΩ7ÔøΩÔøΩÔøΩyrÔøΩÔøΩÔøΩÔøΩ0–§uÔøΩÔøΩ≈ªÔøΩÔøΩvÔøΩÔøΩw\u0007ÔøΩ\u0013R]VÔøΩÔøΩÔøΩ3ÔøΩÔøΩ|4iÔøΩÔøΩCÔøΩÔøΩ?ÔøΩÔøΩÔøΩ[ÔøΩÔøΩqSÔøΩÔøΩvÔøΩ\nÔøΩÔøΩÔøΩÔøΩWÔøΩÔøΩÔøΩÔøΩÔøΩ'ÔøΩvwÔøΩÔøΩ_ÔøΩÔøΩÔøΩYÔøΩ\u001fWÔøΩ?ÔøΩÔøΩÔøΩ\tÔøΩ{ÔøΩUÔøΩ⁄±GÔøΩÁáø[9+ÔøΩ›±A*ÔøΩ|ÔøΩÔøΩ\u000e/\n\u000f_OXFÔøΩÔøΩb\u0000ÔøΩÔøΩ3CÔøΩzÔøΩÔøΩ\u0017G\u0010ÔøΩÔøΩÔøΩQcÔøΩÔøΩ~ÔøΩ{ÔøΩÔøΩÔøΩ3ÔøΩÔøΩ\u000eÔøΩÔøΩÔøΩ&ÔøΩÔøΩfÔøΩjkG,ﬂ∞ÔøΩhÔøΩÔøΩÔøΩg6ÔøΩFcÔøΩ\u0002>ÔøΩƒ©\nÔøΩÔøΩ~ÔøΩzÔøΩ~ÔøΩÔøΩuy(]\nArÔøΩaÔøΩÔøΩ~ÔøΩ∆îÔøΩÔøΩ\\ÔøΩÔøΩÔøΩQNp\u0011ÔøΩ\u001aÔøΩÔøΩ\u0018>tÍëãÔøΩ\u0016ÔøΩYTpÔøΩﬂ®ÔøΩ\u0000ÔøΩÎ®ô\u0015ÔøΩÔøΩ\\vSÔøΩ-ÔøΩ1ÔøΩÔøΩ\nÔøΩ\u0000ÔøΩ\nLÔøΩÔøΩ\u0006ÔøΩÔøΩ$8ÔøΩÔøΩ%ÔøΩ:|%ÔøΩOÔøΩ\u0002ÔøΩXÔøΩ\u000eÔøΩÔøΩzx\u001aÔøΩ-ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0003aÔøΩ\nÔøΩÔøΩÔøΩ6yÔøΩA\u000fJÔøΩ\u0015ÔøΩ2T\u0013\nÔøΩ0ÔøΩ\nÔøΩÔøΩÔøΩ\u0004ÔøΩÔøΩÔøΩÔøΩ$ÔøΩÔøΩmV$LÔøΩÔøΩE\u0016ÔøΩcAe\u001a9^‹¥ÔøΩoÔøΩÔøΩÔøΩ\u000e7ÔøΩux|7ÔøΩsÀòÔøΩ(?#ÔøΩ[ÔøΩ\u001f\u0010ÔøΩBÔøΩ»†\u0005u\u0018uÔøΩ-8ÔøΩ]ÔøΩOÔøΩR\u0005O^[\"ÔøΩ{ÔøΩ8\u00043LÔøΩ0ÔøΩÔøΩÔøΩÔøΩ%jB/ÔøΩÔøΩÔøΩ\u0017\u0001ÔøΩU\u0016ÔøΩRB◊úÔøΩdÔøΩ_ÔøΩx+\u0005ÔøΩ6ÔøΩtÔøΩÔøΩÔøΩÔøΩCÔøΩ^ÔøΩ;ÔøΩÔøΩ\u0010ÔøΩ-ÔøΩÔøΩ\u001f\u0011ÔøΩ\nonÔøΩÔøΩ“•ÔøΩ\u0010\bÔøΩ!\bÔøΩÔøΩ!ÔøΩjÔøΩ<ÔøΩÔøΩ\u0010ÔøΩÔøΩ/0ÔøΩÔøΩ\u001aÔøΩ7ÔøΩ`H\u0012\nA9ÔøΩ1ÔøΩ\u0003ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ+\t\tÔøΩ='8#€ºÔøΩX\bRÔøΩÔøΩoÔøΩÔøΩ9pjÔøΩÔøΩ5\u001bÔøΩ\u001fÔøΩ00'ÔøΩÔøΩÔøΩ{ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩO\nÔøΩÔøΩÔøΩÔøΩWÔøΩÔøΩÔøΩvÿ≤oÔøΩ.,|lÔøΩÔøΩ\nÔøΩ»°ÔøΩ;\u0013'ÔøΩÔøΩÔøΩÔøΩW5ÔøΩ\u0006\u001b`ÔøΩÔøΩCÔøΩ#ÔøΩw*fÔøΩÔøΩvÔøΩÔøΩÔøΩÔøΩÔøΩ\u0007ÔøΩÔøΩÔøΩ>ÔøΩ&ÔøΩÔøΩtÔøΩ/SÔøΩmÔøΩÔøΩi\u0012ÔøΩÔøΩ\u000e?|ÔøΩ+ÔøΩÔøΩ/ÿ±|›Å\u0005ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩgÔøΩÔøΩdJÔøΩG\tÔøΩ2sÔøΩ\u0017\u001aÔøΩ\u0016\"<8\n\u0015ÔøΩÔøΩ\n\\ÔøΩgÔøΩÔøΩ<ÔøΩÔøΩ\u0012ÔøΩÔøΩ%XÔøΩ*ÔøΩ2ÔøΩ7`qÔøΩÔøΩ+ÔøΩÔøΩ6ÔøΩÔøΩÔøΩ«ñ?=ÔøΩÔøΩÔøΩÔøΩ\n<zÔøΩ[ﬂÖÔøΩIZÔøΩÔøΩjÔøΩÔøΩÔøΩ7}\u0016\\ÔøΩÔøΩamÔøΩÔøΩÔøΩCÔøΩ\u001fzuL*\n¬ë\u0004[ÔøΩÔøΩ/ÔøΩ<uÔøΩ\"ÔøΩÔøΩGMÔøΩÔøΩÔøΩxLÔøΩIÔøΩ\"ÔøΩ7ÔøΩYÔøΩ=3qÔøΩ>\n&ÔøΩ'\u0003FÔøΩÔøΩ=ÔøΩÔøΩÔøΩÔøΩÔøΩ}ÔøΩ+ÔøΩÔøΩ\u0016(ÔøΩ22ÔøΩH#LÔøΩ5ÔøΩÔøΩÔøΩkE}^S^CÔøΩÔøΩÔøΩÔøΩCÔøΩÔøΩ6g\n$ÔøΩÔøΩ\u0010ÔøΩs sÔøΩ(]QÔøΩeÔøΩÔøΩ,ÔøΩÔøΩyzÔøΩZ\nG◊≥-\u0019qÔøΩÔøΩÔøΩ√Ñ':ÔøΩs9ÔøΩ&ÔøΩmÔøΩ&—Ñ\u001bÔøΩ'$ÔøΩÔøΩÔøΩÔøΩaÔøΩbCÔøΩ\u0000ÔøΩÔøΩÔøΩ-ÔøΩQ$ \u001fÔøΩÔøΩÔøΩ4tÔøΩ\u0005,q\u0015\u0015\u0013-K\u001bÔøΩÔøΩ\"ÔøΩ\u0018GÔøΩ0ÔøΩBÔøΩ*ÔøΩRÔøΩIÔøΩqÔøΩF&EW8\u0004lÔøΩ'ÔøΩ0\u0000⁄¨ﬁ∞ÔøΩFSÔøΩj^FYJ…Ç\u0016dlK\u000fÔøΩ\u000f5ÔøΩ\\ÔøΩ¬ó}ÔøΩÔøΩÃÖN/\u0017b(ÔøΩ$ÔøΩÔøΩ\nfbÔøΩc\u001bWÔøΩ‘ó.ÔøΩ‹æÔøΩÔøΩÔøΩÔøΩÔøΩ-ƒòÔøΩÔøΩk\\ÔøΩÔøΩÔøΩ04\n{ÔøΩkÔøΩÔøΩC\u0010ÔøΩÔøΩiEBpÔøΩ\u0010ÔøΩ\u0003cÔøΩ\niÔøΩ\u0004TÔøΩa\u0006]ÔøΩX\u0006ÕàÔøΩD9ÔøΩ\u0012ÔøΩ\u0005ÔøΩ\nÔøΩ?ÔøΩÔøΩwgÔøΩ]ÔøΩ\u0011ÔøΩÔøΩÔøΩ!ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ^_ÔøΩZÔøΩÔøΩSB>ÔøΩÔøΩÔøΩCwÔøΩrÔøΩ«à\u0015ÔøΩÔøΩgÔøΩÔøΩÔøΩiÔøΩÔøΩ<ÔøΩÔøΩÔøΩÔøΩÔøΩpÔøΩÔøΩzÔøΩÔøΩ#ÔøΩÔøΩKLLoÔøΩÔøΩÔøΩ{ÔøΩ=ÔøΩh4ÔøΩÔøΩ\nc/ÔøΩ4ÔøΩ\u0000ÔøΩ\u0015P:(ÔøΩÔøΩÔøΩÔøΩ\"ÔøΩÔøΩ\n{ÔøΩÔøΩÔøΩ{œôÔøΩÔøΩÔøΩ{ÔøΩ\u0001ÔøΩ`ÔøΩÔøΩ}ÔøΩÔøΩcÔøΩ{ÔøΩ{ úyÔøΩÃº3ÔøΩ\u0003ÔøΩÔøΩÔøΩAÔøΩ=ÔøΩÔøΩÔøΩY\u0010ÔøΩÔøΩ*\u000eh\u001asÔøΩÔøΩÔøΩnÔøΩÔøΩ\u0001ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\"~ÔøΩÔøΩnÔøΩÔøΩ!ÔøΩÔøΩ\n=&1n\tÔøΩÔøΩ0rÔøΩAŸΩÔøΩ0ÔøΩuÔøΩÔøΩÔøΩ]?ÔøΩÔøΩÔøΩÔøΩÔøΩuÔøΩÔøΩÔøΩÔøΩÂªé\u0019\n àÔøΩÔøΩÔøΩVqÔøΩ6ÔøΩ\n\u0001ÔøΩ›óC\\ÔøΩÔøΩÔøΩÔøΩ-5ÔøΩÔøΩÔøΩÔøΩAÔøΩ\u0018ƒôÔøΩ\"ÔøΩ;$\u0002ÔøΩOZvh–¨ÔøΩ_ÔøΩ{ÔøΩÔøΩÔøΩ;cuÔøΩj€ÑÔøΩÔøΩ@ÔøΩq'.\u0017_ÔøΩÔøΩÔøΩ8\u000f\u001a\u0011,ÔøΩN\u0013ÔøΩ\u0002\"œú>ÔøΩÔøΩÔøΩw/^ÔøΩM9ueÔøΩcRÔøΩnU=ÔøΩkNÔøΩ^ÔøΩÔøΩCvÔøΩÔøΩÔøΩ\u001fFÔøΩ6ÔøΩTÔøΩ/a9ÔøΩÔøΩElÔøΩÔøΩÔøΩ@4\u0007\\ÔøΩÔøΩÔøΩ\u001bt6eÔøΩfÔøΩÔøΩ√£ÔøΩÔøΩÔøΩÔøΩ\u0014\u0003ÔøΩÔøΩÔøΩ’∏$>X\u0013zHÔøΩU\\4YD^aÔøΩqaDÔøΩ]XÔøΩ\u0019hÔøΩÔøΩÔøΩÔøΩ%u\u0019ÔøΩÔøΩÔøΩdÔøΩÔøΩÔøΩ2\u001f\bÔøΩ…éSÔøΩA€ßÔøΩÔøΩ6^xq\u0004~ÔøΩ^;WÔøΩÔøΩÔøΩÔøΩzÔøΩÔøΩ/\nBÔøΩÔøΩÔøΩfMÔøΩ[ÔøΩ[|ÔøΩÔøΩ\u0005)ÔøΩÔøΩÔøΩ\u001aÔøΩÔøΩÔøΩ\nÔøΩ\nMiUÔøΩt\u000fÔøΩJÔøΩÔøΩ5ÔøΩ4mÔøΩ_o“ùÔøΩ\u001fÔøΩÔøΩ\u001b\u0015ÔøΩÔøΩÔøΩW\u001bvVÔøΩ(jÔøΩ&\"ÔøΩÔøΩ⁄§ÔøΩÔøΩ\u0017*%ÔøΩÔøΩ\u0010U:AÔøΩÔøΩÔøΩ\u0015ÔøΩmÔøΩÔøΩ%)\u001bÔøΩr]\u001amÔøΩÔøΩ\"ÔøΩR=ÔøΩzhÔøΩzÔøΩ\u0018ÔøΩYSÔøΩ’∏ÔøΩ\u0014ÔøΩlÔøΩÔøΩ+\\VUÔøΩÔøΩÔøΩ\u0013QÔøΩ5PÔøΩ`7ÔøΩn[ÔøΩÔøΩÔøΩ\u0012ÔøΩq›†⁄ÜÔøΩ\u0014ÔøΩ\u000e–∫ÔøΩBÔøΩÔøΩ*ÔøΩVÔøΩ{ÔøΩÔøΩÔøΩtÔøΩÔøΩ6\n;hÔøΩ\u0014ÔøΩÔøΩuPÔøΩ\u0010LoÔøΩÔøΩD(»á\u0006≈òÔøΩ\u0000K^ÔøΩÔøΩM5ÔøΩÔøΩYÔøΩXÔøΩRÔøΩÔøΩÔøΩ\tÔøΩ\"tÔøΩ\u001fÔøΩ8ÔøΩ\u00183ÔøΩÔøΩ7\u0015\u0003g\u0005ÔøΩ*ÔøΩu\u001fÔøΩÔøΩÔøΩkÔøΩxÔøΩ}.]{\u0000p'ÔøΩ\u0015ÔøΩ\u0014ÔøΩVrÔøΩÔøΩÔøΩCÔøΩÔøΩG&^ÔøΩHÔøΩ\u0018\n_ÔøΩÔøΩ]ÔøΩ]p\u0013`ÔøΩ?<oÔøΩ}BÔøΩÔøΩ\u001bÔøΩÔøΩ;ÔøΩ:\ntÔøΩcmDx\n{\u000f_ÔøΩÔøΩÔøΩﬁ£\u0017\u0000ÔøΩ\u0007ÔøΩ\u00148\u0007ÔøΩÔøΩÔøΩ1qÔøΩÔøΩ1YÔøΩ,\u0015ÔøΩ÷ΩÔøΩÔøΩNÔøΩ\n⁄£ÔøΩoÔøΩNp\u0002ÔøΩ[ÔøΩÔøΩÔøΩ~ÔøΩÔøΩ7ÀüN]\njÔøΩ\u0016ÔøΩ\n.ÔøΩ\u0011ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ.ÔøΩuÔøΩÔøΩ;ÔøΩ'47ÔøΩ‘ïÔøΩwÔøΩ>xÔøΩ\u0012\nÔøΩÔøΩw}ÔøΩm\nÔøΩ!ÔøΩWÔøΩÔøΩ%}fÔøΩÔøΩÔøΩ:ÔøΩ/ÔøΩljVYÔøΩÔøΩ[ÔøΩ\u000eÔøΩYe\u00137zA`ÔøΩ\t{ÔøΩÔøΩÔøΩ\nÔøΩ\nÔøΩnÔøΩjÔøΩ:\u0010ÔøΩkÔøΩÔøΩÔøΩÀ°ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩssBÔøΩ\nÔøΩÔøΩ\tAÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩlÔøΩ◊ÖRÔøΩÔøΩgÔøΩKÔøΩ\u000eu`{ÔøΩ41”î>lÔøΩÔøΩi5ÔøΩ}Àè\u0015lU9DÔøΩÔøΩF<T√ÑﬁÄVu\u0012\u0018ÔøΩKÔøΩÔøΩÔøΩ6ÔøΩET…ùÔøΩ.ÔøΩÔøΩ#4ÔøΩÔøΩ~vÔøΩM&cÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩOÔøΩ$TÔøΩpÔøΩ\b;ÔøΩÔøΩÔøΩÔøΩ.ÔøΩÔøΩÔøΩÔøΩÔøΩiÔøΩ\u0013*ÔøΩ”≤ÔøΩvDVÔøΩ“¢:VÔøΩÔøΩ\u0004?Gvu€æÔøΩÔøΩ›ûÔøΩ\u0004Y8\u0016[\u0011ÔøΩ\u0005ÔøΩ\u0019/ÔøΩpbÔøΩ9ÔøΩ\nA+ÔøΩ9ÔøΩ\u0000yÔøΩÔøΩ◊ãÔøΩ:ÔøΩl‰ØÇBÔøΩÔøΩ'ÔøΩX\u0018ÔøΩÔøΩHÔøΩ.,_ÔøΩ\"ÔøΩ\u0003ÔøΩÔøΩdTÔøΩÔøΩÔøΩÔøΩÔøΩuhÔøΩÔøΩX…¨ByÔøΩÔøΩM–î\"\u0016ÔøΩÔøΩÔøΩ%ÔøΩ&mÔøΩÔøΩ^ÔøΩ$d'\b=lÔøΩ9ÔøΩuÔøΩ*Ó∫¥ÔøΩzIuÔøΩsÔøΩuXÔøΩÔøΩxÔøΩbÔøΩÔøΩÔøΩ\u001aÔøΩOÔøΩ~ÔøΩoIÔøΩÔøΩÔøΩ\u0013\u0018!\u0012ÔøΩÔøΩVÔøΩi3ÔøΩÔøΩO$X]ÔøΩpÔøΩÔøΩÔøΩFÔøΩÔøΩ:ƒ•Z\u0013∆íÔøΩÔøΩz◊§ÔøΩe5_«∫ÔøΩ=7 üÔøΩ\u000140 pÔøΩxÔøΩÔøΩÔøΩÔøΩÔøΩ\u000eÔøΩ>~ÔøΩÔøΩwÔøΩ\n/ÔøΩÔøΩ\tÕõÔøΩ2LÔøΩtÔøΩÔøΩÔøΩÔøΩÔøΩŸõEÔøΩb~\u0019«àÔøΩ!ÔøΩÔøΩWC]ÔøΩÔøΩ0bÔøΩ]^'ÔøΩMÔøΩnÔøΩŸöÔøΩ⁄≥#E\bF\nÔøΩuÔøΩ‚†ø7\n\u0006ÔøΩv;ÔøΩuÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩoGÔøΩÔøΩÔøΩkÔøΩÔøΩ&GAÔøΩ%k`ÃπIK\u000fÔøΩX\u0013ÔøΩ\u0013ÔøΩÔøΩ\u001fÔøΩÔøΩ\u0017ÔøΩÔøΩÔøΩ-ÔøΩÔøΩT\u001fÔøΩj\u0019;]xBaÔøΩÔøΩk;=ÔøΩÔøΩ\nw2ÔøΩzZNÔøΩÔøΩe!ÔøΩÔøΩÔøΩÔøΩ=\u0013ÔøΩRÔøΩÔøΩ)\thzÔøΩÔøΩ\u0010ÔøΩÔøΩ/\u00069ÔøΩ&sÔøΩ\u0019ÔøΩÔøΩ{3ÔøΩ!MÔøΩ2ÔøΩÔøΩ\u000fÔøΩÔøΩGÔøΩ\u000fÔøΩÔøΩÔøΩ√´ÔøΩ„∂πÔøΩÔøΩÔøΩ<\nÔøΩZ|ÔøΩﬁ≥^ÔøΩ|\u001bwÔøΩÔøΩ2Y@8ÔøΩÔøΩÔøΩQ\nZÔøΩÔøΩÔøΩY\u00014jÔøΩÔøΩÔøΩÔøΩzg\u00060ÔøΩÔøΩlQ\nÔøΩ‹ö\t\u0005ÔøΩ÷Å;ÔøΩtÔøΩIL\u0005JÔøΩ3ÔøΩÔøΩd)$<\nÔøΩ|ÔøΩD\u0015NÔøΩ\u000f\u0002jÔøΩÔøΩÔøΩIÔøΩÔøΩ%ÔøΩZÔøΩÔøΩŸ®ÔøΩÔøΩ\u0006+e2R\bÔøΩ*ÔøΩÔøΩ\u00182)qIwÔøΩÔøΩAÔøΩ\u0015\u0018FÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0011Œ≠ÔøΩÔøΩ6ÕûÔøΩ^ÔøΩNWÔøΩÔøΩr;ÔøΩ^ÔøΩR8ÔøΩ'ÔøΩÔøΩhÔøΩÔøΩd\u00013ÔøΩÔøΩ\u0010n]W%MsÔøΩÔøΩ6}‘ÄÔøΩ'eÔøΩÔøΩ,iZÔøΩÔøΩÔøΩÔøΩ^\u0019=ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ6ÔøΩ*ÔøΩÔøΩÔøΩ(\nAÔøΩiÔøΩ8uXÔøΩ\u001b&f26ÔøΩaÔøΩ«™ÔøΩÔøΩÔøΩÔøΩZÔøΩM%ÔøΩ`jÔøΩB-ÔøΩÔøΩÔøΩÔøΩ!(*ÔøΩÓ≠Ø√≤ÔøΩqÔøΩ\u001b)LÔøΩ2W[}\nÔøΩÔøΩAÔøΩBÔøΩgC]ÔøΩÔøΩ0ÔøΩÔøΩi\u0005\u0013ÔøΩEÔøΩÔøΩUÔøΩ\u0015?ÔøΩÔøΩÔøΩ}‘ê9ÔøΩÔøΩÔøΩLÔøΩÔøΩh\"ÔøΩPÔøΩs$ÔøΩ@>FJ#ÔøΩLÔøΩFÔøΩOL!ÔøΩ\u001b\nÔøΩÔøΩhÔøΩ3~ÔøΩ5ÔøΩVÔøΩ3#:#cÔøΩ:ÔøΩÔøΩ\u0019Ã´%ÔøΩcÔøΩ|ÔøΩÔøΩÔøΩ5ÔøΩÔøΩZ^ÔøΩ”ÆÔøΩÔøΩÔøΩ(ÔøΩÔøΩ\u0000xYÔøΩÔøΩﬁàÔøΩÔøΩÔøΩÔøΩ;\ntÔøΩÔøΩÔøΩ∆ÄÔøΩ7tÔøΩÔøΩÔøΩÔøΩs;<ÔøΩ>\u001fÔøΩÔøΩ//GÔøΩÔøΩÔøΩ\u0003ÔøΩ=ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩZ)HtÔøΩ[≈ß_\nÔøΩ8XOÔøΩWÔøΩƒóÔøΩ{ÔøΩ}ÔøΩÊÑ•ÔøΩ~\u0019ÔøΩÔøΩgÔøΩ\u001fÔøΩÔøΩÔøΩÔøΩOÔøΩ,\nÔøΩ◊∑ÔøΩÔøΩvy\nÔøΩÔøΩjÔøΩBÔøΩ8ÔøΩ%ÔøΩÔøΩfÔøΩI{ÔøΩÔøΩÔøΩÔøΩ⁄às%wÔøΩÔøΩÔøΩ_ÔøΩÔøΩJÔøΩÔøΩfÔøΩÔøΩ0WÔøΩÔøΩÔøΩ~ayÔøΩ'ÔøΩÔøΩ÷ÆÔøΩÔøΩg\u0003ÔøΩ:\u000evÔøΩrÔøΩ\nÔøΩsÔøΩÔ£ßÔøΩÔøΩ{ÔøΩÔøΩÔøΩÔøΩ,ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ{ÔøΩ\u001aÔøΩXzÔøΩ\u0010ÔøΩÔøΩ'/\nÔøΩaÔøΩyRÔøΩBYrÔøΩ-ÔøΩÔøΩUX\u0001>\u0016QÔøΩÔøΩ^ÃßÔøΩÔøΩ7ÔøΩÔøΩÔøΩkÔøΩÔøΩÔøΩÔøΩÔøΩYKÔøΩ\u0012ÔøΩÔøΩ3ÔøΩÔøΩ\nÔøΩÔøΩÔøΩ”≠ÔøΩ-ÔøΩÔøΩt'ÔøΩ%r\u0005\u001bw1OmV3\u0015\u0013ÔøΩ\u00151\u0015ÔøΩÔøΩf+ÔøΩÔøΩ})[A\u0017M.ÔøΩÔøΩ&?>3ÔøΩÔøΩAX-«µÔøΩ\u0007ÔøΩ\u0019ÔøΩj@ÔøΩ#—≤ÔøΩD]ÔøΩÔøΩbÔøΩÔøΩ&ÔøΩ_ÔøΩÔøΩ\u0006ÔøΩ|ÿ≥ÔøΩsT-T\nvÔøΩÔøΩÔøΩÔøΩ\u0004ÔøΩ\u0001jZ,ÔøΩLt\u0002€∏+?ÔøΩ`ÔøΩt÷öﬂ´rÔøΩ%ﬂµÔøΩ0ÔøΩÔøΩeMÔøΩ\\ÔøΩÔøΩ\tÔøΩ%ÔøΩÔøΩÔøΩÔøΩÔøΩ\u000edg\nÔøΩÔøΩÔøΩÀçmv'ÔøΩÔøΩÔøΩÔøΩwÔøΩEu ÔøΩÔøΩ\u0000ÔøΩÔøΩÔøΩÔøΩÔøΩ9\u0003ÔøΩÔøΩÔøΩ\n(ÔøΩ\u0004ÔøΩUwDÔøΩ2.}-ÔøΩ!≈ïÔøΩÔøΩÔøΩÔøΩÔøΩ”çÔøΩ“æÔøΩc]ÔøΩ\nÔøΩÔøΩÔøΩ\nÔøΩ÷§ÔøΩÔøΩÔøΩH\u0000ÔøΩ\ndÔøΩ\u000eZÔøΩÔøΩ_’µ\nHÔøΩÔøΩSÔøΩ\u001bÔøΩÔøΩ\u0010-XÔøΩÔøΩB5^ÔøΩÔøΩ\u0015ÔøΩbÔøΩŒ©VÔøΩ1ÔøΩÔøΩÔøΩsÔøΩÔøΩÔøΩ\u0011`ÔøΩB0ÔøΩÔøΩÔøΩIWÔøΩÔøΩK\bAÔøΩÔøΩÔøΩcÔøΩÔøΩ÷¶b('.=ÔøΩLÔøΩDÔøΩ\u000fÔøΩvÔøΩ(ÔøΩ/ÔøΩÔøΩoÔøΩÔøΩfÔøΩÔøΩ\\ÔøΩ-fÔøΩ}ÔøΩÔøΩÔøΩ\t{ÔøΩÔøΩ\n\u0013ÔøΩ(ÔøΩ\nÔøΩÔøΩÔøΩ{ÔøΩ_ÔøΩÔøΩÔøΩGÔøΩÔøΩÔøΩxDÔøΩÔøΩÔøΩGÔøΩ\u001f;^ÔøΩmÔøΩN$kÔøΩ\u0001ÔøΩÔøΩgÔøΩaR\u00012wÔøΩÔøΩÔøΩR=RSÔøΩÔøΩfÔøΩ_ÔøΩqÔøΩÔøΩÔøΩ`ÔøΩÔøΩuÔøΩ\u0011\tÔøΩÕå.ÔøΩ=fA`ZNÔøΩ÷ù\u0002OÔøΩÔøΩ2ÔøΩdÔøΩuÔøΩÔøΩ@ÔøΩ\n+ÔøΩÔøΩy\u0006ÔøΩ^gÔøΩ8osÔøΩÔøΩÔøΩ!ÔøΩgÓùπ.ÔøΩÔøΩÔøΩ$ÔøΩ\n\bÔøΩÔøΩ%yÔøΩsÔøΩ&ÔøΩd\u0000g\u001bÔøΩÔøΩÔøΩÔøΩÔøΩ'rÔøΩ~6ÔøΩÔøΩtÔøΩÔøΩÔøΩt\u000fÔøΩÔøΩLÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩwÔøΩ@ÔøΩÔøΩÔøΩ?\u000eIÔøΩXm”¥BV\\ÔøΩÔøΩÔøΩÔøΩzÔøΩ6f\nÔøΩ\"6ÔøΩÔøΩÔøΩNÔøΩjjÔøΩÔøΩ-ÔøΩ}VÔøΩ>ÔøΩÔøΩ\\jÔøΩU≈ÄQÔøΩ2ÔøΩÔøΩ&ÔøΩ…∫iÔøΩVÔøΩb7-t\u0004N–ô/ÔøΩPÔøΩÔøΩÔøΩÔøΩLFjyÔøΩRÔøΩÔøΩ\u0017ÔøΩ3ÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩ?≈ú\\UÔøΩq8SÔøΩÔøΩgI(„¨∫ÔøΩÔøΩ$ÔøΩLÔøΩC\u001bSÔøΩÔøΩÔøΩÔøΩŸ≤ÔøΩÔøΩh&kLÔøΩÔøΩ.ÔøΩ%ÔøΩBÔøΩ\u001aÔøΩ,?ÔøΩÔøΩÔøΩ9ÔøΩÔøΩÔøΩÔøΩÔøΩk:\n*ÔøΩ«û\u001anIVCA8ÔøΩoÔøΩÔøΩV\u0012\u00158\u0011ÔøΩÔøΩÔøΩSÔøΩ\u0010\u000eÔøΩÔøΩÃú\u0003MÔøΩ\"ÔøΩÔøΩLÔøΩÔøΩÔøΩXÔøΩÔøΩÔøΩÔøΩ+ÔøΩÔøΩ(j\\ÔøΩÔøΩ\u0011ÔøΩt'NÔøΩz(q’à\u001aoÔøΩ\b\n”íÔøΩTÔøΩ?)ÔøΩÔøΩHgcC\u0015ÔøΩÔøΩ}ÔøΩJ\u0005ÔøΩFx,GÔøΩÔøΩÔøΩ5RyÔøΩ>ÔøΩch\nÔøΩ\u0007€ñÔøΩÔøΩÔøΩŸô}ÔøΩÔøΩÔøΩT\bÔøΩÔøΩ5ÔøΩÔøΩ+B\u0007ÔøΩmÔøΩ:ÔøΩÔøΩÔøΩ«çÔøΩ05[GtÔøΩY\u0015ÔøΩWC\\ÔøΩÔøΩzLÔøΩÔøΩ\u00057?\u001bÔøΩhÔøΩÔøΩ5ÔøΩleÔøΩ}ÔøΩÔøΩÔøΩﬁ§e!ÔøΩ\u0011s\u000f\nÔøΩÔøΩwÔøΩÔøΩ\u0000ÔøΩÔøΩÔøΩq\u000e{3ÔøΩ\u000fÔøΩÔøΩÍû∫x[ÔøΩ»π\u0007\u0017oÔøΩ\u0005\u0010ÔøΩ\u0011ÔøΩa>ÔøΩ?|ÔøΩ) SÔøΩ~ÔøΩcÔøΩÔøΩÔøΩ;?ÔøΩÔøΩÔøΩZ\u0005ÔøΩ\bÔøΩ~ÔøΩÔøΩÔøΩÔøΩtIÔøΩÔøΩÔøΩÔøΩkP\nÔøΩ<ÔøΩq\u0017ÔøΩÔøΩgÔøΩÔøΩ:ÔøΩÔøΩ\u0002ÔøΩÔøΩƒ•ÔøΩÔøΩÔøΩÔøΩÔøΩfÔøΩ+ÔøΩelÔøΩÔøΩÔøΩ}\nÔøΩ\u000efÔøΩ\\\u001b9uEÔøΩ\nÔøΩÔøΩ\ni~ÔøΩy\u000eÔøΩ\u0019ÔøΩÔøΩ\nÔøΩÔøΩLZvh»ú}ÔøΩÔøΩÔøΩ}?ÔøΩÔøΩÔøΩ\u0001N›¶xÔøΩ\\}0ÔøΩÔøΩ\u0000FÔøΩÿâi6,(,ÔøΩÔøΩbÃÜÔøΩÔøΩ&ÔøΩm`ÔøΩ\\ÔøΩÔøΩÔøΩÔøΩÔøΩ\u000f\u0018ÔøΩÔøΩÔøΩÔøΩz)ÔøΩkrÔøΩiQÔøΩÔøΩÔøΩ\u001a\n\u0003ÔøΩfŒâVÔøΩrÔøΩ.ÔøΩ\u001aÔøΩlÔøΩÔøΩ\\ÔøΩÔøΩRÔøΩBÔøΩ\tuÔøΩÔøΩ$ÔøΩJÔøΩÔøΩÔøΩaÔøΩFrkÔøΩ;ÔøΩ‹†PÔøΩX\n5ÔøΩ,\u000fÔøΩÔøΩYKÔøΩÔøΩ\b;ÔøΩ\u0001\u0013ÔøΩ3ÔøΩ+{kÔøΩ\u00134K{ÔøΩ'ÔøΩÔøΩ\u0018ÔøΩ\nÔøΩÔøΩÔøΩÛ©úÑÔøΩ]ÔøΩ óÔøΩkÔøΩ*ÔøΩ$TÔøΩÔøΩiÔøΩ\u0016ÔøΩÔøΩÔøΩvÔøΩÔøΩÔøΩÔøΩg ÅÔøΩ\bÔøΩ'ÔøΩÔøΩÔøΩ]ÔøΩÔøΩHÔøΩÔøΩ#\nvÔøΩÔøΩ!ZJmxÔøΩ*ÔøΩÔøΩ2ÔøΩ\nÔøΩÔøΩÔøΩ@0PÔøΩ\u0013ÔøΩÔøΩz}T\bÔøΩÔøΩOÔøΩKÔøΩ(\u0002ÔøΩ\u0014*V-ÔøΩL—ò<ÔøΩ—ö{ÔøΩTÔøΩÔøΩd<ÔøΩnÔøΩ\u00036r\u0013\nE\u0010ÔøΩZÔøΩÔøΩbÔøΩÔøΩÔøΩL’´ÔøΩjÔøΩÔøΩÔøΩÔøΩÔøΩ\u0005\u0019f-{ÔøΩEÔøΩ\u000fÔøΩÔøΩ;ÔøΩUuR∆®ÔøΩÔøΩNÔøΩÔøΩÔøΩ1&ÔøΩDÔøΩÔøΩSÔøΩÔøΩjÔøΩÔøΩÔøΩ\t2kYÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ0–ôÔøΩ ÔøΩ ÔøΩÔøΩÔøΩ\u0012T\u001aÔøΩ$ÔøΩQÔøΩdX∆ñJ\u0006TwÔøΩUÔøΩÔøΩ}kÔøΩbd\nÔøΩÃïÔøΩA2eY»´7\u0015\u0014\tÔøΩÔøΩTÔøΩ\u0001ÔøΩÔøΩ ŒàÔøΩd$W\u0017ÔøΩÔøΩ|ÔøΩÔøΩÔøΩGÔøΩÔøΩ?\u0005xÔøΩ«∫ÔøΩ\u0011s\u000fÔøΩ:ÔøΩÔøΩAÔøΩT)ÔøΩ]?G\u001bÔøΩ\nNIÔøΩ\u000ewÔøΩÔøΩJg[}ÔøΩÔøΩÔøΩ(ÔøΩd{ÔøΩLœà&\u0004v\u0019ÔøΩÔøΩÔøΩ’∂ÔøΩÔøΩÔøΩ@\u0000ÔøΩÔøΩÔøΩÔøΩÔøΩ\u000eÔøΩÔøΩsÔøΩÔøΩc=RÔøΩÔøΩ:\u000e—ôd-ÔøΩgÔøΩÔøΩÔøΩs7ÔøΩ\u001aÔøΩ*ÔøΩÔøΩR]ÔøΩ\u0003ÔøΩ`ÔøΩÔøΩ\u0003Wn<ÔøΩ\nÔøΩÔøΩ0ÔøΩ\tK\u0000/ﬁúÔøΩ«ø\nÔøΩÔøΩG{(2\u0016+ÔøΩÔøΩ—ªÔøΩÔøΩÔøΩ\u001fÔøΩ(hÔøΩÔøΩ(\u001bÔøΩÔøΩ=ÔøΩN/ÔøΩyÔøΩÔøΩÔøΩÔøΩÿØ#\u0012\nÔøΩÔøΩ/ÔøΩ>ÔøΩ'KyaÔøΩ&ÔøΩZ\\ÔøΩUÔøΩÔøΩ\u0000vÔøΩSÔøΩhÔøΩJÔøΩ?ÔøΩsÔøΩÔøΩ\u001f\u0012ÔøΩÔøΩÔøΩÔøΩ<0ÔøΩfÔøΩPÔøΩÔøΩjÔøΩH`\nVÔøΩZ\u0007ÔøΩÔøΩÔøΩ0ÔøΩÔøΩÔøΩÔøΩ«∏ÔøΩÔøΩÔøΩauÔøΩÔøΩ@ÔøΩIÔøΩÔøΩkp/\u0014WÔøΩ'ÔøΩJÔøΩLÔøΩC*ÔøΩ”åÔøΩ>ÔøΩH\u001bbUMFÔøΩZÔøΩ6\u001bÔøΩ\nÔøΩ|6ÔøΩb^?ÔøΩ\u001fÔøΩÔøΩkÔøΩRÔøΩ4VÔøΩg\u0007ÔøΩV\bÔøΩÔøΩÔøΩÔøΩZcÔøΩÔøΩÔøΩÔøΩL\u0018ÔøΩNÔøΩ\tÔøΩNÔøΩ)ÔøΩyuÔøΩ\u0017\nq0>.ÔøΩ\nÔøΩÔøΩSÔøΩ9ÔøΩB\u0013ÔøΩÔøΩ:ÔøΩÔøΩÔøΩ\u000epÔøΩOÔøΩnƒ£ÔøΩaÔøΩKÔøΩÔøΩÔøΩ\nÔøΩCgÔøΩoÔøΩ!ÔøΩÔøΩD—ø-ÔøΩÔøΩG&q\n4ÔøΩqÔøΩ\u0014ÔøΩÔøΩÔøΩÔøΩ|&B&ÔøΩÔøΩ\u0015ÔøΩ…ÖM⁄æ&p;7\nÔøΩ|\buZLÔøΩ\u0002ÔøΩMO\u0001ÔøΩÔøΩ\bÔøΩÔøΩ\u0013ÔøΩÔøΩ4ÔøΩÔøΩÔøΩ8ÔøΩÔøΩÔøΩÔøΩÔøΩœâfÔøΩÔøΩÔøΩÔøΩUÔøΩÔøΩ\u00155ÔøΩ\u0012eÔøΩÔøΩÔøΩRÔøΩyÔøΩ\nÔøΩÔøΩÔøΩ\n?\u001fÔøΩÔøΩqÔøΩ\n\u00104w`ZÔøΩG\u0015ÔøΩÔøΩÔøΩÔøΩÔøΩfÔøΩÔøΩÔøΩ,\u0005ÔøΩ\\ÔøΩÔøΩoxÔøΩG`NE\u0005\n\u0003~ÔøΩÔøΩrÔøΩÔøΩ}ÔøΩÔøΩe\u0011MJÔøΩ»é_\u0014ÔøΩÔøΩÔøΩ[«ΩÔøΩ1iÔøΩÔøΩ\n\u0012ÔøΩwÔøΩÔøΩ«å\u0005ÔøΩ\n=?08ÔøΩÔøΩÔøΩ%ÔøΩÔøΩ2:\ntÔøΩnÔøΩÔøΩÔøΩ\u001b\u000fÔøΩ[\u0014DWbJouK-ÔøΩ|/ÔøΩÔøΩ6ÔøΩÔøΩÔøΩÔøΩ}ÔøΩÔøΩuÔøΩÔøΩÔøΩÔøΩ\u0018\u000fqÔøΩtÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ[PÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ7GÔøΩÔøΩÔøΩŒ±i%?ÔøΩ1ÔøΩÔøΩÔøΩ\u00068\nN.ZÔøΩ-Vj\u0015^ÔøΩn›¶x\u000fÔøΩ\u0019ÔøΩforINÔøΩ.+ÔøΩÔøΩ(0ÔøΩÔøΩÔøΩoÔøΩÔøΩClÔøΩKvÔøΩ.ÔøΩ\u001aÔøΩÔøΩ\b7\" òÔøΩ<$&ÔøΩxÔøΩÔøΩCx\u0011ÔøΩ4ÔøΩ\nÔøΩÔøΩ\n)ÔøΩqÔøΩÔøΩÔøΩÔøΩO8ÔøΩ\\ÔøΩÔøΩÔøΩÿÆ=ÔøΩ++ÔøΩÔøΩÔøΩÔøΩ\\;tÔøΩ\u0005%\tiÔøΩÔøΩÔøΩHÔøΩÔøΩÔøΩ:QÔøΩy‘ü<{ÔøΩÔøΩ|ÔøΩ~ÔøΩÔøΩJC5MnÔøΩÔøΩ.ÔøΩj3\nÔøΩÔøΩk#ƒ°]ÔøΩsR\u0015MÔøΩÔøΩ\ndsfÔøΩ\u0016ÔøΩlÔøΩ\n>ÔøΩÔøΩÔøΩÔøΩÔøΩ\u001aÔøΩsCObÔøΩi\\ÔøΩDÔøΩÔøΩ?lIÔøΩw\u0004:#ÔøΩ4\u0016AÔøΩ2\u0019ÔøΩ4ÔøΩÔøΩÔøΩ%ÔøΩNÔøΩcW„Ö¢\u0018}eÔøΩO\u0003DÔøΩrÔøΩÔøΩP7\u0019ÔøΩÔøΩ]?ÔøΩ\u001aÔøΩJÔøΩj6yÊ∏™ÔøΩ\u0018/(\\/\nÔøΩÔøΩÔøΩÔøΩ\u0019ÔøΩÔøΩÎµë\u0006ÔøΩwQÔøΩ4\u0016\u001amÔøΩ-ÔøΩ\u0002ÔøΩÔøΩÔøΩMÔøΩÔøΩÔøΩnÔøΩMÔøΩ\u0013⁄∞ÔøΩÔøΩÔøΩ\u0018—û\u0010.ÔøΩÔøΩQÔøΩÔøΩ\u0011gx+#ÔøΩ%\u0001CÔøΩSHZV\u0015ie\nÔøΩQÔøΩÔøΩÔøΩÔøΩ‹∏ÔøΩu[mÔøΩj5\u0010uÔøΩÔøΩÔøΩÔøΩHÔøΩÔøΩrY-\u0005ÔøΩ!QAÔøΩÔøΩ[ÔøΩpÔøΩnÔøΩÔøΩÔøΩ5ÔøΩÔøΩ\nTq\nÔøΩÔøΩ\u0014ÔøΩJÔøΩÔøΩPﬁ∫ÔøΩM ÔøΩÔøΩOÔøΩ«ÆÔøΩHT+\u0004ÔøΩÔøΩAÔøΩCÔøΩBÔøΩrÔøΩÔøΩÔøΩ\u00036ÔøΩÔøΩÔøΩDÔøΩ^cÔøΩÔøΩ\u0016ÔøΩÔøΩÔøΩ\u001a6ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ¬êÂûøÔøΩN0?|ÔøΩpÔøΩ.ÔøΩÔøΩ4ÔøΩÔøΩÕµUy\u0010ÔøΩÔøΩ/ÔøΩdÔøΩÔøΩ\u0011s\u000fb–•\\ÔøΩÔøΩ\u0007:\u000fÔøΩ\u00150gCÔøΩÔøΩÔøΩiÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ4ÔøΩc‘ºÔøΩÔøΩmÔøΩm}2ÔøΩuNN9uÔøΩÔøΩÔøΩ*2.,ÔøΩ\u000e\u0003ÔøΩ\u0006ÔøΩ\nXÔøΩ=ÔøΩÔøΩ?ÔøΩ?<ÔøΩÔøΩÔøΩ\u0003GŒäÔøΩVÔøΩÔøΩM»ΩpÎó±ÔøΩ∆ºÔøΩw#ÔøΩÔøΩFÔøΩÔøΩ?|ÔøΩÔøΩ\u0000G=ÔøΩÔøΩ0;ÔøΩTÔøΩ}ÔøΩÔøΩÔøΩ\u0011t?ÔøΩÔøΩ\u0013ÔøΩ\nÔøΩ.e2‚ï∑ÔøΩÔøΩÔøΩÔøΩ|\u0014\n{ÔøΩ+8gÔøΩ[\nÔøΩÔøΩc=X\u0018ÔøΩÔøΩ|ÔøΩÀ§=ÔøΩbœØÔøΩKÔøΩÔøΩKÔøΩÔøΩ\u000eÔøΩÔøΩÔøΩÔøΩ<ÔøΩ\\\u001fÔøΩÔøΩÔøΩÔøΩsbÔøΩÔøΩÔøΩ\u0011\u001fÔøΩiÔøΩÔøΩV}\u0018ÔøΩ\nÔøΩÔøΩPÔøΩ~ÔøΩÔøΩ[ÔøΩ9ÔøΩÔøΩuÔøΩfÔøΩ\u0014ÔøΩÔøΩÔøΩÔøΩFÔøΩNV0w\u0019\u0013QOÔøΩ)ÔøΩy\u0014ÔøΩ\u0007ÔøΩ\nÔøΩÔøΩÔøΩi\u001bÔøΩIÔøΩÔøΩ\tÔøΩT\u0015[ÔøΩÔøΩYrY-E\u0002ÔøΩ&w[1xÿõÔøΩÔøΩÔøΩÔøΩ‰ãßÔøΩ\nÔøΩÔøΩ\nÔøΩrÔøΩÔøΩÔøΩ=ÔøΩÔøΩqÔøΩjÔøΩhÔøΩ.⁄∫ÔøΩÔøΩ\nÔøΩnÔøΩ\u0014\u0002ÔøΩcR\"\u0000ÔøΩ\u0019Q\u0019ÔøΩ\u001bÔøΩÔøΩZo]ÔøΩ)◊™OÔøΩ(ÔøΩÔøΩÔøΩ/ÔøΩ,ÔøΩÔøΩÔøΩxrÔøΩÔøΩÔøΩmzÔøΩp>axEÔøΩÔøΩ\u000e\nÔøΩÔøΩnfÔøΩB0ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ›àÔøΩÔøΩ÷ÄÔøΩÔøΩÔøΩ\u0012KÔøΩÔøΩNfU\u0016!ÔøΩ0%\u0012ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0013ÔøΩÔøΩ`yNt#ÔøΩÔøΩE\u0005ÔøΩ$ÔøΩ\u0006\nÔøΩÔøΩÔøΩÔøΩY'5\u001fÔøΩÔøΩÔøΩÔøΩBhAmÔøΩUUÔøΩÔøΩÔøΩÔøΩylÔøΩF–¶RÔøΩRÔøΩsuÔøΩÔøΩÔøΩFÔøΩÔøΩÔøΩiÔøΩ.GÔøΩuÔøΩÔøΩ&\u0005o◊íhÔøΩÔøΩ|ÔøΩ\u0010ÔøΩ3ÔøΩÔøΩÔøΩ%\u0000ÔøΩnÔøΩE\u0014rÔøΩÔøΩÔøΩ-VÔøΩ4ÔøΩ\u0011ÔøΩMI!ÔøΩÔøΩUi\u0003.ÔøΩ9w(]–∂ÔøΩ\u0003ÔøΩ\u0012T'ÔøΩ\\ÔøΩ_ÔøΩÔøΩHzÔøΩÔøΩPÔøΩMÔøΩ\u0001,ÔøΩ\u0006{ÔøΩ*5ÔøΩ\u0007\u0017ÔøΩÔøΩÔøΩMÔøΩÔøΩwÔøΩ{\u000f_ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩlÔøΩÔøΩ\u0018Ãæ\u0018ÔøΩ'-\u0005ÔøΩÔøΩÔøΩ<!ÔøΩÔøΩÔøΩoÔøΩ\u0016ÔøΩ‘ùÔøΩÔøΩm|3ÔøΩ\u0004ÔøΩÔøΩ-\nNÔøΩÔøΩÔøΩ3z~ÔøΩNIÔøΩÔøΩ9ÔøΩÔøΩc\u0016\u0004~9ÔøΩ\u0015ÔøΩqÔøΩÔøΩ1=ÔøΩÔøΩÔøΩ\nÔøΩÔøΩ\u0007ÔøΩÔøΩÔøΩ}ÔøΩÔøΩ\u0007ÔøΩ<ÔøΩ\\ÔøΩ?J\u0019CÔøΩ7JÔøΩ.ÔøΩ\u000fÔøΩÔøΩS\u0003KÔøΩ\n]ÔøΩÔøΩÔøΩÔøΩÔøΩŸßÔøΩ\u001a(\u0013ÔøΩÔøΩÔøΩÔøΩ<FJÔøΩCÔøΩ’ôÔøΩÔøΩ\nÔøΩ\u001aÍíòYÔøΩÔøΩ.AfÔøΩIe[\u001bvﬁµÔøΩ6nÔøΩÔøΩ}\n\u0007ÔøΩ\u0000,ÔøΩÔøΩ8ÔøΩ\u001fÔøΩÔøΩuwÔøΩy\u0007zLÔøΩ\u0019ÔøΩ8\u0018hÔøΩÔøΩ`ÔøΩgPÔøΩÔøΩÔøΩÔøΩoÔøΩÔøΩwÔøΩÔøΩ_ÔøΩ€ÅSÔøΩÔøΩ\u001fÔøΩg\\ÔøΩ0–©i\u000fÔøΩx\u0003ÔøΩ\u001fÔøΩÔøΩÔøΩÔøΩ\u0003rÔøΩ$ÔøΩ,m√©ÔøΩYÔøΩÔøΩÔøΩÔøΩÔøΩnÔøΩÔøΩÔøΩ?ÔøΩ?–às\u0012rÔøΩY?\n\u0010MÔøΩ?MÔøΩÔøΩ,ÔøΩÔøΩSÔøΩÔøΩ2ÔøΩÔøΩq{Ã¥ÔøΩ5\u0011ÔøΩÔøΩÔøΩÔøΩOÔøΩÔøΩ\u0011\u0003—Ω\u0019gÔøΩÔøΩÔøΩÔøΩÔøΩ9ÔøΩ9ÔøΩYPGÔøΩs\u0014\nB\u0001!-ÔøΩ\nÔøΩÔøΩ*ÔøΩÔøΩY⁄µÔøΩ\u00106];ÔøΩhÔøΩÔøΩ.\u0002ÔøΩÔøΩ_3\u0011ÔøΩ\u0018ÔøΩ\u0006ÔøΩW1\nM.\u001b5ÔøΩkjlMQÔøΩf-ÔøΩ?u\tÔøΩÔøΩ-gÔøΩ–ÖÔøΩÔøΩ$\u0014ÔøΩÔøΩÔøΩ\u00138ÔøΩ1ÔøΩ{ÔøΩ*MÔøΩ\u0018oÔøΩ4ÔøΩ q\\ÔøΩ*f\u0015ÔøΩÔøΩÔøΩ)ÔøΩÔøΩb)ÔøΩvÔøΩÔøΩZÔøΩS\nÔøΩÔøΩŒä\u001fÔøΩÔøΩ\\\u001aÔøΩÔøΩfR\b\ntÔøΩÔøΩÔøΩÔøΩ|#ÔøΩÔøΩÔøΩÔøΩk,ÔøΩÔøΩcÔøΩÔøΩ1%/ÔøΩ:'◊°Y-\\ÔøΩÔøΩ\u0012ÔøΩvy‘•\t\u000f^1\u0019ÔøΩÎ≤ÆÔøΩÔøΩ—ÆÔøΩ)XÔøΩÔøΩI4ÔøΩÔøΩFÔøΩrVWÔøΩÔøΩ\nÔøΩz\u001aÔøΩ\tÔøΩÔøΩÔøΩ3cÔøΩÔøΩÔøΩ\u0010XÔøΩ\tÔøΩÔøΩ`ÔøΩ\\P\bÔøΩÔøΩ\u0015ÔøΩ~ÔøΩÔøΩXÔøΩW\u00172>\nÔøΩ>ÔøΩÔøΩÔøΩ\nÔøΩI\u0015o–™v\u0015ÔøΩÔøΩ/TÔøΩÔøΩPÔøΩÔøΩ9\u001046ÔøΩÔøΩ7\naLÔøΩ\u001f>ÔøΩ|ÔøΩÔøΩÔøΩUa“ü0ÔøΩgÔøΩ\u0013ÔøΩÔøΩ@ÔøΩÔøΩ:R*\u0006ÔøΩ\nÔøΩ~ÔøΩIÔøΩÔøΩ{ÔøΩ\u0018ÔøΩbÔøΩÔøΩ\u001aÔøΩ\nÔøΩGÔøΩOÔøΩ\u000ft^ÔøΩ5ÔøΩ'4oÔøΩŒ£]&ÌÅÇÔøΩlÔøΩ\u0013VÔøΩÔøΩÔøΩÔøΩÔøΩ\u0001ÔøΩÔøΩEp\u0019ÔøΩÔøΩÔøΩÔøΩm€ùÔøΩ?KÔøΩ/\u0002N\u0002ffÔøΩ]ÔøΩ2Y^OJÔøΩﬁøÔøΩÔøΩ\u0017ÔøΩ\u0003ÔøΩ\u001bi\u0003ÔøΩÔøΩeÔøΩÔøΩ$ÔøΩ\u0007'ÔøΩ.ÔøΩ8ÔøΩ% ÔøΩÃëÔøΩÔøΩÔøΩÔøΩÔøΩlÔøΩÔøΩ\u0011ÔøΩ=ÔøΩrÔøΩÔøΩh◊¥ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩqÔøΩ=cMÔøΩ¬≠ÔøΩ4ÔøΩ<~ÔøΩySÔøΩÔøΩ0NÔøΩÔøΩvÔøΩ\nwÔøΩ3ÔøΩÔøΩMÔøΩ_XÔøΩÔøΩÔøΩÔøΩÔøΩ\u000ewkÔøΩ\u001fÔøΩÔøΩ\u0003BÔøΩ4qœ∑ÔøΩÔøΩÔøΩ+\u0000ÔøΩe7\u001fuÔøΩÔøΩIÔøΩ\u0002ÿÖU3ÔøΩÔøΩUÔøΩ~ÔøΩﬂép\u001b0sÔøΩ–øÔøΩWÔøΩL7ÔøΩÔøΩI\u0017+y$ÔøΩÔøΩ\u0007/ÔøΩZÔøΩa%ÔøΩÔøΩR5[UlÔøΩÔøΩÔøΩÔøΩÔøΩ{ÔøΩÔøΩwD.ÔøΩ!ÔøΩ>nÔøΩÔøΩ6ÔøΩgÔøΩ5\nÔøΩlÔøΩrZkÔøΩ\u000fÔøΩÔøΩ=ÔøΩwQ\nQÔøΩOÔøΩfÔøΩÔøΩÔøΩ6ÔøΩÔøΩÔøΩÔøΩÔøΩ&ÔøΩÔøΩÔøΩÔøΩX\u0017+:ÔøΩlÀâ\u000e8ÔøΩÔøΩ=?WRg;Ú¨ü∂ÔøΩÔøΩÔøΩ3AS;[ÔøΩÔøΩ)ÔøΩ9^\u0013]⁄∫wÔøΩÔøΩh ÔøΩVÔøΩÔøΩÔøΩ\n$*ÔøΩ<ÔøΩÔøΩÔøΩa]TÔøΩÔøΩL\\\nxqn{ÔøΩÔøΩI4-ÔøΩÔøΩgsÔøΩ\u001b\u0011ÔøΩÔøΩ)ÔøΩÔøΩ-ÔøΩ\nÔøΩÔøΩuÔøΩ^ÔøΩÔøΩe\u0002`ÿ∏ÔøΩ*'\u001aÔøΩÔøΩÕàÔøΩÔøΩÔøΩ\u0011\u0006ÔøΩÔøΩQU\u0017ÔøΩÔøΩffÔøΩk*\u0004ÔøΩÔøΩÔøΩ\u0014BÔøΩÔøΩÔøΩÔøΩÔøΩ\tÔøΩ(hÔøΩ7XÔøΩf#EN$ÔøΩaxÔøΩÔøΩ9+ÔøΩU0ÔøΩÔøΩÔøΩr[\u0013\u0016\u000eÔøΩ/ÔøΩÔøΩÔøΩZÔøΩÔøΩ\u0010ÔøΩÔøΩÔøΩ/ÔøΩÔøΩÔøΩX+‰¢≠ÔøΩÔøΩÔøΩÔøΩuÔøΩ-JÔøΩÔøΩfﬂó\u001bÔøΩÔøΩ\u0001T\n\u0001ÔøΩÔøΩ¬™ÔøΩÔøΩZi\u0019ÔøΩ4ÔøΩ!MÔøΩÔøΩÔøΩ\u0004nÔøΩ\u001b[\u0016ÔøΩÔøΩ\u0014rÔøΩ`\nÔøΩÔøΩnÔøΩ}ÔøΩRÔøΩj5\"ÔøΩÔøΩPÔøΩ\u0007ÔøΩaÔøΩ\u0001BR\nmÔøΩÔøΩÔøΩ~ÔøΩ ΩÔøΩ8ÔøΩ%-ÔøΩÔøΩ+ÔøΩÔøΩ*ÔøΩÔøΩqÔøΩÔøΩ\u0019ÔøΩ\nÔøΩÔøΩÔøΩTq\u0018CÔøΩ\u001bÔøΩ{<ÔøΩjKÔøΩ»ö\"ﬁßÔøΩ\bÔøΩÔøΩsÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩnÔøΩ\nÔøΩZÔøΩÔøΩÔøΩÔøΩ>qÔøΩK2ÔøΩ\nÔøΩ1ÔøΩwÔøΩM|dÔøΩEÔøΩÔøΩÔøΩ\n\\|4ÔøΩ\u0004ÔøΩP6ÔøΩÔøΩX M\nnÔøΩÔøΩÔøΩKHÀπÔøΩx{lÔøΩ>ÔøΩ^\u0005ÔøΩ\n9v~ÔøΩ]ÔøΩÔøΩ5AÔøΩn<ÔøΩv KÔøΩÔøΩÔøΩÔøΩwc\bnÔøΩC1ÔøΩE\u0007ÔøΩÔøΩÔøΩuÔøΩ\u0017ÔøΩÔøΩÔøΩ0ÔøΩÔøΩÔøΩÔøΩÔøΩ4ÔøΩ\nÔøΩÔøΩÔøΩ\u001fÔøΩ\u000fÔøΩÔøΩ\u001f>yÔøΩ{ÔøΩÔøΩÔøΩLÔøΩÔøΩS[ÔøΩRÔøΩÔøΩ\u000eÔøΩiÔøΩ\u0007\u001aÔøΩÔøΩ1\tÔøΩÔøΩ6≈áÔøΩÔøΩÔøΩ`\u0001ÔøΩgÔøΩÔøΩtÔøΩœúÔøΩ:ÔøΩy\u000f\u001bÔøΩm—ã\u0017ÔøΩÔøΩ~ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ0ÔøΩ:@ÔøΩFÔøΩÔøΩIÔøΩ\u000eÔøΩÔøΩÔøΩÔøΩ\u0005[cVZÔøΩ\n:zÔøΩÔøΩÔøΩghÔøΩÔøΩÔøΩ\u001bÔøΩÔøΩÔøΩTÔøΩcÔøΩÔøΩÔøΩGÔøΩ?H^q\u0016_⁄ï\tLz\nBÔøΩx\\UvÔøΩÔøΩ5ÔøΩ\u0018ÔøΩ\nÔøΩTÔøΩxÔøΩ\nÔøΩVÔøΩÔøΩLÔøΩÔøΩ\u0006\u0014ÔøΩY#ÔøΩÔøΩÔøΩ\u0001ÔøΩÔøΩV\u0011\n)mÔøΩÔøΩÔøΩÔøΩGÔøΩÔøΩ<'›ÄÔøΩknÔøΩÔøΩ0fNsÔøΩ\\Z;TÔøΩTÔøΩ…®ÔøΩÔøΩ(kÔøΩTÔøΩÔøΩ~}»úÔøΩ«°ÔøΩ\u0019ÔøΩÔøΩÔøΩ-MgX\u001aÔøΩÔøΩÔøΩ>rKÔøΩ%qÔøΩÔøΩc{YUÔøΩ\u001aH\u0003ÔøΩ}ÔøΩUÔøΩÔøΩÔøΩBÔøΩJzÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩƒúiÔøΩ\u0010ÔøΩ9RÔøΩi\b+ÔøΩfKGxÔøΩÔøΩÃéÔøΩÔøΩÔøΩÔøΩvÔøΩÔøΩ1ÔøΩÔøΩÔøΩÔøΩÔøΩ\b\n\u0013ÔøΩ!À≥ÔøΩLFÔøΩBÔøΩeÔøΩÔøΩkbÔøΩJÓüæPÔøΩÔøΩ9ÔøΩ\\\u0014\u000eAÔøΩ\u000e\n\u0015*ÔøΩgÔøΩ\u001aÔøΩ<)ÔøΩÔøΩ,ÔøΩÔøΩÔøΩd]ÔøΩÔøΩ«¥ÔøΩÔøΩÔøΩÔøΩÔøΩXmÔøΩÔøΩLÔøΩÔøΩ~ÔøΩÔøΩ,ÔøΩ-lÔøΩÔøΩﬂçT\u0007ÔøΩjÔøΩ\u0003ÔøΩoÔøΩBh\"TrÔøΩw;ÔøΩDÔøΩ\u0019ÔøΩ\n’âu-LÔøΩÔøΩW\u0007B1Ã≠)xÔøΩsÔøΩ(ÔøΩk\u0016D'\u0013∆ëÔøΩÔøΩpmÔøΩ\u001a\b\u00067&ÔøΩÔøΩhÔøΩ\nÔøΩÔøΩ|)ÔøΩRO\\zÔøΩ\u0005fÔøΩddÔøΩ]\u0007HÔøΩqÔøΩÈπí;y\u0017nÔøΩ0ÔøΩ\u0005Q\u0003`\u0016ÔøΩÔøΩÔøΩ\nÔøΩ\u0011ÔøΩ_ÔøΩÔøΩÔøΩv\neÔøΩÔøΩÔøΩÔøΩ1ÔøΩÔøΩ)ÔøΩTtJqÔøΩ\u0019ÔøΩÔøΩ\u0012ÔøΩ6G«•_ÔøΩÔøΩ\nLÔøΩÔøΩÔøΩl23ÔøΩÔøΩ÷ùg'rÔøΩÔøΩ„ê¥ÔøΩÔøΩ\u0018PÔøΩÔøΩFTÔøΩ≈¨ÔøΩ\u001bÔøΩ8&ÔøΩDjÔøΩ≈∫ÔøΩdÔøΩÔøΩÔøΩwÔøΩÔøΩÔøΩ\b=v>,ÔøΩ\u0002\tÔøΩÔøΩÔøΩ%ÔøΩÔøΩÔøΩÔøΩN-ÔøΩÔøΩÔøΩ/ÔøΩÔøΩ.ÔøΩeÔøΩÔøΩ?x\n\u0001\u0012ÔøΩP\bÔøΩL,ÔøΩ/ÔøΩf]yÔøΩÔøΩuRfiÔøΩ~ÔøΩÔøΩ\tÔøΩ\u0000/ÔøΩSpÔøΩFÔøΩÔøΩKW\u001f\\(ÔøΩ{ÔøΩÔøΩ\nRP|ÔøΩ›ªÔøΩÔøΩÔøΩÔøΩ[tÔøΩÔøΩ\nÔøΩÔøΩ.ÔøΩÔøΩÔøΩÔøΩwÔøΩi<s¬íÔøΩÔøΩ3ÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\u0003;[ÔøΩÔøΩÔøΩx_ÔøΩÔøΩÔøΩm1\u0016R@ÔøΩÔøΩÔøΩ*,ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ~ {ÔøΩÔøΩÔøΩGÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ›•ÔøΩ\n.ÔøΩ\u0011+[ÔøΩÔøΩf\u0007ÔøΩzÔøΩ\"TÔøΩÔøΩsB>ÔøΩ\u0018\nÔøΩ!\nMÔøΩgy\tZu1ÔøΩÔøΩf)D[ÔøΩUÔøΩ\u000eoÔøΩÔøΩÔøΩS(Í°ÆÔøΩ`ÔøΩÔøΩ ~ÔøΩÔøΩÔøΩ.k‘ò65EÔøΩJÔøΩœ∂u\\ÔøΩÔøΩ&ÔøΩÔøΩ\u0005eÔøΩÔøΩ\u000fÔøΩÔøΩh\u0001\u0013ÔøΩnC∆êi-ÔøΩ6ÔøΩÔøΩKXDÔøΩÔøΩÔøΩoÔøΩ\nYÔøΩd]\u0013ÔøΩjÔøΩ=ÔøΩÔøΩ^\u001bN0ÔøΩÔøΩ&\u0010]7ÔøΩÔøΩÔøΩÔøΩpÔøΩÔøΩÔøΩqEQÔøΩÔøΩ\u001aAÔøΩÔøΩc∆àDÔøΩml]W ÔøΩ)/ÔøΩÔøΩÔøΩÔøΩ÷ô\u0010ÔøΩ\tﬁΩ\u0005ÔøΩ,SÔøΩ4O[ÔøΩÔøΩÔøΩ|ÔøΩ3ÔøΩÔøΩÏøü\nÔøΩ#ÔøΩ\u001aÔøΩD\u0017CTcÔøΩÔøΩ\"u/,IÔøΩÔøΩÔøΩÔøΩ\n[J ÔøΩÔøΩuÔøΩÔøΩ\u0003\u0012SÔøΩIÔøΩEM\u0019j^ÔøΩ(ÔøΩÔøΩSQ{ÔøΩÔøΩ\u0002\u0015ÔøΩme“ú\\ÔøΩPÔøΩWQf\"ÔøΩ0\tW*ÔøΩÔøΩÔøΩj›î,\u0004ÔøΩÔøΩZJUÔøΩ{ÔøΩ\u001aÔøΩ[(I\u00135Àô8ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0019ÔøΩC\n9ÔøΩ3HÔøΩÔøΩÔøΩÔøΩÔøΩXvRÔøΩ7ÔøΩ“áFÔøΩÔøΩSÔøΩÔøΩgÔøΩÔøΩ\u0018\u0000ÔøΩ\u001f>|ÔøΩÔøΩ|\u001fÔøΩVÔøΩZ0\u0006ÔøΩLÔøΩcÔøΩoÔøΩÔøΩn€ù6k}ÔøΩÔøΩ_&0 ÔøΩ\b”ØÔøΩ\u0014ÔøΩÔøΩ\u000f_ÔøΩZ\u0017)]ÔøΩwÔøΩÔøΩ9ÔøΩeÔøΩÔøΩÔøΩÔøΩÔøΩ\u000eÔøΩÔøΩ“íCs6DmqKq;ÔøΩÔøΩÔøΩ!\u0001\u0018sÔøΩc\u0012ÔøΩÔøΩ`K4vÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩf\nÔøΩÔøΩﬂíÔøΩÔøΩ\u0011s\u000f~ÔøΩ\u0003ÔøΩÔøΩ_ÔøΩ{mvMÔøΩÔøΩÔøΩ\u0017ufoDÔøΩ◊°ÔøΩ'ÔøΩÔøΩ\u0017ÔøΩÔøΩyÔøΩÔøΩÔøΩ[ÔøΩ&{ÁúªÔøΩÔøΩÔøΩÔøΩvÔøΩ4QÔøΩÔøΩNÔøΩÔøΩ*ÔøΩRÔøΩ\nÔøΩÔøΩy\nÔøΩÔøΩÔøΩs\u0002ÔøΩeÔøΩ\u0002ÔøΩtIÔøΩÔøΩÔøΩ\u0000\u0002\u0005ÔøΩÔøΩÔøΩ|ÔøΩbÔøΩw#‹•ÔøΩ\nÔøΩ ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩDN]\u0011ÔøΩ\u000eÔøΩÔøΩÔøΩ6\n0ÔøΩ@\n6Ÿ±\nÔøΩ\u0018,ÔøΩnÔøΩWÔøΩÔøΩ,ÔøΩÔøΩ\u0007tÔøΩ\u00190ÔøΩÔøΩÔøΩ9ÔøΩeÔøΩÔøΩ6bÔøΩ]ÔøΩÔøΩSÔøΩ:'ÔøΩÔøΩ'jÔøΩÔøΩ0\\?ÔøΩÔøΩ\u0003!GÔøΩ/ÔøΩ\nÀçv\u001b GÔøΩukÔøΩ\u0012.\\ÔøΩÔøΩÔøΩ`\u0017 ÔøΩ\tÔøΩÔøΩÔøΩÔøΩÔøΩ0ÔøΩoÔøΩÔøΩÔøΩ\u0001ÔøΩÔøΩy\u0001\nÔøΩ8_/RYÔøΩÔøΩÔøΩÔøΩrÔøΩLÔøΩÔøΩ\nÔøΩÔøΩtcÔøΩÔøΩÔøΩÔøΩWgHJÔøΩÔøΩfÔøΩ-@ÔøΩ’™ÔøΩ‘¨ÔøΩÔøΩ8ÔøΩnkÔøΩÔøΩ\u0014ÔøΩenY}ÔøΩ ÔøΩSÔøΩ)ÔøΩLÔøΩ,T‘≤*bÔøΩOÔøΩÔøΩu\tÔøΩÔøΩPÔøΩÔøΩÔøΩj$ÔøΩ\u0001ÔøΩmÔøΩÔøΩ*ÔøΩÔøΩÔøΩ*qIÔøΩlÔøΩj/ÔøΩ^/Qd]}9ÔøΩ-ÔøΩÔøΩ\u0015€ókÔøΩÔøΩUuÔøΩ{ÔøΩÔøΩNÔøΩÔøΩ[VÔøΩÔøΩÔøΩz\nJ|ÔøΩ-zÔøΩv3ÔøΩTÔøΩV&ÔøΩÔøΩu\tU›™ÔøΩ∆®ÔøΩ4ÔøΩÔøΩ'\u001bÔøΩT7BÔøΩ\u0001ÔøΩPÔøΩÔøΩÔøΩ\n\u0006:}1»πÔøΩ\u0010\u0017ÔøΩ_\u000eq≈øÔøΩ\nÔøΩoeÔøΩRÔøΩÔøΩ]!\u001fÔøΩÔøΩÔøΩ\bDS.ÔøΩÔøΩA4/\u0013i\u001aÔøΩhÔøΩÔøΩ7ÔøΩ\nÔøΩ‰öÅ\u0002'ÔøΩg)Q$\u0000/og\u001a\u0010WÔøΩÔøΩ\u0010ÔøΩÔøΩNÔøΩÔøΩÔøΩÔøΩ\u0006ÔøΩmujÔøΩÔøΩÔøΩDÔøΩÔøΩ«ΩjÔøΩÔøΩ!ÔøΩÔøΩUiÔøΩÔøΩ4ÔøΩKXÔøΩÔøΩlÔøΩÔøΩpSuÔøΩsÔøΩ\u0017,ÔøΩV4IaÔøΩ\u0006ÔøΩÔøΩ:*ÔøΩÔøΩNÔøΩÔøΩ>\u0005ÔøΩzÔøΩÔøΩÔøΩÔøΩÔøΩle\u0019PÔøΩYÔøΩ? >LÔøΩoGÔøΩ\u0003ÔøΩ6ÔøΩŒÇ\n~_\u0013ÔøΩÔøΩÔøΩÔøΩGÔøΩKs\u0006m[e\nÔøΩx[ÔøΩ¬≠ÔøΩÔøΩÔøΩÔøΩÔøΩ?ÔøΩÔøΩÔøΩ8ÔøΩÔøΩÔøΩ7ÔøΩ'$ÔøΩÔøΩÔøΩaÔøΩÔøΩÔøΩ\"^\u0004`ÔøΩ'4w»ú}XÔøΩÔøΩ\u001bÔøΩÔøΩ\u0019ÔøΩ\u0013ÔøΩxÔøΩ1\nÔøΩÔøΩ-ÔøΩÔøΩ€∞3ÔøΩrMÔøΩZÔøΩÔøΩ^\nÔøΩ(XÔøΩ“òÔøΩÔøΩÔøΩÔøΩ6{ÔøΩYÔøΩ\tqÔøΩÔøΩÃî|ÔøΩK\u001aÔøΩiŸ∂ÔøΩcÔøΩ\u0001NÔøΩqÔøΩLFÔøΩÔøΩ:ÔøΩÔøΩ\nÔøΩ[gÔøΩ(ÔøΩ#\u0016\u000eÔøΩÔøΩNITÔøΩBÔøΩÔøΩÔøΩcÔøΩOJVÔøΩG`ÔøΩÔøΩc<\u0001ÔøΩmÔøΩÔøΩÔøΩm:\u0002\nÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩ“£ÔøΩ\u0007zÔøΩ‰Æ∂ÔøΩ√îÔøΩuÔøΩÔøΩÔøΩUaÔøΩ{xÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩ\u0010ÔøΩB>ÔøΩ\b\u001a=3rﬁÅÔøΩÔøΩ_„ôñ<Y\u0015ÔøΩ`ÔøΩÔøΩÔøΩÔøΩÔøΩ|ÔøΩÔøΩw#ÔøΩÔøΩ\u0017≈ÉÔøΩÔøΩÔøΩÔøΩrÔøΩﬂ™|ÔøΩ\u0001\u0002ÔøΩ›æÔøΩ>\u0012rŸúÔøΩÔøΩÔøΩÔøΩ,\\ÔøΩq\bSÔøΩQ}\nÔøΩÔøΩ€πÔøΩŒºÔøΩÔøΩ\u0006\nDÔøΩ3Q}ÔøΩÔøΩÔøΩ‹±jÔøΩE\u0011◊¨ÔøΩcÔøΩÔøΩ\u0005\nSÔøΩÔøΩÔøΩ~:ÔøΩÔøΩÔøΩÔøΩ[ÔøΩXÔøΩÔøΩÔøΩ\n\u0007\u0018ÔøΩ.ÔøΩÔøΩCÔøΩ\b;@GaÔøΩÔøΩÔøΩ\\KÔøΩBÔøΩ.ÔøΩÔøΩÔøΩÔøΩ\u0018ÔøΩ\u0005?=ÔøΩÔøΩFF9—≠Ãú\u0013ÔøΩÔøΩ|gA8fÔøΩ<ÔøΩÔøΩÔøΩI*ÔøΩ\neÔøΩ√¶ÔøΩÔøΩ\u001fÔøΩÔøΩDÔøΩÔøΩg\u0018ÔøΩM\u000fV\u0001ÔøΩÔøΩs\u0004m!ÔøΩÔøΩ>\u000eÔøΩ\u0007ÔøΩ\"ÔøΩ_\n\bÔøΩ1ÔøΩÔøΩw4ÔøΩÔøΩ{ÔøΩÔøΩÔøΩÕö\u0001DP\u0011ÔøΩÔøΩÔøΩ÷ØLÔøΩÔøΩÔøΩI&c\nÔøΩaV\u0004ÕèÔøΩY ÔøΩÔøΩ\u000f\u001aÔøΩ¬¥%ÔøΩÔøΩÔøΩOÔøΩ_\u0004mÔøΩNÔøΩÔøΩ.ÔøΩ\nÔøΩÔøΩ`ÔøΩÔøΩs:\u0018=\u0011ÔøΩÔøΩ7ÔøΩ\u0010ÔøΩUÔøΩÔøΩmÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩVÔøΩ\u0017ÔøΩ4ÔøΩM\u0014ÔøΩbB\u0004ÔøΩ\n\u00183uÔøΩ\u0015\u0000ÔøΩÔøΩWoﬂøÔøΩÔøΩÔøΩÔøΩÔøΩ\nGÔøΩ\u0017`ÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩuÔøΩÔøΩ¬≠1ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩs7egÔøΩ\nÔøΩ9ÔøΩ*÷∫ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0003sFÔøΩ;ÔøΩÔøΩ0ÔøΩF<>\u0001\u0006ÔøΩÔøΩm1.ÔøΩNvÔøΩÔøΩÔøΩÔøΩan;<ÔøΩ6ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩDÔøΩÔøΩJ3(ÔøΩÔøΩ$ÔøΩÔøΩRÔøΩ\u0006ÔøΩ\n(.{ÔøΩqÔøΩ#ÈØÜÔøΩÔøΩwH\u0014sÔøΩÔøΩÔøΩ&YÔøΩaÔøΩPÔøΩqkNÔøΩ\u0003»åE$ÔøΩdÔøΩ}MÔøΩÔøΩu\u0011ÔøΩ4ÔøΩÍûº,ÔøΩÔøΩÔøΩÔøΩCÔøΩGMX\u0012\nADR\u0014ÔøΩ\nÔøΩvÔøΩÔøΩSÔøΩÔøΩ\u0001ÔøΩ\u00026ÔøΩÔøΩluKÔøΩ1ÔøΩ{ÔøΩMÔøΩ€ÅÔøΩ\u001fFÔøΩÔøΩ3i\u0010'.=ÔøΩ72ÔøΩÔøΩ4_@xÔøΩOÔøΩ:ÔøΩ\u0013\nÔøΩÔøΩ\u0016~\nÔøΩQvÔøΩ—îÔøΩ!ÔøΩp<ÔøΩÔøΩÔøΩÔøΩ%W\u001fÔøΩÔøΩÔøΩZÔøΩÔøΩ≈ãÔøΩÔøΩ-2ÔøΩÔøΩ\u000f\u001f++ÔøΩ_ÔøΩÔøΩÔøΩ[5ÔøΩ6jvÔøΩÔøΩJÔøΩfÔøΩ—ç\u0015\u0001AÔøΩÔøΩbdÔøΩÔøΩT\u0006]b\u0017ÔøΩ\u0003iÔøΩÔøΩÔøΩ|ÔøΩa5ÔøΩ\u0006ÔøΩ\u0012WÔøΩÔøΩGÔøΩ#ÔøΩÔøΩÔøΩ{ÔøΩOn\u0002:AVﬁ®ÔøΩÔøΩÔøΩVÔøΩÔøΩTfxÔøΩÔøΩÔøΩ1ÔøΩ,ÔøΩ7\nÔøΩÔøΩÔøΩ!ÔøΩi\u0007x|ÔøΩÔøΩ\u0019ÔøΩÔøΩ:\u001aSP\bÔøΩÔøΩ\nÔøΩ\u0017l\u000eÔøΩ\n#ÔøΩ¬ñLe≈©sÔøΩÔøΩÔøΩT)ÔøΩQ\u0017U\t's'ÔøΩÔøΩoÔøΩ#-ÔøΩMme~;ÔøΩÿ°[ÔøΩÔøΩXÔøΩYw[ÔøΩÔøΩÔøΩÔøΩto\n\bÔøΩ\u0017ÔøΩ\u000eÔøΩÔøΩj\nÔøΩU%ÔøΩ?\n\u0006:ÔøΩ1tÔøΩÔøΩ.ÔøΩ\npÔøΩpÔøΩVUÔøΩÔøΩtÔøΩzÔøΩÔøΩ\u0017iÔøΩ]ÔøΩuP:ÔøΩÔøΩihÔøΩ5ÔøΩBSaÔøΩZ\u001fÔøΩqÔøΩÔøΩ&ÔøΩÔøΩ<QÔøΩÔøΩRÔøΩ\u0012ÔøΩ\u0014\u0002P\nÁπ≤mÔøΩVDÔøΩÔøΩDSv\n—ÉÔøΩAÔøΩuÔøΩNgv|`ÔøΩ\u0015ÔøΩ+ÔøΩAÔøΩÔøΩ?ÔøΩkÔøΩbHÔøΩJm6ÔøΩÔà¢\nÔøΩ\u0004ÔøΩÔøΩIÔøΩ9ÔøΩPM(RÔøΩ\u0018ÔøΩ\bÔøΩÔøΩÔøΩvÔøΩ hÔøΩ\u000e3ÔøΩÔøΩ\nÔøΩÔøΩ3gÔøΩ?}ÔøΩzÔøΩ\u0003ÔøΩÔøΩMÔøΩÔøΩÔøΩk~\u0014ÀåÔøΩÔøΩÔøΩ\u0003\"\u0013/NZvÔøΩ\u0015ÔøΩnrÔøΩJ*r‹õÔøΩÎ±ò\tÔøΩÔøΩÔøΩÔøΩÔøΩLk–ôÔøΩ(ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩs\u000f\nM+\u0006\u0006\u0007>ZÔøΩ=6>ÔøΩ2'.ÔøΩ\u001a4+`ÔøΩsr\u0013ÔøΩÔøΩ>fAÔøΩoX^#ÔøΩÔøΩÔøΩBÔøΩ\u0003YÔøΩÔøΩÔøΩiÔøΩq\u0017\u0016mÔøΩÔøΩÔøΩ\u0019=?\u0010ÔøΩrÔøΩÔøΩ`ÔøΩÔøΩÔøΩÔøΩ:v$ÔøΩ\bÔøΩŸ¥ÔøΩÔøΩÔøΩÔøΩT\u0014\u00123ÔøΩ9ÔøΩÔøΩÔøΩÔøΩT\"ÔøΩ>\u0006ÔøΩpÔøΩ≈òÔøΩ\u0012ÔøΩÔøΩ4^6ÔøΩÔøΩÔøΩVÔøΩI+ÔøΩÔøΩDÔøΩÿ¥\u00121r/ÔøΩTÔøΩÔøΩy\u0007ÔøΩÔøΩXPÔøΩÔøΩ]&ÔøΩÔøΩ[\u000fÔøΩÔøΩÔøΩÔøΩÔøΩ%;ÔøΩÔøΩ]\u0010ÔøΩ\nÔøΩ\n\u00068ÔøΩÔøΩ!nlÔøΩÔøΩaOHnÔøΩiÔøΩÔøΩ3MÔøΩ|\u0004ÔøΩCÔøΩÔøΩfÔøΩÔøΩÔøΩ'ÔøΩÔøΩKÔøΩ\nÔøΩÔøΩftÔøΩÔøΩ3xVÔøΩÔøΩÔøΩ\u0002\"ÔøΩ$d\\\u0016ÔøΩ2ÔøΩÔøΩÔøΩ9'‹áGO_ÔøΩÔøΩÔøΩAnV-iÔøΩOÔøΩ\u0019YJO!\tÔøΩÔøΩwÔøΩzÔøΩÔøΩYÔøΩÔøΩ\u001aÔøΩ\u000eT<W\u0014ÔøΩ.ÔøΩCvÔøΩrZiÔøΩHÔøΩ\nÔøΩÔøΩÔøΩjPDÀì\u0002ÔøΩ%ÔøΩSÔøΩMÔøΩÔøΩÔøΩT*ÔøΩUMÔøΩyÔøΩV5ÔøΩkÔøΩÔøΩwÔøΩdÔøΩÔøΩy\ngÔøΩk ¥\u0015aÔøΩ2ÔøΩ)0LÔøΩ\u0017ÔøΩÔøΩxe%ÔøΩnÔøΩ≈∫]?ÿÑNÔøΩÔøΩ\\XbÔøΩ\u0018ÔøΩÔøΩ*DÔøΩÔøΩÔøΩxÔøΩÔøΩÔøΩqÔøΩÔøΩÔøΩ{BNÔøΩÔøΩd,ÔøΩ\u0016ÔøΩÔøΩ\b7V\\ﬁòÔøΩÔøΩÔøΩgÔøΩf|#nÔøΩJ\n]ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ:ÔøΩÔøΩ;h}_ÔøΩÔøΩ\nÔøΩUÔøΩI!@ÔøΩcDH⁄™ÔøΩPÔøΩÔøΩ%ÔøΩÔøΩ)kÔøΩ8ÔøΩeÔøΩÕò\u0006-ÔøΩ¬°ÔøΩ'Nc]ÔøΩT.\nÔøΩbÔøΩ!*\u000f\n<3ÔøΩZÔøΩ1ÔøΩ\u0002ÔøΩÔøΩt\u0006◊¥ÔøΩÔøΩÔøΩ◊ëÔøΩÔøΩÔøΩKÔøΩTÔøΩCÔøΩÔøΩÔøΩÔøΩZÔøΩ\nT6ÔøΩ[\u0014ÔøΩÔøΩR[30ÔøΩ|ÔøΩÔøΩÔøΩLÔøΩÔøΩÔøΩÿ∂zOÔøΩÔøΩÔøΩÔøΩ\u0004/ÔøΩ\u001fÔøΩÔøΩ\"ÔøΩÔøΩf(ÔøΩQsÏ¶üÔøΩ\u0002ÔøΩÔøΩÔøΩ0◊ø7\nÔøΩÔøΩ:ÔøΩÔøΩÔøΩÔøΩÔøΩ1a»ú}ÔøΩ\u0014ÔøΩÔøΩ|ÔøΩÔøΩdEÔøΩÔøΩrÔøΩ\\a}ÔøΩÔøΩ?ÔøΩÔøΩÔøΩi;ÔøΩtÔøΩÔøΩÔøΩ\u001bÔøΩÔøΩ;\u0019ÔøΩÔøΩ\nN\u0018]ÔøΩÔøΩu>\u0018ÔøΩ{ÔøΩ[ÔøΩÔøΩÔøΩÔøΩBË©¢\u0007ÔøΩ\nÔøΩ\tf{ÔøΩ~\u000etÔøΩ\u0004ÔøΩ‹ékÔøΩ÷ºÔøΩ\u00051LrÔøΩÔøΩÔøΩ~\u0000ÔøΩ#ÔøΩ\u0017ÔøΩ@4H\u0012ÔøΩÔøΩÔøΩ@vÔøΩpÔøΩÔøΩ+e\neÔøΩÔøΩÔøΩTtÔøΩÔøΩ{0ÔøΩvÔøΩ\u00196ÔøΩÔøΩKw\nÔøΩyÔøΩÔøΩÔøΩKÔøΩÔøΩÔøΩÔøΩ‚úá|1»πÔøΩÔøΩÔøΩZÔøΩ~QÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩF!ÔøΩzf2U\u001fÔøΩ5ÔøΩÔøΩ=uw`ÔøΩÔøΩÔøΩÔøΩÓ©£\u0017\u000663ÔøΩ\bÔøΩjÔøΩc(1\u0019ƒÜÔøΩÔøΩ\"ÔøΩj\u0013N\u0011ÔøΩÔøΩÔøΩÔøΩÔøΩANXÔøΩÔøΩÔøΩ'ÔøΩÔøΩÔøΩÔøΩ'\nÔøΩÔøΩÔøΩÔøΩg\u0013ÔøΩÔøΩcÔøΩÔøΩKv#ÔøΩÔøΩYÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ”ú\u0003NntIÔøΩÔøΩÔøΩ\u0010OÔøΩ6ÔøΩÔøΩXY_\nuYÔøΩ%ÔøΩÔøΩ'ÔøΩ+‰¥ìÔøΩ*ÔøΩÔøΩ.\u0013ÔøΩÔøΩÔøΩÔøΩD8ÔøΩÔøΩÔøΩÔøΩÔøΩ„îÉ‹¨ÔøΩÔøΩv„ó≥ÔøΩJc\u0014ÔøΩlÔøΩÔøΩ-⁄≥ÔøΩS-gÔøΩTÔøΩ\nÔøΩÔøΩ\u0002ÔøΩ’®ÔøΩÔøΩ\u0005_ÔøΩbj3-ÔøΩ>3ÔøΩVÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩu\nÔøΩÔøΩ—ª\u001b\u0019ÔøΩ&ÔøΩÔøΩÔøΩ6ÔøΩ\u001b\nkÔøΩ\t\u0015\nÔøΩ\u0000ÔøΩ@ÔøΩÔøΩÔøΩKÔøΩÔøΩÔøΩnÔøΩœ±ÔøΩ gc<3ÔøΩÔøΩWÔøΩ\nÔøΩ`K\nÔøΩ\u0012&ÔøΩ#\u001fÔøΩÔøΩ\u0013ÔøΩÔøΩiÔøΩrb\u001bÔøΩj\"9ÔøΩ0ÔøΩ\u0019*ÔøΩTÁºú]ÔøΩ)USÔøΩÔøΩqz|\u0013\u0007ÔøΩy\u0006ÔøΩ\u0010ÔøΩÔøΩÔøΩG/ÔøΩ7ÔøΩSÔøΩ~ÔøΩ¬†_ÔøΩ{—éI\nÔøΩ5\nÔøΩ\nÔøΩ\u0002]ÔøΩÔøΩ8ÔøΩÔøΩÔøΩo…∏ÔøΩE\u00192{\u001f\u0006ÔøΩÔøΩÔøΩÔøΩÔøΩ:ÔøΩÔøΩpÔøΩK\nÔøΩ3ÔøΩÔøΩw#‹ßÔøΩ\b≈¥ÔøΩÔøΩD⁄ÄY\u0007eÔøΩÔøΩp7ÔøΩÔøΩe;ÔøΩÔøΩÔøΩMÔøΩ\nŒ°ÔøΩZÔøΩ=\u0016ÔøΩÔøΩÔøΩ*NÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ.ÔøΩÔøΩ[ÔøΩ|ÔøΩ%ÔøΩÔøΩÔøΩF5ÔøΩ+a;}U\u0018ÔøΩ'ÔøΩ\u0015ÔøΩ';PL>ÔøΩdÔøΩ&\u000fF\u0019ÔøΩÔøΩœ∏ÔøΩsÔøΩÔøΩ~\noÔøΩ)\u0007\u0011'\u0018ÔøΩ=fa–±\u0013ÔøΩÔøΩ\nÔøΩ:ÔøΩÔøΩÔøΩFÔøΩ\u0000ÔøΩ\nÔøΩ\u000f*#ÔøΩÔøΩ\u0016ÔøΩÔøΩ\u0015VqÔøΩRÔøΩ\u001aÔøΩ\u000edÔøΩ4f7.ÔøΩnÔøΩÔøΩÔøΩuÔøΩ;<ÔøΩÔøΩlÔøΩÔøΩ*[kÔøΩÔøΩNÔøΩÔøΩÔøΩÔøΩÔøΩ\u0003ÔøΩ\u001bvÔøΩÔøΩÔøΩÔøΩ\u001fÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ#ÔøΩeÔøΩZÔøΩ=ÔøΩr\u000e\n=OÔøΩ/ÔøΩq4#ÔøΩ:~eÔøΩÔøΩÔøΩYÔøΩ#GÔøΩ;ÔøΩuÔøΩÔøΩÔøΩMÔøΩ\n\u0005ÔøΩÔøΩ\u001bÔøΩh4ÔøΩÔøΩx3eyÔøΩÔøΩ&\tUÔøΩÔøΩÔøΩyUIÔøΩt*ÔøΩÔøΩ4ÔøΩ#ÔøΩÔøΩÔøΩ1ÔøΩ\u000f\u001fDÔøΩ9wSÔøΩ1ÔøΩ—´*ÔøΩ\u0015›ò_x[ÔøΩ\u0012=ÔøΩ|ÔøΩÔøΩ\u000fÔøΩ\nÔøΩUÔøΩÔøΩ=ÔøΩnYÔøΩ-FÔøΩ”∂ÔøΩiZÔøΩÔøΩ\u0013ÔøΩ+^ÔøΩÔøΩÔøΩÿπÔøΩÔøΩÔøΩ»â\u000eÔøΩ<#^6naÔøΩÔøΩ\u0010L!hÔøΩÔøΩÔøΩnÔøΩzÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩgP\u000e^ÔøΩ`ÔøΩÔøΩ;ÔøΩ`\u000e~ÔøΩÔøΩÔøΩ\u0006_ÔøΩÔøΩÔøΩÔøΩ[!ÔøΩ\u001fÔøΩÔøΩ>ÔøΩÔøΩﬁ£\u0017ÔøΩ+ÔøΩVÔøΩ{ÔøΩÔøΩMÔøΩÔøΩGÔøΩsÔøΩBÔøΩÔøΩÔøΩÔøΩkÔøΩPbÔøΩaÔøΩD$^ÔøΩuÔøΩŸá\u000fÔøΩÔøΩcÔøΩ\u0010Wn<ÔøΩ\u000fÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩLVÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩWÔøΩ+ÔøΩlÔøΩEÂåÖÔøΩVPrGÔøΩ\"ÔøΩ4K\u0003\bUÔøΩÔøΩÔøΩ\nÔøΩ÷öoÔøΩÔøΩ9ÔøΩÎ≠ªOÔøΩÔøΩ9B/UNÔøΩTzÔøΩÔøΩeÔøΩ_\u001f=}%∆ßÔøΩÔøΩÔøΩn\u0014-lÔøΩÔøΩ ÔøΩKÔøΩ\nÔøΩÔøΩÔøΩÔøΩ{ÔøΩÔøΩqŸòfXyÔøΩy\u0002ÔøΩ\n’íÔøΩ\u0018ÔøΩÔøΩ2Ê±íÔøΩÔøΩÔøΩÔøΩQÔøΩÔøΩÔøΩ\u0019\u0019ÔøΩ\u0016ÔøΩ7ÔøΩÔøΩmÔøΩeÔøΩ*ﬂøz]qÔøΩÔøΩÔøΩÔøΩÔøΩBÔøΩCÔøΩ>ÿπ1ÔøΩÔøΩ>]ÔøΩH(4ÔøΩ\nÔøΩÔøΩyÔøΩ\u001b;MÔøΩs_ÔøΩlÔøΩ|ÔøΩÔøΩÔøΩv~\u0019=ÔøΩ\u00195ÔøΩIÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0019ÔøΩÔøΩÔøΩ\u0004=vÔøΩÔøΩsÔøΩQ\u0014:{jÔøΩ.ÔøΩ(ÔøΩ+ÔøΩe(ÔøΩÔøΩ\u0007ÔøΩÔøΩUÔøΩ\u001aÔøΩÔøΩÔøΩ[ÔøΩYÔøΩ}ÔøΩÔøΩÔøΩ–π\u0018ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩ»ûÔøΩoÔøΩÔøΩÔøΩEÔøΩÔøΩ?sÔøΩÔøΩFÔøΩÔøΩÔøΩÔøΩÔøΩœ±mÔøΩ\u0005vÔøΩÔøΩ{ÔøΩdÔøΩ@S}?ÔøΩ]yq»±ÔøΩ+ﬂºyÔøΩÔøΩJI\u0006ÔøΩ\nP6ÔøΩDÔøΩ5ÔøΩdÔøΩÔøΩÔøΩQmjÃñ\u0018ÔøΩ%^ÔøΩ\nÔøΩÔøΩ<ar\u0010\u001b@\u001b\u0016ŒäÔøΩÔøΩYcÔøΩ ^\tÔøΩÔøΩÔøΩ\nyÔøΩ\u00009$ÔøΩkÔøΩ~ÔøΩ—ìWÔøΩ~ÔøΩ9ﬂú\u0002NJWÔøΩ/ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0007ÔøΩ5ÔøΩÔøΩÔøΩÔøΩa`8`ÔøΩE&^DÔøΩÔøΩ\u0015ÔøΩ«èÔøΩ\nÔøΩH>ueÔøΩSÔøΩÔøΩ\u000eÔøΩM\n,\u0016ŸΩÔøΩyÔøΩÔøΩ\u000fpÔøΩ\u0015Y:w”ëjÔøΩV»ÖK\u001aÔøΩ&]ÔøΩÔøΩXÔøΩ\u0000ÔøΩÔøΩÔøΩGm(\nÔøΩM\u0019v&&ÔøΩÔøΩWoÔøΩÔøΩÔøΩÔøΩWoÔøΩÔøΩÔøΩ\u0000hÔøΩœú\u000fE]xÔøΩÔøΩÔøΩaÔøΩÔøΩ.ÔøΩ9ÔøΩÔøΩÔøΩOTWÔøΩÔøΩÔøΩÔøΩÔøΩ<22\u0001`1ÔøΩÔøΩlÔøΩ\nh\nÔøΩU\u001f>|ÔøΩ\"ÔøΩ.ÔøΩ\t\u001bÔøΩÔøΩ\u001fÔøΩ\u0006ÔøΩÔøΩY¬ù⁄ç&>ÔøΩÔøΩÔøΩŒ∞fÔøΩ\\ÔøΩÔøΩZacÔøΩzSqÔøΩÔøΩoh\nŸ´ÔøΩÔøΩÔøΩ=ÔøΩ?\"ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩeO@#1yÔøΩÔøΩ'/\u000fÔøΩY6ÔøΩMu0;ÔøΩÔøΩQ*ÔøΩ9jÔøΩ]ÔøΩÔøΩXGM4ÔøΩ~`)iÔøΩ)Zi+\u0018\nÔøΩ0\n?{ÔøΩÔøΩP\u0017]/\u0005ÔøΩÔøΩÔøΩÔøΩÔøΩ+ÔøΩ]\u0017KÔøΩÔøΩ\u0004ÔøΩjÔøΩ\u0013ÔøΩÔøΩpÔøΩÔøΩsÔøΩÔøΩ+\u000fTÔøΩc\n5\ndÔøΩÔøΩÔøΩŸõwÔøΩÔøΩ!ﬂ£ÔøΩÔøΩ\nÔøΩÔøΩÔøΩ:NVcH}ÔøΩg@dÔøΩÔøΩÔøΩ'r’ä\nÔøΩÔøΩÔøΩÔøΩRÔøΩÔøΩ/ÔøΩÔøΩoÔøΩÔøΩ\"6~ŒÜÔøΩ\nÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩ\u001bq\u0006ÔøΩZ9ÔøΩÔøΩ\n6Ox<c6ÔøΩ\u000fÔøΩ\u0001\u0006eÔøΩÔøΩÔøΩÔøΩ€±ÔøΩÔøΩÔøΩ$hÔøΩÔøΩHLBÔøΩÔøΩÔøΩHÔøΩTikÔøΩT\b\u0000\u0016vÔøΩoRﬁíÔøΩ\u00057\tsA-ÔøΩÔøΩÔøΩÔøΩ]ÔøΩÔøΩ<xÔøΩÔøΩ-8ÔøΩÔøΩÔøΩÔøΩ\u0013ÔøΩÔøΩEvÔøΩÔøΩÔøΩÔøΩ7\u0003ÔøΩ\u0017ÔøΩÔøΩ\u0014ÔøΩ+ÔøΩ{LÔøΩÔøΩ:\nÔøΩ{ÔøΩ\u0004ÔøΩ\u0014`ÔøΩÔøΩxÔøΩHÔøΩ,ÔøΩ\bÔøΩ∆ñ—¥ÔøΩ5ÔøΩ\u0015ÔøΩ_ÔøΩ\u0011TyÔøΩAgÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ4lGÔøΩÔøΩÔøΩÔøΩÔøΩ\u0015orÔøΩ?ÔøΩÔøΩ\nÔøΩÔøΩÔøΩŸ´ÔøΩC\\ÔøΩÔøΩÔøΩÔøΩ\u0002D€æ\nÔøΩÔøΩ&ÔøΩ‘ô\u001bD)FÔøΩZ\u0015ÔøΩÔøΩ\nÔøΩ,FÔøΩ1vÔøΩno’áÔøΩÔøΩ6ÔøΩVÔøΩ\u0011ÔøΩÔøΩzg\u0014ÔøΩÔøΩ\u0003ÔøΩÁûøÔøΩÔøΩ\"ÔøΩÔøΩLnÔøΩwÔøΩ.÷æayÔøΩ\nÔøΩ\u0019[$ÔøΩgLjq+f\"ÔøΩBÔøΩÔøΩÔøΩQÔøΩEÔøΩ\u000ewÔøΩc]ÔøΩÔøΩÔøΩg0ÔøΩÔøΩÔøΩÔøΩSÔøΩzLÔøΩÔøΩÔøΩ‡Ø∞ÔøΩ\u0003\"ÔøΩ78&√úÔøΩ0ÔøΩiÔøΩ]<ÔøΩ\u0014ÔøΩ\u00188ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩFÔøΩÔøΩÔøΩG\u0015ÔøΩÔøΩdG,~ÔøΩÔøΩ\u001f<{_ÔøΩÔøΩﬁ™`ÔøΩÔøΩÔøΩÔøΩy…≤0\u0006ÔøΩÔøΩÔøΩ=?ÔøΩ\u00064[auÔøΩTÔøΩDÔøΩ<ÔøΩblÔøΩÔøΩÔøΩJÔøΩ\u000emÔøΩÿ∑ÔøΩaÔøΩi\u0015h\u0018ÔøΩÔøΩÔøΩÔøΩdÔøΩ*ÔøΩÔøΩ)ÔøΩ$ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩg.ÔøΩKWÔøΩ9tZÔøΩÔøΩ\u0011s\u000fÔøΩÔøΩKmÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ0WQÔøΩDiÔøΩ\"ÔøΩÿéÔøΩ'`‹âcÔøΩY\u000fÔøΩÔøΩEÔøΩ\u0003ÔøΩÔøΩÔøΩÔøΩjÔøΩzU!&ÔøΩbÔøΩ\u0006F\u0017\u0018l€•ÔøΩ\u000fÔøΩw\u001a\nÔøΩJÔøΩ10!ÔøΩÔøΩxcÔøΩxÔøΩÔøΩFÔøΩ\nÔøΩ\u0006l^ÔøΩc%ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩv1QÔøΩGÔøΩÔøΩ\nÔøΩ{3+\u0014ÔøΩÔøΩ÷ß-ÔøΩ\u001aÔøΩgÔøΩ^mÔøΩÔøΩÔøΩ+:ÔøΩHÔøΩÔøΩ\nXDÔøΩmÔøΩÔøΩ~ÔøΩb ªÔøΩÔøΩ\u0012)ÔøΩ\u0003ÔøΩUÔøΩÔøΩÔøΩÔøΩFJ~\u000fiÔøΩﬁæÔøΩ≈™ÔøΩÔøΩ\u0001ÔøΩÔøΩÔøΩ\n\u0001ÔøΩÔøΩÔøΩ>yÔøΩZÔøΩÔøΩEÔøΩ\u00150ÔøΩÔøΩÔøΩaÔøΩÔøΩÔøΩ\"ÔøΩFÔøΩ@[\u001aÔøΩ\n\u001bÔøΩtÔøΩÔøΩÔøΩÔøΩR\u000fÔøΩV\\\u0005ÔøΩÔøΩ8nÔøΩcCÔøΩ]ÔøΩ=ÔøΩQÔøΩÔøΩÔøΩ\u0019ÔøΩÔøΩÔøΩ\n_ÔøΩoÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩt\u0018ÔøΩÔøΩt]TÔøΩEc\n\u0005\nPv#ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ–ñÔøΩqÔøΩÔøΩCÔøΩÔøΩÔøΩÔøΩeÔøΩCÔøΩ\u000eÔøΩÔøΩÔøΩÔøΩF/ÔøΩ1 \u0017ÔøΩÔøΩÔøΩK\u001avÔøΩRÔøΩ$\u0016b€∞ÔøΩÔøΩÔøΩ'ÔøΩÔøΩ[ÔøΩÔøΩÔøΩÔøΩdÔøΩ]ÔøΩÔøΩX,\nÔøΩÔøΩYJÔøΩKÔøΩ\u0001BYÔøΩ%FOwÔøΩÔøΩÔøΩaÔøΩ]gÔøΩÔøΩÔøΩl\u0010#ÔøΩ\nP\u001aÔøΩ\u000f\nÔøΩÔøΩÔøΩ\u001f2\tÔøΩÔøΩÔøΩ|>L\n=ÔøΩÔøΩuwÔøΩÔøΩÔøΩ»Ω\u0006ÔøΩ +E\u0007ÔøΩQÔøΩ\u0006ÔøΩ\nK\u0002ÔøΩÔøΩ@aÔøΩ-Y*ÔøΩ5ÔøΩÔøΩzÔøΩDÔøΩJÔøΩSÔøΩÔøΩƒäb?p\nÔøΩÔøΩZÔøΩxÔøΩÔøΩ\nI.ÔøΩeÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩiÔøΩÔøΩ“µÔøΩUcÔøΩÔøΩmÔøΩ\b\u001a-w›ü\u0005\u0014ÔøΩe\u00145?^ÔøΩZ6VIÔøΩÔøΩÔøΩÔøΩRÔøΩlÔøΩÔøΩÔøΩÔøΩ‹´ÔøΩÔøΩvÔøΩÔøΩhÔøΩ?*ÔøΩ\u0000ÔøΩÔøΩOÔøΩ\n-ÔøΩ\u001aÔøΩ:ÔøΩ:\u0016KVvÔøΩÔøΩÔøΩ=ÔøΩ\nÔøΩÔøΩJAÔøΩFkÔøΩT$\n\u00069_,ÔøΩÔøΩÔøΩkÔøΩ\nÔøΩÔøΩ\u0011ÔøΩÔøΩÔøΩ\t\bÔøΩBBÔøΩjÔøΩ\u0002s,ÔøΩa≈éÔøΩÔøΩ/ÔøΩÔøΩÔøΩjT\u0015ÔøΩÔøΩsÔøΩÔøΩÔøΩ\u0002ÔøΩ=zaÔøΩÔøΩWo>ÔøΩBÔøΩFÔøΩ\u0004ÔøΩyÔøΩÔøΩ\u0017/\nÔøΩ\u0013ÔøΩHN{OÔøΩ\u0011-3n\u0015ÔøΩZÔøΩÔøΩÔøΩc'JÔøΩ?d\u0005ÔøΩUQ|ÔøΩÔøΩÂ´∑ ªZkÔøΩc\nÔøΩ\u0011\u001fÔøΩZÔøΩ%;ÔøΩ*ÔøΩ)EeÔøΩ[rÔøΩ\u0014vÔøΩÔøΩ}\u0002\u0014ÔøΩ\u0006ÔøΩ\\uÕïÔøΩZQ*,ÔøΩGO_ÔøΩÔøΩ(ÔøΩ\bkÔøΩ\u0012e7~?ÔøΩÔøΩÔøΩ\n9vVÔøΩhÔøΩpÔøΩovÔøΩÔøΩ]ÔøΩÔøΩEÔøΩÔøΩ-ÔøΩyÔøΩÔøΩ\u0010\u0017oÔøΩ}ÔøΩÔøΩ?e\u001fX2ÔøΩ>\u0019zBw~ÔøΩTÔøΩÔøΩFR2ÔøΩZÔøΩÔøΩ\nÔøΩÔøΩxÔøΩÔøΩRœà\u0018/P_ÔøΩ\u0018&LÔøΩÔøΩÔøΩGXeÔøΩÔøΩ\u0004\u0001ÔøΩ-ÔøΩÔøΩÔøΩhÔøΩ\u0000ÔøΩEÔøΩ\u0004\u0004ÔøΩÔøΩÔøΩÔøΩ+ÔøΩÔøΩÔøΩÔøΩÔøΩWc“äOÔøΩÔøΩÔøΩ]p\u0003k\u0010\ng‰ºÉÔøΩÔøΩ;MÔøΩÔøΩZ,\u001bÔøΩ\n\u0006ÔøΩ\u0015ÔøΩ}gÔøΩcÔøΩÔøΩÔøΩ\u0007gÿ†ÔøΩ9$ÔøΩX\n÷∞3ÔøΩZÔøΩ\u000frÔøΩÔøΩ!jÔøΩÔøΩq(aÔøΩÔøΩÔøΩ\niÔøΩNÔøΩ¬£~[\u0015ÔøΩ`K4f)PÔøΩ—¥ÔøΩ_ÔøΩ{bJÔøΩÔøΩ^K…∫ÔøΩÔøΩ\u0014VÔøΩ\u0014ÔøΩÔøΩ+#ÔøΩ\nÔøΩ\u0019ÔøΩÔøΩ\u0002ÔøΩÔøΩ:ÔøΩ\u001fÔøΩÔøΩDÔøΩÔøΩ‹ø\u0010EÔøΩV\u0012Ë∑≠ÔøΩAÔøΩ\u0005ÔøΩ\u000frÔøΩFÔøΩÔøΩ\u0012\nwAfÔøΩÔøΩ\\\u0017ÔøΩ\u0013ÔøΩÔøΩFm\u0016b7ÔøΩ\nÔøΩ{ÔøΩÔøΩÂô¢ÔøΩÔøΩKÔøΩÔøΩyXn!GÔøΩÔøΩ\u000eÃÜÔøΩÔøΩtÔøΩ—±\nÔøΩÔøΩÔøΩ\nS}l}ÔøΩ\u0001{ÔøΩÔøΩ\u0001.ÔøΩÔøΩ\u0013\u0005ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0018ÔøΩÔøΩ)ÔøΩÔøΩVÔøΩ\u0017ÔøΩÔøΩÔøΩ7s]$ÔøΩwÔøΩ}ÔøΩ,∆Ü#hÔøΩÔøΩÔøΩÔøΩ\n5?ÔøΩÊùßmxHssNÔøΩÔøΩvbÔøΩ\u001fÔøΩ pÔøΩÔøΩ` ÔøΩÔøΩÔøΩ4\u0002?ÔøΩ\u001bÔøΩÔøΩÔøΩÔøΩk\nËõäwvÔøΩ\u0019ÔøΩÔøΩxJ!ÔøΩÔøΩÔøΩÔøΩJÔøΩ-yÔøΩ!#ÔøΩÔøΩÔøΩ\bÔøΩ#ﬂé1ÔøΩ,xlÔøΩ»îh&\u0004\nÔøΩE'hÔøΩÔøΩÔøΩÔøΩ\u000eÔøΩÔøΩ2ÔøΩ}IÔøΩ\u0003\u001aÔøΩ\u0006ÔøΩOÔøΩÔøΩR:\u000erÔøΩ\u0018IÔøΩ\n;qI|ÔøΩFÔøΩdcÔøΩ\u0006ÔøΩÔøΩÔøΩRUÔøΩfRnEu\u00114'\u001ff'ÔøΩ]'{ÔøΩ7bÔøΩÔøΩÔøΩtl%÷æ.UÔøΩÔøΩ\u0004«û3ÔøΩ9ÔøΩylÌùÆ+ÔøΩL+ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩa@&xÔøΩÔøΩ~ÔøΩQ }:\u000evfÔøΩÔøΩjzy^ÔøΩUÔøΩÔøΩÔøΩbÔøΩ5ÔøΩcn\u0004MÔøΩ\u0007ÿ£ÔøΩ\nAÔøΩm@9ÔøΩ2ÔøΩ∆ßÔøΩ]ex|a+ÔøΩ5ÔøΩÃçÔøΩ?ÔøΩŸæÔøΩÔøΩA\u001fÔøΩÃµ\u0011ZoÔøΩÔøΩÔøΩÔøΩ+ÔøΩÔøΩk*\u0004MÔøΩ_\nÔøΩoÔøΩÔøΩÔøΩ?3B!`ÔøΩ«¶ÔøΩ| \u0016\nÔøΩÔøΩ\u0018ÔøΩLÔøΩÔøΩÔøΩÔøΩ@ÔøΩÔøΩ‚é¥\"hÔøΩÔøΩ_ÔøΩ 7ÔøΩÔøΩÔøΩ:ÔøΩAaÔøΩV\nAÔøΩÔøΩXÔøΩd\u0007ÔøΩ~u\u0016ÔøΩ\t\u0013ByÀâÔøΩ◊îj_ÔøΩÔøΩs(sÔøΩ\"%ÔøΩÔøΩÔøΩ\"\u00016ÔøΩÔøΩÔøΩWJ:\n”Ä\niqÔøΩÔøΩÔøΩJ\nÔøΩ@ÔøΩÔøΩÔøΩ\u0010ÔøΩÔøΩ\u001fLÔøΩÔøΩÔøΩb\n;o?rÔøΩÔøΩAÔøΩÔøΩ%ÔøΩÔøΩetÔøΩUV.ÔøΩuTWNÔøΩÔøΩ\u0010ÔøΩÔøΩÔøΩXLÔøΩ}ÔøΩWÔøΩ–øÔøΩ_ÔøΩm\u0000ÔøΩÔøΩÔøΩÔøΩ\u0007?ÔøΩÔøΩÔøΩÔøΩl]\bÔøΩDÔøΩU]ÔøΩ#C–ím ÔøΩ\\ÔøΩÔøΩÔøΩ*ÔøΩpblrIÔøΩ3\u0004\u0004ÔøΩ[)ÿ´ÔøΩÔøΩ\u000eÔøΩÔøΩ–®\n=0ÔøΩ@iÔøΩÔøΩÔøΩ{N\u0018ÔøΩ<ÔøΩÔøΩÔøΩ>,ÔøΩ<ÔøΩ3ÔøΩ\u0012 \u000fÔøΩ◊äÔøΩ_ÔøΩÔøΩÔøΩ0ÔøΩÔøΩ.ÔøΩLÔøΩ!hX\u0005ZÔøΩx\u0004-\n]ÔøΩ…©37ÔøΩÔøΩ\u0014ÔøΩÔøΩÔøΩÔøΩe^TÔøΩSÔøΩÔøΩpÔøΩ\u0001\u0004\nÔøΩpÔøΩÔøΩ=ÔøΩÔøΩ\u0004ÔøΩÔøΩiÔøΩÔøΩxÔøΩWC]ÔøΩÔøΩ\nﬁà}\u0001ÔøΩ^ÔøΩÔøΩ2\u0004=}e\u0018M0YÔøΩ\u0007\u0006ÔøΩÔøΩ7ÔøΩZC\tÔøΩÔøΩ+ÔøΩZ+ÔøΩÔøΩG\u0019ÔøΩI\u0007ÔøΩ@pÔøΩ5&DÔøΩmMÔøΩÔøΩÔøΩ3{ÔøΩ\u0012ÔøΩ\u0003ﬂΩyÔøΩ\u0010ÔøΩÔøΩcÔøΩEÔøΩ\u0001\u0010ÔøΩÔøΩkÔøΩLÔøΩX\u0017\tÔøΩÔøΩU/{ÔøΩwÔøΩ1ÔøΩx[LAÔøΩ\nÔøΩuku:ÔøΩ.€ÉÔøΩn\u0002\u0010mpLÔøΩ∆äÔøΩ?ÔøΩF@ÔøΩ\u0000ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0011ÔøΩ\u0003ÔøΩJÔøΩ\u0012_\nsÔøΩ\nÔøΩ\n5ÔøΩ`@‘ô\nÔøΩ|ÔøΩ/ÔøΩÔøΩÔøΩ~3ÔøΩu—∂XÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩl äÔøΩÔøΩÔøΩÔøΩÔøΩ◊ÜÔøΩ<ÔøΩÔøΩÔøΩÔøΩTÔøΩÔøΩÔøΩGRN]ÔøΩJÔøΩÔøΩÔøΩ5,\u0013SsÔøΩ\u0002ÔøΩYÔøΩIOÔøΩ.ÔøΩ÷å\u0017ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ%c\u0017\u0006 ö=iÔøΩ!ÔøΩÔøΩ,1KÔøΩU/;;ÔøΩÔøΩÔøΩWÔøΩÔøΩxÔøΩÔøΩ.ÔøΩ0.ÔøΩN\u0001ÔøΩ{\u0005ÔøΩtÔøΩÔøΩ\u0007ÔøΩ\nÔøΩÔøΩÔøΩYÔøΩ\u0015ÔøΩqÔøΩ\u0007ÔøΩ\u0001UÔøΩoÔøΩvÔøΩ{r⁄äPÔøΩ\nÔøΩ\nÔøΩEÔøΩjkÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ0\u0001*ÔøΩÔøΩ?ÔøΩÔøΩÔøΩ?u/ÔøΩÔøΩzÔøΩDXbÔøΩÔøΩ\nÔøΩ\\}ÔøΩ>ÔøΩ{ÔøΩaÔøΩ2ÔøΩÔøΩÔøΩÔøΩk<ÔøΩ\tFÔøΩDÔøΩY\u00192%ÔøΩ»¢f,iÔøΩN]L”é\u0012ÔøΩy9]\u001bYÔøΩÔøΩV\u0004ÔøΩÔøΩ_X\nÔøΩ\u0019ÔøΩÔøΩ\u00174C\n\u0010ÔøΩWo\u0001uÔøΩ\u0015r2_ÔøΩÔøΩÔøΩqoÔøΩtÔøΩÔøΩoÔøΩQÔøΩ\u0003ÔøΩ\nÔøΩÔøΩÔøΩÔøΩo+$ÔøΩFDÔøΩh^ÔøΩÔøΩ~\u0015ÔøΩTÔøΩa)-ÔøΩ\n+>DTÔøΩÔøΩ+F':ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0001ÔøΩÔøΩ>ÛûØæÔøΩÔøΩ\u0002ÔøΩw2wkWEÔøΩÔøΩ\u00104ÔøΩÔøΩ|_HaÔøΩ/ÔøΩÔøΩÔøΩÔøΩÔøΩ=XÔøΩ,V\u0019fÔøΩÔøΩÔøΩÔøΩVTÔøΩ2tWÔøΩÔøΩGÔøΩÔøΩÔøΩÔøΩiÔøΩÔøΩÔøΩœµZÔøΩÔøΩuÔøΩ\tÔøΩ8OÔøΩ·πíÔøΩÔøΩÔøΩÔøΩÔøΩ>tÔøΩHJÔøΩÔøΩk\u000fdgÔøΩ\u0018ÔøΩÔøΩs\u000fj}ÔøΩV\u0004ÔøΩÔøΩÔøΩ-ÔøΩ#?1ÔøΩÔøΩ(ÔøΩ\u0002ÔøΩÔøΩqÔøΩÔøΩÔøΩ&#\u0015i’õ\u0002ÔøΩÔøΩÔøΩ çGÔøΩÔøΩÔøΩ|,;\bÔøΩvÔøΩÔøΩx\u0001:J\u0016oo\u0010A\u0013ÔøΩ–óC]ÔøΩÔøΩÔøΩ\u0001gJÔøΩ?ÔøΩ:ÔøΩÔøΩmÔøΩyÔøΩIÔøΩwÔøΩPBÔøΩÔøΩÔøΩ<xÔøΩÔøΩÔøΩPÔ≤áÔøΩ\u00104^PÔøΩ\n*\bÔøΩÔøΩÔøΩÔøΩ\u000fÔøΩ-ÔøΩ5+C)\u00114ÔøΩÔøΩÔøΩÔøΩ\u000f\u0018;Y4#>ÔøΩÔøΩÔøΩtÔøΩÔøΩÔøΩ\b\u001aÔøΩ\u000eÔøΩÔøΩÔøΩÔøΩRÔøΩÔøΩ$2\u0004ÔøΩ\u001fEÔøΩÔøΩ<1ÔøΩÔøΩÔøΩ~ÔøΩÔøΩoÔøΩÔøΩ/6`D\u0019EÔøΩÔøΩ0ÔøΩ$XÔøΩÔøΩÔøΩ2ÔøΩ[ÔøΩ1ÔøΩ\t\u00114-ÔøΩ\u0016ÔøΩxÔøΩ\u0017\u0004ÔøΩ:\u0013K\u000fÕãN-\u000eÔøΩ>ÔøΩÔøΩu\u0005=FÔøΩ”ØÔøΩT`\u000fU>mÔøΩmÔøΩÔøΩ\u0012xÔøΩÔøΩÔøΩÔøΩ!GÔøΩ{\u001fÔøΩEÔøΩCÔøΩ…ñ\u0006~bÔøΩ,ÔøΩÔøΩM+ÔøΩF_ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ9“ßZ\b\u001a\u0003ÔøΩeÔøΩ\nÔøΩÔøΩÔøΩÔøΩAeÔøΩYÔøΩ÷ûvÔøΩÔøΩZÔøΩ\n'N_UÔøΩÔøΩc\nÔøΩk<ÔøΩsÔøΩÔøΩfA\n,ÔøΩ@ÔøΩlÔøΩÔøΩ;ÔøΩÔøΩ\nÔøΩdÔøΩQÔøΩÔøΩ~ÔøΩÔøΩÔøΩJ\u0018ÔøΩÔøΩÔøΩh*\u0004ÔøΩgnpJÔøΩÔøΩ(ÔøΩoﬁÖÔøΩÔøΩÔøΩÔøΩ.dÔøΩÔøΩQ~ÔøΩ9EÔøΩÔøΩ\u0004ÔøΩ7ÔøΩ;ÔøΩ*\u0014PÔøΩ]\u0001eÔøΩg\\nÔøΩÔøΩLÔøΩoÔøΩÔøΩDÔøΩ\t\u0011ÔøΩÔøΩ\u001f9ÔøΩÔøΩ]ÔøΩÔøΩÔøΩO–ÖÔøΩ-ÔøΩÔøΩcwkÔøΩ\u0003ÔøΩÔøΩ\bÔøΩ#^ÔøΩ]\u001bk95ÔøΩÔøΩ/<oÔøΩÔøΩ30ÔøΩÔøΩpd\nÔøΩÔøΩÔøΩ;Z\u0019ÔøΩ\u0001ÔøΩ\nÔøΩA$ÔøΩ\u000e[KÎõä\bzÔøΩ]ÔøΩÔøΩuÔøΩC\u0015ÔøΩÔøΩÔøΩX\u000fÔøΩ\u0002ÔøΩ2lÔøΩ\u0011\tÔøΩÔøΩÔøΩi9ePD43ÔøΩ>|ÔøΩÔøΩAÔøΩ9\nÔøΩ3V<ÔøΩÔøΩyÔøΩ«≠ÔøΩ\u001aÔøΩ\n\u0015wÔøΩÔøΩcÔøΩÔøΩÔøΩg\u00132.ÔøΩW1C>rÔøΩ4ÔøΩÔøΩÔøΩÔøΩÔøΩ\u000eÔøΩÔøΩ_ÔøΩMcÔøΩÔøΩ?\u000f'\u0016ÔøΩÔøΩ\nÔøΩÔøΩÔøΩI([;ÔøΩ-WYÔøΩ5S\u0011ÔøΩhÔøΩskEÔøΩÔøΩ`\u0015PÔøΩ[kÔøΩ\nGÔøΩÔøΩ\u0011ÔøΩÔøΩ[ÔøΩGÔøΩÔøΩÔøΩ\u0002ÔøΩÔøΩiÔøΩ|e6ÔøΩGÔøΩÔøΩ1ÔøΩ>ÔøΩaÔøΩbÔøΩÔøΩ]C\b\u001a\u0006ÔøΩ8ÔøΩÔøΩuÔøΩÔøΩÔøΩDWÔøΩÔøΩ_ÔøΩ\u0006√¨#5ÔøΩÔøΩY*\u0003DÔøΩ6GÔøΩÔøΩI\\vÔøΩÔøΩÔøΩÔøΩÔøΩ\u0005ÔøΩq\u0017ÔøΩ\noﬂæÔøΩÔøΩ5\u0007b1ÔøΩÔøΩÔøΩi CÔøΩSWÔøΩÔøΩŒ¥!*\u001bÔøΩÔøΩ0ÔøΩ_ÔøΩÔøΩ2ÔøΩÔøΩÔøΩ^ÔøΩÔøΩÔøΩÔøΩ/`b+ÔøΩrÔøΩÔøΩÔøΩ*ÔøΩ.\u0004}ÔøΩÔøΩ„ØáÔøΩÔøΩ5\u0003ÔøΩSÔøΩÔøΩÍ∂õ]SÔøΩkLÔøΩÔøΩÔøΩ]ÔøΩÔøΩÔøΩPÔøΩ@ÔøΩ\"~\u00048ÔøΩ\u0010nÔøΩÔøΩÔøΩLÔøΩÔøΩÔøΩ{NÔøΩzLÔøΩÔøΩ\u000fÔøΩ\u000fÔøΩ<”éÔøΩÔøΩ5@\u0007~\u001aFÔøΩÔøΩ◊âÔøΩSÔøΩÔøΩÔøΩ\u0010wÔøΩ.KÔøΩÔøΩÔøΩ\u0007ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩb\n\u0000<ÔøΩÔøΩÔøΩÔøΩa\u0000+ÔøΩ\bL⁄â\u001f=S9{ﬂ∞ÔøΩ9ÔøΩ\nÔøΩ\nÔøΩÔøΩfÔøΩÔøΩY\u0001n\u0007ÔøΩ&,\tNÔøΩ,ÔøΩ)ÔøΩ5ÔøΩﬂ®/X\u0012ÔøΩmkÔøΩY'ÔøΩ;ÔøΩﬂ±1ÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩD\u001blÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ_«´|A0}ÔøΩd–åÔøΩcÔøΩ'q\u0017ÔøΩuÔøΩ+ÔøΩÔøΩÔøΩ(\u000fL…ë(ÔøΩÔøΩn6ÔøΩÔøΩÔøΩ«Ñ,(*«ÑÔøΩÔøΩN«èBÔøΩNÔøΩÔøΩÔøΩÔøΩÔøΩM-ÔøΩvﬂögTuÔøΩÔøΩÔøΩuÔøΩ4[ÔøΩ\u0003ÔøΩFÔøΩ=ÔøΩ\u0019ÔøΩÔøΩ\u001fÔøΩÔøΩÔøΩ«ØÔøΩÔøΩ#ÔøΩ\bAÔøΩ\u001aÔøΩ∆îAÔøΩH-\n._{ÔøΩ\u001bYÔøΩvÔøΩ]+mÔøΩcŒ°\nÔøΩ\nwÔøΩjÔøΩkÔøΩÔøΩÔøΩÔøΩÔøΩ.ÔøΩ5ÔøΩÔøΩÔøΩÔøΩ›à>ƒä~œ´{ÔøΩ|ÔøΩÔøΩÔøΩ$ÔøΩÔøΩÔøΩ\t\u0019$\u000eC~\u0016fK^>ÔøΩ\"]\b\u001aÔøΩÔøΩÔøΩ\u0013ÔøΩjÔøΩÔøΩ\u0001jHÔøΩÔøΩEÔøΩ>ÔøΩÔøΩ)ÔøΩÔøΩPÔøΩ3ÔøΩ»≥nw\u0007e+\u001bÔøΩÔøΩcUp\u000em\nÔøΩÔøΩUNÔøΩg/ÔøΩtÔøΩXÔøΩÔøΩÔøΩÔøΩÔøΩ^PÔøΩÔøΩYebÔøΩ\u0019&ÔøΩ–øÔøΩÔøΩsHÔøΩ\"5ÔøΩDI\bZ“ñ2\u0004\n\u0016_ÔøΩÔøΩ}UA\nÔøΩÔøΩÔøΩÔøΩIÔøΩOXD\u0004\n}HÔøΩ&VÔøΩÔøΩÔøΩz.ÔøΩyÔøΩÔøΩ\u0010\b\nÔøΩ0wÔøΩ\u0011ecÔøΩOÔøΩmÔøΩ\u0017KÔøΩ\nÔøΩÔøΩO<ÔøΩÔøΩ9ÔøΩaÔøΩ_h\u0004\u0014\u0015\u0014ÔøΩÔøΩÔøΩÔøΩ\"ÔøΩ\u0006–ñ\n\u0017ÔøΩ\tÔøΩÔøΩ9LÔøΩÕìÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩO`ÔøΩ\u001b?ÔøΩyÔøΩÔøΩÔøΩ$XV2KÔøΩÔøΩT\u001fÔøΩ\u0002ÔøΩ^ÔøΩÔøΩm\u0000A\u0013yEoÔøΩÔøΩFÔøΩ…Ç\n\"htÔøΩ\u000fÔøΩÔøΩuÔøΩÔøΩÔøΩW\u0013\n7ÔøΩ\u0010ÔøΩ:ÔøΩDÔøΩÔøΩU\u0002@\u0019ÔøΩ\u000eÔøΩ9']ÔøΩ'ÔøΩÔøΩÁÉò\u001a«ªÔøΩ\u000e#ÔøΩ\b\u001aÔøΩ0ÔøΩ4,ÔøΩÔøΩxÔøΩ]\u0014ÔøΩÔøΩÔøΩ\"ÔøΩ≈∏D[1\b\nM\u0004\nÔøΩ\u0000ÔøΩ\\{ÔøΩ(ÔøΩ/C–∞WÔøΩTÔøΩ\u0004mÔøΩBXÔøΩÔøΩlÔøΩÔøΩÔøΩq2HÔøΩÔøΩP\u0017XÔøΩt1ÔøΩ\u0018ÔøΩ√§\u0018\u0006ÔøΩ‹•;bÔøΩÔøΩYÔøΩ\u0016A[ÔøΩÔøΩ\"ÔøΩVÔøΩO\";g)ÔøΩÔøΩ| ≤\u00101\"\u0014ÔøΩ=\u0017\u001b\u001fzRiÔøΩ`ÔøΩ?|ÔøΩ\u0011ÔøΩ\bÔøΩÔøΩmKÔøΩÔøΩ'-\nÔøΩÔøΩÔøΩ0ÔøΩÔøΩNc\u0011AcÔøΩ»àÔøΩ}\u0018\u0010)\u000f\u0012 ÔøΩ9ÔøΩGÔøΩf<~ÔøΩZÔøΩÔøΩ ÔøΩÔøΩ3%EÔøΩÔøΩÔøΩrÔøΩÔøΩÔøΩÔøΩ›™ÔøΩ\nZ\u0002ÔøΩFÔøΩÔøΩOÔøΩW=_ÔøΩÔøΩÔøΩ~ÔøΩ\u0015ÔøΩ\u0011ÔøΩ8\u00156ÔøΩÔøΩ*\u0012!ÔøΩÔøΩÔøΩ:\u0012ÔøΩÔøΩ\u0016ÔøΩ\u0006ÔøΩÔøΩÔøΩL-lÔøΩq4\u0005…∏ÔøΩ7\nÔøΩÔøΩÔøΩÔøΩ\\PÔøΩÔøΩ(ÔøΩ>ÔøΩfÔøΩ\u0014ÔøΩÔøΩÔøΩÔøΩHÔøΩÔøΩÔøΩ\"OÔøΩÔøΩÔøΩpKÔøΩÔøΩi6ÔøΩÔøΩ\u0003ÔøΩ.ÔøΩÔøΩÔøΩÔøΩÔóØ+ÔøΩ\nÔøΩ=\n\u0015ÔøΩÔøΩÔøΩ9\nÔøΩ.ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0010/hÔøΩ\u0019«¢\u0007A[ÔøΩI\u0002—Å;=ÔøΩÔøΩb'`7~7ÔøΩÔøΩÔøΩ*ÔøΩÔøΩÔøΩ\nÔøΩÔøΩ\n\u0011/ÔøΩÔøΩ1IÔøΩÔøΩÔøΩÔøΩÔøΩ#ÔøΩÔøΩM\nDÔøΩÔøΩEÔøΩÔøΩ[ÔøΩÔøΩJÔøΩÔøΩÔøΩÔøΩ\nB8z\\ÔøΩÔøΩ\u0016ÔøΩnÔøΩÔøΩ~0&7hkÔøΩÔøΩÔøΩD\u000e#ÔøΩ=ÔøΩcœâÔøΩÔøΩÔøΩ7ÔøΩÔøΩ<*\u0016ÔøΩÔøΩÔøΩÔøΩ\u0019ÔøΩ\n4ÔøΩƒÉÔøΩÔøΩÔøΩ{\u001aÔøΩhÔøΩ\u0006ÔøΩÔøΩ\n\u0015\u0007\u0002ÔøΩ\u0006PÔøΩ\u0015MÔøΩ/\u0016ÔøΩÔøΩÔøΩ8ÔøΩC3!'ZDÔøΩÔøΩ>\n{ÔøΩÔøΩÔøΩMÔøΩuÔøΩ(ÔøΩ1j~ÔøΩÔøΩÔøΩ\\s\u001f7ÔøΩÔøΩE0ÔøΩÔøΩN√•LÔøΩpÔøΩiÔøΩ>\u0019t\u0001ÔøΩhÔøΩ\\ÔøΩrÔøΩ\u0019gÔøΩÔøΩeA\u001aDÔøΩIÔøΩÔøΩÔøΩ\n^ÔøΩÔøΩÔøΩ}MÔøΩÔøΩÔøΩÔøΩuPÔøΩ\u0007ÔøΩ\u0014\nÔøΩS~ rÔøΩhUÔøΩ\nÔøΩUdÔøΩrÔøΩ#ÔøΩ_ÔøΩ{ÔøΩ]\u0018ÔøΩ7\"ÔøΩÔøΩd\u0001%ÔøΩÔøΩP\u001bÔøΩ,'ÔøΩ;ÔøΩ\u0004ÔøΩÔøΩ#K/ÔøΩ\u00163uyÔøΩ\u0018ŸÖW\nÔøΩÔøΩ~ÔøΩ\nrÔøΩÔøΩKÔøΩÔøΩÔøΩ\u00104\u001aÔøΩ\n¬åp*\u0014Œ´\u001a1GÔøΩÔøΩÔøΩÔøΩ5\tÔøΩVÔøΩ\u0006ÔøΩÔøΩ\u0017^ÔøΩÔøΩPÔøΩPÔøΩÔøΩ\u0006ÔøΩ\u0001ÔøΩ\u0014ÔøΩ-ÔøΩ>\npÔøΩwH\u0014ÔøΩV@\u0015@3,ÔøΩ\u001fFÔøΩ\nÔøΩ◊Å.ÔøΩX\n+ÔøΩÔøΩÔøΩ\u0016ÔøΩÔøΩÔøΩwhnD\u00023ÔøΩÔøΩÔøΩÔøΩ\u0004GÔøΩ\u0015ƒÄ\u0016TﬂìÔøΩÔøΩ\u0016lÔøΩÔøΩ}0{ÔøΩÔøΩXÔøΩÔøΩ\u0003ÔøΩÔøΩ\u0012p\n3\nwaÔøΩ\u001fÔøΩ)ÔøΩWX\nÔøΩ\u0013rzÃÇ@lÔøΩÔøΩI\u0017ÔøΩÔøΩÔøΩ<ÔøΩSÔøΩÔøΩ=\u0015mÔøΩlÔøΩÔøΩ\u0004\n\u0006ÔøΩ\u0004ÔøΩhÔøΩÔøΩQÔøΩ{'ÔøΩNÔøΩBÔøΩÒÄüòÔøΩ24ÔøΩÔøΩÔøΩ#)≈∑ÔøΩ=«øq{ÔøΩ\u0019~'ÔøΩÔøΩXÔøΩÔøΩKÔøΩÔøΩS}ÔøΩÔøΩ\u000eÔøΩÔøΩ\u0017ksÔøΩÔøΩÔøΩÔøΩÔøΩ\u0002lpÔøΩÔøΩ[ÔøΩGÔøΩ\u000ftÔøΩwÔøΩÔøΩMGÔøΩÔøΩGÔøΩ\u000fl›óÔøΩM`ÔøΩÔøΩ;E]dÔøΩ9\u0004ÔøΩÔøΩ<ÔøΩ^ÔøΩÔøΩ!ÔøΩ3=\u0005ÔøΩ-9\u0015\u000fÔøΩÔøΩÔøΩ\u0005ÔøΩÔøΩÔøΩÔøΩb\u0000zÔøΩÔøΩÔøΩÔøΩzƒØ`\u0013ÔøΩÔøΩÔøΩ\bÔøΩÔøΩnÔøΩ+\u000eÔøΩƒ•_ÔøΩt~tÔøΩYÔøΩ\u0011ÔøΩ\u0017ÔøΩÔøΩŒíÔøΩŸåe5ÔøΩÔøΩ:ÔøΩÔøΩ#ÔøΩÔøΩ\u0002e.ÔøΩÔøΩÔøΩÔøΩ\u0007ÔøΩÔøΩÔøΩŸãwÔøΩ\nÔøΩÔøΩT0DkÔøΩÔøΩ”≤ÀåÔøΩ\u0018ÔøΩ4V\bÔøΩLÔøΩ(ÔøΩ\u00153ÔøΩLÔøΩjÔøΩÔøΩÔøΩÔøΩ@JKÔøΩ.ÔøΩ)ÔøΩ\u0002\u0002ÔøΩÔøΩ=\u0010\u0007hÔøΩÔøΩ(ÔøΩO\u001f\u0018ÔøΩÔøΩ\u0006\u001aE#1ÔøΩ\u001b\u001bWÔøΩÔøΩ6 K\n\bZ\u0019+5ÔøΩÔøΩÔøΩ0ÔøΩ\nUÔøΩ(\u0011ÔøΩÔøΩM.\u001a\u0007+ÔøΩÔøΩZÔøΩ^\u0007ÔøΩLÔøΩ\u001aÔøΩ\b]ÔøΩX\u00052ÔøΩÔøΩu\u0011ÔøΩÔøΩD\u0004]zÔøΩÔøΩ-50aÔøΩVÔøΩ\nÔøΩ\nÔøΩDJÔøΩ7ÔøΩ\t=ÔøΩÔøΩÔøΩ\u0015ÔøΩÔøΩ\u0014ÔøΩÔøΩÔøΩ\u0018HÔøΩ\u0014<9ÔøΩÔøΩmÔøΩi?ÔøΩŒò#—èÔøΩ-xj\u0012ÔøΩÔøΩiÔøΩG\n\u0010tG\u0005ÔøΩHÔøΩÔøΩEjÔøΩ\nAÔøΩ“õ8&ÔøΩ\nA\u001f<R ;ÔøΩÔøΩÔøΩ85ÔøΩÔøΩNÔøΩDÔøΩPÔøΩÔøΩ_ÔøΩAÔøΩÔøΩ\n\u0016ÔøΩÔøΩ%ÔøΩ\u0019\u00016}ÔøΩÔøΩ\"CÔøΩ\u0006\u00036$Q\"hÔøΩÔøΩ]?GIÔøΩnuK\u0011\u001bÔøΩ\u001bÔøΩÔøΩÔøΩic\u0016\u0004IFÔøΩÔøΩG/1:ÔøΩÔøΩ“ΩÔøΩÔøΩÔøΩÔøΩWi19ÔøΩÔøΩÔøΩ\u001fÔøΩ(HD\u0010ÔøΩÔøΩa&UÔøΩ!ÔøΩ\u00075\\ÔøΩÔøΩœ∫&\nÔøΩ<ÔøΩÔøΩ\n€±ÔøΩ2\u0011AcÔøΩÔøΩÔøΩÔøΩ/_ÔøΩUÔøΩyÔøΩÔøΩÔøΩ\u0005PÔøΩ\"ÔøΩ4\u0006AgÔøΩ_ÔøΩÔøΩ?ÔøΩ\\‘äÔøΩÔøΩ\u001buÔøΩÔøΩ,\u0004hÔøΩ\u001bÔøΩ\u0010ÔøΩÔøΩO_EÔøΩ\u0014KÔøΩÔøΩ\u00104Cd}ÔøΩ{NÔøΩ@ÔøΩ0\u0017ÔøΩs\u0010ÔøΩÔøΩÔøΩ}ÔøΩÔøΩiDÔøΩ\u0018D–∞ÔøΩÔøΩ\nJÔøΩ\u0014ÔøΩ1ÔøΩÔøΩagÔøΩr\u0000\nA9ÔøΩ\u000500ÔøΩZÔøΩÔøΩ h4F<,ÔøΩRÔøΩfx5ÔøΩ[JÔøΩ\u0017\u000fÔøΩÔøΩ:ÔøΩGÔøΩÔøΩ\nÔøΩ&ÔøΩÔøΩ[y\u0019ÔøΩj\bÔøΩÔøΩ'v\u0017ÔøΩ\u0000ÔøΩÔøΩÔøΩ~\u0004mÔøΩÕ§vÔøΩ\n\b\nfOPtÔøΩVnplÔøΩ@ÔøΩÔøΩ”≤\nn*\u001b\u0000ÔøΩnÔøΩ\nﬁæ}'\nÔøΩuQDÔøΩXh\"ÔøΩuÔøΩÔøΩd\nKuÔøΩÔøΩÔøΩ+e6ÔøΩ1B\nÔøΩ-\u0014ÔøΩÀ§=ÔøΩOÔøΩÔøΩÔøΩwÔøΩÔøΩGVÔøΩÔøΩhÔøΩÿ∂ÔøΩ/ÔøΩË¢äÔøΩrÔøΩÔøΩ,ÔøΩ\u0017XÔøΩ\bfÔøΩÔøΩÔøΩrÔøΩs\u0011A√¥ÔøΩÔøΩ\u000f{ÔøΩ\u000fÔøΩÔøΩaÔøΩÀñÔøΩÔøΩ9\u0015\u0015ÔøΩÔøΩ'4\u000eV\n\"ÔøΩÔøΩÔøΩ2i\nN*jÔøΩ\n\u0006ÔøΩÔøΩTÔøΩ\n\u0000CÔøΩÔøΩtÔøΩÔøΩ{ÔøΩŒó‹ïÔøΩ\u0005ÔøΩfgXÔøΩLÔøΩÔøΩÔøΩ\n\"ÔøΩRa\u0013/.{ÔøΩ?o]ÔøΩ\n!ÔøΩ\u0004ÔøΩHÔøΩÔøΩJÔøΩ6ÔøΩÔøΩz’äÔøΩ-ËòèÔøΩ)tÔøΩÔøΩÔøΩÔøΩÔøΩ5œûÔøΩ\u00199O{ÔøΩÔøΩÔøΩÔøΩ]oi&ÔøΩÔøΩÔøΩiÔøΩÔøΩ\u0014\u00114ÔøΩÔøΩÔøΩ!ÔøΩ-{ÔøΩÔøΩ\u0003TÔøΩJB1ÔøΩE–ºﬁ´]KÔøΩ|ÔøΩÔøΩÔøΩ/ÔøΩÔøΩiÔøΩÔøΩÔøΩÔøΩÔøΩWÔøΩÔøΩÔøΩWDÔøΩÔøΩÔøΩ\n\u0018*t\u001aÔøΩÔøΩÔøΩÔøΩs\u00184+ÔøΩr*ÔøΩkÔøΩÔøΩ_ÔøΩ\t…ï\n\u0006ÔøΩÔøΩGÔøΩYÔøΩÔøΩ)Ã¨IWVUÔøΩÔøΩU·Ø≥7D\u0001ÔøΩbÔøΩ'/;ÔøΩ\u0013ÔøΩ;wÔøΩa(XÔøΩh@ÔøΩ9ÔøΩ\n.ÔøΩÔøΩhÔøΩuÔøΩÔøΩÔøΩÔøΩÔøΩ\u0019ÔøΩFÔøΩ`\u001aqÔøΩs}$ÔøΩ\nÔøΩÔøΩÔøΩ/ÔøΩ»∑yÔøΩÔøΩÔøΩÔøΩÔøΩ\u001fÔøΩÔøΩ\u000eÔøΩ=wÔøΩÔøΩ”ãÔøΩÔøΩÔøΩÔøΩ'*ÔøΩ‘ïÔøΩ√ïÔøΩÿ¥ÔøΩmÔøΩi>RÔøΩ.ÔøΩÔøΩÔøΩ5\u0005Vz\n!LÔøΩ\nÔøΩ\u0001ÔøΩ\u0002J\u0003DÔøΩQCfÔøΩÔøΩÔøΩ\n(\nHÔøΩ\u0017ÔøΩ7nQ0VÔøΩÔøΩ\u001b\u000f\u000fÔøΩÔøΩaÔøΩ4aÔøΩtlTY<zIÔøΩ\u0019i0_\u001aÔøΩyÔøΩÔøΩÔøΩÔøΩÔøΩZÔøΩjÔøΩÔøΩ¬≠1r⁄±_waAQÔøΩ\u0001L9ÔøΩÔøΩÔøΩÔøΩsTÔøΩÔøΩ‘ô\u001bÔøΩÔøΩÔøΩÔøΩ\u0003AÔøΩiÔøΩ\u0018\u001frw\u001b\u0011AÔøΩÔøΩÔøΩ\nÔøΩ\u0000>ÔøΩw≈âÔøΩÔøΩÔøΩ\u00104ÔøΩ_ÔøΩxzÔøΩÔøΩxÔøΩÔøΩ}ÔøΩÔøΩdÔøΩ\nÔøΩ)ÔøΩÔøΩÔøΩÔøΩ`„ââ*ÔøΩpE*ÔøΩÔøΩÔøΩRqÔøΩÔøΩÔøΩ2TÔøΩÔøΩ\u0007\u0005ÔøΩ7ÔøΩ&7ÔøΩ\nÔøΩ!\tÔøΩÿ¥\u0013ÔøΩSÔøΩAÔøΩx,#yÔøΩAÔøΩf\u0010AÔøΩ4ÔøΩCÔøΩi\u0000\u0017ÔøΩ.ÔøΩÔøΩd\u0004kÔøΩ\u0007u9\u00010√£ÔøΩ4ÔøΩXÔøΩLÔøΩ\u0017ÔøΩ\b\u001aÔøΩrÔøΩjÔøΩÔøΩ\u0016ÔøΩkÔøΩ/m\u0013ÔøΩÔøΩÔøΩ\u000fc\u0003k?ÔøΩ1_=%`ÔøΩ(ÔøΩ„ßáÔøΩ-;ÔøΩ\nÔøΩ^ÔøΩ\bÔøΩÔøΩÔøΩÔøΩPÔøΩÔøΩÔøΩ|\bÔøΩB\u0000—≤ÔøΩ\u0011]\bÔøΩ ë”¥ÔøΩ|ÔøΩSÔøΩ\u0015ÔøΩÔøΩCÔøΩÔøΩ ü0!ÔøΩÔøΩFeD\"ÔøΩEÔøΩÔøΩx/1S#*Q;y\nIlZ\u0015ÔøΩÔøΩv≈£`ÔøΩJÔøΩÔøΩ—ìWÔøΩÔøΩÔøΩ|ÔøΩ\u0003ACD#\u001fJF\u0019 Æ_0\u000feÔøΩnÔøΩt?\u0001\u001aRÔøΩÔøΩÔøΩÔøΩÔøΩ2\u0011AC%JÔøΩÔøΩÔøΩ4H1vPÔøΩRÔøΩCÔøΩÔøΩÔøΩÔøΩ\nÔøΩbÔøΩÔøΩ\"ËÜùwÔøΩ'\u0014JÔøΩÔøΩÔøΩÔøΩÔøΩTÔøΩ\u001av#eÔøΩÔøΩÔøΩ\u0003`J^ÔøΩÔøΩƒãIÔøΩWÔøΩÔøΩ\bÔøΩ\tÔøΩC[ÔøΩÔøΩÔøΩ4qÔøΩÔøΩÔøΩ\"ÔøΩV[_*\u001ad\u0019ÔøΩÔøΩ\nÔøΩD\u0004ÔøΩÔøΩÔøΩ\u001bÔøΩ/ÔøΩrÔøΩÔøΩyÔøΩNT>ÔøΩ\u0011\u000f>\u0017ÔøΩ\u0019ÔøΩ7ÔøΩÔøΩÔøΩ1\bZdÔøΩÔøΩ\u0014\nZMÔøΩÔøΩ$ÔøΩÔøΩn‹ÆÔøΩ\u0013&¬±ÔøΩÔøΩbÔøΩÔøΩÔøΩÔøΩ+^cr\u0004ÔøΩfÔøΩ\u0006ÔøΩ|\u0001\\ÔøΩÔøΩ ÔøΩ\t›ù&xÔøΩ[ÔøΩÔøΩ7\u0015ÔøΩ+ÔøΩÔøΩd\b\u001am\u0016ÔøΩ7ÔøΩ>ÔøΩÔøΩH9ÔøΩ\nAS\u0015$ÔøΩ6>\u001ayÔøΩRqR#ÔøΩÔøΩgNÔøΩÔøΩÔøΩ\u0006ÔøΩÔøΩyÔøΩcÔøΩJÔøΩ[ÔøΩ\u0017ÔøΩO6\"z\n'[ÔøΩ\u0017)(ÔøΩ”öÔøΩ\nvjCÔøΩ\u0014\u0000;\"ÔøΩÔøΩÔøΩMTogo+ÔøΩÔøΩ\u0012ÔøΩmyhÔøΩDÔøΩZ~ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\n\"h`\u0010)$ÔøΩFÔøΩS\ngÔøΩwhÔøΩG\u0015yÔøΩÔøΩ\u0004O…ª\u0002\u0013ÔøΩlÔøΩ\nÔøΩ^ hÔøΩÔøΩ6ÔøΩ$ÔøΩ0ÔøΩÔøΩÔøΩÔøΩÔøΩ\n8T\u0017OÔøΩRÔøΩÔøΩÔøΩ\u001b\u001f?{-\u0005ÔøΩ\u0012-ÔøΩ~C]\u0017ÔøΩÔøΩPÔøΩxÔøΩÔøΩeVÔøΩÔøΩÔøΩ%z\u0002ÔøΩÔøΩ\u0004F|ÔøΩ\noCÔøΩ…Æ\u0011\u00114ÔøΩ_ÔøΩÔøΩYÔøΩ-)YW:\nrj!\u0014J39ÔøΩÔøΩ*6UU,ÔøΩÔøΩgÔøΩ\u001aÔøΩs#\u0005ÔøΩ\b%A\u0003\u0005ÔøΩ\u0014ŒÜ–ü\n\u0007;Àä:}3ÔøΩÔøΩ;^\u0019\u0007z\nÔøΩ(FARÔøΩÔøΩÔøΩB\u0005«•.e‹ø}ÔøΩiÔøΩÔøΩÔøΩ\u0000i'/\u000fÔøΩ\u0001+ÔøΩ\nÔøΩÔøΩÔøΩ(wÔøΩÔøΩÔøΩ3ÔøΩ\u0001ÔøΩ=ÔøΩrÔøΩÔøΩN]\n ´KX7ÔøΩ5ÔøΩ1ÔøΩ\u0007#\u0003ÔøΩÔøΩnÔøΩ\u000156kq\tÔøΩÔøΩ\nÔøΩ|[\nÔøΩÔøΩ~\u0019ÔøΩi?}u\u0018+u7ÔøΩ\u0015zÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ-ﬂ•ÔøΩÔøΩÔøΩÔøΩÔøΩ.ÔøΩÔøΩÔøΩEÔøΩÔøΩÔøΩ=l$SÔøΩ6;ÔøΩbÔøΩlPL\u00014\u000fÔøΩÔøΩÔøΩ\u0004/ÔøΩ}'14xÔøΩVÔøΩTÔøΩÔøΩY\u0000ÔøΩÔøΩÔøΩÔøΩJÔøΩÔøΩ\u0010ÔøΩÔøΩÔøΩ\u0017ÔøΩLÔøΩÃ¥EW\u0007F\u0017ÔøΩÔøΩvÔøΩ\nÔøΩqJlÔøΩI}ÔøΩA\u0001fWUÔøΩÔøΩYZÔøΩÔøΩT\u0012ÔøΩÔøΩÔøΩ;ÔøΩ*M‘£`V\n\u0013ÔøΩÔøΩ\u0018ÔøΩ9\u001bÔøΩƒ§ÔøΩ\nÔøΩÔøΩjÔøΩÔøΩÔøΩ\nAC\u000e\nÔøΩP⁄ΩÔøΩWUÔøΩ^e\n'}ÔøΩÔøΩ\n{ÔøΩÔøΩfÔøΩ!TY\u001aÔøΩ8X|ÔøΩÔøΩ&ÔøΩ\u0016ÔøΩKÔøΩÔøΩj0Êß≠\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ&ÔøΩkÔøΩÔøΩÔøΩl[hÔøΩÔøΩ5ÔøΩÔøΩÔøΩ\u0003ÔøΩÔøΩÔøΩÔøΩ\nÔøΩA\u0004\nÔøΩA\nÔøΩ\nÔøΩÔøΩÔøΩa*ÔøΩÔøΩÔøΩxAÔøΩuddÔøΩQX\\ÔøΩ\u0014ÔøΩQDÔøΩÔøΩeÔøΩÔøΩÔøΩ2ÔøΩÔøΩÔøΩ}0ÔøΩj26ÔøΩÔøΩÔøΩÔøΩ{?ÔøΩÔøΩ-ÔøΩw&d^ÔøΩ\u000eÔøΩ\u0015ÔøΩÔøΩÔøΩJÔøΩÔøΩb\u000e\u0004mÔøΩÔøΩÔøΩ~\u0019ÔøΩUg\b\u001a3ÔøΩjÔøΩKÔøΩÔøΩfEÔøΩ|JXÔøΩÔøΩÔøΩ*ÔøΩÔøΩBÔøΩ<ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩh2ÔøΩ≈ùÔøΩÔøΩÔøΩ\u0015MÔøΩÔøΩyqÔøΩ*O\u0014ÔøΩÔøΩxTGÔøΩiZÔøΩÔøΩ\u0011nÔøΩ\u0003ÔøΩ+\u0001ÔøΩÔøΩeÔøΩi\u001aÔøΩ\nkÔøΩ\u0013ÔøΩ\u0002s hÔøΩOÔøΩ\u0010ÔøΩŸã7ÔøΩÔøΩ0P9N&ÔøΩ\u0015\u000f4ÔøΩÔøΩÔøΩ0SY0ÔøΩÔøΩHÔøΩbÿ•ÔøΩkD\u0004ÔøΩÔøΩsUÔøΩ>\u0014+ÔøΩÔøΩ&ÔøΩ\u001bc3ÔøΩÔøΩ:!]L\bÔøΩRÔøΩÔøΩÃ´6ÔøΩfq>ÔøΩÔøΩ:ÔøΩ(ÔøΩ\u001ae\nÔøΩ:ÔøΩD\u001aÔøΩÔøΩÔøΩÔøΩÔøΩDÔøΩ\u0012ÔøΩ&H\bÔøΩ\nmÔøΩRÔøΩ]ÔøΩeÔøΩ^ÔøΩNÔøΩÔøΩÔøΩ\u0015XQ\u0007ÔøΩ\u0010ÔøΩ{`ÔøΩÔøΩj\u0015\u00114ÔøΩÌè£=ÔøΩﬁÜ--n:ÔøΩÔøΩÔøΩr(ÔøΩ5wx(#{ÔøΩfÔøΩ\u0002;ÔøΩÔøΩÀÜ\u0010ÔøΩ\u0018\nÔøΩOÔøΩfÔøΩTÔøΩdÔøΩ¬†7ÔøΩÔøΩwÔøΩÔøΩÔøΩ<Gi‰ºÉÔøΩÔøΩ\bÔøΩROq.QÃÅÔøΩ}C5P$pÔøΩ~\u0014ÔøΩ\u0011\u0017ÔøΩ_a%gÔøΩ\u0013\u00114VVÔøΩÔøΩ^ÔøΩÔøΩbÔøΩÔøΩÔøΩ3ÔøΩeÔøΩ}\"ÔøΩÔøΩTÔøΩÔøΩÔøΩÔøΩ_a\u0004ÔøΩÔøΩxÔøΩÔøΩÔøΩÔøΩÔøΩ\u0003ÔøΩ,ÔøΩÔøΩ\u0003ÔøΩ8ÔøΩÔøΩ¬û\u0001NÔøΩ%\nÔøΩÔøΩ[-NÔøΩÔøΩÔøΩtW|ÔøΩtmÔøΩ#u\nÔøΩÔøΩOÔøΩ&\u000e\n'QNÔøΩr!dÔøΩÔøΩÔøΩ\"ÔøΩÔøΩwHÔøΩÃπÔøΩÔøΩ giÔøΩ'g]q\n8)>ÔøΩ ÔøΩÔøΩ\u0017u&#W5ÔøΩÔøΩXÔøΩ,\b$ÔøΩ\u0013J\u0018ÔøΩÔøΩ9osUÔøΩÔøΩÔøΩI\u0017\u0005?!\u00104ÔøΩÔøΩUÔøΩÔøΩÔøΩÔøΩ[ÔøΩ\bÔøΩHJ\u0015\u000f\u0006>\u0011\tÔøΩ\u0006-\u0004ÔøΩÔøΩÔøΩ\u001aÔøΩÔøΩ‘¨+ÔøΩ{D,ﬂ§k\n\u0011ÔøΩ\u0005wÔøΩÔøΩeÔøΩÔøΩi+ÔøΩÔøΩÔøΩ\n:gÔøΩxqÔøΩÔøΩÔøΩJÔøΩÔøΩÔøΩ\u0006oÔøΩ?ÔøΩiÔøΩÔøΩxÔøΩÔøΩÔøΩÔøΩÔøΩ\u0019k4ÔøΩ\u001fMÔøΩÔøΩ\u001bÔøΩ[3!hÔøΩ\u0018tx\u0013ÔøΩ\u0001cF\n4!!ÔøΩÔøΩo*ÔøΩ/\nÔøΩÔøΩ«¨\u0000\u0016\u0018ÔøΩ p\nÔøΩi+BÔøΩ@}ÔøΩr'.=dÔøΩÕ≥ÔøΩÔøΩÔøΩ;<ÔøΩÔøΩ\u0014 ÔøΩÔøΩÔøΩÔøΩ\u0000ÔøΩŸ∞3u\u0002ÔøΩokÔøΩOÔøΩFÔøΩÔøΩ6\nÔøΩÔøΩ~\nÔøΩ—ºÔøΩ-\u0005\u001a`ÔøΩKÔøΩnÔøΩÔøΩÔøΩÔøΩ&ÔøΩ/<ÔøΩ!/ÔøΩ\n@jÔøΩÔøΩÔøΩÔøΩJÔøΩz\nÔøΩ)\b9v^l\u00060xÔøΩÔøΩGÔøΩÔøΩVÔøΩÔøΩ6ÔøΩaÔøΩ'ÔøΩ4ÔøΩMÔøΩÔøΩ>ÔøΩ<ÔøΩ!8ÔøΩ‹°ÔøΩÔøΩ;\nÔøΩÔøΩ>\u0002`ÔøΩÔøΩ,ÔøΩ}?GÔøΩ\u0001TÔøΩ7EÔøΩÔøΩ{ÔøΩÔøΩ\u0000tEÔøΩÔøΩÔøΩ\u0007ÔøΩ6:'-ÔøΩutÔøΩc\"+ÔøΩÔøΩyWÔøΩ>\u000ecÔøΩ\u0007ÔøΩÔøΩÔøΩkÔøΩ]ÔøΩÔøΩZpÔøΩjKÔøΩÔøΩÔøΩÔøΩYjÔøΩÔøΩ\u000exÔøΩpr\u0011t)ÔøΩ}ÔøΩKÔøΩNI\n\u0006:—°dÔøΩÔøΩÔøΩÔøΩK\u000eQÔøΩ4\nPÔøΩÔøΩÔøΩ\u0011ÔøΩSÔøΩ!4ÔøΩÔøΩV{ÔøΩÔøΩEÃÉÔøΩ\u001bÔøΩÔøΩ\t#ÔøΩÔøΩ|ÔøΩÔøΩ5)cÔøΩÔøΩEÔøΩEÔøΩ\u0000A#ÔøΩ\nÔøΩÔøΩ\u0001QÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩrÔøΩ3ÔøΩÔøΩxÔøΩTÔøΩ\u0014CHM4ÔøΩÔøΩ\nÔøΩ(xÔøΩ{ÔøΩ^`ÔøΩ)=AÔøΩ\nÔøΩQÔøΩ\u000erÔøΩÏ™üÔøΩmsoﬁæK:YÔøΩÔøΩ\u0004Un>FÔøΩÔøΩÔøΩÔøΩRÔøΩ \u001a7ÔøΩÔøΩ&ÔøΩ~\u0004ÔøΩ*7ÔøΩÔøΩlÔøΩ\u0010qÔøΩ\b\u001a+ÔøΩ\u0010\u0015ÔøΩÔøΩPÔøΩÔøΩwÔøΩÔøΩ%ÔøΩ\u000el4ÔøΩÔøΩÔøΩÔøΩd\nfÔøΩÔøΩÔøΩœîÔøΩÔøΩÔøΩjDq?ÔøΩ`IÔøΩ{ÔøΩÔøΩ\nÔøΩ\u0017NÔøΩ◊∞OdÔøΩ:\n\u0006:÷òzÔøΩÔøΩ\bÔøΩÔøΩÔøΩ7ÔøΩWÔøΩAIÔøΩdÔøΩ ∞EÔøΩbeÔøΩÔøΩ\u0006\u00114lÔøΩÔøΩVÔøΩ…¶1ÔøΩÔøΩÔøΩR›∞!…äÔøΩ\"ÔøΩLÔøΩÔøΩÔøΩ\n/QÔøΩ\u0007“∂ÔøΩC#ÔøΩ“ØJÔøΩ◊äÔøΩ\u001bÔøΩÔøΩÔøΩÔøΩÔøΩ'-\n\u0011/;_rWi'ÔøΩÔøΩ\t+hŸÆÔøΩÔøΩ6hÔøΩ9.C–òÔøΩ@IR\u0011(ÔøΩÔøΩSg4fÔøΩT›ÜÔøΩ\u0006G\n^ÔøΩ\u0011ÔøΩ\u0013\u000f]-:F.]{ÿ∫ÔøΩNÔøΩk\u000e\u0004=nqÔøΩÔøΩ\u0018oÔøΩ}ÔøΩÔøΩrÔøΩeÔøΩÃΩbPÔøΩÔøΩÔøΩ\u0015ÔøΩYV0vbÔøΩÔøΩ„ßØÔøΩ◊à\b:ÔøΩÔøΩ\nLÔøΩÔøΩÔøΩZ9\u000f\nÔøΩÔøΩv/^ÔøΩÔøΩ5Yz\u0005ÔøΩ\u0000ÔøΩÔøΩQ‹øÔøΩÔøΩÔøΩÕå\u0005ÔøΩÔøΩÔøΩ`?RÔøΩ\u000fÔøΩ\u0001'ÔøΩÔøΩKÔøΩ\u001bÔøΩR,kÔøΩÔøΩÔøΩ_ÔøΩÔøΩÔøΩ:ÔøΩÔøΩ\\qÔøΩQ\u0014CÔøΩ<2ÔøΩSÔøΩ<ÔøΩ2.nÔøΩÔøΩ”¢ÔøΩÔøΩq7k‚Ñ±P hÔøΩÔøΩ\nLoYÔøΩ\bÔøΩÊùßX\nÔøΩDÔøΩNÔøΩÔøΩEwÔøΩHEÔøΩI\n;8ÔøΩÔøΩÔøΩÔøΩ›ÅÔøΩÔøΩK\n!hÔøΩ}\u001a`\u0004ÔøΩiÔøΩ'X@ÔøΩ\u0019it`ÔøΩÔøΩÔøΩE3nÔøΩiÔøΩÔøΩ◊çÔøΩÔøΩHÔøΩ%19ÔøΩÔøΩÔøΩÔøΩYeÔøΩ\u0005ÔøΩÔøΩNÔøΩGÔøΩ\"ÔøΩÔøΩÔøΩ\u0010ÔøΩEÔøΩ\u0005\"ÔøΩÔøΩgÔøΩÔøΩ\u0000,(qÔøΩ|WY9C\u0013iÔøΩ\bÔøΩ\u001fÔøΩ$i\"ÔøΩ\u0000'ÔøΩÔøΩ\n}ÔøΩÔøΩU\u0005ÔøΩÔøΩl\u0015(ÔøΩ\u0007v2ÔøΩÔøΩ…¶ÔøΩ—æ\nÔøΩ&ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ*OÔøΩUy¬ÅÔøΩÔøΩ\u000f\u0007&ÔøΩÔøΩÔøΩxS!yN\bÔøΩÔøΩÔøΩkÔøΩ⁄¶ÔøΩÔøΩÔøΩFÔøΩÔøΩÔøΩ3ÔøΩÔøΩÔøΩE\u001fÔøΩÔøΩ5XyWÔøΩÔøΩÔøΩÔøΩxxÔøΩf.ÔøΩ\nA7ÔøΩÔøΩ'ÔøΩÔøΩKhÔøΩyq\u0001∆¶ÔøΩÔøΩÔøΩŒ™\"ÔøΩ‹æ\u001bÔøΩ.\u00164\u0007ÔøΩÔøΩÔøΩ\bÔøΩ\u000eby&ÔøΩ\bzÔøΩÔøΩ\u0010—ùÔøΩÔøΩƒ∂\u000eÔøΩÔøΩ`ÔøΩÔøΩ\u001fkÔøΩEÔøΩJÔøΩ\u001al\u0001ÔøΩPÔøΩxÔøΩÔøΩÔøΩÔøΩ4ÔøΩ\b⁄ÇmdÔøΩOÔøΩWYÔøΩoÔøΩ9ÔøΩPÔøΩ0ÔøΩd◊à\bÔøΩÔøΩÔøΩgXÔøΩÔøΩk\"ÔøΩÔøΩÔøΩ'/?\u00132ÔøΩMÔøΩÔøΩ14TÈ¶ñÔøΩÔøΩ*\nx9»¶ÔøΩmkl(J\u0002t,!ÔøΩqB\u0006\u0004ÔøΩgFﬁµÔøΩÔøΩÔøΩÔøΩr\tÔøΩÔøΩÔøΩYWÔøΩN\\ÔøΩpÔøΩ.ÔøΩ\u00049\u0007ÔøΩ\\ÔøΩÔøΩÔøΩ\nS\u0013ÔøΩÔøΩ\u0006ÔøΩ⁄àÔøΩÔøΩ3uD\bZÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩ@FmÔøΩÔøΩiÔøΩÔøΩ:ÔøΩrÔøΩÔøΩVÔøΩTÔøΩÔøΩÔøΩÔøΩ\u0001\u0012cSKÔøΩ\nÔøΩÔøΩÔøΩTÔøΩÔøΩÔøΩWÔøΩ\bÔøΩZIÔøΩÔøΩDj&\bnÔøΩÔøΩ>R*ÕÄÔøΩÔøΩÔøΩ{ÔøΩÔøΩÔøΩ+ÔøΩ\u001fÔøΩÔøΩhÔøΩ\u0017ÔøΩÔøΩOÔøΩ\\]ÔøΩÔøΩÔøΩ2ÔøΩ\u0001ÔøΩVÔøΩiÔøΩÔøΩTJÔøΩnÔøΩK\n\u0010ÔøΩH\u0000ÔøΩ\u0003ÔøΩ$\u0017ÔøΩÔøΩÔøΩoÔøΩ\bÔøΩ\nÔøΩ-\nÔøΩÔøΩ\u0012ÔøΩqY$;ÔøΩ\u001aÔøΩÔøΩÔøΩJ\u001aÔøΩÔøΩÔøΩÈì∂@ÔøΩQIE…ßJ\tdÔøΩ}=ÔøΩÔøΩaÔøΩ%ÔøΩÔøΩÔøΩ}ÔøΩFÔøΩŸõkÔøΩÔøΩKÔøΩÔøΩÔøΩÔøΩÔøΩYYÔøΩ\u001044ÔøΩÔøΩ|8ÔøΩ\u0014\u0011A\u0007≈ûC?ÔøΩÔøΩÔøΩ_ÔøΩyÔøΩÔøΩ\nÔøΩÔøΩ~ÔøΩ\u0019ÔøΩÔøΩ\u0005ÔøΩÔøΩSÔøΩjVÔøΩ\u0010ÔøΩ\u0018ÔøΩ@ÔøΩYÔøΩtÔøΩdÔøΩ\u0003\u0000Œ™ÔøΩÔøΩÔøΩÔøΩ^uÔøΩÔøΩgÔøΩ ÔøΩ49ÔøΩTÔøΩÔøΩÔøΩ\u0011ÔøΩ-\u0007] é}ÔøΩ\u0013I\u001a‹ø7\nyÔøΩÔøΩ\u0003ÔøΩ=ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩAÔøΩÔøΩzÔøΩÔøΩ]\nÔøΩ,ÔøΩÔøΩÔøΩe\b:ÔøΩÔøΩÔøΩÔøΩ-ÔøΩ$\nÔøΩD/ÔøΩytÔøΩÔøΩ1\u0018oÔøΩSÔøΩ'ÔøΩ\u0010ÔøΩÔøΩÔøΩOÔøΩÔøΩÔøΩ\n+z‰ºÉÔøΩ{ÔøΩO]\u0011€â)ÔøΩdÔøΩÔøΩ[ÔøΩh7V/ÔøΩÔøΩ\u0005_ÔøΩbÔøΩ,:\nÔøΩÔøΩÔøΩ*ÔøΩ–®ÔøΩÔøΩ\bOÔøΩÔøΩzÔøΩE-ÔøΩ\n1ÔøΩ\u0007ÔøΩ]bÔøΩÔøΩ\nÔøΩÔøΩ!+ÔøΩÔøΩÔøΩ0ÔøΩf5eÔøΩÔøΩ%ÔøΩÔøΩ÷ßÔøΩ’¨ÔøΩÔøΩ-ÔøΩaÔøΩw\u001fÔøΩ#ÔøΩLÔøΩ\nAcÔøΩIÔøΩEÔøΩÔøΩÔøΩD)ÔøΩÔøΩÔøΩÔøΩf5eYÔøΩ\u0005ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ|ÔøΩ2\u0004ÔøΩqÔøΩ3ÔøΩÔøΩ<ÔøΩFT}?ÔøΩÔøΩÔøΩÔøΩÎ∑ü(ÔøΩÔøΩ\u001aSÕ™ÔøΩÔøΩ#eWÔøΩ\u00104VÔøΩ»π\u0007\u0007ÔøΩ\nÔøΩÔøΩÔøΩ{ÔøΩÔøΩÔøΩlÔøΩÔøΩ~ÔøΩÔøΩC]ÔøΩ\u0013ÔøΩZÔøΩÔøΩ4nÔøΩ?ÔøΩÔøΩÔøΩÔøΩvWÔøΩÔøΩ0EÔøΩÔøΩuÔøΩÔøΩ9\u00104\u0016ÔøΩÔøΩÔøΩ\nÔøΩÔøΩ[j\u0006ÔøΩ\u0018ÔøΩÔøΩÔøΩBÔøΩ'ÔøΩ-*7xÔøΩhÔøΩÔøΩxPF\u0000ÔøΩ\b:ÔøΩÔøΩ6zO\nbÔøΩnÔøΩÔøΩÔøΩKU\u0005aÔøΩcÔøΩ\u0017cS\u001f>yÔøΩ}ÔøΩ\u000fUTÔøΩÔøΩ\u000eÔøΩ1ÔøΩ\u0004.nÔøΩ\bÔøΩÔøΩR<gÔøΩÔøΩ bÔøΩIÔøΩ%ÔøΩ3ÔøΩ\u0012ÔøΩÔøΩt/\u00104ÔøΩSÔøΩÔøΩ(ÔøΩÔøΩÔøΩjVA9ÔøΩte\u0004Y}UÔøΩ\u00152\u0004ÔøΩv\nÔøΩÔøΩÔøΩ\u0018\nÔøΩ/\"ÔøΩÔøΩÔøΩœ©N4ÔøΩDÔøΩqÔøΩUÔøΩO^ÔøΩÔøΩ-ÔøΩ?@0ÔøΩ»Ø~ÔøΩÔøΩcÔøΩ\u001a hÔøΩgÔøΩÔøΩÔøΩÔøΩÔøΩ1!ÔøΩ\u0017ÔøΩÔøΩ+ÔøΩŒ†ÔøΩuÔøΩÔøΩ\u0013\u0018]@]ÔøΩÔøΩ,ÔøΩ\u0006B#\u0019S_ÔøΩÔøΩ\b\u001a\u0013O÷ìÔøΩÔøΩÔøΩ6ÔøΩ\u0016ÔøΩÔøΩkÔøΩÔøΩ+ÔøΩ#ÔøΩ@ÔøΩÔøΩ'-eÔøΩm÷∫HÔøΩkÔøΩWx[tzÔøΩ\bzÔøΩs2ÔøΩÔøΩÔøΩlÔøΩjÔøΩxeÔøΩÔøΩ\u0017ÔøΩÔøΩz\u0006ÔøΩÔøΩ=ÔøΩWi\u0007ZÔøΩrÔøΩ\nZh;}&ÔøΩ,\u0015AÔøΩVGÔøΩÔøΩÔøΩ\u0015-ÔøΩ+ÔøΩÔøΩÔøΩLDK\nÔøΩ8z~’ælÔøΩOpÔøΩh\u0013ÔøΩÔøΩ\u000e\u0003ÔøΩÔøΩLd|&,ÔøΩ:ÔøΩC/ÔøΩÔøΩWÔøΩÔøΩÔøΩ\u000f_ÔøΩÔøΩÔøΩ\nÔøΩ\b'h√ìÔøΩ\u001bp#\u0001f\u0012ÔøΩm&\nD^ÔøΩÔøΩa\n?U;\u0001ÔøΩVTÔøΩ\u0015ÔøΩ~ÔøΩÔøΩ)AÔøΩ‘¨ÔøΩ&]ÔøΩÔøΩzgÔøΩÔøΩÔøΩÔøΩI<ÔøΩ4ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩZ\u0017ÔøΩbÔøΩ}ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÒ§¢Æ\u0016ÔøΩÔøΩ“µ\u0007ÔøΩ3ÔøΩ\nVÔøΩ\u0018DÔøΩÔøΩÔøΩÔøΩÔøΩ\u0007\u0006ÔøΩ\u00123_lÔøΩp\u0005OÔøΩÔøΩÔøΩÔøΩÔøΩ`\b\u0001ÔøΩÔøΩ\nVÔøΩBUM\u0013\"hKÔøΩZÔøΩ#ÔøΩÕíÔøΩ⁄ÄÔøΩÔøΩ7ÔøΩa[{FnM\u0004ÔøΩq\u0006=}U\u0018?\u0004T\tÔøΩ2ÔøΩÔøΩ)ÔøΩC\bAc3ZcÔøΩÔøΩÔøΩEÔøΩÏ§¢ÔøΩiÔøΩ«éJWcYÔøΩÔøΩ\\ÔøΩRÔøΩCÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ.VÔøΩUÔøΩÔøΩR`\u0006ÔøΩ?6:%ÔøΩÔøΩ_ÔøΩÔøΩ8ÔøΩÔøΩÔøΩ\u001awÔøΩ\u0002*ÔøΩn\n\u0014ÔøΩÔøΩSÔøΩÔøΩÔøΩÔøΩÔøΩ_ÔøΩ{aÔøΩÔøΩJg\nÔøΩÔøΩÔøΩ&.=ÔøΩtÔøΩJŸçG\u0014SÔøΩU\u0018\u000fÔøΩÔøΩÔøΩ.VÔøΩÔøΩÔøΩCÔøΩÔøΩ\u000f øPÔøΩÔøΩ\bWÔøΩÿ¥ÔøΩ3~<ÔøΩÔøΩÔøΩÔøΩÔøΩ‰¥äÔøΩ\u0011ÔøΩ%'ÔøΩÔøΩNÔøΩLÔøΩÔøΩÔøΩ]÷ºÔøΩÔøΩJÔøΩo}3ÔøΩ\u0015\u00031z~ S2ÔøΩwaÔøΩÔøΩÔøΩ2\bAﬂºÔøΩ69ÔøΩÔøΩ\u0013\u0011AG%ÔøΩÔøΩÔøΩÔøΩÔøΩŒà+\u001aÔøΩÔøΩ%MÔøΩÔøΩÔøΩrÔøΩÔøΩÔøΩÔøΩ}CsÔøΩNÔøΩ\u001a#h\nÔøΩÔøΩÔøΩt\u0002ÔøΩÔøΩÔøΩƒÑÔøΩ([ÔøΩÔøΩÔøΩ\u0018ÔøΩÔøΩeÔøΩÔøΩ#5]-ÔøΩpÔøΩXFÔøΩÔøΩ@ÔøΩ—∂\u0016ÔøΩ:ÔøΩÔøΩLÔøΩ‚ö¢,ÔøΩ\u0000ÔøΩÔøΩÔøΩÔøΩ]ÔøΩ\n–∫ÔøΩ\u000fÔøΩ >ÔøΩ5”ãÔøΩ?pÔøΩ8IÔøΩ,ÔøΩ*'YÔøΩ!h\\VQQ\tyÔøΩÓΩ≤ÔøΩÔøΩÔøΩ-x\n+ÔøΩÔøΩÔøΩ#{Ey$D}\u0003etÔøΩÔøΩÔøΩÔøΩÔøΩZÔøΩ\u0015¬øNUÔøΩÔøΩÔøΩÔøΩ'\nÔøΩÔøΩlrNÔøΩÔøΩ\u0011\u000fÔøΩÔøΩÔøΩ=ÔøΩÔøΩÔøΩ3·ö•mp/\nÔøΩÔøΩ\nkÔøΩÔøΩÔøΩAÔøΩÔøΩyÔøΩ\u0002ÔøΩ\u001b\u0019ÔøΩÔøΩÔøΩ\u0016ÔøΩÔøΩO^vÔøΩ$ÔøΩ\tR-\u0004mÔøΩv\nÔøΩÔøΩÔøΩÔøΩd\bÔøΩÔøΩÔøΩÔøΩ4SÔøΩI\u0000ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩic*\u0004ÔøΩÔøΩÔøΩÔøΩÔøΩ5ÔøΩÔøΩÔøΩ\u000eÔøΩÔøΩ|ÔøΩÔøΩ}ÔøΩ9njWÔøΩ\u0005ÔøΩÔøΩ%ÔøΩ\u0018\u000fÔøΩ\nÔøΩÔøΩ!ÔøΩ@ÔøΩKw\n\u0015\u0011Abf)ÔøΩÔøΩ4\nDÔøΩ€§ÔøΩ&ÔøΩÔøΩÔøΩÔøΩ-\u0005BF!>ÔøΩ\u001b+w”â\b\u001aÔøΩ\u001aÔøΩÔøΩ\u0016ÔøΩ\u0014u\u0017}ÔøΩÔøΩRÔøΩ\nÔøΩÔøΩ7_ÔøΩÔøΩÔøΩ\u0019ÔøΩÔøΩ\nB\u0005Z\u0003ÔøΩÔøΩÔøΩÔøΩy0ÔøΩCSÔøΩ’¨\u0002\"ÔøΩ\u0019BWÔøΩ7^\tGc\u0016ÃÆÔøΩrÔøΩ-ÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩ»Ω÷ÆÔøΩÔøΩ\bÔøΩ-ÔøΩ’ÉÔøΩ-.2``\t(Û†âã\nÔøΩ/ÔøΩFÔøΩLbÔøΩÔøΩÔøΩÔøΩ}ÔøΩFÔøΩSQÔøΩ=ÔøΩRF5+ÔøΩ&ÔøΩ≈∫x\u0000bÔøΩÔøΩ\u001f$-\nÔøΩGÔøΩ+ÔøΩTÔøΩ2LÔøΩfÔøΩ\"YÔøΩSÔøΩH|ÔøΩ\u0018ÔøΩÔøΩÔøΩ€™\n–è*\nÔøΩÔøΩÔøΩÔøΩÔøΩ\nAÔøΩÔøΩÔøΩÔøΩ#\n^\nÔøΩo\\ÔøΩÔøΩÔøΩQÔøΩÔøΩÔøΩ'ÔøΩ\nd\bZÔøΩÔøΩ&ÔøΩ~ÔøΩ}m}ÔøΩÔøΩÔøΩ\u0006ÔøΩvInÔøΩ0ÔøΩÔøΩ\u0007ÔøΩh)ÃçÔøΩ@uÔøΩÔøΩ?%ÔøΩÔøΩÔøΩÔøΩX<ÔøΩNÔøΩÔøΩÔøΩ\u0013ÔøΩ^\nÔøΩÔøΩJÔøΩ>ÔøΩÔøΩÔøΩÔøΩ\u0000\u0016\u001b\u0003ÔøΩdÔøΩSeÔøΩ-ÔøΩÔøΩi\u0010ÔøΩarÔøΩÔøΩÔøΩÔøΩÔøΩZ~ÔøΩyÔøΩ!ÔøΩkÔøΩ5ÔøΩ~ÔøΩ*\u0016\u0002\u0003!}\u001f\u0014ÔøΩL\bÔøΩ\u0002ÔøΩ\u0007*WÔøΩÔøΩ√£/lÔøΩ”´\u0010ÔøΩÔøΩÔøΩ,4ÔøΩEÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ‚û™ÔøΩ‰èøÔøΩÔøΩÔøΩY~ÔøΩÔøΩ*ÔøΩ_BÔøΩmÔøΩ84\u0013ÔøΩ\u0006\n\"h\n~ÔøΩ%UdÔøΩÔøΩ&,C@*]ÔøΩ◊§ÔøΩÔøΩhÔøΩJ\u001flÔøΩÔøΩÔøΩÔøΩR\u0006ÔøΩIb\u0010Aw÷§n8STnÔøΩ\u0004!-[ÔøΩnÔøΩÔøΩ\bÔøΩ\u0017\u0011ÔøΩÔøΩÔøΩ/0[ÔøΩQÔøΩgÔøΩ\u0015OCÔøΩÔøΩ5Q/aÔøΩv^m\u0011tÔøΩ]JÔøΩvÔøΩ\nÔøΩTÔøΩÃüÔøΩ»ò\u0005AÔøΩ\u00104fÔøΩ8ÔøΩRc\u0017\u0006aÔøΩÔøΩ#h\nu\tÔøΩvÔøΩ\nÔøΩs!4ÔøΩÔøΩÔøΩxF\u000fcÔøΩbÔøΩÔøΩÔøΩ!ÔøΩ>ÔøΩ\u0007ÔøΩ\u0003ÔøΩÔøΩÔøΩ-^ÔøΩ6\u001f\u0011OW\tzÔøΩT\nÔøΩÔøΩJÔøΩ8ÔøΩÔøΩ\nÔøΩYÔøΩ\nÔøΩDg\u0015‹¥ÔøΩNœΩp+:ÔøΩ8ÔøΩ¬≠ÔøΩcÔøΩÔøΩ\u000e\n9ÔøΩ|ÔøΩJÔøΩÔøΩk4KÔøΩJÔøΩÔøΩvÔøΩ\u001bÔøΩÔøΩ\u0018ÔøΩnÔøΩÔøΩÔøΩu?ÔøΩÔøΩÔøΩ&ÔøΩZÔøΩnÔøΩÔøΩﬂÇW\u00013x1\u0010ÔøΩ\u0017ÔøΩ5\u00104ﬁÖÔøΩÔøΩp\u0016CÔøΩYQuÔøΩt\tA\u001bO\u0015XÔøΩ\"\"hÔøΩDÔøΩ\u0012ÔøΩ&ÔøΩCÔøΩ\u0010\u001a9ÔøΩ`ÔøΩÔøΩÔøΩ\u0010A\u00045ÔøΩ\u0011ÔøΩÔøΩlÔøΩ\n*'\nÔøΩÔøΩ6\bzÔøΩ{ÔøΩdÔøΩ›πÔøΩÔøΩÔøΩÔøΩÔøΩ.BÔøΩ\u0011ÔøΩ\u0003ÔøΩ\u0015ÔøΩÔøΩÔøΩ5UÔøΩIÔøΩÔøΩ>3XDDÔøΩÔøΩÔøΩ\u0012ÔøΩ∆óƒÑIeÔøΩÔøΩdÔøΩVÔøΩ%#<ÔøΩÔøΩBÔøΩÔøΩ8ÔøΩÔøΩÔøΩCÔøΩÔøΩÔøΩ=ÔøΩÀñ`ÔøΩÔøΩnÔøΩ*ÔøΩeCÔøΩ\u0010ÔøΩÔøΩÔøΩÔøΩe\bZÔøΩ\u0007[ÔøΩB\n4ÔøΩk5kÔøΩhÔøΩ\u0012ÔøΩÔøΩHÔøΩÔøΩÔøΩÕª>ÔøΩÔøΩZÔøΩÔøΩ>R\bx\u0017ÔøΩ.\nÔøΩÔøΩÔøΩÔøΩ]`\u0006KÔøΩÔøΩÔøΩ◊§ÔøΩÔøΩÔøΩUÔøΩp\u0003\u0007ÔøΩ\\\u0017ÔøΩqÔøΩ‘ÑWÔøΩ9‰ØñÔøΩÔøΩ\u001b#ÔøΩ\u0007\u00104ÔøΩ0ÔøΩÔøΩÔøΩ\b\nÔøΩ!hÔøΩ\u001flÔøΩÔøΩPÔøΩÔøΩ\"h\nu·ïØÔøΩÔøΩJgÔøΩÔøΩ\u0001ÔøΩÔøΩÔøΩRn-GÔøΩÔøΩeaE\u0016ÔøΩC–∫>0ÔøΩÔøΩiÔøΩÔøΩ&ÔøΩ2\u0001ÔøΩ*\u0013ÔøΩÔøΩ√ÖÔøΩgÔøΩÔøΩ:ÔøΩ3\u0007ÔøΩ^nuLÔøΩ*GÔøΩÔøΩTÔøΩÔøΩÔøΩÔøΩ8+ÔøΩ ºÔøΩÔøΩbWH\nYÔøΩÔøΩÔøΩÔøΩ/ÔøΩd\u0017ÔøΩ\b\u001aÔøΩ\n+qÔøΩÔøΩ*ÔøΩWfÔøΩ5FÔøΩ€ÖE~z\u0005ÔøΩÔøΩÔøΩÔøΩgMXr\b-\u000fÔøΩ/ÔøΩÔøΩ4ÔøΩ\n4ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ6ÔøΩ\u0011Ã∑\"uÔøΩ\nE~x1ÔøΩ53ÔøΩ:\u0010ÔøΩÔøΩkÔøΩ0ÔøΩÔøΩ)–ég+ÔøΩhÔøΩÔøΩnÔøΩMEÔøΩDÔøΩv\u0011AÔøΩÔøΩÔøΩ+ÔøΩaÔøΩÔøΩÔøΩÔøΩ[KtÔøΩÔøΩ~ÔøΩhd5ÔøΩWo\u0001FhÔøΩY}d^\u0002[ÔøΩiÿòÔøΩÔøΩ∆ónP0ƒêÔøΩwÔøΩÔøΩ\u0006«ÇÔøΩÔøΩq\nÔøΩÔøΩw?xÔøΩAÔøΩÔøΩÔøΩ\b\u001a*ÔøΩf&ÔøΩÔøΩ%ÔøΩÔøΩZÔøΩÔøΩf4ÔøΩFÔøΩ:tÔøΩ~ÔøΩtÔøΩÔøΩ›ßÔøΩ\nd\b\u001a6$}?rÔøΩAÔøΩÔøΩÔøΩÔøΩIÔøΩÔøΩ%2!lrIÔøΩ\u001aÔøΩ\n+\u0011ÔøΩ\u0012ÔøΩAÔøΩ\u000fÔøΩ1ÔøΩÔøΩMIÔøΩRÔøΩÔøΩÔøΩgQÔøΩLÔøΩ#ÔøΩkÔøΩ ÔøΩÔøΩÔøΩÔøΩ:^\u0003AÔøΩ\u001aÔøΩ,\u001fÔøΩÔøΩAÔøΩÔøΩÔøΩzÔøΩÔøΩZ!0ÔøΩÔøΩÔøΩÔøΩ\bÔøΩÔøΩÔøΩYyÔøΩÔøΩ\u001bÔøΩg{ÔøΩ:tÔøΩ›ªÔøΩ?ÔøΩF`ÔøΩ\u0004Fk8\u0019ÔøΩ\n\u0010j`\u0016\u0002ÔøΩ=\nÔøΩ\"F>$dÔøΩZrÔøΩ,ÔøΩÔøΩÔøΩÔøΩÔøΩ\u000f}ÔøΩÔøΩ>-ÔøΩÔøΩÔøΩÔøΩOÔøΩÔøΩr‰Ñ†\u001bÔøΩ\u0012ÔøΩÔøΩTL7\u001aÔøΩÔøΩÔøΩÔøΩt\nÔøΩ:ÔøΩÔøΩFU\u0002ÔøΩ`vÔøΩÔøΩÔøΩÔøΩuxÔøΩÔøΩp7ÔøΩÔøΩ\\1x@ÔøΩ`[ÔøΩ\u0002ÔøΩ ª\n\"ÔøΩN\u00134\u00104ÔøΩÔøΩÔøΩÔøΩÔøΩb-ÔøΩÔøΩÔøΩÔøΩÔøΩ\u001ay\u0014ÔøΩÔøΩÔøΩ\u001f=y\u0005ÔøΩÔøΩ\bÔøΩÔøΩÔøΩÔøΩBÔøΩ1ÔøΩJi\u0011\u0012ÔøΩjÔøΩÔøΩ)ÔøΩÔøΩÔøΩVÔøΩOÔøΩ$ÔøΩZW\u00147ÔøΩÔøΩVÔøΩ\u0014q\tcÔøΩÔøΩÔøΩ8NÔøΩfJ\u0004\nÔøΩÔøΩÔøΩÔøΩ(ÔøΩ\bsÔøΩ]?\u0007ÔøΩÔøΩ_\nviÔøΩﬂ°ÔøΩPLÔøΩxÔøΩVu$ÔøΩxÔøΩÔøΩ\u0006u! \u0015@ÔøΩÔøΩ[ÔøΩDc\u000fkmÔøΩÔøΩq1ÔøΩ\u0004mÿµÔøΩxljq[^XÔøΩÁ±ûÔøΩi%ÔøΩ\u0018ÔøΩ~ÔøΩÔøΩÔøΩ\u001fFÔøΩ\u0006|ÔøΩÔøΩÔøΩqÔøΩ\t6ÔøΩÔøΩÔøΩÔøΩCÔøΩÔøΩÔøΩ\u0004ÔøΩÕ∫ÔøΩÔøΩÔøΩÔøΩHÔøΩÔøΩ>ÔøΩ#|>ÔøΩÔøΩyÔøΩI…≠m…èÔøΩÔøΩÔøΩ\u001fÔøΩ_xÔøΩÔøΩÔøΩK:kÔøΩR\u0014\nd\u0004ÔøΩÔøΩ\u0004ÔøΩ\u0015œ†qÔøΩ\u0006ÔøΩÔøΩÔøΩFÔøΩIÔøΩÔøΩ\u001bsÔøΩ]p^ÔøΩ\u001a\u001b\u0001A1ÿπÔøΩÔøΩu \"ÔøΩÔøΩ\nMj\nJrÔøΩ}ÔøΩXwÔøΩ⁄≠«≠ÔøΩÔøΩ@\u000eÔøΩ⁄©oX.ÔøΩ\nÔøΩÔøΩÔøΩÔøΩH>fAÔøΩÔøΩ\n^ÔøΩÔøΩÔøΩtÔøΩÔøΩLJÔøΩ\nÔøΩÔøΩÔøΩÔøΩ7vaÔøΩÔøΩ\u0013—∂xÔøΩDÔøΩcsÔøΩ{*]\u0019$hxÔøΩÔøΩ<\n∆Ç^ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩSUÔøΩ\nÔøΩÔøΩÔøΩ{ÔøΩÔøΩ 1ÔøΩ\u0017\u0003ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩJ\nÔøΩÔøΩ4ÔøΩ…≠\nÔøΩÔøΩ^sE`ÔøΩY\u00114PÔøΩV|G2{ÔøΩ\u0006ÔøΩÔøΩ2ÔøΩÔøΩBÔøΩ‘≤jÔøΩEEÔøΩÔøΩJÔøΩ?{ÔøΩÔøΩÔøΩ4ﬂ¶ÔøΩ4ÔøΩÔøΩ$vÔøΩÔøΩÔøΩC\\0ÔøΩKÔøΩÔøΩ\u000fÔøΩÔøΩÔøΩÔøΩ:ÔøΩ‘ì\u0001o\u0001%\u0000ÔøΩ\"C–≠YÔøΩVÔøΩVÔøΩ\u0000ÔøΩjÔøΩÔøΩZ\"ÔøΩÔøΩÔøΩÔøΩoÔøΩyÔøΩUÔøΩÔøΩ=\u0013ÔøΩ\u0000?\u001aÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩxÔøΩÔøΩ…±ÔøΩÔøΩÔøΩÔøΩ\u0019ÔøΩ\"ÔøΩaÔøΩ7\u0015ÔøΩÔøΩ$WÔøΩ\u0007\"ÔøΩ\u0001\u0003vÔøΩ\bÔøΩÔøΩ<ÔøΩÔøΩR\u0007+ÔøΩÔøΩ'X\u0014ÔøΩ\"\u0016NDÔøΩÔøΩÔøΩÔøΩÔøΩOÔøΩmHÔøΩvxj\nÔøΩNÔøΩ*ÔøΩ3ÔøΩIJÔøΩÔøΩﬂ´7\u001f\u0003JÔøΩ\u0012&\u0011Q\u000f4/~TÔøΩ3\u0007ÔøΩ^ÔøΩ5FÔøΩ'I'K-$ÔøΩsÔøΩ@ÔøΩÔøΩÔøΩÔøΩ\u0013ÔøΩl\nÔøΩÔøΩH)ÔøΩ{ÔøΩÔøΩ\n\"ÔøΩÔøΩxÔøΩ>ÔøΩ\u0015ÔøΩÔøΩÔøΩ\\ÔøΩÔøΩ\u0019ÔøΩÔøΩcÿì\u0012ÔøΩ-ÔøΩ=ÔøΩ\nÔøΩ\n\u0011–±1\b:(ÔøΩ\u0005'‹ñÔøΩ\u0002ÔøΩEÔøΩaÔøΩÔøΩ^@_∆®WÔøΩHGÔøΩÔøΩ{ÔøΩÔøΩ\"ÔøΩNÔøΩÔøΩÔøΩIƒåÔøΩÔøΩ/+ÔøΩÔøΩÔøΩÔøΩÔøΩ:ÔøΩÔøΩÔøΩÔøΩR\u0001AÔøΩu$\u0010ÔøΩuÔøΩÔøΩXÔøΩ\nÔøΩ>3ÔøΩÔøΩRÔøΩJ\nÔøΩV=h\u0006ÔøΩÔøΩ9oOjfÔøΩ/ÔøΩ\u0012”êÔøΩÔøΩWK\u0019~?jÔøΩXa6#ÔøΩ\u001aÔøΩv(UÔøΩ\u0017\u0018xEÔøΩ4ÔøΩ|ÔøΩÔøΩÔøΩÔøΩZ&&GÔøΩÔøΩa1H\u0018\u001f:ÔøΩÔøΩÔøΩZ6M\n\u0005\u0010.PÔøΩÔøΩÔøΩ\u0010ÔøΩÔøΩNIÔøΩg *ÔøΩ\u0006Î†∫ÔøΩaÔøΩuNnÔøΩ\u001bÔøΩÔøΩ=^A9@ÔøΩJ\u001bÔøΩbÔøΩ=,ÔøΩ∆ºÔøΩÔøΩÔøΩÔøΩR`ÔøΩÔøΩ\bNÔøΩÔøΩÔøΩ\nÔøΩNÔøΩ\u0019XÔøΩ\u0006o9ÔøΩ_eÔøΩÔøΩÔøΩ\u0011sÔøΩ\u0007\"\u00177TÔøΩw#ÔøΩ—üx5ÔøΩÔøΩÔøΩÔøΩÔøΩ\n’ìDÔøΩÔøΩ~ÔøΩÔøΩUÔøΩÔøΩ^0ÔøΩd55:\u000eqVÔøΩ\u0010D\u0004ÔøΩtÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩaÔøΩÔøΩ0\u0015%,ÔøΩuÔøΩ&ÔøΩÔøΩDÔøΩ\u0019!h\nu–îÔøΩ\u0004ÔøΩDÔøΩ$ÔøΩÔøΩÔøΩKÓê±-\u001bÕΩÔøΩÔøΩz\nJ\u0000ÔøΩhÔøΩH\n/}ÔøΩEÔøΩQÔøΩÔøΩA\u0004ÔøΩ%)ÔøΩÔøΩÔøΩ1ÔøΩh—®∆ΩÔøΩÔøΩ#\u0011AcÔøΩ\u001fÔøΩÔøΩ>\nÔøΩ\u0001zÔøΩÔøΩYxzÔøΩr÷û%;ÔøΩ\bckÔøΩÔøΩ9ÔøΩÔøΩC\n0cÔøΩÔøΩÔøΩ\nÔøΩ\u001aAÔøΩ÷ÅÔøΩ!ÔøΩÔøΩ'\u0001Ka@2,<ÔøΩeÔøΩÔøΩ#ÿ™hDÔøΩÔøΩÔøΩÔøΩ6ÔøΩÔøΩpÔøΩ;ÔøΩÔøΩÔøΩ’Ü\".⁄™SÔøΩÔøΩ_LcÔøΩÔøΩÔøΩÔøΩÔøΩj\tÔøΩhLjÔøΩ,X\nÔøΩÔøΩÔøΩD/QÔøΩ\u0000ÔøΩÔøΩÔøΩÔøΩ:pÔøΩÔøΩO\u0003ÔøΩZi\nÔøΩÔøΩÔøΩiÔøΩÔøΩÔøΩ{ÔøΩo|ÔøΩÔøΩÔøΩ\nÔøΩÔøΩ|ÔøΩkm\u0013ÔøΩÔøΩ|BsÔøΩN\\ÔøΩOÔøΩ'\u0014\u0002ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩVqÔøΩv#\nÔøΩÔøΩ0]ÔøΩÔøΩ\nÔøΩÔøΩPÔøΩÔøΩÔøΩ\u0006ÔøΩ7ÔøΩÔøΩÔøΩÔøΩF\u0005ÔøΩÔøΩÔøΩÔøΩÔøΩ;ÔøΩyÔøΩkÔøΩÔøΩ\nÔøΩ>ÔøΩyEÔøΩÔøΩÔøΩ\nnfÔøΩÔøΩÔøΩÔøΩS\u0006ÔøΩÔøΩÔøΩÔøΩTÔøΩÔøΩÔøΩÔøΩAÔøΩÔøΩÔøΩÔøΩÔøΩTÔøΩ\u001aÔøΩ?-ÔøΩ^D+ÔøΩÔøΩÔøΩÔøΩ~ÔøΩÔøΩÔøΩ2\nÔøΩ\u0001ÔøΩ\n\u0015'&ÔøΩ`X)5\u0006/ÔøΩ|5=\u0018ÔøΩÃ§e\u001adDiÔøΩeMXÔøΩ4ÔøΩ\n\u0006(ÔøΩÔøΩOÔøΩ,ÔøΩÔøΩÔøΩÔøΩ\u0001@$ÔøΩ\njÔøΩÔøΩ\u0014\u0015ÔøΩÔøΩÔøΩ\"LÔøΩÔøΩ\u0002ÔøΩE*ÔøΩ+ÔøΩ\u001fÔøΩi8\nT ÔøΩyKÔøΩfÔøΩ ÔøΩÔøΩ\"3\n>2Z\nÔøΩ\u0005ÔøΩYÔøΩ\u0003MÔøΩ%\u0012\u0019ÔøΩ.ÔøΩrÔøΩ;$wœ°\\ÔøΩÔøΩlÔøΩ\u0003UÔøΩÔøΩWÔøΩ!h,|ÔøΩeÔøΩÔøΩw0ÔøΩEÔøΩI\nÔøΩÔøΩÔøΩ\u0017[.ÔøΩÔøΩEÔøΩÔøΩÔøΩ<eÔøΩ\u0014ÔøΩ,ÔøΩÔøΩﬁ£\u0017?ÔøΩÔøΩ-ÔøΩ_ÔøΩ\b\u001a_JÔøΩ\u0018ÔøΩÔøΩÔøΩÔøΩ&zX\u001a\u001aÔøΩ|<\u001fÔøΩ!2AÔøΩÔøΩ-ÔøΩÔøΩ≈®ÔøΩÔøΩÔøΩÔøΩ_ÔøΩÔøΩÔøΩmÔøΩÔøΩiÔøΩÔøΩ\t\u00177\u0014\u0005ÔøΩ5ÔøΩÔøΩK2\u0004ÔøΩ\u0005\"N\u0012\u0012[ÔøΩ\ne\u001aÔøΩÔøΩ\\ÔøΩJÔøΩÔøΩÔøΩŒ†u17N\\rHÔøΩÔøΩÔøΩ%\u0003LbÔøΩÔøΩÔøΩfÔøΩÔøΩÔøΩm6€ÖÔøΩÔøΩ\u001bÔøΩÔøΩÔøΩ\u001fÔøΩ!sÔøΩÔøΩ-|ÔøΩÔøΩ\u0002ÔøΩ\nfÔøΩ$2ÔøΩÔøΩÔøΩ\u0011ÔøΩuÔøΩÔøΩ_\u0013.ÔøΩÔøΩhgÔøΩa3ÔøΩÔøΩÔøΩhÔøΩOÔøΩ\u0019,ÔøΩvÔøΩÔøΩ\nÔøΩÔøΩ'\u0002ÔøΩÔøΩÔøΩÔøΩ@ÔøΩÔøΩÔøΩÔøΩdÔøΩ 0ÔøΩÔøΩ\u0003i„ùéÔøΩÔøΩsÔøΩÔøΩÔøΩe$ÔøΩ(ÔøΩ\nA7`\n’™\u0018!ÔøΩÔøΩ\u0000n\n,ÔøΩqÔøΩÔøΩRÔøΩ9+ÔøΩU1fÔøΩ`ÔøΩÔøΩAÔøΩ\u0018ÔøΩÔøΩÔøΩÔøΩ\u0007M\u0014ÔøΩ0~ÔøΩ\nÔøΩBDÔøΩxYYÔøΩ'ÔøΩAnrÔøΩÔøΩgÔøΩ\u0016ÔøΩÔøΩ⁄ÜÔøΩ\u0010ÔøΩ\u0014azlﬂùFQÔøΩDÔøΩÔøΩ+ÔøΩ\u001aÔøΩ\ngÔøΩ\u0013%Fl>xÔøΩÔøΩf;ÔøΩsqÔøΩ}Ã¥ÔøΩÔøΩ4ÔøΩÔøΩ\nÔøΩÔøΩ\u0001ÔøΩÔøΩÔøΩ\b\u001aÔøΩ#ÔøΩ=ÔøΩÔøΩdÔøΩqÔøΩ\u0011F\n\"ÔøΩÔøΩK5ÔøΩÔøΩÔøΩ3ÔøΩ\u0011\u00172\u0004\n+W2ÔøΩÔøΩÔøΩÔøΩÔøΩ]\u0000aÔøΩÔøΩÔøΩÔøΩ\u0019ÔøΩÔøΩ1I?ÔøΩÔøΩÔøΩÔøΩ\u0013;ÔøΩÔøΩ'ÔøΩÔøΩ\u000eÔøΩÔøΩÔøΩ3,ÔøΩY[ÔøΩ*ÔøΩ`\u000f}ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0000qHÔøΩÔøΩG∆©ÔøΩ⁄†ÔøΩxO0\u0012***)\u0012O\u0014\u0011AÔøΩÔøΩÔøΩRÔøΩÔøΩÔøΩ\u000eÔøΩÔøΩ0ÔøΩÔøΩ8ÔøΩÔøΩ\u0007h ÔøΩ^XÔøΩÔøΩrÔøΩÔøΩÔøΩ4ÔøΩÔøΩÔøΩhÔøΩ\u001fÔøΩ\b\u001aKÔøΩzPﬁº}G\t≈òÔøΩR^ÔøΩV~\u0002ÔøΩ\nA[ÔøΩZÔøΩÔøΩXÔøΩ\u0010ÔøΩ\u0005_ÔøΩÔøΩ\u0017\u0007GÔøΩ\u0014ÔøΩÔøΩ\n9ÔøΩÔøΩ\u0000ÔøΩ\u0015\n`ÔøΩÔøΩ[\u0014|$ÔøΩHÔøΩ ÔøΩÔøΩÔøΩ[%ÔøΩÔøΩA\u0004ÔøΩ\nJV\nWÔøΩÔøΩ#ÔøΩ\u0005ÔøΩÔøΩxÔøΩcÔøΩ<ÔøΩWÔøΩÔøΩÔøΩQÔøΩiÔøΩÔøΩ&ÔøΩ&ÔøΩÔøΩÔøΩf6Pm\u00104\u001aÔøΩ\nvh1zM+<ÔøΩ“ÆÔøΩÔøΩ\bÔøΩ\u0001!hÔøΩÔøΩd!\u0012\u000fÔøΩqÔøΩK\\ÔøΩ% ÔøΩÃºÎÖóÔøΩaÔøΩÔøΩ:\u0019?ÔøΩÔøΩwÔøΩÔøΩ7ÔøΩÔøΩÔøΩ9paÔøΩÔøΩ\u0017}ÔøΩÔøΩÔøΩfÔøΩ]ÔøΩ\u0001ÔøΩÔøΩzÔøΩPU/QÔøΩTÔøΩ\u0011z8ÔøΩ\u0014-ÔøΩGDÔøΩ\u0019lmÔøΩ>ÔøΩÔøΩv\nmÔøΩÕ∫ÔøΩt?,:ÔøΩÔøΩ\n-\u0019ÔøΩ0ÔøΩÿâKÔøΩ'KOÔøΩÔøΩ\u0015ÔøΩX\u0018ÔøΩ~ÔøΩÔøΩÔøΩ\"ÔøΩÔøΩÔøΩ#ÔøΩ\u0016>\tgÔøΩb\u0007ÔøΩÔøΩ!ÔøΩÔøΩÔøΩyÔøΩ\u0015ÔøΩÔøΩÔøΩ\n⁄≥\u001aÔøΩYÔøΩÔøΩÔøΩÔøΩ%\tÔøΩaÔøΩÔøΩuÔøΩ^≈Ω\u0000_ÔøΩÔøΩ\u001fÔøΩÔøΩÔøΩÔøΩ$ÔøΩÔøΩÔøΩWÔøΩKÔøΩ\u0006;kœ†\nÔøΩfUxÔøΩ.EÔøΩÔøΩR}ÔøΩ\u0010ÔøΩ÷éÔøΩÔøΩÔøΩ2tÔøΩX2\u0006ÔøΩ\u000e&ÔøΩ\u0018K&c\u0012kÔøΩÔøΩ\u0001ÔøΩÔøΩ\bÔøΩÔøΩ\u001fÔøΩÔøΩBÔøΩÔøΩÔøΩu”åq\u0005\u00188ÔøΩÔøΩV[rÔøΩÔøΩtÔ©≥7ÔøΩÔøΩÔøΩ+ÔøΩÔøΩÔøΩ[ÔøΩÔøΩÔøΩ\u0012i«ÄÔøΩo\nÔøΩ5ÔøΩÔøΩG\u0002ÔøΩ|iÔøΩÔøΩ\u0018ÔøΩ\u0006^J$ÔøΩsÔøΩÔøΩÔøΩÔøΩ5ÔøΩÔøΩÔøΩÔøΩ*\n52\u0004\nÔøΩxÔøΩE/ÔøΩ&FÔøΩ\n]\\ÔøΩ0ÔøΩEÔøΩ<ÔøΩÔøΩ.?\u000fÔøΩ\u0014_ÔøΩÔøΩÔøΩÔøΩ\u0013w\nf5ÔøΩ\u0007H\nÔøΩÔøΩÔøΩ[UU\u0014\u001bÔøΩÔøΩÔøΩ\u0007ÔøΩ*ÔøΩZ\u0011ÔøΩÔøΩ\u0015ÔøΩ\u0012:\u0000ÔøΩÔøΩrpÔøΩWÔøΩ1d\u0007+[chÔøΩ–ÜÔøΩ-ÔøΩpzÔøΩÔøΩq3ÔøΩ|\b\u001aXÔøΩnÔøΩYa,ÔøΩÔøΩEÔøΩÔøΩÔøΩ’¨ÔøΩÔøΩ9 cÔøΩÔøΩL\u0007ÔøΩÔøΩÔøΩ\u0019M>yEÔøΩuÔøΩ\nAKÔøΩ}÷∫\bqÔøΩÔøΩd]iÔøΩÔøΩÔøΩ ÔøΩ4ÔøΩÔøΩÔøΩ(ÔøΩA[ÔøΩPs hÔøΩÔøΩÔøΩÔøΩ}ÔøΩÔøΩÔøΩÔøΩ*ÔøΩÔøΩF<ÔøΩÔøΩÔøΩÔøΩÔøΩ@ÔøΩÔøΩÔøΩÔøΩ4\u000eC\nÔøΩÔøΩ{\u0006ÔøΩ;C√ÅVÔøΩÔøΩÔøΩJDÔøΩÔøΩe\u000f`03ÔøΩ>ÔøΩO\n1:_rÔøΩÔøΩnTÔøΩz@\u001bÔøΩÔøΩf\b⁄Ç\u0007rKÔøΩ¬ë{ÔøΩÔøΩÔøΩAÔøΩÔøΩÔøΩÔøΩÔøΩ.ÔøΩÔøΩÔøΩ>ÔøΩ@ÔøΩﬂçt\u0017ÔøΩ\u0012ÔøΩ\u0011ÔøΩÔøΩ\nh_ÔøΩJﬂæ[√≤ÔøΩ<\u0011/ÔøΩtÔøΩ5BfÔøΩÔøΩÔøΩ-xQÔøΩ[wÔøΩvÔøΩÔøΩwÔøΩ0WÔøΩ\n}R\u0003\u0004MÔøΩÔøΩ%ElXÔøΩÔøΩsMÔøΩ€®ÔøΩJU|ÔøΩHGoÔøΩBFÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ4ÔøΩÔøΩcÔøΩjVÔøΩzÔøΩ{#ÔøΩ\u0006ÔøΩhÔøΩ[ÔøΩÔøΩwÔøΩ\u0004ÔøΩjÔøΩ\n+ÔøΩ3ÔøΩ\u0010ÔøΩÔøΩÔøΩiÔøΩ\nÔøΩÔøΩÔøΩÔøΩgÔøΩnc&ÔøΩn\nÔøΩÔøΩ{\nl»¨ÔøΩ\u001aÔøΩRÔøΩÔøΩÔøΩaÔøΩ1ÔøΩP\u0006[CÔøΩMÔøΩl\\ÔøΩ\\ÔøΩÔøΩ\u0005{ÔøΩÔøΩ\u0004ÔøΩeÔøΩ~\u001aÔøΩ!^ÔøΩÔøΩÔøΩK%OÔøΩÔøΩ\nÔøΩÔøΩÔøΩ=\u0018ÔøΩÔøΩÔøΩ\"ÔøΩ\u0005UPTÔøΩÔøΩÁ∞∑j\u001fÔøΩÔøΩ9ÔøΩ\"*ÔøΩÔøΩÔøΩÔøΩ\u001bÔøΩfE\b\u001a_z\u0004VÔøΩxÔøΩÔøΩgÔøΩÔøΩÔøΩÔøΩÔøΩﬂΩ{ÔøΩ-◊áÔøΩ\u0011ÔøΩ.ÔøΩ4ÔøΩÔøΩV\u0017AKÔøΩÔøΩÔøΩH\u0011ÔøΩ~ÔøΩNBÔøΩÔøΩ`ÔøΩÔøΩÔøΩAÔøΩ<\u0016\u0018ÔøΩ?r\u0007ÔøΩÔøΩJc\u0010ÔøΩÔøΩÔøΩ\u001aCÔøΩ-ÔøΩ9'ÔøΩiÔøΩY√ÖÔøΩÔøΩ\u001aNBÔøΩ‘èÔøΩ\u0013\u0013\u0011A?~ÔøΩÔøΩÔøΩfÔøΩdŒπÔøΩÔøΩÔøΩ\\ÔøΩ#vÔøΩÔøΩÔøΩX\u001b\u0004ÔøΩÔøΩ@ÔøΩÔøΩÔøΩ\t”≠ÔøΩ\u0000ÔøΩÔøΩ\\ÔøΩ⁄ëÔøΩQÔøΩ\u0013ÔøΩÔøΩwÔøΩÔøΩgÔøΩÔøΩÔøΩÔøΩÔøΩvÔøΩ\nÔøΩÔøΩÔøΩ∆öÔøΩqÔøΩÔøΩÔøΩÔøΩ@\t7ÔøΩ5\u0019aÔøΩÔøΩg\\ÔøΩ\nŒÅÔøΩÔøΩÔøΩ5ma<ÔøΩÔøΩ\u0018*ÔøΩ\u0019'ÔøΩÔøΩ?x)\nUÔøΩ\u0013xÔøΩ\"zLÔøΩ\u0001ÔøΩ”•(,ÔøΩBÔøΩÔøΩ?kÔøΩÔøΩhÔøΩ!ÔøΩJA\nÔøΩÔøΩ2ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u000fÔøΩÔøΩ/5\nÔøΩÔøΩËúº?ÔøΩ,ÔøΩÔøΩÔøΩs73ÔøΩÔøΩ?|61ÔøΩ\u0014;ÔøΩA,ÔøΩFÔøΩ'ÔøΩBÔøΩÔøΩÔøΩÔøΩ^ÔøΩ\u0018kÔøΩRhÔøΩ÷ªÔøΩÔøΩÔøΩ\nw\u001b0sÔøΩÔøΩ!ÔøΩDÔøΩÔøΩÕèÔøΩÔøΩ\u001aÔøΩ\u0016ÔøΩ\nW⁄û»π\u001a\u0010yÔøΩ“üÔøΩ\u001aÔøΩJÔøΩh)ÔøΩÔøΩoÔøΩÔøΩ@hÔøΩÔøΩÔøΩ|\nheÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\u0018ÔøΩVÔøΩ\u0018ÔøΩÔøΩtNÔøΩÔøΩ\bÔøΩ\u0001gWC;ÔøΩÔøΩ`ME\nÔøΩ=!ÔøΩ\nV\u0018ÔøΩ≈çÔøΩmÔøΩ}ÔøΩÔøΩL[\u0016CÔøΩTÔøΩ4ÔøΩÔøΩÔøΩ#0GÔøΩÔøΩÔøΩÎäïÔøΩU6ÔøΩ,ÔøΩWÔøΩ\"ÔøΩÔøΩIÔøΩÔøΩÔøΩ.AÔøΩd\nÔøΩmK\u00115ÔøΩÔøΩÔøΩÔøΩÔøΩ.ÔøΩÔøΩ@1^ZÔøΩÔøΩ0UÔøΩ\u0018ÔøΩÔøΩ\u0013.ÔøΩÔøΩUÔøΩÔøΩX\u0017ÔøΩFÔøΩ\u0019ÔøΩoﬁæ”ö6nÔøΩWÔøΩÔøΩÔøΩÔøΩ*ÔøΩÔøΩÔøΩ\u0012ÔøΩ\u0001W\bÔøΩ&ÔøΩÔøΩ=ÔøΩrƒã—áÔøΩÔøΩ—äÔøΩÔøΩpÔøΩ0ÔøΩÔøΩÔøΩÔøΩ{ÔøΩÔøΩ+yÔøΩa\u0006\u0003ÔøΩPÔøΩ.\u0004Õó\u0003@ÔøΩ\n/ÔøΩdT8ÔøΩÔøΩ\u0010ÔøΩ\u0017ÔøΩÔøΩÔøΩ\u0006ASTFÔøΩ\"hÔøΩ:ÔøΩ\u0006\u0019ÔøΩÔøΩ\\\u001bÔøΩlÔøΩÔøΩÔøΩ|ÔøΩFÔøΩÔøΩVu“Ñ(&G–òÔøΩÔøΩ\tÔøΩÔøΩ3u—àAÔøΩÔøΩ÷å\u00145ÔøΩÔøΩ\u000e\u0019ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ<\b\u001aÔøΩ@dF¬ãÔøΩT6\u0014ÔøΩO ZÔøΩ\nÔøΩÔøΩÔøΩ2bÔøΩe;ÔøΩQ7ÔøΩÔøΩk\u0014ÔøΩÔøΩÔøΩHÔøΩﬁ∞\nÔøΩ\n\u0013kÔøΩ\u0016_ÔøΩﬂæ?;ÔøΩ≈ü$_ÔøΩCÔøΩFÔøΩG*,\u0005mÔøΩqÔøΩjÔøΩÔøΩ\u0018AwÔøΩÔøΩ-≈µÔøΩ:s\u0003ÔøΩÔøΩsÔøΩÔøΩdGÔøΩÔøΩÔøΩÔøΩ\u000fÔøΩB\u0010ÔøΩ\"hÔøΩÔøΩB8QJVW^\u0010ÔøΩ9;\u001aÔøΩÔøΩ]ÔøΩ[\u0015ÔøΩÔøΩÔøΩ#HQ\u0001ÔøΩvÔøΩn\u0018(\nÔøΩÔøΩ]ÔøΩÔøΩv7uËîê\u0010\u0005\u0011$\u0004ÔøΩA$D\u0010[ÔøΩÔøΩÔøΩÔøΩÔøΩkÔøΩ\nsÔøΩ\u0004ÔøΩÔøΩxÔøΩÔøΩÔøΩ\u0007aÔøΩƒûÔøΩÔøΩ^ÔøΩÔøΩkÔøΩÔøΩgÔøΩÔøΩ&ÔøΩ\u0006?)\nÔøΩf_ÔøΩ\u0012AÔøΩÔøΩÔøΩ|PÔøΩ\u0000pÔøΩÔøΩSÔøΩÔøΩÔøΩÔøΩÔøΩ~dÔøΩx]ÔøΩ∆ùgVÔøΩ\u0001ÔøΩÔøΩÔøΩnÔøΩbÔøΩÕπ\"S69ÔøΩ{D\tÔøΩ:ÔøΩÔøΩ0:ÔøΩ|ÔøΩIÔøΩÔøΩg/ﬂµR[\na\b\u0004ÔøΩT$@ÔøΩ/rD\b\nÔøΩÿ°ÔøΩÔøΩÔøΩÔøΩ\u0003ÔøΩÔøΩÔøΩZÔøΩÔøΩÔøΩœä\u0018PÔøΩÔøΩÔøΩ\u0006\u0017ÔøΩS(!pÔøΩw$ÔøΩ\u0015ÔøΩÔøΩÔøΩÔøΩÔøΩb63ÔøΩaÔøΩ*ÔøΩÔøΩ ÔøΩÔøΩ~ÔøΩÔøΩÔøΩÔøΩVÔøΩ*ÔøΩG+ÔøΩÔøΩvÔøΩ\u0014 ÔøΩ~ÔøΩÔøΩÔøΩÔøΩZÔøΩ\u0019\u0014ÔøΩÔøΩÔøΩÔøΩ-ÔøΩ\u000ff+ÔøΩ\nÔøΩÔøΩ\u0012\u0004ÔøΩnDh€ûÔøΩ?ÔøΩÔøΩ\u0017ÔøΩ~ÔøΩÔøΩ{OÔøΩÔøΩ√ÉYÔøΩ}SÔøΩÔøΩuOÔøΩÔøΩ\bÔøΩÔøΩÔøΩP!YÔøΩ÷ΩÔøΩÔøΩ\u0013#ÔøΩÔøΩ\u0016ÔøΩsÔøΩ=j\u00104t\u0018aÔøΩOk\u0004mÔøΩÔøΩÔøΩÔøΩ!ÔøΩÔøΩ^ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩF,ÔøΩ-JÔøΩ?~ÔøΩÔøΩÔøΩHM\u0010ÔøΩÔøΩeÔøΩe’éÔøΩ0ÔøΩ\tDÔøΩtnÔøΩÔøΩÔøΩHtÔøΩ_ÔøΩÔøΩhÔøΩÔøΩÔøΩ&&ÔøΩ\u0003\u0004-!ÔøΩ\u001aÔøΩdÔøΩxÔøΩ\n<ÔøΩ\u001aÔøΩ:*\u001b]\u001048\"ÔøΩYÔøΩz7\"ÔøΩbÔøΩÔøΩ:ÔøΩÔøΩ„®ùÔøΩÔøΩÔøΩ\n\u0004ÔøΩÔøΩÔøΩgÔøΩÔøΩ.J\u001b\u0006ÔøΩÔøΩ\u0004+_ÔøΩ]ÔøΩÔøΩÔøΩoÔøΩÔøΩ!pÔøΩÔøΩF\u0012ÔøΩÔøΩƒ•ÔøΩT\u0015.\"ÔøΩCÔøΩF\u000fJ\\ÔøΩÔøΩÔøΩozÔøΩÔøΩÔøΩp/l_\u0015ÿÆÔøΩjÔøΩÔøΩ5ÔøΩ\u0018ÔøΩ9\u0000ÔøΩ\u0003fÔøΩÔøΩ8zÔøΩ_ÔøΩC~\u001f\u0012\nÔøΩ\u0013ÔøΩ\nKÔøΩrÔøΩÔøΩÔøΩz(2ÔøΩ\u001awQ\u001bÔøΩt;v\nvÔøΩ(ÔøΩbÔøΩÔøΩÔøΩ.\bÔøΩsJ/ÔøΩÔøΩWÔøΩDÔøΩÔøΩUﬂÄÔøΩToÔøΩ\tÔøΩ>ÔøΩÔøΩ\u001aÔøΩÔøΩAÔøΩÔøΩ*OÔøΩbÔøΩÔøΩÔøΩ\u001fÔøΩC?ÔøΩÔøΩ\u0007\u0007ÔøΩ@v\u0018ÔøΩs–úÔøΩik\u000eÔøΩÔøΩÔøΩÔøΩÔøΩU\u001aÔøΩÔøΩ\u001an>`w\u00058ÔøΩY\u001bÔøΩ\u0001\bÔøΩ%VÔøΩÔøΩ4ÔøΩÔøΩ\u0006\u0004}ÔøΩÔøΩ-/\\X3ÔøΩÔøΩÔøΩÔøΩ\u0005PÔøΩlÔøΩÔøΩ>ÔøΩ\u000f\tÔøΩÔøΩm\u0003NLÔøΩ9KÔøΩe57ÔøΩÔøΩ\u0003ÔøΩ ñÔøΩ:ÔøΩE\u001b\u001aÔøΩ$\b\u001afÔøΩ&ÔøΩÔøΩiÔøΩÔøΩ\nÔøΩ8ÔøΩ)1œûÔøΩ\u0015\u0017!jÔøΩÔøΩÔøΩÔøΩ^%ÔøΩVÔøΩoÔøΩÔøΩ\u0000MÔøΩD!Q*ÔøΩ&ÔøΩwxÔøΩ\u0016|&\u001bÔøΩÔøΩÔøΩz\u001aÔøΩÔøΩÔøΩ<NÔøΩ\u001a\u0006ÔøΩÔøΩÔøΩlÔøΩÔøΩDÔøΩ#ÔøΩ\"JÔøΩ_\u0007*ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0003ÔøΩ\u0013ÔøΩÔøΩÔøΩrÔøΩ\n:ÔøΩp\u00068-ÔøΩ%\tÔøΩ]TÔøΩÔøΩ\u0015ÔøΩÔøΩ\u0004AÔøΩ?tFC?ÔøΩF\u000fÔøΩÔøΩ(!ÔøΩ_ƒÉÔøΩÔøΩ(\nÔøΩ\u0019\u0017ÔøΩdÔøΩÔøΩÔøΩÔøΩ\u0007ÔøΩ\u000e\nlÔøΩÔøΩ^ÔøΩÔøΩÔøΩ~ÔøΩÔøΩÔøΩÔøΩ\n~JÔøΩCÔøΩ\"hÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩ\u0014ÔøΩÔøΩ\u0019UÔøΩ@ÔøΩÔøΩ\u0006AÔøΩÔøΩ#ÔøΩÔøΩDwÔøΩ_ÔøΩ Ä\bÔøΩÔøΩÔøΩÔøΩAÔøΩ_ÔøΩ hÔøΩ\u0005–ÖÔøΩÀíÔøΩ\u001b\u0013A#IÔøΩ€∑ÔøΩuÔøΩ}ÔøΩ%ÔøΩ\u001fÔøΩÔøΩXÔøΩÔøΩ\nÔøΩÔøΩÔøΩJt=ÙÖ†ØÔøΩz{KgÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ7ÔøΩ+ÔøΩ{\u0005ÔøΩÔøΩÔøΩ\u0007ÔøΩÔøΩaÔøΩ)5ÔøΩ_LÔøΩÔøΩ\nÔøΩÔøΩÔøΩDzÔøΩÔøΩÔøΩ(ÔøΩ\u0015ÔøΩ–ÇÔøΩÔøΩ'ÔøΩ&e;ÔøΩÔøΩzÔøΩ!ÔøΩÔøΩ\u0017ÔøΩ*UÔøΩ_ÔøΩ\u001fÔøΩS\ndÔøΩgÔøΩW.ÔøΩ3ÔøΩTNÔøΩUÔøΩo\u0017nÔøΩÔøΩÔøΩÔøΩ:hÔøΩÔøΩ\u0010$\u0003ÔøΩwÔøΩYÔøΩ\u0006\nÔøΩÔøΩp\tÔøΩ\u00114ÔøΩ%BÔøΩÔøΩ'ÔøΩBxÔøΩ2x/\nÔøΩfpMÔøΩ,\u0013ÔøΩÔøΩ*EÔøΩ0ÔøΩÔøΩ\u0013ÔøΩSÔøΩxÔøΩNQÔøΩÔøΩT\b%RJÔøΩ\u0017ÔøΩ+ﬁ™ÔøΩ,n2ÔøΩkÔøΩÔøΩ èÔøΩÔøΩÔøΩÔøΩ\b\u001aL(0ÔøΩœéÔøΩbÔøΩsÔøΩ`\u0004ÔøΩÔøΩ\u0018ÔøΩpJ”å õ-{ÔøΩ\u0011ÔøΩÔøΩÔøΩ<\"ÔøΩ:\u0000=ÔøΩÔøΩ/xÔøΩÔøΩÔøΩÔøΩaÔøΩCÔøΩÔøΩÔøΩ\nÔøΩÔøΩ\u0019\u0010 ÔøΩÔøΩa ûÔøΩ1]ÔøΩÔøΩXÔøΩÔøΩÔøΩ«ö9ÔøΩ\nII2ÔøΩÔøΩkÔøΩ\u0014ŸäÔøΩ\b:ÔøΩHrÔøΩ\u0003fÔøΩ\u0016?ÔøΩÔøΩÔøΩÔøΩÔøΩlf+\u0014\u0010tsÔøΩÔøΩÔøΩburÔøΩ>\nÔøΩiÔøΩÔøΩÔøΩdÔøΩÔøΩÔøΩÔøΩÔøΩ{\neÔøΩÔøΩÔøΩÔøΩ\u0002ÔøΩ\u0005ÔøΩÔøΩÔøΩH\bÔøΩÔøΩÔøΩwÔøΩÔøΩgÔøΩUÔøΩÔøΩ\tA\u00034ÔøΩ(NÔøΩÔøΩ>C+TÔøΩÔøΩ\u0013ÔøΩN\nÔøΩÔøΩ\u001aAÔøΩ\tÔøΩ\u0018\u0010:&esÔøΩ\u0001pÔøΩ\n~]T\u0015ÔøΩ6ÔøΩÔøΩOÔøΩÔøΩ\u00104Â†™jL\u00185ÔøΩÔøΩJÔøΩÔøΩÔøΩHZeÔøΩÔøΩE}\u0003ÔøΩzÒΩùΩ,\nÔøΩÔøΩ hIÔøΩ…ùÔøΩ/\u0000ÔøΩ\bixbB\u0018ÔøΩ)N…≠\tÔøΩÔøΩ8%9\u001b\nÔøΩiLÔøΩpÔøΩ\"ÔøΩÔøΩÔøΩ8\"*ÔøΩÔøΩÔøΩpCÔøΩÔøΩ.\b\u001an\u00150ÔøΩ\u0011Ëõà’πÔøΩÔøΩL\u0017XÔøΩÔøΩAA\u0002ÔøΩÔøΩ\nX ÔøΩZÔøΩÔøΩXÔøΩ\nÔøΩÔøΩe“Æ\u0011\nÔøΩÔøΩm>ÔøΩ5ÔøΩ(6ÔøΩDFÔøΩ\u0005ÔøΩÔøΩÔøΩÔøΩ- ÔøΩ\u0002f=E\u0016\u000eÔøΩ\n!ÔøΩ\u0017w?*\nrÔøΩ\u0018\tÔøΩts`aÔøΩÔøΩ1]∆ÖÔøΩ5\"ÔøΩÔøΩÔøΩA?\u000f\bÔøΩ;ÔøΩ\nÔøΩ=5zÔøΩÔøΩ=cÔøΩÓõ≤ÔøΩÔøΩÔøΩÔøΩis7ÔøΩ/‹ñ\u0011ÔøΩÔøΩÔøΩÔøΩVxdÔøΩXÔøΩ2ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u000fÔøΩ0ÔøΩi@\u0000{vÔøΩÔøΩWÔøΩ\u0005lÔøΩA9ÔøΩ7\u0000ÔøΩ\u00013wO^ÔøΩ<bA<D»íÔøΩ%ÔøΩ\u0004'.ÔøΩ?ÔøΩÔøΩÔøΩÔøΩ\u001a3f…æ\tÀìÔøΩÔøΩNÔøΩÔøΩÔøΩ @cÔøΩ?\nÔøΩd@ÔøΩÔøΩ⁄ê6eÔøΩÔøΩÔøΩKÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩPÔøΩÔøΩ ÔøΩjÔøΩ√áXÔøΩÔøΩÔøΩÔøΩÔøΩ3BÔøΩ\u000fSÔøΩMg7ÔøΩ_ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u000f(JÔøΩy<0ÔøΩ\u0002ÔøΩ\n@AÔøΩIÔøΩÔøΩÔøΩÔøΩyÔøΩÔøΩZRÔøΩYÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0000?\u0012ÔøΩ\u0006\u000fCÔøΩ\u0001ÔøΩ\nÔøΩÔøΩ“≠ÔøΩ\u0000ÔøΩ\u0015\u00114ÔøΩÔøΩ[ÔøΩwÔøΩ\u0011Q\"\nÔøΩÔøΩÔøΩtÔøΩ\u0011ÔøΩ0LÔøΩ8\u0017:-LÔøΩDAÔøΩÔøΩ⁄´9ÔøΩ1 ∂NÔøΩÔøΩ>\u0018\nÔøΩÔøΩÔøΩÔøΩoÔøΩÔøΩÔøΩÀå\u001aÔøΩ0Aÿû\u0010o)ÔøΩÔøΩÔøΩ<ÔøΩIÔøΩ<ÔøΩ'ÔøΩdÔøΩ\u00191MÔøΩÔøΩhÔøΩtbq\u0012\u0017ÔøΩ$-\u0015ÔøΩ\u0006*ÔøΩ{ÔøΩ(cÔøΩÔøΩ hÔøΩÔøΩÔøΩÔøΩÔøΩ h3ÔøΩT%«≥]Xq\u0015nÔøΩ⁄ê4È∞¥ÔøΩ[ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ‰ú≥0ÔøΩ%ÔøΩÔøΩ}HÔøΩÔøΩswÔøΩÔøΩÔøΩÔøΩ◊≠ÔøΩ+ÔøΩ\u0019U!ÔøΩ\tÔøΩÔøΩÔøΩÔøΩ5ÔøΩ\u0010ÔøΩ@ÔøΩ\n3#[ÔøΩÔøΩÔøΩF=ÔøΩ6ÔøΩÔøΩ.ÔøΩÔøΩ+qÔøΩÔøΩ#h\n\u0006\u0014ÔøΩBc\"hÔøΩÔøΩ\u0004 ,ÔøΩÔøΩÔøΩmPÔøΩÔøΩÔøΩ~\u001b\u0018ÔøΩÔøΩw^ÔøΩ\n\bÔøΩ|K`ÔøΩÔøΩs\u001a\u0002AÔøΩH\\ÔøΩ#\u0017@\u0002\u0014U|}\u0011ÔøΩÔøΩÔøΩ\u0003ÔøΩ'CÔøΩÔøΩe|ÔøΩÔøΩ9ÔøΩÔøΩÔøΩÔøΩ-\nyÔøΩÔøΩsk\u0007ÔøΩ[0\u0010ÔøΩÔøΩ4:\n\"\u0016ÔøΩÔøΩ0\u0018ÔøΩIÔøΩÔøΩÔøΩeÔøΩ\n,q\u0017ÔøΩ(ÔøΩ8ÔøΩ\u0019ÔøΩD\\\n”±0ÔøΩÔøΩE|>TrQ\u00116ÔøΩÔøΩ?ÔøΩÔøΩÔøΩ8ÔøΩ\u0006ÔøΩÔøΩylxÔøΩnÔøΩÔøΩ\u0013ÔøΩQ≈â2afÔøΩÔøΩ2ÔøΩÔøΩ\u0019`ÔøΩÔøΩÔøΩ|!$ÔøΩDÔøΩÔøΩÔøΩ5ÔøΩ\u0006\u001f%,ÔøΩA_\u0012\n-ÔøΩ4√âÔøΩÔøΩÔøΩÔøΩÔøΩ‚Ü∞ÔøΩÔøΩWÔøΩÔøΩ\u000fÔøΩÔøΩÔøΩ%\u0005KÔøΩ\u001b\u0011s\u0001ÔøΩ{ÔøΩR)yÔøΩÔøΩIl`ÔøΩ‰Äæ3bÔøΩÔøΩÔøΩÔøΩÔøΩN\u0017\u0004ÔøΩ#ÔøΩXÔøΩx·∞©ÔøΩ\u000fHÔøΩ!ÔøΩ\u000eÔøΩDÔøΩÔøΩ—§_@ÔøΩÔøΩ\u000f}ÔøΩ%y\u0017s7ÔøΩw\n\u0001\u0006\u0001aÔøΩi1pEÔøΩÔøΩÔøΩ‰≥öÔøΩ\u000e_T\u0003ÔøΩ\nÔøΩÔøΩÔøΩbSÔøΩTÔøΩÔøΩIÔøΩÔøΩÔøΩÔøΩ6ÔøΩhÔøΩÔøΩÔøΩlÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ{*\u0015ÔøΩCÔøΩ,nÔøΩÔøΩ7ÔøΩ+\u0010\u0006ÔøΩÔøΩ#vb\u0012\u0004\nÔøΩ!&ÔøΩ&<ÔøΩJ`S\u0011ÔøΩ\u000f}ÔøΩÔøΩ\nÔøΩ,ÔøΩÔøΩÔøΩÔøΩFlÔøΩQÔøΩ[ÔøΩÔøΩs\u000eÔøΩ\\R\nÔøΩdÔøΩ\tS8ÔøΩÔøΩÔøΩ\u0004ÔøΩÔøΩ\ntÔøΩÔøΩ'oÔøΩ\"ÔøΩi\u0014ÔøΩ$ÔøΩÔøΩV\\\u0014\u000f?ÔøΩ)VÔøΩÔøΩÔøΩÔøΩuÔøΩhs\u0013ÔøΩ\nÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩ5)\nÔøΩÔøΩSœÑ\u001b\u0016gÔøΩËÇ†\u0003ÔøΩRg'ÔøΩ0:@ÔøΩœ®\u0006A[bÔøΩ0ÔøΩ9ÔøΩ\u0011tÔøΩ1a\u0010ÔøΩÔøΩÔøΩÔøΩ%ÔøΩCÔøΩÔøΩÔøΩ\n-ﬁÉÔøΩÔøΩÔøΩÔøΩ»†aÔøΩÔøΩ\u0017\bl{ÔøΩÔøΩIFÔøΩMÔøΩÈ±ØÔøΩÔøΩÔøΩAg\u0016KÔøΩB5AÔøΩ\nÔøΩs#_ÔøΩÔøΩÔøΩÔøΩ2 \u0010ÔøΩ[\n€®&\u001aÔøΩGFQÔøΩÔøΩÔøΩrÔøΩÔøΩyÔøΩtÔøΩ\u0011<ÔøΩÔøΩÔøΩHÔøΩ\u0018E\u0004MÔøΩSÔøΩ $v\\:\"hÔøΩHÔøΩ@ÔøΩhÔøΩ[ÔøΩÔøΩÔøΩo{ÔøΩ\"ÔøΩf\b:,ÔøΩÔøΩÔøΩ-ÔøΩ\"ÔøΩÔøΩ\tÔøΩÔøΩ^zpÔøΩÔøΩ=xÔøΩÔøΩW\nÔøΩoÔøΩÔøΩ_ÔøΩ?ÔøΩ\nÃïÔøΩt\u0010{ÔøΩ\nÔøΩaÔøΩ$\\4ÔøΩÔøΩ\u0016EÔøΩ3pD\u0010xÔøΩ~ÔøΩ\u0001ÔøΩ @Œ•ÔøΩÔøΩV{ÔøΩÔøΩÔøΩÔøΩ\u0014PÔøΩÔøΩ;\u000fÔøΩÔøΩÔøΩUÔøΩ#\u0016&ÔøΩÔøΩ;ÔøΩaT\u0018xÔøΩ\u001fÔøΩÔøΩÔøΩÔøΩÔøΩ3=ÔøΩ“çÔøΩ0Mw\u0018\u0019ÔøΩ}rÔøΩÔøΩ9qÔøΩÔøΩ%;2!‘á{ÔøΩÔøΩ_ÔøΩÔøΩ7oÔøΩW.xÔøΩÔøΩÔøΩ$\u000eÔøΩÔøΩÔøΩ€§HÔøΩÔøΩa\n\u0006ÔøΩ\u0004ÔøΩFÔøΩ\u0016.'6m—ûÔøΩ‹Ä[ÔøΩp\nÔøΩÔøΩouÔøΩ{◊∞ÔøΩ{'ÔøΩHÔøΩÔøΩ%\u0003NÔøΩÔøΩ\u000fNÔøΩÔøΩÔøΩ/\u001fnrÔøΩkÔøΩÔøΩ\u0015ÔøΩ0ÔøΩAGÔøΩ9}ÔøΩo\nÔøΩÔøΩ9\u0012ÔøΩXÔøΩ:\u000fÔøΩÔøΩÔøΩÔøΩ:ÔøΩNÔøΩbÔøΩÔøΩÔøΩ;nÔøΩ~\u0000◊Ä|ÔøΩ0\bÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩw\nÔøΩÔøΩuÔøΩyÔøΩ\nÔøΩ]ÔøΩRÔøΩ\t&ÔøΩ;ÔøΩ&.OÔøΩQÔøΩÔøΩ@T\u0013cÔøΩ\u0000ÔøΩÔøΩ=\u0007O\u0002vÔøΩGÔøΩ ÔøΩrÔøΩuÔøΩÔøΩÔøΩx\u001f^(ÔøΩÔøΩƒ¨3\u0002^6ÔøΩ2\u0001XÔøΩGC%~ÔøΩnR\u0004ÔøΩtÔøΩÔøΩ\u0010≈∫ÔøΩÔøΩ9ÔøΩ%tÔøΩ\tÔøΩ\u0006◊∑xG\u0016ÔøΩ(ÔøΩ-ﬁû9{c\u001a#0wpÔøΩyÔøΩVÔøΩQÔøΩÔøΩ≈ñŸ¢ÔøΩ9!ÔøΩ~ÔøΩÔøΩ$O\tÔøΩP\n-ÔøΩ\u0018aÔøΩÔøΩÔøΩ>ÔøΩÔøΩÔøΩtNÔøΩeÔøΩa%ÔøΩ\u0016ÔøΩ.ÔøΩ21rÔøΩ=\u0019ÔøΩrÔøΩpÔøΩÔøΩp-I8ÔøΩ*ÔøΩ2ÔøΩÔøΩ\u0004nÔøΩÔøΩ\u0001\u0010tÔøΩÔøΩnb-QÔøΩÔøΩÔøΩÔøΩ\nÔøΩUÔøΩ9MFjÔøΩÔøΩÔøΩÔøΩ\u0012\n ãÔøΩ\u001f-s?\u0004ÔøΩAÔøΩÔøΩQ\u0003gÔøΩÔøΩ\u0016RtWÔøΩa\tŒ∂ÔøΩC9vSÔøΩÔøΩ€è\bÔøΩyPÔøΩ\u0007 ÔøΩvÔøΩ\u0010ÔøΩ\u0016\u0016ÔøΩz\u0011ÔøΩ\u0019FÔøΩÔøΩÔøΩuÔøΩ'\u0016kT\bÔøΩÔøΩÔøΩ\u000fÔøΩy$›òLÔøΩ\nÔøΩ\u0015,1LIÔøΩÔøΩ\u0017ÔøΩ.=~]ÔøΩÔøΩÔøΩQÔøΩ%\u000e\u0010ÔøΩ'ÔøΩ$zÔøΩÔøΩ\u0006WxdÔøΩÔøΩÔøΩÔøΩ,-ÔøΩÔøΩÔøΩIÔøΩÔøΩÔøΩÔøΩÊ¶™Zi1ÔøΩ~ÔøΩÔøΩWTÔøΩÔøΩ{ÔøΩÔøΩBÔøΩE5\b⁄åMInÔøΩNÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0004ÔøΩaz\u0015«®ÔøΩ~ÔøΩÔøΩÔøΩÔøΩ7SÔøΩ\nÔøΩÔøΩJ\nXÔøΩ\bÔøΩÔøΩÔøΩwÔøΩZ\u0012ÔøΩÔøΩ\u0002ÔøΩVo0ÔøΩÔøΩ/\nÔøΩ\tfÔøΩikS`$ÔøΩ|\u0004ÔøΩe ™\u0003ÔøΩÔøΩÔøΩ/]ÔøΩÔøΩÔøΩÁûìÔøΩÔøΩ\u0007ÔøΩ\u0001d\u0015gV\n]ÔøΩÔøΩk$\u0002ŸµW\u001f-wœ¶ÔøΩÔøΩÔøΩÔøΩÔøΩ!EbÔøΩÔøΩR{[HÔøΩÔøΩÔøΩÔøΩu'N\u001bBXÔøΩVÔøΩÔøΩ+BÔøΩÔøΩÔøΩ@\n(IÔøΩ”Æ5ÔøΩ\u0006\u0003ÔøΩÔøΩÔøΩÔøΩ'\u001by8!ÔøΩÔøΩsb\nAÔøΩ2ÔøΩÔøΩÔøΩSÔøΩÔøΩ=ÔøΩÔøΩÔøΩCÔøΩÔøΩ\u001aÔøΩkjÃà\u0005ÔøΩi\u0005ÔøΩ%\u0007xÔøΩ+/{\u0014ÔøΩYAÔøΩÔøΩXohÔøΩ:>JÔøΩ\u000euAÔøΩÔøΩÔøΩ<ÔøΩ+oÔøΩÔøΩÔøΩS|ÔøΩ\u0011\nÔøΩ!ÔøΩ\u0001ÔøΩ7;gSzBÔøΩihÔøΩ\u0013ÔøΩHKÔøΩ-ÔøΩwr\u0013+\u0010ÔøΩÀêÔøΩKpÔøΩLi\u0005ÔøΩ\nÔøΩÔøΩB]fÔøΩ ÔøΩOÔøΩ\u0005LÔøΩ\u001fÔøΩzÔøΩÔøΩ\u0010ÔøΩ_ÔøΩCÔøΩ?ÔøΩ#\u0007ÔøΩ[⁄óuzÔøΩ?ÔøΩ=ÔøΩDÔøΩEÔøΩ/K îœ≤ÔøΩÔøΩÔøΩ\u001aÔøΩÔøΩpÔøΩ) ÔøΩMÔøΩ\u0005ÔøΩ\u0015ÔøΩpÔøΩNÔøΩÔøΩ\u0010@\t\u001f\tÔøΩÔøΩÔøΩG<ÔøΩÔøΩ>ÔøΩ\u001fÔøΩxÔøΩ\u0000`ÔøΩ.ÔøΩ\t,\u0006(ÔøΩÔøΩ*ÔøΩn’ô;\ne\u0003ÔøΩ)\u0017ÔøΩÔøΩŸ∞}ÔøΩ`k\u0006ÔøΩRÁâªÔøΩÔøΩÔøΩuÔøΩ\"ÔøΩ\n?{\u0007ÔøΩÔøΩ4eÔøΩf\u001bÔøΩ\nÔøΩe ÔøΩÔøΩ_m\u000f)ÔøΩ!\u000f\u0003\u001fÔøΩ\u0016ÔøΩt8ÔøΩÔøΩ\u0000ÔøΩÔøΩÔøΩmÔøΩÔøΩ\u001aÔøΩ\nÔøΩÔøΩF\u0000ÔøΩbn=8\u000fÔøΩÔøΩHi%s\n\u0013.ÔøΩ]ÔøΩÔøΩÔøΩ~D›ñÔøΩ.\bz¬≤ÔøΩÔøΩ\u0013JÔøΩÔøΩÔøΩB\nÔøΩ\u001a\u0004mFÔøΩ!›ºÔøΩEÔøΩÔøΩ&q\u0014\u0010ÔøΩ^ÔøΩÔøΩÔøΩ8\u0005\u000e\u0016ÔøΩ\u0019B\u0011\u0018MKvdÔøΩxUÔøΩÔøΩa*ÔøΩ[^ÔøΩÔøΩÔøΩ‚ã∑ÔøΩgÔøΩKÔøΩ7swÔøΩ\u0019ÔøΩ–ºÔøΩÔøΩY1ÔøΩÔøΩfÔøΩ\u0012ÔøΩuÔøΩH\u0003\u0004\nÔøΩ9ÔøΩPÔøΩ*ÔøΩq\n\u001b4'ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ«ØÔøΩŒª _.\nÔøΩRÔøΩÔøΩqgÔøΩ!\u0005ÔøΩÔøΩ\u0011ÔøΩaJ\u0011ÔøΩ\u0019fA(u\\Z#h\u0002SÔøΩÔøΩWÔøΩ\u0019ÔøΩÔøΩÔøΩÔøΩÔøΩ =bI[AÔøΩÔøΩÔøΩÔøΩ;h\u0003\u0018ÔøΩÔøΩ%ÔøΩÔøΩ\u001fÔøΩJc ÔøΩ\nÔøΩ ÔøΩÔøΩÔøΩÔøΩm{ÔøΩ,‹ñAWYÔøΩ-ÔøΩÔøΩ,\u0019ÔøΩÔøΩÔøΩÔøΩ/?TÔøΩ_@ÔøΩÔøΩÔøΩ\b\u001a~ÔøΩÔøΩ'ÔøΩÕª\u000f\u0000+~\u0019\u0018ÔøΩMO_;ÔøΩÔøΩÔøΩwbÔøΩ\u0016\u0000ÔøΩÔøΩ\u0006\u0007\u0001ÔøΩÔøΩÔøΩ{÷Ü4\u0000ÔøΩ0ÔøΩ}ÔøΩÔøΩ\u0000ÔøΩ?|ÔøΩ\u001aP6\u0004$\u0003g«ÅoÔøΩy@ÔøΩwÔøΩÔøΩZÔøΩx!\u0002bÔøΩÔøΩpÔøΩ?ÔøΩ\u000fÔøΩ}HpÔøΩ1apÔøΩÔøΩ\niÔøΩ«∂\u0006\u0016yF\nÔøΩ*ÔøΩÔøΩ\nÔøΩeÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u000f}\tÔøΩ?8ÔøΩjYÔøΩÔøΩÔøΩ\u0012ÔøΩ]ÔøΩÔøΩÔøΩ#[ÔøΩÔøΩ\u0000ÔøΩÔøΩ/\u0007ÔøΩÔøΩÔøΩ0z'ÔøΩÔøΩ‡∫°eHÔøΩ\u001aÔøΩﬂ¶ÔøΩOÔøΩ^>ÔøΩÔøΩ&\u0007\u0006v\n\u0011ÔøΩÔøΩ\".ÔøΩÔøΩ2ÔøΩCpfÔøΩ28ÔøΩÔøΩ`v\u0012\u0000ÔøΩCÔøΩÔøΩu\n\u001b\u000eÔøΩ\u0016n\tnÔøΩ2(ÔøΩcÔøΩÔøΩ|ÔøΩÔøΩÔøΩÔøΩo\u0007ÕéÔøΩÔøΩ\nÔøΩÔøΩ\u0004GÔøΩe\\ÔøΩmW\u000f@ÔøΩ!{+ÔøΩMÔøΩ-ÔøΩÔøΩZvÔøΩ:ÔøΩ\u000fÔøΩ\bh\npÔøΩÔøΩÔøΩ{ÔøΩ{ÔøΩ\u0012\u00104\nÔøΩ\u0010ÔøΩÔøΩÔøΩ+ÔøΩÔøΩv¬öhÔøΩ\u0012ÔøΩI\u00104t-ÔøΩÔøΩ/ÔøΩ»øÔøΩ-Kq`ÔøΩÔøΩ\u0019ÔøΩVÔøΩÔøΩyVÔøΩ\u001bÔøΩ.ÔøΩ\n|,ÔøΩÔøΩÔøΩ\u0001\u0005ÔøΩÔøΩ ÔøΩt1\u0010ÔøΩIÔøΩ<ÔøΩ∆ø\u0018INTÔøΩÔøΩeÔøΩ#hÔøΩ|ÔøΩÔøΩÔøΩÔøΩÕáÔøΩÔøΩÔøΩ\u0003\nÔøΩ\\\n:|€ûRÔøΩ\u0006 äTL\u001bÔøΩrpmÔøΩÔøΩ[ÔøΩ\b\u001aÔøΩÔøΩ]\nÔøΩEÔøΩÔøΩ\u0016_ÔøΩyÔøΩgz\nÔøΩÔøΩstÔøΩÔøΩ^XÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩZÔøΩjhÔøΩÔøΩÔøΩ\u0001ÔøΩ'UlœÇÔøΩÀ™\u0004ÔøΩT!hÀéÔøΩ5ÔøΩÔøΩÔøΩÔøΩ\"ÔøΩ\u001bÔøΩÔøΩr(C\u0013\u0004MÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩAtÔøΩBÔøΩj>ÔøΩ*ÔøΩu{ÔøΩ^ÔøΩ<ÔøΩÔøΩ\u0010ÔøΩÔøΩ\u000fÔøΩÿÆl\u0006ÔøΩÔøΩÔøΩƒÅ)ÔøΩK\u0010ÔøΩÔøΩ\u000fD;?ÔøΩUÔøΩ·Æâ\nÔøΩÔøΩÔøΩHÔøΩÔøΩ#h3ÔøΩc\u0012ÔøΩÔøΩÔøΩÔøΩÔøΩP'ÔøΩL\u0016ÔøΩ\u001f\u0010ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0005ÔøΩ%ÔøΩH|°íãÔøΩÔøΩ+)\nWÔøΩQÔøΩÔøΩÔøΩÔøΩ:ÔøΩ\n\u0017#}ÔøΩ@t\u0004ÔøΩ\u0004ÔøΩÔøΩÔøΩÔøΩG\u0001\nCÔøΩ\u0005√ôÔøΩÔøΩhÔøΩÔøΩ\u0005:^ÔøΩ\u001a~ÔøΩ\u001f ÔøΩÔøΩ\u0017]ÔøΩ0ÔøΩ\u0015o2ÔøΩÔøΩEÔøΩ\u0005&fÔøΩ\u0011ÔøΩÔøΩ|ÔøΩ\n}rK/ÔøΩÔøΩÔøΩ.\b\u001a\u0006ÔøΩ8ÔøΩÔøΩœ¶ÔøΩB[ÔøΩJÔøΩhÔøΩ\\\u0003.n0hÔøΩÔøΩw&YÔøΩÔøΩÔøΩÔøΩ1ÔøΩÔøΩÔøΩ{ÔøΩ~ÔøΩ/›π#ÔøΩ ÔøΩ\u000e#BÔøΩ\nÔøΩ-ÔøΩ4>ÔøΩÔøΩ;ÔøΩÔøΩÔøΩW7\u0004\n6qy“á\u000fÔøΩ\u0017\u0007ÔøΩ\u0015ÔøΩt0ÔøΩÔøΩÔøΩ\u0007\u0001\u0004-ÔøΩÔøΩÔøΩÔøΩbÔøΩet2*ÔøΩlÔøΩ›ãh%ÔøΩeÔøΩÔøΩÔøΩ:aYh\u0019ÔøΩ\nUÔøΩR\u0003!h\bfÔøΩÔøΩÔøΩT\nÔøΩ!ÔøΩÔøΩhÔøΩÔøΩ/ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0002ÔøΩ&AÔøΩÔøΩ}ÔøΩr!ÔøΩÔøΩ\u0017IÔøΩOÔøΩÔøΩ\n\u000fÔøΩ\bZÔøΩ@ÔøΩÔøΩÔøΩÔøΩ\u0018@ÔøΩÔøΩ\u0004ÔøΩ\n4GÔøΩ-ÔøΩÔøΩo{ÔøΩÔøΩWÔøΩkÔøΩw\u0007O\nWÔøΩÔøΩ+ÔøΩJ\u00114ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0011\u0003ÔøΩÔøΩ≈ùDÔøΩÔøΩDÔøΩÔøΩ–êfÔøΩÔøΩL#ÔøΩ9UÔøΩ;zÔøΩ\nl!pÔøΩÔøΩÔøΩZÔøΩÔøΩÔøΩ]`ÔøΩ\u0012>UgnÔøΩ\u0003Q]\u0010ÔøΩ/\u0003\u0003ÔøΩ+tÔøΩÔøΩÔøΩÔøΩÔøΩ\u0017ÔøΩA–ù=`:ÔøΩÔøΩ\u0018AÔøΩ\u00075ÔøΩ>B\u0018\u0006ÔøΩIÔøΩ\u0013`ÔøΩÔøΩV\n)~\u0011^\u0010DÔøΩÔøΩ/ÔøΩÔøΩÔøΩ@ÔøΩÔøΩkÔøΩÔøΩÔøΩÔøΩ%*ÔøΩÔøΩQÔøΩ*/\\}\bÔøΩF4ÔøΩvÔøΩp‰∑û03\nÔøΩÔøΩB–ÄqﬁºU&ÔøΩÔøΩ-ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩDhÔøΩ5k^ÔøΩLÔøΩmh\nÔøΩÔøΩ )aÔøΩ{ÔøΩ,ÔøΩrÔøΩ\tÔøΩDÔøΩÔøΩÔøΩÔøΩÔøΩ\u001b^ÔøΩ\u0016%\u0010FÔøΩu``–ûÔøΩ\u0000N!\u0004]ÔøΩÔøΩa$cÔøΩ\u0006<\bPÔøΩWÔøΩÔøΩ$3\nÔøΩÔøΩ\nkÔøΩAÔøΩ\u0004ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0010\u0002ÔøΩÔøΩÔøΩ\u0005ÔøΩÔøΩgÔøΩÔøΩ\u0014ÔøΩÔøΩkSÔøΩÔøΩ>0i\u0005À©ÔøΩ=-\u0006ÔøΩÔøΩvÔøΩC~ÔøΩ\nÔøΩ.ÔøΩ.ÔøΩ\".^{4|~|ÔøΩÔøΩÔøΩÔøΩ/ÔøΩÔøΩÔøΩ*yÔøΩÔøΩ\nÔøΩfÔøΩﬂí1gS:eP\u0003ÔøΩ\nÔøΩÔøΩÔøΩÔøΩs‚†áÔøΩ\u0006ÔøΩÔøΩ}ÔøΩZ\"ÔøΩ\u0019ÔøΩn@ÔøΩ…πgÔøΩa\u000eÔøΩ3ÔøΩ\u000fp\u0015x\nÔøΩÔøΩ$T\nÔøΩÔøΩÔøΩZf\u0000\u0017ÔøΩ∆∞ÔøΩh\u0000ÔøΩÔøΩÔøΩIÔøΩ3ÔøΩﬁê>ÔøΩÔøΩÔøΩÔøΩrpÔøΩLÔøΩÔøΩ\nÔøΩl? YÔøΩÔøΩ2ÔøΩc\u0010ÔøΩ\u0003ÔøΩÔøΩvpÔøΩÔøΩ\u0005ÔøΩ\n'ÔøΩÔøΩlÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\"\u0019ÔøΩ\u000e\"ÔøΩÔøΩÔøΩÔøΩ;\u000f_ÔøΩ8w{HÔøΩÔøΩÔøΩcÔøΩÔøΩD\bÔøΩ`t”´ÔøΩÔøΩ.5ÔøΩÔøΩRzÔøΩÔøΩÓì£ÔøΩÔøΩÔøΩ\nxF\n\u0011ÔøΩÔøΩAÔøΩHjÔøΩ0ÔøΩoQÔøΩ‰ÅØÔøΩtbU\u0006JKVÔøΩf\u0012\u0004\nÔøΩÔøΩÔøΩ\u0019ÔøΩÔøΩ\u0000ÔøΩU\u001apÔøΩ\nAK\nÔøΩÔøΩbÔøΩÔøΩÔøΩÔøΩ!ÔøΩ[ÔøΩÔøΩ4ÔøΩHÔøΩÔøΩ2ÔøΩaÔøΩÔøΩ|gJÔøΩÔøΩU}ÔøΩÔøΩ%dÔøΩÔøΩm;1ÔøΩ6t\u0012ÔøΩ6ÔøΩÔøΩ\nX`NÔøΩ}ÔøΩÔøΩÔøΩ\u0005ÔøΩÔøΩ\u0004ÔøΩÔøΩaÔøΩÔøΩÔøΩwL|ÔøΩÔøΩLÔøΩ\\IÔøΩ\u0006ÔøΩMÔøΩÔøΩÔøΩ5pÔøΩ\u001f\b\u000fÔøΩÔøΩÔøΩ+ÔøΩÔøΩT!hÔøΩM\u0001\u0005ÔøΩSAﬂûÔøΩ:\u0005N%D_\u001a\"h3ÔøΩ&⁄ÉÔøΩÔøΩÔøΩÔøΩ\nA;ÀîÔøΩÔøΩ\u0019\u0001ACÔøΩmÔøΩZÔøΩÔøΩ^ÔøΩ%5AÔøΩ0ÔøΩ?}ÔøΩfOÔøΩIÔøΩbÔøΩÔøΩCÔøΩ0ÔøΩ%LwyeÔøΩ\u0005_\u0004'\u0014ÔøΩIÔøΩÔøΩÔøΩh}gƒä\u000fÔøΩuÔøΩ$\u001fÔøΩp\b\u001aÔøΩÔøΩÔøΩÔøΩ+\n\u0010XÔøΩ\u0003#qÔøΩÔøΩ8ÃµfÔøΩ\u0000ÔøΩ\u0005ÔøΩaÔøΩ\u001a\u0016RyÔøΩ\"<TÔøΩÔøΩÔøΩÔøΩ*ÔøΩÔøΩÔøΩ\u0015\u000f5\bzÔøΩÔøΩTI \u0007nVÔøΩ]]\u00104XRÔøΩÔøΩ&\u000e\\ÔøΩÔøΩÔøΩXÔøΩ6ÔøΩ&\b\u001aÔøΩÔøΩ\u001fCÔøΩ\u0015c~ÔøΩ\u000fÔøΩÔøΩÔøΩÔøΩ\u0007ÔøΩrBÔøΩ\u0014\u0010ÔøΩ√®ÔøΩJÔøΩP\u0002ÔøΩ*\u0014ÔøΩ[\n\u0011ÔøΩ\u0019\u0016ÔøΩÔøΩVXZQÔøΩÔøΩ\u0011tSÔøΩ\u0010^ÔøΩDÔøΩ?|ÔøΩ;-\u0006ÔøΩC\u0018ÔøΩÔøΩÔøΩÔøΩ\u000fÔøΩkÔøΩ\u0001\nMÔøΩÔøΩ◊¶ÔøΩÔøΩLyÔøΩMU·çÅ\u00104ÔøΩ*YÔøΩSy}mUÔøΩGO^CÔøΩS\u0015ÔøΩH\u0010ÔøΩZÔøΩ<{ÔøΩÔøΩÔøΩ\n÷¶ÔøΩÔøΩÔøΩ\u001bJÔøΩ[ÔøΩ\bZÔøΩ\u0007b\u000fÔøΩÔøΩÔøΩ\nÔøΩ‹∞\nrÔøΩÔøΩ\nMÔøΩÔøΩR\u0004\n\u0006\u00039ÔøΩÔøΩÔøΩbÔøΩÔøΩ\u0007ÔøΩrÔøΩÔøΩmÔøΩvZ`5\nÔøΩ\bÔøΩÔøΩf\nLbÔøΩRZÔøΩxQSÔøΩÔøΩ+ÔøΩÔøΩÔøΩ.)+ÔøΩ\u0005AÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\u0007ÔøΩ*^ÔøΩRÔøΩÔøΩ!*nÔøΩÔøΩ\"ÔøΩ\u0007\nAÔøΩJ?ÔøΩ\\ÔøΩÔøΩ\u0007Œñ*eÔøΩÔøΩEj1ÔøΩ\"\u0012ÔøΩÔøΩ\bÔøΩÔøΩ\n+ÔøΩ^ÔøΩÔøΩÔøΩ\u001b\u000eÔøΩKÔøΩ!ÔøΩ=ÔøΩDÔøΩÔøΩPÔøΩ≈≠\u0014AÔøΩÔøΩﬁòÔøΩ\u00189kÔøΩÔøΩ\u0019ÔøΩ\u001bÔøΩ^jÔøΩÔøΩ\u0017ÔøΩÔøΩ)ÔøΩÔøΩ\u0014V5ÔøΩÔøΩ\u0015\u0017@ÔøΩ\u0000ÔøΩ\u0011>ÔøΩÔøΩ\"ÔøΩÔøΩE\u0012?ÔøΩuÔøΩÔøΩÔøΩÔøΩ\u000eP$\u0000Fp\nÔøΩÔøΩÔøΩlÔøΩÔøΩÔøΩ\u0004\u000fÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u000eÔøΩ-ÔøΩÔøΩAAÔøΩjÔøΩuV`\u000eƒ∞ÔøΩÔøΩÔøΩX+ÔøΩMÔøΩuÔøΩLÔøΩ<ÔøΩ7ÔøΩ‰ôãÔøΩ\u0001\u0018ÔøΩÔøΩÔøΩEÔøΩV0\n\u0001\u000eÔøΩ< \u0010ÔøΩÔøΩ.c#\u0000ÔøΩ\u0002Ju\u000f+\tÔøΩW\u0015ÔøΩR\u0003_ÔøΩÔøΩ\u001fF_ÔøΩÔøΩÔøΩ\n\u0014HÔøΩÔøΩÔøΩ¬òJÔøΩ\u0002~ÔøΩ[ÔøΩÔøΩ\u0001ÔøΩ\u0007ÔøΩÔøΩcrÔøΩÔøΩE\t0ÔøΩÔøΩwÔøΩE&\nÔøΩIÔøΩÔøΩJ>ÔøΩ\u001aZÔøΩ_~\u0019\u0010ÔøΩo\u0003ÔøΩ`ÔøΩ|ÔøΩÀßÔøΩÔøΩp@ÔøΩÔøΩ|ÔøΩ}ÔøΩÔøΩÔøΩ\u0013ÔøΩÔøΩ\u000fÔøΩDÔøΩÔøΩÔøΩ`ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩsh0ÔøΩ\nÔøΩÔøΩ\u001fCÔøΩÔøΩÔøΩÔøΩÔøΩaLÔøΩÔøΩDNEK\u0010p\tÔøΩÔøΩ/ÔøΩ_ÔøΩÔøΩ\u0013]\u000e_g,aÔøΩÔøΩÔøΩ\"ÔøΩÔøΩÔøΩ9zq\u0002 \\ÔøΩ0ÔøΩ3VÔøΩ\nÔøΩ0ÔøΩÔøΩÔøΩ\u0000ÔøΩ\nÔøΩ=ÔøΩ7ÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩ+(ÔøΩBÔøΩ\nÔøΩ\u0000\u0002ÔøΩÔøΩ6ÔøΩ\u0001\u000eCÔøΩ6baÔøΩ\nO\ndÔøΩ|ÔøΩÔøΩ>ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩ\u00061ÔøΩÔøΩÔøΩ\u0000D7H'WÔøΩ&FÔøΩ\u0005\u0015ÔøΩt\nDt#9ÔøΩÔøΩ:nUÔøΩc \u0004ÔøΩÔøΩÔøΩ@ÔøΩ&\u000eÔøΩ\u0005ÔøΩek\u001a)›º8eÔøΩÔøΩ;ÔøΩKsÔøΩ\u0001ÔøΩ\tÔøΩJÔøΩi7ÔøΩ\u0017<{ÔøΩÔøΩ_’üÔøΩÔøΩ?ÔøΩÔøΩÔøΩ%ÔøΩ2;ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩP$JXÔøΩ\u0001ÔøΩ=ÔøΩ`MÔøΩ\u0017\u0004}ÔøΩÔøΩSp ÔøΩ\u001f $ÔøΩÔøΩ»ì\n\u0006ÔøΩÔøΩ_–£Vz*ÔøΩ\\–ìÔøΩcÔøΩ\u0005ÔøΩ\u0014?0/ÔøΩﬁòÔøΩÔøΩHP05\bÔøΩu\u000foÔøΩÔøΩÔøΩÔøΩÔøΩ%jfiÔøΩÔøΩÔøΩÔøΩÔøΩhp}JÔøΩ\u0004ÔøΩj\u0004\n”ΩPZ%1# h\\ÔøΩÔøΩ\u0012ÔøΩÔøΩ&\b\u001aFwTRuÔøΩÔøΩ!ÔøΩŒòÔøΩCÔøΩtÔøΩb~]ÔøΩÔøΩÔøΩÔøΩsÔøΩÔøΩÔøΩ-ÔøΩWÔøΩÔøΩa[‹µ\nÔøΩFÔøΩ«èÔøΩG,J\u0010\u001f`P\u0004\n\u0006shFÔøΩ\u0005ÔøΩÔøΩ4ÔøΩ9\u0000\u0019{VÔøΩÔøΩ&\u0011xeÔøΩ\tÔøΩÔøΩÔøΩ.{ÔøΩF\nÔøΩzÔøΩÔøΩÕÇÔøΩ\u0007ÔøΩÔøΩIÔøΩAÔøΩÔøΩNÔøΩ:ÔøΩÔøΩ:$4ÔøΩ:\"ÔøΩ1KÔøΩÔøΩ[ÔøΩÔøΩÔøΩC\\’©ÔøΩ[\n\u00114ÔøΩÔøΩÔøΩÔøΩÔøΩ^EÔøΩÔøΩ\b(ÔøΩ\njÔøΩ|ÔøΩÔøΩlÔøΩ|\nÔøΩÔøΩ\u0006\u0007\u0013ÔøΩÔøΩ\u0004DÔøΩ7\u0013SÔøΩÔøΩGw\u0004\n}ÔøΩÔøΩÔøΩs\ngÔøΩ|rDÔøΩÔøΩ\u0010√ãÔøΩÔøΩÔøΩ[O\u0005o\nÔøΩÔøΩ$J;{Pr\u0014MÔøΩÔøΩÔøΩ\"\nÔøΩ\u0011\\È¢Ç\u000eÀ†\b\u001a:s«ë;ÔøΩÔøΩÔøΩÔøΩ4ÔøΩTÔøΩ\nÔøΩÔøΩÔøΩ<qÔøΩDÔøΩJlJ\u0010ÔøΩÔøΩ\u0012\u0004mÔøΩÔøΩlE\u001f\tÔøΩ\u0006ÔøΩÔøΩ\u0013UÔøΩ\nÔøΩ<|ÔøΩÔøΩÔøΩ…∫ÔøΩ\u000fÔøΩÔøΩ\nqÔøΩb}ÔøΩÔøΩ\u0003sÔøΩÔøΩ\u0005{UÔøΩ\u0001ÔøΩÔøΩÔøΩÔøΩ^ÔøΩ[ÔøΩxÔøΩÔøΩoLÔøΩ\u000fÔøΩÔøΩÔøΩBÔøΩÔøΩ\"ÔøΩQ9ÔøΩ∆ÉÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ^IÔøΩÔøΩtDÔøΩ\u0012n\u0016ÔøΩ*ÔøΩ/ÔøΩGÔøΩBÔøΩPE\u0011ACÔøΩÔøΩ:}[ÔøΩÔøΩ=~ÔøΩÔøΩR66ÔøΩ ÔøΩÔøΩÔøΩ'ÔøΩﬂÅÔøΩPiÔøΩjÔøΩÔøΩÔøΩpÔøΩfÔøΩkÔøΩsÔøΩa'ÔøΩ\u0002;B\u000fÔøΩ…ßÔøΩ5ÔøΩ\nU{\u0000\u001aÔøΩÔøΩ\u001a\"hÔøΩNÔøΩ!\u0010}ÔøΩF–îÔøΩÔøΩ@ÔøΩÔøΩÔøΩ\u000fÔøΩ\u0004ÔøΩj:\u0013ƒ§XÀ∏xQÔøΩ\u001fÔøΩ0xBkÔøΩÔøΩÔøΩjxÔøΩ\u0004\u0016\u0002ÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩ\u0017ÔøΩ>@ÔøΩ9ÔøΩÔøΩ\u0006ÔøΩÔøΩ#\u0004ÔøΩÔøΩRÔøΩÔøΩ\u0010ÔøΩ<ÔøΩÔøΩÔøΩ«ÆÔøΩQP\u0000aQÔøΩÔøΩÔøΩ<ÔøΩ_\u0006\u0004ÔøΩÔøΩ:\nÔøΩÔøΩ\u0012ÔøΩÔøΩÔøΩÔøΩb\u0016ÔøΩ#\n\u0016ÔøΩÔøΩÔøΩÔøΩÔøΩ\n}ÔøΩÔøΩ\\ÔøΩÔøΩ\u0004\u0000ÔøΩÔøΩ5)ÔøΩÔøΩeÔøΩÔøΩÔøΩÔøΩ+XÎì∑ÔøΩ5kÔøΩÔøΩ\u0014ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ7c7ÔøΩ^JÔøΩ\u00060ÔøΩvCÔøΩÔøΩÔøΩ-\u0018\u0017\u0013ÔøΩÔøΩÔøΩ_=}mÔøΩÔøΩCgÔøΩq;ÔøΩÔøΩ#gÔøΩ[ÔøΩÔøΩÔøΩ\u0007ÔøΩ,a52\n\u000fÔøΩÔøΩS8ÔøΩm\"wÔøΩÔøΩ\u0016SÔøΩÔøΩ\u000f\u000fÔøΩ9%\nÔøΩ\n\u0000ÔøΩ»Ö\t0)OY}\u0000<ÔøΩ\u001aÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩr\u0001,\u001f?wgÔøΩÔøΩÔøΩÔøΩSÔøΩ€ç\bÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩ‘¶ÔøΩÔøΩÔøΩ\njÔøΩcÔøΩ#\u0012ÔøΩ`RVÔøΩwÔøΩh–åÔøΩ\u0019ÔøΩ\u00003ÔøΩÔøΩ\n\u0017ÔøΩÔøΩ-\u0019kÔøΩÔøΩH2ÔøΩ”òÔøΩI+ÔøΩÔøΩ_IÔøΩ 4\nÔøΩ(ÔøΩÔøΩ\u0005ÔøΩ≈•ÔøΩ`BGÔøΩÔøΩh[\u0013ÔøΩDCP\u0011\u0012_IÔøΩÔøΩ#\u001bÔøΩ\u0004ÔøΩ|ÔøΩfÔøΩbÔøΩJÔøΩ#\nI6IYÔøΩÔøΩ\u001fCC$ÔøΩ)5œà#ÔøΩn\tÔøΩÔøΩ#ÔøΩÔøΩÔøΩm!ÔøΩÔøΩÔøΩ\u0003\u0016MÔøΩ1\u0015ÔøΩÔøΩÔøΩ=GÔøΩ%ÔøΩ{\tÔøΩ#ÔøΩ\nXÔøΩÔøΩÔøΩ\u0003ÔøΩwT)\u0004ÔøΩ\u0010ÔøΩ\n;}\u001bÔøΩ0ÔøΩÔøΩÔøΩÔøΩÔøΩ–çÔøΩÔøΩÔøΩÔøΩ∆ÖÔøΩLj\\ÔøΩ\n,ÔøΩqÔøΩÔøΩÿ≤cXÔøΩÔøΩotÔøΩpÔøΩÔøΩ6ÔøΩiÔøΩÔøΩÔøΩÔøΩÔøΩc+ÔøΩÔøΩÔøΩG%ÔøΩ\u0000ÔøΩ6!ÔøΩ)ÔøΩÔøΩÔøΩ!ÔøΩV;D;!{ÔøΩZ\u0003ÔøΩj5,ÔøΩ0ÔøΩÔøΩq=\u0004ÔøΩÔøΩ/ÔøΩRyÔøΩ\u0016L\"\u0015'nfÔøΩ\\ÔøΩ3ÔøΩ\\ÔøΩÔøΩÔøΩÔøΩDl0ÔøΩÔøΩkÔøΩÔøΩ<l/OJÔøΩ9ÔøΩPÔøΩÔøΩ\nÔøΩlbÔøΩ\u0014zÔøΩÔøΩM\u0007ÔøΩ/N=T7ÔøΩ7ÔøΩÔøΩÔøΩÔøΩAÔøΩÔøΩÔøΩÔøΩ·•àÔøΩVÔøΩÔøΩ\u0003\u001aÔøΩkÔøΩÔøΩ2UÔøΩ\u0010ÔøΩ3ÔøΩX8\u0006BÔøΩ]IÔøΩCÔøΩ)ÔøΩ5[ÔøΩ#KÔøΩÔøΩ<R\nÔøΩÔøΩqÔøΩ\nlÔøΩRÔøΩ\\0l~ÔøΩÔøΩÔøΩ}\u0015ÔøΩU\u0019<ÔøΩÔøΩ;ÔøΩ!@ÔøΩ\nmzÔøΩÔøΩ%&QKÔøΩÔøΩÔøΩW<ÔøΩ/ÔøΩÔøΩmÔøΩaxÔøΩ\u0010\u0006(’∑RjÔøΩÔøΩ_ÔøΩ*ÔøΩÔøΩÔøΩÔøΩ\u0004”Å/ÔøΩ\u0002AÔøΩCÔøΩ√ºÔøΩÔøΩÔøΩ}ÔøΩÔøΩ*ÔøΩvÔøΩÔøΩÔøΩYÔøΩ}¬ÑHm\u0002ÔøΩÔøΩÔøΩN[#w?ÔøΩ|ÔøΩ4ÔøΩRS3ÔøΩÔøΩÔøΩ€ΩÔøΩÔøΩÔøΩc&ÔøΩ;R}ÔøΩÔøΩÔøΩ\np8ÔøΩCjÔøΩyÔøΩ]ÔøΩÔøΩ<\u0011ÔøΩ1?\u0003ÔøΩ=2G5ÔøΩZ*ÔøΩlDw\u00003ÔøΩrÔøΩl\u0018ÔøΩyÔøΩ%ÔøΩ\u0019ÔøΩÔøΩÔøΩÔøΩBÔøΩ9OÔøΩ\u0014ÔøΩeS`ÔøΩ\u0018ÔøΩB'YÔøΩWÔøΩ\u000eÔøΩP$ﬂùÔøΩ1MÔøΩÔøΩ,ÔøΩTÔøΩ\u0000ÔøΩ-:fÔøΩ|ÔøΩÔøΩw}ÔøΩ`\u0018ÔøΩ@ÔøΩÔøΩn9eÔøΩ\u0001ÔøΩÔøΩ{xIÔøΩÔøΩ„û≠ÔøΩ\u0005$KÔøΩ\u0010:¬§\u000fÓ∫®ÔøΩ*ÔøΩFÔøΩÔøΩ;%UÔøΩ!ÔøΩÔøΩ\nX\u0004!\u0004‹ûzÔøΩ\u001bÔøΩHÔøΩ\nÔøΩÔøΩ2—íÔøΩ\u0004DÔøΩZÔøΩ ÔøΩ\u0006ÔøΩjÔøΩd[ÔøΩÔøΩÔøΩ\u0003JÔøΩÔøΩhÔøΩÔøΩ!ÔøΩ\u0002ÔøΩÔøΩU|\u0001\"ÔøΩÔøΩ3w*OﬁÇ0\nÔøΩ1xÔøΩÔøΩsÔøΩXÔøΩÔøΩoGÔøΩÔøΩÔøΩ*ÔøΩbÔøΩÔøΩÔøΩ2<;{ÔøΩtÔøΩY1ÔøΩMÔøΩÔøΩÔøΩÔøΩ\u0011:\u0012ÃïÔøΩÔøΩ8ÔøΩ\nÔøΩ9ÔøΩÔøΩPÔøΩpÔøΩoÔøΩ\u0002ÔøΩg^ÔøΩ=S}\"\\\ng\u0019ÔøΩ\u0005\nÔøΩÔøΩÔøΩ ÔøΩÔøΩ∆πÔøΩÔøΩÔøΩÔøΩÔøΩ\u000fOÔøΩÔøΩWÔøΩÔøΩÔøΩÔøΩÔøΩ~ahÔøΩ\nÔøΩNÔøΩÔøΩÔøΩ\"\u0018ÔøΩ\u0010\u0017ÔøΩ]ÔøΩÔøΩYqÔøΩ“®xÔøΩ–™~ÔøΩ\u0015ÔøΩ\u0000#ÔøΩÔøΩÔøΩÔøΩUÔøΩÔøΩ0ÔøΩ)ÔøΩ\u0013ÔøΩÔøΩMÔøΩ\u0001ÔøΩÔøΩf\nÔøΩXzÔøΩ∆âÔøΩÔøΩ``ÔøΩ\\\tmÔøΩp[∆∑jÔøΩo47ÔøΩÔøΩaÎ∞£BÔøΩÔøΩQ\u000fÔøΩ\u000e◊ÇÔøΩ#0ÔøΩÔøΩÔøΩ\niÔøΩZÔøΩ\u001aÔøΩW\tÔøΩ\u000f\n\u0007ÔøΩ)ÔøΩÔøΩ\nÔøΩaÔøΩCÔøΩ\u0012ÔøΩz\u0002ÔøΩ9‘™ÔøΩd<ÔøΩA#ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u000fÔøΩ\u0015ÔøΩÔøΩÔøΩ\u0018ÔøΩ\u001a\u0016\u0002'ÔøΩÔøΩ0ÔøΩ$ﬂÖÔøΩ\\ÔøΩ^$\nÔøΩÔøΩ\n\u0012\u0013ÔøΩÔøΩ\nÔøΩ]!ÔøΩU\\ÔøΩhÔøΩÔøΩ\u0011ÔøΩ\nÔøΩSÔøΩ\u001bY\u0016\u0018\u0012C\u0003')\n\u0003QÔøΩ\"ÔøΩ\n\\ÔøΩÔøΩÔøΩÔøΩ^yÔøΩ)'ÔøΩ\nÔøΩh\u0002ÔøΩxÔøΩÔøΩwTÔøΩÔøΩbÔøΩFÔøΩ6i\u0017\u0004ÔøΩ1\u0007jÔøΩaRfÔøΩ\u0005ÔøΩÔøΩr\u00002÷™ÔøΩqa@ÔøΩ[ÔøΩÔøΩh%ÔøΩt\u0012\u0003,\u0003N24ÔøΩ\u0018ÔøΩÌäìÔøΩZp≈ºÔøΩÔøΩ0NWxÔøΩt\u0018\u0019j+Jn4o«±ÔøΩ\u0011ÔøΩ\u0013ÔøΩ\u0012\u0002WÔøΩÔøΩhﬂ´&5ÔøΩ3=F|{`ÔøΩÔøΩÔøΩj—ÅQ\u0018Bfd\u0004\n/\u0014\u0010ÔøΩX\u0015ÔøΩÔøΩJÔøΩ&#\u0015xÔøΩÔøΩ\u0019ÔøΩ\"\nP'⁄æÔøΩ;ÔøΩ\u0010~ÔøΩÔøΩ’õÔøΩÔøΩÔøΩÔøΩ[‹∞rpÔøΩÔøΩ_ÔøΩ\u0010T√∑\u0000*B0fÔøΩÔøΩ|ÔøΩÔøΩ\tÔøΩR\u0016tÔøΩÔøΩÔøΩ‚•¥ÔøΩÔøΩ'\u0011\u0006Œ¶ÔøΩBÔøΩ6ÔøΩÔøΩNÔøΩ◊è\u0014ÔøΩ,ÔøΩ\ngCwÔøΩÔøΩÔøΩPÔøΩ_cÔøΩa\u0007ÔøΩZÔøΩA\bÔøΩMoÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u001bÔøΩÔøΩ5)\u0000«∂\u0004\u0016ÔøΩ\b)ÔøΩ\u0011Z\nÔøΩ\u0017ÔøΩÔøΩÔøΩkSÔøΩ\n<5wÔøΩAÔøΩÔøΩPÔøΩ\n\u0017\u0005g\u000eÔøΩÔøΩÔøΩ%\t0l\u0001\u0001m\n(ÔøΩ\nz\u0018BÔøΩ-ÔøΩE\u001bÔøΩ\n\u0000ÔøΩÔøΩ'ÔøΩIÔøΩÔøΩÔøΩhÔøΩ\u001bm{ÔøΩT7RyÔøΩ\u0007ÔøΩ@≈•ÔøΩk\u000fÔøΩÔøΩUaÔøΩÔøΩÔøΩ\nÔøΩiÔøΩÔøΩÔøΩÔøΩÔøΩcÔøΩ=rÔøΩ4yÔøΩ\u0010ÔøΩUÔøΩÔøΩ\nÔøΩÔøΩigÔøΩ_\u0006\u0006ÔøΩ\u0013ÔøΩ#ÔøΩÔøΩÔøΩvÔøΩ)ÔøΩÔøΩÔøΩcÔøΩÔøΩÔøΩ\u0007ÔøΩÔøΩÔøΩ>#y5ÔøΩŸ´ÔøΩ\u001fÔøΩ\n\u0019A_\u0017\u0013ÔøΩsÔøΩ)\u00183rMÔøΩoÔøΩÔøΩ,pÔøΩ\u0015Mw·Üí-ÔøΩLÔøΩ(#ÔøΩÔøΩzÔøΩ\u001fB5K\n<ÔøΩ ÔøΩÔøΩ\u001bqÔøΩ6\u0013!AÔøΩÔøΩÔøΩÔøΩ’ÉÔøΩcMÔøΩÔøΩbÔøΩÔøΩxÔøΩÔøΩH\u0013ÔøΩgÔøΩ,QhFÔøΩCpÔøΩCÔøΩÔøΩÿ∞ÔøΩÔøΩ\u001fÔøΩ\u0018ÔøΩ\nÔøΩ.XÔøΩÔøΩÔøΩÔøΩJ7ÔøΩ@+NÔøΩÔøΩIÔøΩÔøΩ\u0006ÔøΩÔøΩÔøΩ{(\u0015ÔøΩ÷ΩÔøΩlÔøΩÔøΩ8$ÔøΩr-ÔøΩ+\u000ekﬁãÔøΩ\u0010ÔøΩP\u0011ÔøΩÔøΩ\u0007>t{€Æ2qÔøΩÔøΩ\u0011ÔøΩÔøΩÔøΩuÔøΩÔøΩÔøΩÔøΩpF‘∏ÔøΩ`\u0014?ÔøΩ%+WÔøΩ1rÔøΩÔøΩ>\u0000\u0000i)ÔøΩ\n\u0002ÔøΩÔøΩ:r3ÔøΩ\u0011ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0003ÔøΩR\u001bÔøΩ-ÔøΩx}XÔøΩiÔøΩÔøΩÔøΩBÔøΩ=!ÔøΩjÔøΩÔøΩ[ÔøΩÔøΩCÔøΩTYÔøΩw?ÔøΩÔøΩÔøΩsÔøΩE\u0003ÔøΩ>qÔøΩ7WÔøΩ|'ÔøΩSpkMÔøΩÔøΩ4NÔøΩNÔøΩÔøΩÔøΩÔøΩ«¶lÔøΩÔøΩ\u0001ÔøΩ#ÔøΩ25ÔøΩÔøΩÔøΩUÔøΩÔøΩÔøΩCÔøΩw\u0007ÔøΩÔøΩTÔøΩ@8\u001b#ÔøΩ\u0005\u000fS\u001f\t\u0018#ÔøΩA∆™ÔøΩÔøΩÔøΩÔøΩÔøΩ@ÔøΩÔøΩÔøΩ\u0007◊ÑÔøΩ\u0006\"j.ÔøΩÔøΩ\b\u0001ÔøΩÔøΩﬁâ-ÔøΩ\u0011ÔøΩ*ÔøΩ<VÔøΩÔøΩQÕçÔøΩÔøΩ\u0014ÔøΩÔøΩ-XÔøΩÔøΩ.\t~Ln\u0010ÔøΩ3rÔøΩÔøΩÔøΩ\u0015ÔøΩ!th@;\nﬂÇÔøΩ—™ÔøΩÔøΩÔøΩÔøΩ{ÔøΩÔøΩÔøΩÔøΩÔøΩyÔøΩÔøΩÔøΩ\u000fÔøΩDÔøΩ\u0016ÔøΩÔøΩ\nÔøΩÔøΩOﬁñ)2ÔøΩ\u0019ÔøΩ\u0015ÔøΩÔøΩ9ÔøΩÔøΩzc^ÔøΩ!ÔøΩnÔøΩÔøΩÔøΩÔøΩ]Hl∆µe\u0019ÔøΩK4utW\u001fSÔøΩ68ÔøΩÔøΩÔøΩ7LÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩuÔøΩÔøΩÔøΩ\u0019zÔøΩa\u000eÔøΩÔøΩÔøΩ9{ÔøΩ\u0006\u0017e\u0015_‹πÔøΩ\u0018ÔøΩaÔøΩÔøΩ\u0000ÔøΩ\t8NÔøΩb\u0000«∑PÔøΩÔøΩÔøΩ\u000e\n_„ùªÔøΩ+wÔøΩ_ÔøΩÔøΩÔøΩÔøΩﬂá\u00045ÔøΩÔøΩ@\u0006ÔøΩU;ÔøΩ1ÔøΩÔøΩÔøΩ\nÔøΩÔøΩJ6KÔøΩ\u0015PﬂÄ\u001bÔøΩ8jgh|ÔøΩoLÔøΩ»Ö\tI9g\u000f\u0016ÔøΩn\n.\u0002\nÔøΩtÔøΩ⁄î1KÔøΩÔøΩ5@oÔøΩÒëÄëÔøΩÔøΩÔøΩ◊¶\u0007+ÔøΩiÔøΩ)ÔøΩ-T$y\nÔøΩÔøΩKÔøΩ\u001fwRÔøΩÔøΩ 1sÔøΩI/ÔøΩÔøΩ9fqBLJ\n<ÔøΩÔøΩUÔøΩÔøΩN^LÔøΩmtÔøΩÔøΩG/\u0007ÔøΩÔøΩ\nÔøΩÔøΩa!A{ÔøΩÔøΩÔøΩ\n[ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩmzxÔøΩ94ÔøΩ#ÔøΩDxÔøΩ–ûyeWÔøΩ|aÔøΩ\u0012nOÔøΩÔøΩ#ÔøΩsÔøΩDÔøΩ5>FÔøΩÔøΩÔøΩfÔøΩÔøΩÔøΩfÔøΩ3ÔøΩÔøΩÔøΩ$ÔøΩvÔøΩpÔøΩjÔøΩ+MÔøΩÔøΩ0ÔøΩÔøΩÔøΩ 8ÔøΩ%}ÔøΩÔøΩ!ÔøΩÔøΩÔøΩC]ÔøΩÔøΩÔøΩÔøΩ?ÔøΩÔøΩÔøΩÔøΩI,ÔøΩÔøΩÔøΩCÔøΩfv3MDÔøΩÔøΩ-p\u000f\nÔøΩwÔøΩÔøΩ÷ñ\u0018‹øÔøΩ\u0018ÔøΩÔøΩÔøΩÔøΩbÔøΩÔøΩ`Db=\u0002ÔøΩÕîmjÔøΩÔøΩ8ÔøΩÔøΩÔøΩÔøΩÔøΩ\u001aÔøΩYÔøΩpÔøΩÔøΩLÔøΩ\u0014D⁄âc]ÔøΩÔøΩÔøΩÔøΩÔøΩÙéçülcﬁéÔøΩÔøΩ1/ÔøΩÔøΩ\u0011,yÔøΩeÔøΩ!ÔøΩvÔøΩ\u0012ÔøΩzÔøΩÔøΩÔøΩÔøΩDn#ÔøΩ#\u0019ÔøΩÂ≠ëÔøΩ\u000e⁄øI√ª%ÔøΩJEZKUfÔøΩÀø⁄≠\u0002Y9ÔøΩ\nQÔøΩWÔøΩÔøΩÔøΩpÔøΩ ™c=wKÔøΩ#ÔøΩÔøΩ\u0016|V\u0003ÔøΩgÔøΩ&GC\u001bÔøΩDÔøΩ\u0013<ÔøΩ\u000fÔøΩRÔøΩRÔøΩF3ÔøΩc\n[ÔøΩ⁄ábÕÇ9fÔøΩÔøΩ\u0013»çÔøΩÔøΩDÔøΩn≈¥VÔøΩÔøΩ:CÔøΩ|0”º+cÔøΩÔøΩÔøΩ\nQ9ÔøΩmWÔøΩJ\u0014ÔøΩÔøΩvÔøΩNÔøΩi\ty$o\u0019\\bCÔøΩ–µ0LqÔøΩ55ÔøΩWÔøΩ1ÔøΩk&ÔøΩÔøΩE{@ÔøΩÔøΩ]WO\u0013ÔøΩ\u00003qÔøΩÔøΩqQ<ÔøΩÔøΩ02jZOnÔøΩt.ÔøΩdÔøΩj\nÔøΩuÔøΩÔøΩc‹∏ÔøΩ\nÔøΩrBÔøΩU}ÔøΩÔøΩ<\"JÔøΩ\nÔøΩ?xÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩ;\u001a\u001aÔøΩ\nÔøΩ\u0006œç{ÔøΩÔøΩÔøΩÀ§:\u000e\nÔøΩ4ÔøΩÔøΩKÔøΩÔøΩ\u0000-sÔøΩÔøΩÔøΩÔøΩK\u0012\u0001wÔøΩÔøΩ|\u0010ÔøΩ‰ÄôqÔøΩ\u000fŸÅÔøΩYÔøΩvQÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ|ÔøΩÔøΩÔøΩd8ÔøΩ1ÔøΩÔøΩu>y\u0001ÔøΩ\u0015pÔøΩÔøΩyÔøΩwÔøΩ ù\\aÔøΩÔøΩ\u0016ÔøΩ[ÔøΩÔøΩ\n5e\u0004\u0011ÔøΩÔøΩoÔøΩÔøΩl\u0003zÔøΩŒÑÔøΩ\u00039ÔøΩ\u0000ÔøΩ√É;OÔøΩ\\‚ö©ÔøΩÔøΩ\\'ÔøΩHzÔøΩÔøΩÔøΩÔøΩ\u0001ÔøΩOtÔøΩÔøΩÔøΩwÔøΩ]D%\u001f«µ#ÔøΩÔøΩÔøΩÔøΩm\nÔøΩÔøΩ%kÔøΩÔøΩÔøΩÔøΩÂáüÔøΩÔøΩÔøΩÔøΩÔøΩmMÔøΩ!ÔøΩÔøΩP“ªÔøΩÿπ!~ÔøΩyÔøΩhÔøΩ\u0001ÔøΩ0ÔøΩ\nQ]X+VÔøΩÔøΩ ∫ÎçãÃë{ÔøΩ-ÔøΩÔøΩÔøΩnX\nÕ∞\u0015ÔøΩRÔøΩ\u00039ÔøΩ}f3ÔøΩ&ÔøΩ\n\u000eÔøΩz\u0016ÔøΩu–ÉÔøΩ\nEÔøΩ_EÔøΩÔøΩÔøΩÔøΩ\u0012ÔøΩ^ÔøΩ+9mlÔøΩ\u0002fÔøΩ\u0018ÔøΩPÔøΩÔøΩÔøΩﬂôÔøΩ\u0012ÔøΩÔøΩ9ÔøΩ`2ÔøΩgK\nÔøΩ”©pÔøΩ≈üÔøΩ\u0001\u0016ÔøΩZÔøΩ\n‹åÔøΩ,ÔøΩ)ÔøΩt4ÔøΩYÔøΩJ)BfÔøΩ6=|\u0018ÔøΩ2z\u0012pY\u00103ÔøΩÔøΩÔøΩ\"L~ÔøΩÔøΩ2ÔøΩqÔøΩgÔøΩÔøΩ.fÔøΩÔøΩÔøΩ4ÍêñÔøΩÔøΩiÔøΩÔøΩﬁç\u0005ÔøΩt€≤tÔøΩ`IÔøΩÔøΩ\u001aÔøΩ-KÔøΩ\nÔøΩhÔøΩ\n7\u001f…åÔøΩhÔøΩ ¨0\u0005\nÔøΩsÔøΩ{vÔøΩTÔøΩ'ÔøΩcWÔøΩ\u00018MjÔøΩ\n2ÔøΩÔøΩ+ƒç\nƒÅÔøΩ&ÔøΩ–çÔøΩf1ÔøΩÔøΩÔøΩÔøΩbÔøΩÔøΩÔøΩCÔøΩtÔøΩ&ÔøΩ!8\u0011\".C#zÔøΩÔøΩpÔøΩÔøΩÔøΩ\nÔøΩBÔøΩmO_{\u0017ÔøΩÔøΩ\u0017ÔøΩ,ÔøΩ“•L\u0015≈´\nÔøΩ2ÔøΩÔøΩÔøΩ/\u0017ÔøΩÔøΩa @ÔøΩLÔøΩÔøΩÔøΩÔøΩ◊†\u0013B3ÔøΩÔøΩÔøΩdÔøΩVÔøΩ>\u0017W^\u0013ÔøΩzÔøΩÔøΩ\nuÔøΩÔøΩÔøΩ:ÔøΩÔøΩY\u001bÔøΩ\u0002ÔøΩ~ÔøΩÔøΩ\u0017zÔøΩÔøΩÔøΩ)ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ_ÔøΩÔøΩÔøΩ\nÔøΩÃ∂\u0015(7FÔøΩ\nÔøΩ\u0018\u001a|ÔøΩÔøΩÔøΩÔøΩÔøΩ+ÔøΩÔøΩÔøΩcSOÔøΩ\u0017ÔøΩ.ÔøΩŒ°HÔøΩÔøΩ_)ÔøΩÔøΩ*’∞ÔøΩSÔøΩ{ÔøΩ9%\u001a`fJﬁπ{\u000f_ÔøΩ20@ÔøΩ\n\"??yeÔøΩÔøΩÔøΩÔøΩÔøΩF\\pÔøΩÔøΩAE1ÔøΩ'4ÔøΩ~ÔøΩ&ÔøΩW\nÔøΩÔøΩ\u000e_ÔøΩ<u\u001bÔøΩtÔøΩ!ÔøΩxÔøΩ<#ÔøΩÔøΩÔøΩÔøΩÔøΩ\u000e/ÔøΩ◊ÅL\u000fzÔøΩ\u0004ÔøΩÔøΩÔøΩÔøΩÔøΩl?`ÔøΩÔøΩcÔøΩj\"ÔøΩÔøΩ\u001f=}MÔøΩpÔøΩDbÔøΩÔøΩ\nÔøΩtÔøΩÔøΩÔøΩaÔøΩv]E\u0013ÔøΩ|\u001aÔøΩ:ÔøΩÔøΩ\u0006ÔøΩP›∂og,ÔøΩÔøΩ\u001aCÔøΩÔøΩÔøΩ”ìÔøΩ~bÔøΩÔøΩ◊éÔøΩ…ÖNÔøΩ\nÔøΩ|mPwÔøΩÔøΩXwÔøΩÔøΩ!SÔøΩÔøΩAÔøΩpÔøΩCÔøΩÔøΩÔøΩ“≥ÔøΩ!ÔøΩÔøΩÔøΩ0CÔøΩ≈†ÔøΩ)\n]_n÷≤\u0003\u0017\u0019ÔøΩÔøΩGÔøΩ=ÔøΩÔøΩÔøΩ.ÔøΩÔøΩ]ÔøΩÔøΩ’ø\bÔøΩ\u0001ÔøΩs:=%WpÔøΩ\t\u0006ÔøΩOKÔøΩ\u0018ÔøΩ^ÔøΩqÔøΩElÔøΩQ&ÔøΩ$kÔøΩÔøΩvh:#ÔøΩ@ÔøΩ\u0018bÔøΩ\u0000ÔøΩ\u0016ÔøΩNKÔøΩÔøΩÔøΩN\t*6ÔøΩÔøΩ^0ÔøΩÔøΩÔøΩÔøΩÔøΩ\u001aÔøΩv›âOÔøΩ”®√òsÔøΩ]ÔøΩuÔøΩ\u0007ÔøΩL2\u0012ÔøΩ6yÔøΩ7ÔøΩ\u0010ÔøΩ\u0019ÔøΩÔøΩcÔøΩ\u0012ÔøΩ\u0013ÔøΩ!ÔøΩDÔøΩaNÔøΩÔøΩwÔøΩ$ÔøΩ\u000fÔøΩX=ÔøΩTG\u0004MÔøΩÔøΩÔøΩ2\u000evÔøΩ\u0018KÔøΩrƒ¨\u0000$DÔøΩxÔøΩ\tÔøΩÔøΩÔøΩ‘àÔøΩÔøΩ\u0002ÔøΩUÔøΩZÔøΩÔøΩÔøΩtÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩ7aÔøΩÔøΩÔøΩÔøΩÔøΩ\nAÔøΩx5ÔøΩÔøΩeÔøΩ‰Ä†ÔøΩÔøΩÔøΩ€≤\u0011ÔøΩ\u000f\"h/C$—ôcÔøΩ:\u0015)ÔøΩÔøΩ\u001aÔøΩÔøΩXfÔøΩI\\ÔøΩB\u0012ÔøΩvÁá±ÔøΩ;ÔøΩÔøΩ'ÔøΩ]Wn>ÔøΩuPÔøΩ0ÔøΩ@ÔøΩ\u0002^ÔøΩQÔøΩh{∆ç;ÔøΩzOÔøΩn?\"4ÔøΩÔøΩÔøΩ\u0004ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÿ©ÔøΩÔøΩ2O\nÔøΩÔøΩÔøΩ(nÿºÔøΩn;\u000fÔøΩV_ﬂùz\" ÔøΩÔøΩÔøΩÔøΩÔøΩqi'ÔøΩÔøΩ\u0012ÔøΩYsÔøΩQ#ÔøΩ\tpÔøΩÔøΩjÔøΩ\b\nÔøΩÔøΩÔøΩ}KP·ãóÔøΩ\u0016lÔøΩPz\u00004ÔøΩoLyÔøΩÔøΩÔøΩ\u0019ÔøΩÔøΩ≈ïWÔøΩ\n\u001a\\ÔøΩ9\nFÔøΩÔøΩÔøΩÔøΩÔøΩ\tÔøΩ\n4⁄ΩÔøΩ/ÔøΩbSkÔøΩ_yÔøΩ\u0011q$\"ÔøΩZÔøΩ\n[XqÔøΩ=ÔøΩƒä\u0011ÔøΩÔøΩÔøΩYÔøΩÔøΩc?ÔøΩŸπZÔøΩ\u000eÔøΩ›∑\nÔøΩ\n6fi\"ÔøΩU~ÔøΩÔøΩEXÔøΩ–®}\u0018ÔøΩc=ÔøΩ»â\nÔøΩOmÔøΩ\tÔøΩ@f.p\u0011ÔøΩÔøΩÔøΩ\nÔøΩÔøΩ2C6~ÔøΩ.Ln\u0005ÔøΩvÔøΩIÔøΩ\nÔøΩﬂòÔøΩ9ÔøΩÔøΩae\u0019ÔøΩÔøΩ%ÔøΩ\u0019ÔøΩ<ÔøΩ∆§sÔøΩÔøΩ\n\u0012vÔøΩPÔøΩ-ÔøΩ9W”™ÔøΩ9ÔøΩ\u0014\nP'ÔøΩHÔøΩUlÔøΩÔøΩÔøΩÔøΩ\"ÔøΩÔøΩÔøΩÔøΩ@%ÔøΩFYÔøΩÔøΩÔøΩ!ÔøΩÔøΩY\u000fÔøΩÔøΩÔøΩ\"ÔøΩ\u0014ÔøΩÔøΩkÔøΩ}ÔøΩÔøΩ]ÔøΩÔøΩfÔøΩ\u0019ÔøΩÔøΩ\nÔøΩÔøΩ.6ÔøΩÔøΩMÔøΩÔøΩ\u0000\n\u0002_ÔøΩJABSÔøΩÔøΩfÔøΩÕõ9r+Q÷¨ ≈É`ÔøΩÔøΩÔøΩ(ÔøΩÔøΩ\u0016ÔøΩ\bÔøΩj=ÔøΩÔøΩI\u001at\u0006ÔøΩ\nÔøΩkÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ<\u000fRÕÖÔøΩ\nuÔøΩ#O ÅÔøΩ\n;ÔøΩ\u0007rZÔøΩ\u0019y9ÔøΩ\nÔøΩ\u0018ÔøΩÔøΩÔøΩfÔøΩÔøΩ\u0015_\u0010…É¬üTÔøΩÔøΩÔøΩRÔøΩ4rÔøΩ;ÔøΩ!KÔøΩÔøΩLÔøΩÃµ3ÔøΩ\u00100\bÔøΩr0l.=m)ÔøΩ[\n!‘§7SÔøΩÔøΩfdÔøΩ\u0017D\nÔøΩÔøΩ\u0004VÔøΩ3gkZOÔøΩÔøΩ¬¨ÔøΩ\n=ÔøΩÔøΩ=W\u0013mÔøΩÔøΩ\u0007\u000fÔøΩÔøΩEÔøΩ\bÔøΩ\u0005*ÔøΩÔøΩ\u0004\u0013\\3~’éiuuÔøΩJFÔøΩÔøΩ≈í+ÔøΩvÔøΩÔøΩ\u0010\u000eÔøΩÔøΩ-ÔøΩÔøΩÔøΩ–èÔøΩÔøΩÔøΩ3-ÔøΩÔøΩÔøΩ€ªÔøΩÔøΩÔøΩÔøΩÔøΩPÕπÔøΩ\u0013ÔøΩ'E$V’úÔøΩ\u001bÔøΩX%ÔøΩÔøΩÔøΩ^ÔøΩ'ÔøΩÔøΩmÔøΩSÔøΩÔøΩXhÔøΩÔøΩÔøΩrÔøΩWnÔøΩ1aSV'ÔøΩÔøΩ`TÿâYg\u0000JÔøΩ{\u0011ÔøΩÔøΩv&b1\u0011ogG◊úÔøΩÔøΩJe_\u001aj\u0010ÔøΩÔøΩÔøΩ\u0011\u001b\u0018WÔøΩ\u0018ÔøΩÔøΩÔøΩÔøΩ:ÔøΩ)€ÆÔøΩÔøΩ;qÔøΩnÔøΩ—´+=s,0AÔøΩÔøΩÔøΩ~\u0019\u0018H4ÔøΩ\n/ÔøΩ5ÔøΩÔøΩÔøΩ&X{ÔøΩÔøΩ}+e9ÔøΩ{+\nFÔøΩÔøΩÔøΩÔøΩÔøΩWÔøΩ%ÔøΩÔøΩÔøΩe\u0001hÔøΩ+7\n\u000fDÔøΩgxÔøΩ\tÔøΩÔøΩWxÔøΩtÔøΩ\n\u0005ÔøΩ\u0017ÔøΩMÔøΩT\u0003oÔøΩ?ÔøΩÔøΩÔøΩÔøΩO$◊ÆÔøΩFLYgsÔøΩÔøΩ\u0012#3\u0011ÔøΩ(ÔøΩÔøΩ\u0013Z\njTÔøΩ@\u0002Lh\\ÔøΩt\u0013VÔøΩÔøΩ%ÔøΩÔøΩpÔøΩ-!/⁄èÔøΩdKÔøΩÔøΩ\u0016\tY\u0010[\u0005ÔøΩ\u0010\n‹ö:ÔøΩ\nÔøΩ\u0013i2ÔøΩwÔøΩU\u0019eÔøΩ*2ÔøΩÔøΩnD,FÔøΩlFËçñÔøΩÔøΩÔøΩÔøΩÔøΩ,ÔøΩÔøΩ7\u0012\nÔøΩ\u0015ÔøΩÔøΩ;ÔøΩs\nCÔøΩÔøΩÔøΩpTÔøΩÔøΩ\u000fÔøΩÔøΩÔøΩlÔøΩ\"6ÔøΩÔøΩ\u000eCÔøΩ7ÔøΩv;ÔøΩ\nBÔøΩ:ÔøΩÔøΩ\tE\nÔøΩsiÔøΩNxÔøΩ\u0010\u0003ÔøΩ#ÔøΩ4\u0002ÔøΩÔøΩ!ÔøΩÔøΩpÔøΩGÔøΩ>\u0006ÔøΩÔøΩÔøΩ\u0006u\u0018ÔøΩÔøΩ]ÔøΩÔøΩÍÄ†ÔøΩhÔøΩ\u0012=ÔøΩÔøΩÔøΩ(ÔøΩ5$ÔøΩR…¶ÔøΩÔøΩYpE”®ÔøΩÔøΩMÔøΩ\\—¥ÔøΩÔøΩ@eÔøΩ\u000e)HÔøΩ !ÔøΩrÔøΩfÔøΩ/QJnÔøΩ\n5\\T\u0015ÔøΩÔøΩÔøΩSLÔøΩÔøΩ\bÔøΩ(ÔøΩÔøΩ+9\u0004*mÔøΩÔøΩ√ñÔøΩ@5ÔøΩ=ÔøΩÔøΩ\u0014\u0013ÔøΩÔøΩÔøΩ0D,ÔøΩh0;ÔøΩ\u0018DÔøΩcÔøΩÔøΩÔøΩ^ÔøΩ&FE\nÔøΩÔøΩ«†\u0006gÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0012ÔøΩ0ÔøΩ€ô\nD\u0013yÔøΩDvPGÔøΩÔøΩ‹ñ›º\u0001ÔøΩÔøΩd\u0007ﬁÜÔøΩÔøΩ\u0014\u000fÔøΩ5|ÔøΩ\u0007ÔøΩE\tÔøΩﬂº\u0007ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ;ÔøΩÔøΩ.ÔøΩÔøΩIÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ1<ÔøΩÔøΩ€†@ÔøΩ\u0017ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩO\u0000ÔøΩcÔøΩA\u0014ÔøΩÔøΩ\u0016ÔøΩ>QeÔøΩ\n&ÔøΩJÔøΩ3-\u0016N\u0012ÔøΩR\u0003`ÔøΩqÔøΩ\n+;ÔøΩ0n\n<ÔøΩ9”∏ÔøΩ\u0005‰®üwÔøΩÔøΩÔøΩÔøΩ,ÔøΩÔøΩ:+ÔøΩ4ÔøΩÔøΩÔøΩÔøΩ?ÔøΩÔøΩÔøΩÔøΩ\u0007rœïÔøΩÔøΩ<Q{\u000fÔøΩdÔøΩÔøΩÔøΩÿîÔøΩƒ¨”ΩÔøΩFO[ÔøΩÔøΩÔøΩ=ÔøΩÔøΩÔøΩÔøΩÔøΩ_LÔøΩÔøΩeÔøΩÔøΩÔøΩjÔøΩ‹éÔøΩvzÔøΩ*ÔøΩKÔøΩÔøΩ\nÔøΩvÔøΩÔøΩ\"ÔøΩKÔøΩ([ÔøΩÔøΩ\\WyÔøΩ<ÔøΩÔøΩ=ÔøΩ\n\u0016ÔøΩÔøΩwÔøΩÔøΩÔøΩÔøΩK\u000f\u0000ÔøΩÔøΩ\"ÔøΩÔøΩXÔøΩÔøΩ<1R|uÔøΩ-%UÔøΩ)ÔøΩÔøΩ)ÔøΩ5p\u0017ÔøΩ“πÔøΩÔøΩ\u0019s'ÔøΩÔøΩÔøΩÔøΩ∆òM\n3oÔøΩq|!\u0000ÔøΩÔøΩÔøΩFÔøΩBÔøΩWÔøΩIÔøΩ8ÔøΩÔøΩy#GÔøΩ/ÔøΩ€∫\u0007ÔøΩ(z–¢ÔøΩ-ÔøΩ\u0003“∑X0ÔøΩÔøΩ!ÔøΩ'ÔøΩ\u0002ÔøΩ’¥\u0016ÔøΩÔøΩÔøΩÔøΩÔøΩ\u000fÔøΩ\nkÔøΩJ|ÔøΩ5NÔøΩP{ÔøΩ\u0010{@ÔøΩ0BO\u0010ÔøΩnÔøΩÔøΩ641*)ÔøΩ\u000eÔøΩ«ªÔøΩÔøΩvCÔøΩ-)ÔøΩ\tQo'$\u001fÔøΩ\u0015n8yÔøΩChÔøΩÕõ+\u0015ÔøΩ‡™°C`\u001bÔøΩqÔøΩDÔøΩKBÔøΩ\tÔøΩ\"\u0007]vÔøΩÔøΩÔøΩhÔøΩÔøΩÔøΩKZÔøΩ4\"ÔøΩÔøΩËÆû÷ç\u000fDÔøΩÔøΩ%1ÔøΩ\u001aÔøΩ\u0012<ÔøΩ]SL6fÔøΩÔøΩ0”π»∏R&ÔøΩÔøΩ\u0006ÔøΩÔøΩÔøΩÔøΩWÔøΩ\\\u0005ÔøΩ\u0012EG0ÔøΩ%16GÔøΩÔøΩIÔøΩÔøΩ5\nRzÔøΩÔøΩÔøΩft|5ÔøΩC@M\u0004ÔøΩÔøΩÔøΩÔøΩ\nwÔøΩ‹âÔøΩÔøΩÔøΩÔøΩe\u0003ÔøΩ.ÔøΩX\nIÔøΩL\\'nhÔøΩ\nÔøΩÔøΩ0\u0002_ÔøΩ ¶ÔøΩ7ÔøΩÔøΩEjn\n!ÔøΩÔøΩAÔøΩ'41RÔøΩPZÔøΩÔøΩÔøΩY\"s&ÔøΩ6◊ª2ÔøΩÔøΩÔøΩUÔøΩ0kb\u000e\nÔøΩzÔøΩ)ÔøΩq'eÀïÔøΩ\u0002÷õÔøΩ:EÔøΩ\nÔøΩ\u0015ÔøΩÔøΩ\b?bÔøΩ[iT{ÔøΩxpÔøΩÔøΩÔøΩÔøΩÔøΩ\u0015ÔøΩÔøΩfmHÔøΩ>9ÔøΩ>~iÔøΩÔøΩWxÔøΩ\u0000ÔøΩÔøΩÔøΩp]M4Jh\nÔøΩ=ÔøΩ)ÔøΩPÔøΩGÔøΩÔøΩÔøΩœúÔøΩÔøΩ3÷•¬≥(=ÔøΩ\\ÔøΩ[ÔøΩ\nÔøΩÔøΩÔøΩ'F,ÔøΩÔøΩ[\u0010ÔøΩ\u0000VÔøΩÔøΩ6ÔøΩÔøΩWÔøΩL\u0006ÔøΩ<dÔøΩ\u0015\nŸ£\u0017'ÔøΩÔøΩÔøΩÔøΩ-ÔøΩÔøΩ\u001b]ÔøΩTÔøΩÔøΩ0KÔøΩWTÔøΩÔøΩÔøΩ\u000eÔøΩ\u0001ÔøΩÔøΩÔøΩÔøΩAÔøΩ1ÔøΩÔøΩÔøΩCÔøΩÔøΩÔøΩÔøΩÔøΩÀÄÔøΩS\u0017ÔøΩ)}ÔøΩf<sÔøΩAÔøΩÔøΩ*ÔøΩ/@ÔøΩÔøΩÔøΩÔøΩÔøΩ}:M`ÔøΩ9\"ÔøΩÔøΩÔøΩÔøΩÔøΩGO^ÔøΩUÔøΩ:ÔøΩ\u000eÔøΩÔøΩÔøΩ\u0005ÔøΩAÔøΩJÔøΩ÷¢\u001bÔøΩÔøΩudÔøΩpkÔøΩnÔøΩ\"hÔøΩÔøΩÔøΩ\u000f\u0006ÔøΩ\nÔøΩ\u0015M\u0013\n&÷ÖQ\"ÔøΩ\nÔøΩÔøΩÔøΩ`YÔøΩwÔøΩ#ÔøΩ\u0017^:RÔøΩ1fxÔøΩ3mÔøΩÔøΩNÔøΩdÔøΩÔøΩÔøΩÔøΩ+ÔøΩ\u000eÔøΩ\bÔøΩPIVÔøΩ\u0003EÔøΩÔøΩ\u0006œºEÔøΩz\bÔøΩ5ÔøΩÔøΩÔøΩÔøΩ@ÔøΩBÔøΩÔøΩÔøΩÔøΩM-;pAÔøΩÔøΩ$ÔøΩLÔøΩÔøΩÔøΩÔøΩXR÷¶q:\"ÔøΩÔøΩI\nq.ÔøΩ\bsÔøΩÔøΩ\u000enÔøΩÔøΩ“ÄÔøΩÔøΩÔøΩUhÔøΩ\nÔøΩbÔøΩÔøΩhUÔøΩÔøΩÔøΩ<u_ÔøΩaaÔøΩÔøΩWCÔøΩÔøΩÔøΩH^aÔøΩYÔøΩAA>AÔøΩÔøΩ\u0010\u0003ÔøΩ9RYjÔøΩOÔøΩÔøΩÔøΩB\u0010W4ÔøΩÔøΩÔøΩLT4MÔøΩÔøΩJÔøΩkwÔøΩ3bÔøΩÔøΩ\u001aÔøΩhpz\u0010ÔøΩJb6\u0006FTpk3ÔøΩo'%ÔøΩÔøΩzk(\u0014$ÔøΩÔøΩ\u0014\\RrÔøΩNÔøΩŒ±!1ÔøΩ`ÔøΩÔøΩ,\u0019?yÔøΩ\n/€ÖÔøΩZBFÔøΩfÔøΩ;ÔøΩEÔøΩ\u0015√•ÔøΩÿ°ÔøΩ5!ÔøΩ^SÔøΩU\\)ÔøΩ6oÔøΩÔøΩÔøΩTÔøΩÔøΩ4XÔøΩÔøΩD◊†SÔøΩ\u0006\u0016ÔøΩ&ÔøΩ^\nÔøΩjnÔøΩOHÔøΩÔøΩÔøΩÔøΩ\u001b\b⁄Ö?ÔøΩ>$ÔøΩÔøΩAA*oÔøΩ›éÔøΩÔøΩYÔøΩ<ÔøΩÔøΩÔøΩ/ÔøΩevÔøΩTJ\bÔøΩsJÔøΩÔøΩÔøΩ\u0002ÔøΩns6ÔøΩw\n\u0001ÔøΩOÔøΩ<ÔøΩwzlIÔøΩ5≈¢\u0003ÔøΩI\u0003ÔøΩÔøΩÔøΩÔøΩÔøΩo\u0007ŒéÔøΩÔøΩÔøΩÔøΩMÔøΩuÔøΩÔøΩbTÔøΩÒòîöÔøΩÔøΩkÔøΩÔøΩN\nÔøΩ\u0019'ÔøΩ8RD–§>6qER`\\\u0005 ÔøΩUÔøΩ\nÔøΩ;^ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ+ÔøΩÔøΩgn`ÔøΩÔøΩ€ÇÔøΩ\u0002b+ÔøΩÔøΩ.\u001f,ÔøΩÔøΩcX]ÔøΩvÔøΩi1yÔøΩÔøΩÔøΩÕ•h?\u000f\bÔøΩÔøΩ*ÔøΩ9%jÔøΩ\u001fÔøΩpÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ hÔøΩÔøΩÔøΩWU›∞ÔøΩ8/_ÔøΩÔøΩÔøΩ<ÔøΩÔøΩ\n_!ÔøΩFLÔøΩeÔøΩ$ÔøΩÔøΩvÔøΩÔøΩ\u0010ÔøΩ-ÔøΩ7ÔøΩV\u001fÔøΩ7ÔøΩÔøΩÌáπÔøΩ\u000fjÔøΩÔøΩÔøΩeUCÔøΩDÔøΩÔøΩ/ÔøΩ8ÔøΩrc6ÔøΩlÔøΩiﬁ≠ÔøΩ{\u0003jÔøΩÔøΩÔøΩÔøΩÔøΩ}ÔøΩÔøΩÔøΩ√ÑfÔøΩ[ÔøΩ@2m\u001b}Se~ÔøΩFKÔøΩ\u0006ÔøΩ\n&ÔøΩ\u0015ÔøΩÔøΩÔøΩu6ÔøΩÔøΩ\nÔøΩÔøΩ\u0012ÔøΩ,0\nÔøΩN+ÔøΩ\nÔøΩ—∑\u0012KÔøΩÔøΩkÔøΩ1E‹ìWÔøΩ—©ÔøΩÔøΩ\nÔøΩÔøΩÔøΩEÔøΩ\u0019@ÔøΩ\u0016ÔøΩ\u0018D\u001b@ÔøΩ\u0015ÔøΩnXÔøΩÔøΩIt\nƒ•ÔøΩ\nÔøΩ\u0011ÔøΩÔøΩÔøΩ\u0002gÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩ=ÔøΩÓú∫wÔøΩ\u0003DSÔøΩÔøΩ*—úÔøΩ58_\nÔøΩÔøΩ\u0015\u000f&ÔøΩoG\u000fÔøΩÔøΩÔøΩÔøΩÔøΩ\u0001ÔøΩ+ÔøΩ€ÇjZÔøΩÔøΩ\n“†ÔøΩÔøΩÔøΩ\u0002NÔøΩ\u0018\"CÔøΩÊéûÔøΩ4ÕíÔøΩÔøΩ8-\n1IsÔøΩÔøΩÔøΩƒ±\nAÔøΩÔøΩÔøΩ?R:ÔøΩbÔøΩ$ÔøΩÔøΩÔøΩÔøΩ[@\u000em*m&\u0012ÔøΩÔøΩ\nÔøΩ\tÔøΩ\u0010ÔøΩ^[ÔøΩ\u0018ÔøΩÔøΩÔøΩe√Ü$ÔøΩDSÔøΩiÔøΩhÔøΩÔøΩÔøΩ\u0015TÔøΩ`ÔøΩGÔøΩRM7n\u0019JÔøΩÔøΩ\neÔøΩÔøΩSÔøΩoÔøΩÔøΩ'v5\u0013ÔøΩ\n[ÔøΩÔøΩ=8|}ÔøΩ_ÔøΩÔøΩ\u000fÔøΩÔøΩ\u0016l⁄ö\u0003ÔøΩV\nÔøΩ0ﬁ¨ÔøΩÔøΩÔøΩ\u0019@ÔøΩÔøΩ\u001fÔøΩÔøΩÔøΩ|?y~ÔøΩ)‹≥÷ßMYu\u0000ÔøΩ⁄º-\u0007;ÔøΩ\t\u0007ÔøΩ\u0010ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ?ÔøΩ)ÔøΩ2nÔøΩÔøΩœ∞y{ÔøΩÔøΩx€¶ÔøΩ:|jÔøΩ$teGÔøΩÔøΩÔøΩ⁄ê\u0006\u0003\u0016b\u0015ÔøΩÔøΩÔøΩ\u0016ÔøΩg\u001fÔøΩ$!\nÔøΩ hÔøΩ\u0005xÔøΩ^p>5ÔøΩ<ÔøΩÔøΩ–ΩÔøΩ[\u0002ÔøΩÔøΩ\u0013ÔøΩÔøΩÔøΩÔøΩ<ÔøΩ{ÔøΩ‘Ö{ÔøΩQÔøΩ3÷•ÔøΩ?ÔøΩ9ÔøΩP\u0016Y\nÔøΩ7ÔøΩÔøΩ\u0002iKÔøΩÔøΩ< ÔøΩÔøΩÔøΩÔøΩ+ÔøΩÔøΩ&\u0003,ÔøΩ-ÔøΩxÔøΩÔøΩEÔøΩÔøΩÔøΩ\u0000Nw\n\u0013ÔøΩ6ÔøΩCÔøΩÔøΩ%ÔøΩ\u0012ÔøΩ;yÔøΩÔøΩUkÔøΩEoÔøΩÔøΩ#2ÔøΩÔøΩ\u0019&\u0018ÔøΩDÔøΩ`ÔøΩÔøΩm7<TÔøΩ3ÔøΩC@\\\u0005ÔøΩÔøΩGO^uÔøΩ\nÔøΩuÔøΩCÔøΩ\u0017\nD0%qCÔøΩXpÔøΩDÔøΩlZ\nw\u0013\u001a_ÔøΩ»à\u000e0Tv\u0007\u0010\u0004ÔøΩÔøΩÔøΩÔøΩ\nAÔøΩ\b\u001a:\u0000ÔøΩgrÔøΩÔøΩ\u00180ÔøΩ?ÔøΩ}6ÔøΩ:ÔøΩ\u0007ÔøΩ\u000f\u001bÔøΩÔøΩ\u0010ÔøΩÔøΩÔøΩFÔøΩÔøΩDSÔøΩÔøΩÔøΩÔøΩaÔøΩÔøΩ\u0006^ÔøΩÔøΩFÔøΩgÔøΩÔøΩ)ÔøΩ2JÔøΩÔøΩ%oÔøΩ?ÔøΩ\u0005_qfB\u000fCLÀ∂\u001aKmZ÷âÔøΩÔøΩÔøΩÔøΩ\tÔøΩÔøΩ.\u0013ÔøΩÔøΩSÔøΩ8Q\u001bÔøΩÔøΩ0tœ§xÔøΩÔøΩ]ÔøΩÔøΩÔøΩÔøΩÔøΩE\u0012KÔøΩ'ÔøΩÔøΩVÔøΩFVÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩuQ’∂ÔøΩ+-ÔøΩÔøΩÔøΩ6(/ÔøΩÔøΩ\u0003ÔøΩ\n\u0002;÷ùÔøΩtMÔøΩU'ÃëÿçG\u0019ÔøΩÔøΩ~\n]9ÔøΩoÔøΩ7\u0016ÔøΩÔøΩÔøΩ$ÔøΩÔøΩjÔøΩyKOUÔøΩ2\n2TmvÔøΩÔøΩJÔøΩ›ÖgÔøΩÔøΩwÔøΩ\u001bg<@)ÔøΩZt`sÔøΩDÔøΩT \u001aÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\n\\gÔøΩÔøΩaQÔøΩ‘©ÔøΩnÔøΩ\u001a\n,ÔøΩa%\u001bF\\ÔøΩ19ÔøΩ\u0016ÔøΩAÔøΩ\u0003\u0000ÔøΩÔøΩ_ÔøΩ#\bÔøΩ/ÔøΩ8LÔøΩÔøΩ%#>ÔøΩÔøΩ\u0006ÔøΩÔøΩ%;ÔøΩ\u0016oœúÔøΩ>uÔøΩÔøΩ4\u0000ÔøΩ.ÔøΩ\")ÔøΩ2ÔøΩÔøΩbRÔøΩY*DR<3ÔøΩ\u0011YÔøΩ\nÔøΩÔøΩÔøΩ\u000eÔøΩÔøΩfu\nÔøΩ\u0013ÔøΩŒñÔøΩÔøΩ~3bÔøΩÔøΩe\nÔøΩÔøΩ|ÔøΩ\u0019BÔøΩuÔøΩÔøΩÔøΩ\u000fÔøΩQ?\nÔøΩcÔøΩÔøΩ\u0012ÔøΩÔøΩÔøΩkÔøΩ\tuÔøΩ\\ÔøΩIÔøΩ\n\u0016\\P~eÔøΩÔøΩ,!|\u0012#h\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ`aÔøΩÔøΩiÔøΩ\u000fÔøΩÔøΩ\b?\u0002œæ;ÔøΩÔøΩ{XÔøΩÔøΩ‡¢ºÔøΩÀÉÔøΩÔøΩYÔøΩ=\nÔøΩÔøΩÔøΩrÔøΩl\n*\u0012ÔøΩ\u000fÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩWE!kÔøΩaÔøΩ2ÔøΩÔøΩYÔøΩSÔøΩÔøΩVNYuÔøΩÔøΩÔøΩlÔøΩgŒêy{ÔøΩ\u0007hÔøΩÔøΩ*TÔøΩÔøΩﬁä+ÔøΩÔøΩ\nÔøΩÔøΩÔøΩ^ÔøΩMÔøΩ9;ierÔøΩQa\u0000ÔøΩÔøΩÔøΩÔøΩX7ÔøΩ;ÔøΩÔøΩ*ÔøΩÔøΩÔøΩÔøΩÀóOÔøΩ?WÔøΩÔøΩÔøΩm/?]ÔøΩ\u0012-∆∂@?fÔøΩÔøΩ8ZÔøΩ6ÔøΩ\u000eoc0ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0007ÔøΩÔøΩu#_MÔøΩW\u0014\u000f0mÔøΩ^>ÔøΩÔøΩÔøΩÔøΩÔøΩ€∂ÔøΩ\u000fr p| t\n+ÔøΩÔøΩDIÔøΩÔøΩÔøΩÔøΩ\u001aÔøΩÔøΩi ÔøΩd<ÔøΩ^\u0006ÔøΩ<ÔøΩÔøΩE\u0007ÔøΩÔøΩ€Ä5—∏ÔøΩÔøΩ<cÔøΩÔøΩÔøΩ1 wÔøΩUGÔøΩÔøΩ0ﬁæÔøΩP\u0013ÔøΩTÔøΩÔøΩPFMÔøΩÔøΩ{\u001b„ûï\u001aIOÔøΩ\t&ÔøΩJÔøΩs\bÔøΩ2ÔøΩH\u0005ÔøΩ√™\u0003\u000eÔøΩÔøΩzÔøΩS\"\u0011\u0013ÔøΩ…•ƒ£€òÔøΩÔøΩÔøΩEÔøΩY_KjM:\u0013YÔøΩ6SRsÔøΩ!h3ÔøΩ\nÔøΩÔøΩ2ÔøΩÔøΩQmKÔøΩ.ÔøΩEoÔøΩÔøΩÔøΩ9*d1ÔøΩG\nÔøΩH◊ïÔøΩ%,;ÔøΩÔøΩ$ÔøΩ]ÔøΩ\u0015ÔøΩVÔøΩr N–ö\u0014BJZ„í∞ÔøΩrd>ÔøΩÔøΩ\"ÔøΩÔøΩÔøΩ\u0004ÔøΩ)ÔøΩ\u0013ÔøΩbÔøΩu\u0013\nBÔøΩÔøΩÔøΩÔøΩJÔøΩ\u0005\u0004sÔøΩgbMÔøΩ)@4ÔøΩÔøΩËÅªXÔøΩ\\ÔøΩHuÔøΩHÔøΩ\u0010bcj/ÔøΩÔøΩÔøΩgu\u0007wÔøΩÔøΩÔøΩÔøΩÔøΩ9ÔøΩ\u0003\u0015ÔøΩ\u0010F3ÔøΩ<ZÔøΩ\\\bÔøΩO_ÔøΩOBHg/?h#ÔøΩÃÇÔøΩÔøΩ—øÔøΩUwÔøΩ_\u0007\u0006yGÔøΩÔøΩ\u001b\nÔøΩxÔøΩL≈âÔøΩÔøΩ«ÜÔøΩ\u0001ÔøΩ◊•ﬁºÔøΩÔøΩÔøΩÔøΩP[e\b\u001aÔøΩRVÔøΩEÔøΩÔøΩ_lyA≈ïÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ9G.ÔøΩÔøΩÔøΩÔøΩ4zÔøΩÔøΩmÔøΩJÔøΩ\u00008ÔøΩÔøΩÔøΩÔøΩ!sÔøΩ0ÔøΩ>ÔøΩ nÔøΩ?ÔøΩÔøΩ?|\u001aÔøΩd\u001fÔøΩÔøΩÔøΩ~ÔøΩÔøΩ\u0015W\u0001ÔøΩw\u001a\nVuÔøΩÔøΩÔøΩ]tÔøΩÔøΩÔøΩYÔøΩ\u0014\u0013\nb\n\u0015\u0010!lÔøΩ+\u0000ÔøΩÔøΩY‹πQÔøΩ«èÔøΩÔøΩsÔøΩÔøΩ\"‹û{ÔøΩÔøΩÔøΩ7ÔøΩd\u0014]\u0000ÔøΩÔøΩkjÔøΩÔøΩÔøΩlÔøΩ@qÔøΩhÔøΩ_ÔøΩgÔøΩ\u0011UÔøΩ\u0006>6,ÔøΩjÔøΩÔøΩÔøΩÔøΩW\n\u0002Nﬂì~ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ+ÔøΩÔøΩÔøΩÔøΩ_~ÔøΩ\u0013ÔøΩÔøΩLÔøΩÔøΩÔøΩX8ÔøΩÔøΩÔøΩÔøΩÔøΩ_ÔøΩ]ÔøΩÔøΩi\u0005ÔøΩ+Oﬁ∫uÔøΩyJﬁπÔøΩ+\u000fÔøΩXÔøΩÔøΩÔøΩÔøΩ\u0006ÔøΩÔøΩ~ÔøΩÔøΩ_ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ]ÔøΩÔøΩuÔøΩtÔøΩd«©,ÔøΩ\u0000ÔøΩ\b\u0011ÔøΩÔøΩÔøΩ+\u0004mÔøΩe\b”Ñ(#ZiÔøΩ\u0010cÔøΩ!6ÔøΩÔøΩI’ΩU7ÔøΩo\u0010>ÔøΩÔøΩTvJÔøΩÔøΩÔøΩ6ÔøΩÔøΩÔøΩBÔøΩÔøΩÔøΩRiÔøΩ?fVÔøΩÔøΩjÔøΩÔøΩV.5\u001aÔøΩÔøΩ\nt\tÔøΩKmÔøΩÔøΩP5hÔøΩ\u0012rÔøΩ |6n0OÔøΩÔøΩÔøΩ‘§ÔøΩÔøΩÔøΩÔøΩÔøΩ~Àén$ÔøΩÔøΩÔøΩmÔøΩ\u000f\bÔøΩÔøΩ\u0014xÔøΩÔøΩ\u0014G—Ø_m/2ÔøΩÔøΩÔøΩ2SÔøΩ¬Ç%&ÔøΩÔøΩ9\u0019\\\nU;cÔøΩÔøΩ\bZÔøΩÔøΩÔøΩÔøΩ9ÔøΩÔøΩ@\u001aÔøΩ]D:z\\bÔøΩÔøΩ/ÔøΩsÔøΩCqÔøΩÔøΩF{ÔøΩMÔøΩÔøΩ4ÔøΩ\u0017cHÔøΩÔøΩ6ÔøΩÔøΩÔøΩ)MsÔøΩÔøΩEÔøΩ\u001ad\u0001DÔøΩ9ÀïÔøΩ\u0012AÔøΩ\u0010\\)ÔøΩiÔøΩœûÔøΩ8ÔøΩÔøΩ}\nÔøΩv\u0012/ÔøΩ\nÔøΩ%ÔøΩÔøΩÔøΩMÔøΩÔøΩÔøΩ\u001aÔøΩ\u0012ÔøΩtI\u001aaÔøΩÔøΩn|TE\bÔøΩ\u0000\u0002ÔøΩ\\ÔøΩ1ÔøΩy;\u000e>kÔøΩÃé\u001bÔøΩ\nFÔøΩÔøΩÔøΩa\u001aÔøΩF`pUo<ÔøΩÔøΩÔøΩÔøΩ\u0011ÔøΩÔøΩ\u0000ÔøΩ}ÔøΩÔøΩÔøΩgoÔøΩÔøΩÔøΩ#ÔøΩÔøΩÔøΩAÔøΩ9ÔøΩ\u0010PÔøΩÔøΩÔøΩbÔøΩ.hÔøΩÔøΩwÔøΩÔøΩ\nÔøΩwQÔøΩ\nÔøΩSÔøΩÔøΩÔøΩÔøΩ*ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩD\u001fÔøΩ\u00190kwÔøΩQ;ÔøΩÔøΩJÔøΩ,ÔøΩ\u0010ÔøΩTÔøΩ\u0013UÔøΩ\u0014ÔøΩÔøΩpÔøΩ%eÔøΩÔøΩ\u0006p\u0004ÔøΩÔøΩ0ÔøΩ:ÔøΩ\nFÔøΩÔøΩﬂíÔøΩÔøΩ3«ôÔøΩ∆ÄÔøΩÔøΩ?z\tÔøΩ0ÔøΩÔøΩÔøΩ)ÔøΩÔøΩÔøΩ√óÔøΩRÔøΩ\u0001ÔøΩÔøΩ∆îSÔøΩ%ÔøΩÔøΩ\u0011qƒÆ+[ÔøΩ\u0011\u00068ÔøΩ\u0018ÔøΩ\u0013ÔøΩÔøΩ<&lÔøΩÔøΩ\u0003yÔøΩjÔøΩ›´:sÔøΩÔøΩÍ£§ÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩjÔøΩsJtrÔøΩYUÔøΩ{ÔøΩ8q3ÔøΩÔøΩÔøΩGO_\u000fÔøΩ\nÔøΩe\\\u0004ÔøΩÔøΩÔøΩÔøΩÔøΩpÔøΩ#\u0016ÔøΩ{ÔøΩ\u001fQ,ÔøΩÔøΩ\u001b+<z\u0015ÔøΩÔøΩÔøΩ=ÔøΩtÔøΩÔøΩÔøΩ–§\u0015ÔøΩ#\u0017&\u0004ƒñWÔøΩÔøΩ\n\u00074\u0015≈ñ~ÔøΩÔøΩ?ÔøΩÔøΩÔøΩ\t\u001a–ÉkÔøΩ%ÔøΩÔøΩ\n9\u0015ÔøΩÔøΩWÔøΩI?ÔøΩY'\u0019ÔøΩÔøΩoÔøΩÔøΩÔøΩ\u0006ÔøΩKÔøΩZÔøΩ\u0015\u0015.5a\u001aUÔøΩÔøΩ\nÔøΩ\u0014\u0001\u0007CÔøΩoÔøΩ›ª\u0015rt\u0013…òPÔøΩBÔøΩfSÔøΩ\u0004]'\u0003ÔøΩ¬Ø,”ªÔøΩTBÔøΩ›¥N\nÔøΩÔøΩ\u0004jD\u0003ÔøΩvNÔøΩBŸ™cÔøΩÔøΩpÔøΩaV,{ÔøΩÔøΩT{aÔøΩƒòÔøΩÔøΩvÔøΩ‹ç!1ÔøΩ\nÔøΩ@ÔøΩnÔøΩ4›êÔøΩ6ÔøΩ\u0010<ÔøΩ*ÔøΩ\nB3NÔøΩÔøΩÔøΩÔøΩ\u0017ÔøΩ\u0005V(ÔøΩ{$ÔøΩ1ÔøΩÔøΩvÔøΩ1ÔøΩÔøΩqÔøΩ[ÔøΩ\u0015ÔøΩ”äÔøΩv8\u0011wÔøΩÔøΩ{ÔøΩcCYÔøΩÔøΩDÔøΩÔøΩ\u000fÔøΩmÔøΩdÔøΩ&\u0004j<\u0006ÔøΩ)\nÔøΩfÔøΩ*ÔøΩ0ÔøΩhÔøΩÔøΩ‘ÄÔøΩiÔøΩUUÔøΩÔøΩÔøΩ\u0002\u0013\\ÔøΩ\u0000\tÔøΩ%_C›∞ÔøΩzVÔøΩÔøΩ\nÔøΩ\u0007:\u0004ÔøΩÔøΩ=8ÔøΩ\u0003ÔøΩÔøΩFWÔøΩÔøΩ›àÔøΩJ«ú(ÔøΩ&ÔøΩÔøΩ!ÔøΩÃï\u001aIÔøΩÔøΩÔøΩÔøΩÔøΩIX\u001a—ó“öÔøΩ\nÔøΩÔøΩ◊Äni!ÔøΩÔøΩ6tÔøΩs%\u000fÔøΩÔøΩÔøΩ\u0016ÔøΩ9ÔøΩwÔøΩWÔøΩ{OÔøΩ!\u0014ÔøΩÔøΩÔøΩÁÄ∏\nÔøΩb\u0017ÔøΩ\u0007ytÔøΩ\nÔøΩÔøΩÔøΩ[ÔøΩÔøΩ#ÔøΩÔøΩ\u0004zÔøΩ\u0006ÔøΩÔøΩ;\u000f^8ÔøΩ\u000eS<ÔøΩÔøΩÔøΩÔøΩ\u0015ÔøΩ9p{ÔøΩÔøΩE_ÔøΩÔøΩÔøΩHÔøΩÔøΩÔøΩÔøΩkÔøΩ57O_ÔøΩÔøΩÔøΩÔøΩÔøΩ\u001b-ÔøΩÔøΩÓøÜÔøΩÔøΩn@\u00196XÔøΩÔøΩ\n]ÔøΩjÔøΩ‹óÔøΩÔøΩ%dÔøΩÔøΩ\b/ÔøΩÔøΩÔøΩeÔøΩÔøΩLE\nÔøΩÔøΩÔøΩseÔøΩoÔøΩÔøΩ\n3kCÔøΩÔøΩÔøΩOÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩAÔøΩl?: ÔøΩÔøΩÔøΩÔøΩ}YJ3ÔøΩÔøΩzÔøΩ:ÔøΩÔøΩÔøΩ[)Q\u0000e\u0007ÔøΩÊüüÔøΩ:ÔøΩÔøΩÀ∑ÔøΩG.m\n(ÔøΩmPÔøΩÔøΩE?ÔøΩpiUuÔøΩ\u0010qÔøΩÔøΩ#x“óÔøΩﬂùÔøΩpÔøΩÔøΩÔøΩgÔøΩﬁºOÔøΩ8Õñ\u0016ÔøΩxÔøΩÔøΩÔøΩÔøΩÔøΩ%_\u00190sÔøΩÔøΩÔøΩÔøΩ\u0005\u000e1—ΩÔøΩp\n;ÔøΩ{j4\u0000Oh…•ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ*–åÔøΩÔøΩÍ≠ßz\\8ÔøΩÔøΩ\u0012ÔøΩ[\u0013\nÔøΩLÔøΩÔøΩÔøΩ\nÔøΩ\u001aÔøΩÔøΩ&#ÔøΩÔøΩÔøΩÔøΩ]HÔøΩ$ÔøΩzÔøΩÔøΩHÔøΩ&ÔøΩHÔøΩ\u0003\u0013ÔøΩ\u0004ÔøΩgœ¶\u0018\u000fpÔøΩÔøΩ\nÔøΩJÔøΩ\nÔøΩ%kÔøΩ\n\u0016\u0000ÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\nÔøΩ)`ÔøΩÔøΩPTy`{ﬂÜhL\"ÔøΩÔøΩe[ÔøΩÔøΩ\n1ÔøΩF9\u0002XÔøΩ-ÔøΩMÔøΩemÔøΩ ÔøΩD7ÔøΩ&WXx7ÔøΩÔøΩ\nÔøΩÿäÔøΩÔøΩyÿ∞ÔøΩÔøΩ:ÔøΩ–ÇÔøΩqÔøΩÔøΩeÔøΩ;ÔøΩD\u0004ÔøΩaÔøΩÔøΩÔøΩPhÔøΩh\nÔøΩ\u0015'\\ÔøΩRÔøΩÔøΩÔøΩ\u001fÔøΩ\tChÔøΩÔøΩÔøΩÔøΩÔøΩmÔøΩÔøΩÔøΩpOÔøΩ\u0011ÔøΩ0ÔøΩ\u0002ÔøΩ\u0001\"ÔøΩÔøΩr4ÔøΩxIbO*ÔøΩ\"ÔøΩaÔøΩOÔøΩÔøΩÔøΩz«ù q’îÔøΩDs\u0007€ªpÔøΩsÔøΩÔøΩ\u0005ÔøΩ+,wÔøΩÔøΩÔøΩGÔøΩ]V\u0018$4ÔøΩoÔøΩ\n7\nÔøΩÔøΩm4kÔøΩ\u0011ÔøΩÔøΩt”ºNÔøΩWSC\u0012ÔøΩ(ÔøΩ\u0015iÔøΩ;ÔøΩ\nÔøΩPÔøΩÔøΩ ìÔøΩ\ndÔøΩÔøΩbcWmP!\u0011¬†NÔøΩ\u0001Q?ÔøΩ$eÔøΩ\u001aÔøΩYÔøΩ\u0018NÔøΩÔøΩ':B_ÔøΩ<uÔøΩ@ÔøΩÔøΩ+\u000f~\u0019\u0018 ÔøΩÈáæ~ÔøΩÔøΩNÔøΩÔøΩpÔøΩwÔøΩÔøΩaÔøΩÔøΩÔøΩ\u0003ÔøΩ\u0014\nBÔøΩ1awÔøΩXÔóØÔøΩ=&,ÔøΩÔøΩÔøΩnÔøΩyÔøΩAÔøΩi1]ÔøΩG8OÔøΩ\nM8ÔøΩyLÔøΩwTYÔøΩÔøΩÔøΩÔøΩ73zÔøΩ»§j\nÔøΩZÔøΩÔøΩÔøΩÔøΩ]o?ÔøΩ]ÔøΩp\u000fÔøΩ&ÔøΩÔøΩ@\u0017ÔøΩ\u0017SÔøΩÔøΩÔøΩI\u001bGÔøΩ6=|ÔøΩjnÔøΩÔøΩ+8Q{wÔøΩÔøΩozÔøΩ\nÔøΩ\u0000O\u0004@ÔøΩ√®ÔøΩÊòå\u0004—ãOT\u0019KÔøΩWÔøΩ\u0002pÔøΩÔøΩÔøΩS?ÔøΩÔøΩ/ÔøΩÔøΩÔøΩcrÔøΩÔøΩÔøΩ\u0012$ÔøΩÔøΩÔøΩÔøΩÔøΩ\nVŒ™=yUÔøΩÔøΩ5)N\u0013\"ÔøΩÔøΩœõÔøΩO\u0018ÔøΩ(aÔøΩwÔøΩ\u0019ÔøΩÔøΩeÔøΩ\u0006ÔøΩ-√øAÔøΩÔøΩ\u0007O\n=uÔøΩ6ÔøΩ[0ÔøΩ\nÔøΩkjLÔøΩ)ÔøΩ]ÔøΩG¬ø\u0001ÔøΩ+ÔøΩÔøΩ>ÔøΩ~ÔøΩÔøΩo{ÔøΩ\nÔøΩÔøΩ{e2ÔøΩœü?ÔøΩÔøΩ\u0005\u000eÔøΩoÔøΩbÔøΩ\n\b^ÔøΩÔøΩÔøΩ@ÔøΩ8ÔøΩÔøΩÔøΩ6\u0006cUK\n\\\t8ÔøΩ4\u0015ÔøΩzÔøΩ\t—ñÔøΩ\u0010-xÔøΩF≈Ø[tÔøΩ$ÔøΩ( ÔøΩÔøΩÔøΩÔøΩ\nS>XSÔøΩ\bMÔøΩ\bÔøΩq*k.,ÔøΩb\u0006\u0017jJÔøΩÔøΩÔøΩ^-ÔøΩÔøΩi\\ƒ∂ÔøΩ}]Xc0kÔøΩ6mÔøΩhlÔøΩH3Ryp4TY\u0010ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩXÔøΩÔøΩÔøΩGÃºeWÔøΩ\u0004mÔøΩp\u000e⁄ü@tÔøΩ⁄üH\u0006LÔøΩ\u0018&8\u0004\nÔøΩÔøΩ\u0019\u0002'ÔøΩÔøΩÔøΩ_b~PÔøΩ\u0010\n€òXÔøΩŸ†ÔøΩ(K^bÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩrÔøΩ!ÔøΩÔøΩÔøΩMdÔøΩÔøΩPQ/ÔøΩ[ÔøΩÔøΩ!ÔøΩÔøΩÔøΩ\u0007b\n\bÔøΩÔøΩÔøΩC\u0002ÔøΩF'ÔøΩÔøΩÔøΩÔøΩ(ÔøΩÔøΩÔøΩ ña^nÔøΩ'HÔøΩ`\u001aÔøΩÔøΩ…ë)ÔøΩtÔøΩ\tÔøΩ\u0018<SÔøΩ\u001b\t6ÔøΩÔøΩqÔøΩjÔøΩE@[ÔøΩ\nÔøΩlsJ8ÔøΩÔøΩÿ∫€ÄÔøΩY;ÔøΩÔøΩ\nÔøΩhKÔøΩ\u001bÔøΩÔøΩÔøΩ\n=ÔøΩÔøΩ#ÔøΩdÔøΩ\nÔøΩ\u0019ÔøΩ&ÔøΩÔøΩ.ÔøΩÔøΩ\\oÔøΩ\bÔøΩ\bTÔøΩ¬ã\u00102ÔøΩÔøΩvJ\u0016\nC5Àá4PÔøΩnnx\nnÔøΩMÔøΩ\u0013–∫Õ∑\u0004\u0016ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0001ÔøΩ\t^\u0005~ÔøΩ\nP0oKÔøΩÔøΩÔøΩÔøΩ\u000e\nÔøΩÔøΩÔøΩoÔøΩ…ΩuwÔøΩ-ÔøΩEÔøΩgÔøΩ(\u0016ÔøΩ\u0004ÔøΩ\nÔøΩe\u0000\u0003ÔøΩ–ÖÔøΩÔøΩÔøΩaÔøΩ\u0014ÔøΩÔøΩ?bÔøΩ\u0019ÔøΩRÔøΩnÔøΩJ/ÔøΩ`qÔøΩy\u0018ÔøΩ6ÔøΩ5ÔøΩQÔøΩ6\u001f\u0004ÔøΩ\nÔøΩÔøΩ29ÔøΩLÔøΩÔøΩÔøΩ?4ÔøΩÔøΩÔøΩÔøΩ&xÔøΩÔøΩ\u0006\u0007]ÔøΩÔøΩ8ÔøΩÔøΩRÔøΩ)1%UÔøΩ\u0016l=Hk\bpÔøΩ\u001fÔøΩÔøΩ\u0017ÔøΩ_ÔøΩe`\u0000ÔøΩ\u0017ÔøΩ*\u0003\u0018ÔøΩ\u0019qDMwÔøΩ«øtÔøΩ1ÔøΩÔøΩÔøΩÔøΩ\u001fÔøΩÔøΩ;vÔøΩvbÔøΩiÔøΩs\u00074»æÔøΩÔøΩ#\u0016ÔøΩ+ÔøΩÔøΩ\u0000ÔøΩ\n«Ü3:ÔøΩ\u0010F\n\u0006ÔøΩ3`VÔøΩwT\u0019ÔøΩuÔøΩBÔøΩÔøΩÔøΩÔøΩ\u0007ÔøΩ|ÔøΩÔøΩÔøΩAsÔøΩ\u0010ÔøΩƒ¥ÔøΩ)O_ÔøΩ}ÔøΩÔøΩÔøΩÔøΩ\u0007/nÔøΩ}vÔøΩÔøΩÔøΩ[ÔøΩÔøΩﬂºÔøΩlÔøΩÔøΩ8ÔøΩ'ÔøΩ/F'ÔøΩÔøΩÔøΩb)ÔøΩoÔøΩÔøΩÔøΩÔøΩ[ÔøΩÔøΩjÔøΩ1ÔøΩ\u0000\u0003kÔøΩÔøΩ0ÔøΩÔøΩÔøΩ+~uÔøΩ⁄äÔøΩ&D*mvÔøΩ÷êaB\u0004ÔøΩO[ÔøΩX…®ÔøΩÔøΩÔøΩÔøΩCÔøΩ'4ÔøΩÔøΩ6ÔøΩ\u0006H\nÔøΩr7ÔøΩMÔøΩÔøΩÔøΩrÔøΩ€ùÔøΩPkÔøΩKÔøΩCbÔøΩ-9ÔøΩ\u0012ÔøΩLÔøΩÔøΩ0*pÔøΩtz=ÔøΩÔøΩ&ÔøΩÔøΩÔøΩd\u0010ÔøΩÔøΩÔøΩ\u0005ÔøΩÔøΩÔøΩÔøΩ|ÔøΩÔøΩÔøΩÔøΩ,ÔøΩHFÔøΩÔøΩ\u0013ÔøΩyÔøΩls\u0003ÔøΩzÔøΩn:7ÔøΩŒ§`[ÔøΩ\u0016ÔøΩ+ÔøΩ^T'kBÔøΩS3\\ÔøΩÔøΩÔøΩ\u001aÔøΩÔøΩÔøΩÔøΩaZÔøΩh\u001bÔøΩrÔøΩhÔøΩÔøΩn\t3ÔøΩÔøΩ%ÔøΩMXÔøΩ-ÔøΩ\n÷ã!ÔøΩ\u001fÔøΩÔøΩ”Ø^ÔøΩf\u000fÔøΩNÔøΩG_ÔøΩÔøΩ`ÔøΩÔøΩ%0ÔøΩ`ÔøΩÔøΩÔøΩ0qÔøΩÔøΩ-÷ºJÔøΩÔøΩ]ÔøΩÔøΩ⁄íKÔøΩÔøΩÔøΩ\u0018\ntÔøΩ&ÔøΩ›≠QÔøΩÔøΩÔøΩÔøΩÔøΩ’ÇQÔøΩÔøΩÔøΩÔøΩ9\u0004\u0000ÔøΩ,ÔøΩÃöSmÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0019x}UOcÔøΩDÔøΩ{iÔøΩÔøΩ\\ÔøΩÔøΩ\tÔøΩ\n\u0015ÔøΩDQJ\ncc”∂ÔøΩÔøΩÔøΩÔøΩÔøΩXÔøΩ@ÔøΩÔøΩÔøΩﬂúO3k$ÔøΩÔøΩÔøΩÔøΩlÔøΩÔøΩ{ÓΩ¶FÔøΩÙä∂°ÔøΩÔøΩœä\u001bÔøΩÔøΩpAy\nZÔøΩmOÔøΩÔøΩ[3ÔøΩÔøΩÔøΩuÔøΩÔøΩu\u0000»àd\u0003OÔøΩÔøΩÔøΩ5ÔøΩHÔøΩ x(ÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\bÔøΩÔøΩÔøΩD)<CPÔøΩQ3ÔøΩ\u0000ﬂùvB2ÔøΩ \u000eÔøΩÔøΩÔøΩXÔøΩJxÔøΩÔøΩÔøΩ[ÔøΩÔøΩgÔøΩÔøΩ?tÔøΩÔøΩÔøΩ\u0017CÔøΩÔøΩÔøΩ}p–çÔøΩÔøΩ*N‹Ñ3\u0003ÔøΩ\u000eMÔøΩÔøΩ-ÔøΩ\\qÔøΩÔøΩ\u001aÔøΩ<ÔøΩÔøΩGÔøΩÔøΩo\u000f)ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩWbPÔøΩ\n=wÔøΩ\nÔøΩƒ†ÔøΩ%dÔøΩ\u000eÔøΩstÔøΩÔøΩTÔøΩÔøΩÔøΩ?ÔøΩ\u0004ÔøΩÔøΩ:SvÔøΩF[\u0011ÔøΩÔøΩÔøΩ>\u0010ÔøΩÔøΩÔøΩÔøΩÔøΩiÔøΩ\n\u0003\\+ÔøΩÔøΩ\u001fÔøΩÔøΩOZÔøΩ4l^<ÔøΩ\u001bÔøΩiﬂòr:ÔøΩaTXÔøΩÔøΩjaÔøΩÔøΩaÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩoÔøΩoÔøΩ\u0018ÔøΩ(ÔøΩ7ÔøΩ|ÔøΩÔøΩÔøΩ#BÔøΩ\u0018\u001aÔøΩnxhÔøΩ1ÔøΩ?ÔøΩÔøΩÔøΩÔøΩ~ÔøΩÔøΩwÔøΩÔøΩ\u0003B–ßjÔøΩtÔøΩ?ÔøΩ ›π\n_ÔøΩ;\nkÔøΩÔøΩ\"hÔøΩÔøΩÔøΩCÔøΩDnﬁµÔøΩ“äePk\nÔøΩPÔøΩ'fÔøΩÔøΩÔøΩÔøΩ◊°ÔøΩÔøΩrIb\u0016‹ä\"ÔøΩÔøΩ5ÔøΩcÔøΩ+ÔøΩ!\u0016ÔøΩÔøΩjÔøΩÔøΩ›∫[=5VÔøΩ0ÔøΩÿÑwÔøΩ3\nÔøΩÔøΩÔøΩIÔøΩ|AÔøΩFÔøΩ{4KÔøΩÔøΩÔøΩob\u00181oÔøΩ\u001aXÔøΩÔøΩÔøΩFÔøΩÔøΩ-4÷âF\n)mdÔøΩ\u001ajÔøΩÔøΩÔøΩÔøΩ/qXÔøΩQ\u0013ÔøΩ\u0011ÔøΩeÔøΩRÔøΩ=ÔøΩÔøΩÔøΩ\u001bLÔøΩxÔøΩÔøΩÔøΩÔøΩ\u0004zÔøΩL\u0006ÔøΩ\tÔøΩ\u0018ÔøΩÔøΩ\nWÔøΩ\u0018NÔøΩe}ÔøΩ Å4p\nÔøΩÔøΩ»Ø`\u001b\u001bD\u0013/ÔøΩÔøΩ\u0013QÔøΩnÔøΩÔøΩ\n/KNo]ÔøΩRÔøΩInrlÔøΩÔøΩ  ”óÔøΩUwoÔøΩ.nÔøΩzyÔøΩÔøΩkÔøΩIÔøΩZp\btÔøΩfÔøΩ\nÔøΩÔøΩÔøΩ ÔøΩNUÔøΩRÔøΩaÔøΩÔøΩÔøΩ}h.bÔøΩnnÔøΩEQÔøΩg\\\u00131ÔøΩÔøΩ\nFÔøΩÔøΩÔøΩ@ÔøΩ`zÔøΩ) ÔøΩ+⁄ÇÔøΩo\nF9QÔøΩgWSeÔøΩ(ÔøΩÔøΩKR\u001ftÔøΩÔøΩÔøΩ\u001bÔøΩ\u0010A?xÔøΩÔøΩ◊ÅrUÔøΩÔøΩÔøΩÔøΩWÔøΩm/ÔøΩYÔøΩÔøΩÔøΩOÔøΩ<sÔøΩÔøΩzÔøΩÔøΩÔøΩsÔøΩ\u0000ÔøΩÔøΩ;ÔøΩÔøΩkS`ÔøΩÔøΩÔøΩÔøΩ\nyÔøΩnÔøΩ\nÔøΩÔøΩÔøΩsÔøΩ“ú\u0002ÔøΩgS@!qÔøΩÔøΩÔøΩÔøΩÔøΩ=ÔøΩ\nÔøΩ“ÉÔøΩÔøΩ\u0012ÔøΩ\nÔΩ§ÔøΩÔøΩÔøΩÔøΩÔøΩhÔøΩÔøΩÔøΩÔøΩ+ÔøΩ\n'ÔøΩKOÔøΩgÔøΩÔøΩ5)a\tUÔøΩn?M<t\u0006ÔøΩ9oÔøΩÔøΩÔøΩ#ÔøΩ*OÔøΩZ%ÔøΩMÔøΩ=WxÔøΩÔøΩ\u001bÔøΩoÔøΩ}\u000eÔøΩ~ÔøΩGÔøΩ-cÔøΩÔøΩÔøΩÔøΩ9uÔøΩT#ÔøΩﬂá\u0004\u0007ÔøΩ9ÔøΩÔøΩÔøΩÔøΩÔøΩ3ÔøΩyÔøΩÔøΩÔøΩ/ÔøΩÔøΩ\u0011\u001b\u0018wtÔøΩÔøΩxumÔøΩ\u0002AÔøΩÔøΩ{ÔøΩ\n\u0015ÔøΩZ\u0003ÔøΩÔøΩÔøΩÔøΩ?ÔøΩÔøΩvÔøΩ\u0010\t ÔøΩ@(tÔøΩÔøΩ\u0007OÔøΩÔøΩÔøΩ={ÔøΩYÔøΩÔøΩ?~5z1G*.(ÔøΩÔøΩpÔøΩ{eÔøΩGƒëQÔøΩ\u0013~\u001b\u0014ÔøΩgZ\n<QK\u0004\u0014pÔøΩ¬≠ÔøΩÔøΩ0mÔøΩ√áO\u001bÔøΩ\nÔøΩÔøΩeÔøΩ…çÔøΩÔøΩt\u0001ÔøΩÔøΩ|ÔøΩÔøΩWÔøΩ\u0018fNÔøΩÔøΩ\bÔøΩ\u0005ÔøΩÔøΩ\u0016$/ÔøΩ\u0015j6ÔøΩÔøΩ\tiHvÔøΩ\u0005nÔøΩÔøΩÔøΩÔøΩ=ÔøΩ\u0002FÔøΩÔøΩÔøΩ\t]pYÔøΩ Äv~ÿúÁÖÉ;GÔøΩ\u0004%Èπ¨:6j4MÔøΩÈ¶ΩCLÔøΩÔøΩ@ÔøΩ\nÔøΩXÔøΩ\u0000-ÔøΩeHÔøΩ\"ÔøΩg=ÔøΩ>\u0004\u0006YÔøΩÔøΩÔøΩÔøΩÔøΩdu7+ƒ°<'@==ÔøΩÔøΩÔøΩUÔøΩ\u0010ÔøΩÔøΩÔøΩUaÔøΩÔøΩaeÔøΩÔøΩÕä\u001bÔøΩ,ÔøΩv2\u0006ÔøΩWo6ÔøΩiÔøΩÔøΩÔøΩeÔøΩÔøΩÔøΩmm\\\u0010M\u0015XT6n bpxÔøΩ-ÔøΩ{ÔøΩ»äOÔøΩ\u0012ÔøΩÔøΩ0'ÔøΩvÔøΩÔøΩ%\u0002ÔøΩFÔøΩ4IÔøΩÔøΩÔøΩI\u0015M(ÔøΩÔøΩ√ôÔøΩ#ÔøΩvÔøΩÔøΩ1nÕ≥ÔøΩ\u0010ÔøΩ\u0018\n–∂ÔøΩoÔøΩ\n>DÔøΩÔøΩFÔøΩ\u0003ÔøΩÔøΩÔøΩ\n\u0002'AÔøΩÕ≥D\u0004ÔøΩÔøΩ[ÔøΩP’åÔøΩÔøΩbÔøΩ\u0017ÔøΩjŸ¥]mnÔøΩ1ÔøΩ^\ntÔøΩÔøΩÔøΩcÔøΩ\"ÔøΩFÔøΩs!ÔøΩf\u001aFÔøΩÔøΩu\u0016ÔøΩ\\D\"\u000ezÔøΩÔøΩpÔøΩÔøΩK\u0017ÔøΩJÔøΩ\u001bÔøΩ\u0019JÔøΩÔøΩ4lÔøΩÔøΩ\nŸ¥\u0007ÔøΩÔøΩÔøΩg\u0000ÔøΩÔøΩÔøΩwz,ÔøΩÔøΩ'oÔøΩÔøΩÔøΩrÔøΩÔøΩAÔøΩÔøΩ4ÔøΩJ\u0006ÔøΩagÔøΩÔøΩ_ÔøΩÔøΩ{ÔøΩÔøΩÔøΩÔøΩ}kÔøΩ,g—∂L*ÔøΩÔøΩe[p1mÔøΩÔøΩÔøΩ\u0006\u0015uÔøΩ;\nÔøΩÔøΩ\nUÔøΩmÔøΩ.ÔøΩm@\u001f8vÔøΩÔøΩÔøΩ9{ÔøΩaÔøΩÔøΩ_R}ÔøΩÔøΩÔøΩcr\u0014ÔøΩCÃÅÔøΩÔøΩo?\u0005(\nÔøΩjÔøΩ‹ΩÔøΩxKƒùÔøΩÔøΩÔøΩ\u0000ÔøΩQa\u000e\n\u0001\u0017«™ÔøΩ\"G\u0013ÔøΩ2ÔøΩ\nA√ÖÔøΩÔøΩÔøΩ;s}*ÔøΩÔøΩoFÏôãÔøΩÔøΩÔøΩÔøΩ\b=ÔøΩq\\8‹ßÔøΩ\u0016C\u0004]ÔøΩÔøΩÔøΩ!ÔøΩ\u0002DÔøΩÔøΩÔøΩPÔøΩ.ÔøΩÔøΩÔøΩÔøΩ!nÔøΩÔøΩ8ÔøΩÔøΩÔøΩwÔøΩÔ∫á\nÔøΩ2.ÔøΩ\n\u00179ÔøΩ+ÔøΩ$dÔøΩÔøΩ√ä\u0000ÔøΩyLb\u0017ÔøΩÀéÔøΩÔøΩÔøΩ\u0001ÔøΩN/ÔøΩ\nÔøΩ4\u0011ÔøΩÔøΩ%ÔøΩÃ∂ÔøΩÔøΩ]{ÔøΩÔøΩ\u0017V\u0004ÔøΩÔøΩÔøΩÔøΩ'ÔøΩ\bQsÔøΩÔøΩÔøΩyÔøΩ^ÔøΩÔøΩ \u001asÔøΩÔøΩÔøΩÔøΩmÔøΩ«¥ÔøΩÔøΩ/\u0012iEÔøΩ&kŸç+\\b\u001a\u0019ÔøΩ\u0011ÔøΩu’íe«∫ÔøΩgÔøΩE)ÔøΩSÔøΩ\u0012YÔøΩ\u0018ÔøΩ\\ÔøΩ1ÔøΩÔøΩuÔøΩÔøΩ5ÔøΩ\n0ÔøΩ\nHÔøΩÔøΩ\u0005_ÔøΩÔøΩ\nÔøΩ:\tE”çÔøΩ ÔøΩ6ÔøΩqKÔøΩÔøΩ5ÔøΩÔøΩXÃû[LÔøΩÔøΩ”±ÔøΩO-ÔøΩn\n*ÔøΩÔøΩOÔøΩÔøΩÔøΩ\u0013ÔøΩsÔøΩ]ÔøΩBÔøΩ\u0000\u0019ÔøΩ6jÔøΩÔøΩ\u0005*pÔøΩ7ÔøΩÔøΩ‡´õÔøΩ\nÔøΩ>–íÔøΩdd5bD‹™ÔøΩÔøΩÔøΩÔøΩmÔøΩÔøΩKÔøΩÔøΩrÔøΩ÷éWÏ∏åÔøΩ\u001fÔøΩiÔøΩÔøΩÔøΩ ÔøΩÔøΩuœÖÔøΩoÔøΩÔøΩÔøΩ1ÔøΩ=g\u0003*ÔøΩÔøΩÔøΩevÔøΩÔøΩÔøΩHÔøΩ!ÔøΩ%ÔøΩxÔøΩ[ÔøΩZÔøΩÔøΩÿÅSSÔøΩjeFiÔøΩÔøΩR\u0010ÔøΩB?iÔøΩÔøΩÔøΩ^ÔøΩÔøΩÔøΩÔøΩ ÔøΩ6\u0016ÔøΩ[@ÔøΩ¬öR5t\t\u0012ÔøΩFÔøΩÔøΩÔøΩÔøΩ\u0015\u0003ÔøΩÔøΩ\u0014{m\nÔøΩÔøΩMÔøΩÀîrÔøΩÔøΩ\u0013ÔøΩ\u0018F!KÔøΩmÔøΩ\nXÔøΩ\u0004ÔøΩ\u0016fÔøΩÔøΩÔøΩÔøΩ1E\u001f0#ÔøΩ3ÔøΩ6KCMÔøΩÔøΩwÔøΩ\nnÔøΩ{ÔøΩÔøΩT_\n=}\u0005hTÔøΩÔøΩÔøΩÔøΩÔøΩnaÔøΩÔøΩÃúX\u0006RÔøΩL2hÔøΩ\nÔøΩÔøΩÔøΩgÔøΩÔøΩÔøΩ/6ÔøΩÔøΩÔøΩËøÜÔøΩÔøΩ5<ÔøΩqlÔøΩlyÔøΩÔøΩÔøΩÔøΩ3ÿ¶ÔøΩ¬ïÔøΩ\nÔøΩI\u0006ÕâÔøΩÔøΩ!]|\u0015\u0018)ÔøΩÔøΩn\nÔøΩ\u0013'lÔøΩ\u0002ÔøΩ\u0004ÔøΩ\\RuÔøΩ√áÔøΩÔøΩ.?<{ÔøΩÔøΩÔøΩ\u000fÔøΩ éﬂÄ\t\u0005@(\u0000’®ÔøΩÔøΩÔøΩ÷§ÔøΩÔøΩ%ÔøΩÔøΩ\u0005ÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\u0007ÔøΩ\nA\u0003Œ≠8qÔøΩÔøΩk'ÔøΩÔøΩ}ÔøΩÔøΩÔøΩŸã\u000fÔøΩrÔøΩ\u0015WÔøΩÔøΩœ´ÔøΩÔøΩÔøΩ{\u000fÔøΩ\nÔøΩtÔøΩÔøΩSÔøΩÔøΩÔøΩLÔøΩn7\"\u0014\nÔøΩl\u00054ÔøΩÔøΩÔøΩpÔøΩ\u0011\nÔøΩSrœπLÔøΩEÔøΩÔøΩÔøΩ_0~ÔøΩ~ÔøΩHÔøΩÔøΩ\\ÔøΩËöæ6ÔøΩÔøΩŸª\nFÔøΩ\u0014\u000faÔøΩ@ÔøΩm>ÔøΩ}_?ÔøΩfj ™\u0003\u001f?}ÔøΩÔøΩÔøΩÔøΩÔøΩ'h\u0001ÔøΩtÔøΩ&HÔøΩ5ÔøΩ\u0000%ÔøΩÔøΩ<ÔøΩTe<ÂàåÔøΩ\bÔøΩÔøΩCTÔøΩÔøΩ\u0000E\nUÔøΩ'Y+\u001bÔøΩÔøΩ\na9Np}ÔøΩ\u0003ÔøΩ›à\u000f\u0013ÔøΩXsÔøΩÔøΩ\u0018ÔøΩWÔøΩÔøΩ\u0004ÔøΩ`\"ÔøΩÔøΩ\u0015xw}-ÔøΩÔøΩbHÔøΩÔøΩAsYc\bE,9ÔøΩÔøΩz@ÔøΩÔøΩF\bZÔøΩÔøΩ\u001bÔøΩ\n0ÔøΩP\u001f+QÔøΩ|ÔøΩÔøΩŒµÔøΩOÔøΩÔøΩVÔøΩÔøΩÔøΩhgÔøΩÔøΩÔøΩÔøΩBÔøΩ*\u0019\u00119r\u000eÔøΩOÔøΩ\u0010\nBÔøΩ=ÔøΩÔøΩÔøΩ{sÔøΩÔøΩÔøΩdÔøΩ\u0013DÔøΩÔøΩ\\\n:ÔøΩÔøΩ\nvÔøΩJÔøΩnÔøΩÔøΩÔøΩZiÔøΩ\u0015PÔøΩq\"&ÔøΩÔøΩÔøΩÔøΩAÔøΩÔøΩÔøΩmqÔøΩeÔøΩkq\nÔøΩÔøΩ\u0017jÔøΩfÔøΩ1ÔøΩÔøΩ!ÔøΩÔøΩ{3nÔøΩŸ´MOÔøΩÔøΩzÔøΩ}ÔøΩÔøΩ\u000fÔøΩÔøΩÔøΩÔøΩokLÔøΩÔøΩ\nÔøΩÔøΩÔøΩYC#ÔøΩ;ÔøΩ’¥2\u0010ÔøΩYÔøΩv]\n^\u0013\n—ó\nJ{ÔøΩÂ∂âÔøΩ≈†\u0001\n\u0013ÔøΩÔøΩÔøΩAÔøΩ/\u0006Z\u0003ÔøΩ\u0007ÔøΩj9ÔøΩ2ÔøΩY5J\u0004MÔøΩBÔøΩ\nÔøΩ5ÔøΩy\u001f>|ÔøΩmÔøΩÕÅÔøΩu|bÔøΩvt\u001a\u00136lÔøΩ^qÔøΩqÔøΩÔøΩÔøΩ+ÔøΩÔøΩ\nÔøΩ~ÔøΩÔøΩÔøΩÕ∂=}a6ÔøΩ,ÔøΩ\u0000ÔøΩ\u0014ÔøΩÔøΩ\nœúu>ÔøΩyÔøΩÔøΩWyÔøΩÔøΩÔøΩ+<ÔøΩÔøΩÔøΩVÔøΩÔøΩÔøΩ\u0007\bÔøΩÔøΩ\u0001\u0001ÔøΩ1ÔøΩÔøΩÔøΩ\u0005g+,ÔøΩ2~YÔøΩpi['ÔøΩQ\n\u0013ÔøΩÔøΩKÔøΩÔøΩÔøΩÔøΩ-‹ñyÔøΩÔøΩÔøΩEÔøΩ2ÔøΩÔøΩÔøΩ^y\u0018ÔøΩÔøΩhÔøΩÔøΩ\ng/?\u0010ÔøΩÔøΩÔøΩKÔøΩSÃöÔøΩ\nAÔøΩÔøΩ\nÔøΩÔøΩ\u0004ÔøΩ?s}ÔøΩﬁÉÔøΩ2ÔøΩ.ÔøΩÔøΩ+=sÔøΩÔøΩÔøΩ+ÔøΩÔøΩ⁄¶ÔøΩ\u0016\nÔøΩÔøΩÔøΩ0ÔøΩÔøΩoÔøΩ\n>\u0011ÔøΩÔøΩÔøΩÔøΩÔøΩ;wsÔøΩÔøΩcÔøΩÔøΩlLÔøΩÔøΩ!\nÔøΩ~yÔøΩÔøΩ\u0005[3ÔøΩgÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ|ÔøΩÔøΩ\u0000J_ÔøΩÔøΩ\u0014~\u0010ÔøΩÔøΩ—ãÔøΩ\u0001ÔøΩ?uÔøΩ~DbUÔøΩq\u0011|\u0007fÔøΩŒ¥ÔøΩHÔøΩÔøΩÔøΩÔøΩQÔøΩÔøΩÔøΩÔøΩ\u000fÔøΩF-J0N\u0007ÔøΩÔøΩ/ÔøΩÔøΩÔøΩÔøΩ)ÔøΩ\u0019ÔøΩ\u0014ÔøΩ^ÔøΩ ÔøΩÔøΩÔøΩX,‹ïÔøΩd$ÔøΩVÔøΩÔøΩÔøΩU_q/ÔøΩÔøΩ2ÔøΩ\u0010>ÔøΩÔøΩÔøΩ…≠uw\u001fÔøΩj\nÔøΩvI\n\u0004xÔøΩE,KÔøΩÔøΩ;ÔøΩ\u0000ÔøΩ\u0013ÔøΩÔøΩ\u0012o[ÔøΩ.L\u0010ÔøΩ4ÔøΩÔøΩÔøΩÔøΩamÔøΩÔøΩMnÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩH\tcJRÔøΩ\u001b.ÔøΩÔøΩ.ÔøΩ\u0016\u001b\u0019ÔøΩlÔøΩÔøΩÔøΩn\n\u0019\u0005ÔøΩÔøΩL\u0000‹Ä÷©“Ñ\n{\u0000%ÃêB+ÔøΩ6kQÔøΩhÔøΩÔøΩÔøΩvNÔøΩÃ£ÔøΩÔøΩVÔøΩiÔøΩDÔøΩOÔøΩHÔøΩÔøΩÔøΩ\t\u0004ÔøΩÔøΩÔøΩÔøΩ,1Z;BÔøΩE=!h\u0003ÔøΩÔøΩ\u0010ÔøΩÔøΩÔøΩÔøΩaÔøΩ\u0007ÔøΩ9\nÔøΩÔøΩ≈á`\u0012ÔøΩyÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩÔøΩw}ÔøΩ~ÔøΩÔøΩ\u000f \u001aÔøΩ;ÔøΩoÔøΩ\n>\u0010ÔøΩ\u0019ÔøΩÔøΩÔøΩqÔøΩÔøΩÔøΩ;ÔøΩ9/pl\u0004ÔøΩHZ,ÔøΩÔøΩ<ÔøΩ0\nJ7M¬¶-ÔøΩIÔøΩŸÄÔøΩBÔøΩXÔøΩÔøΩ+--ÔøΩÔøΩ4ÔøΩÔøΩ4ÔøΩ~uiÔøΩvÔøΩBnÔøΩ{ÔøΩrÔøΩÔøΩ|ÔøΩ>s\u0007:ÔøΩÔøΩ'ÔøΩ.ÔøΩÔøΩ}ÔøΩ'D~ÔøΩÀá\u001aÔøΩÔøΩÔøΩ8ÔøΩ%ÔøΩ9OÔøΩÔøΩqÔøΩÔøΩ÷†ÔøΩÔøΩ\u0007\u0007o\u000e(ÔøΩÔøΩ)\np+ÔøΩ,ÔøΩÔøΩ{{ÔøΩ*ÔøΩ\u0005\u0007ÔøΩÔøΩÔøΩGÔøΩÔøΩÔøΩpg¬±ÔøΩcÔøΩÔøΩÔøΩUÔøΩ:ÔøΩu08[LJÔøΩÔøΩ-\u0007%ÔøΩ\u0006ÔøΩÔøΩBÀØÔøΩÔøΩÔøΩÔøΩ*mÔøΩÔøΩ{ÔøΩÔøΩÔøΩÔøΩ3ÔøΩ\u0003vW\u0004ÔøΩV‹∏ÔøΩlÔøΩLÔøΩDÔøΩÎ¥ú\u0000ÔøΩÔøΩÔøΩfÔøΩ\u0011ÔøΩoÔøΩÔøΩÔøΩ\n<ÔøΩÔøΩÔøΩ doÔøΩÔøΩ}ÔøΩÔøΩÔøΩ\u0016€ªÔøΩ<ÔøΩKV{ÔøΩ.ﬁû%\u0016_ÔøΩ\u0018\u0004\u0012ÔøΩ'o*\u0016ÔøΩ\u001b\nÔøΩÔøΩÔøΩÔøΩÔøΩ\u0003ÔøΩÔøΩ€≥ÔøΩÔøΩ~ÔøΩÔøΩÔøΩM<t\u0006ZÔøΩÔøΩÔøΩÔøΩÔøΩcÔøΩfÔøΩNÔøΩ>\u0003ÔøΩ\u0006X-ng8\nÔøΩqkpQzaÔøΩÔøΩÎèΩÔøΩÔøΩ\u0000GO\\ÔøΩDODÔøΩÔøΩÔøΩH~ÔøΩÔøΩ\nÔøΩ‹üÔøΩx_ÔøΩu#X\u0013LÔøΩkÔøΩÔøΩÔøΩ\u001fÔøΩÔøΩMÔøΩ\u0018fAÔøΩÔøΩ(\nÔøΩÔøΩÔøΩu\u001bLÔøΩ]<\nÔøΩzOÔøΩÔøΩD\nÔøΩÔøΩwÔøΩÔøΩÔøΩmO\u001fIGjÔøΩEÔøΩ\nÔøΩÔøΩÔøΩÔøΩ\u0007RÔøΩÔøΩ\u001b\u0002Z\nÔøΩnH3.\u0013ÔøΩ?ÔøΩÔøΩÔøΩÔøΩ\u0018ÔøΩÔøΩÔøΩÔøΩ\u0000&ÔøΩy\u0019(1LÔøΩWÔøΩTO5ÔøΩ\u0010\u0012ÔøΩ\nM\nÔøΩGÔøΩÔøΩ>\u0012√∏ÔøΩgÔøΩF7ÔøΩÔøΩÔøΩgÔøΩ\u0016ÔøΩÔøΩÔøΩKÔøΩX\nÔøΩÀÜ/kÔøΩIÔøΩ:ÔøΩ\u000e\u0001ÔøΩÔøΩÔøΩ>WFIÔøΩMUÔøΩÔøΩ4ÔøΩ}a\nÔøΩÔøΩ\u0016ÔøΩÔøΩyÔøΩc=ÔøΩÔøΩA”πÔøΩ;rR‘ÜFpÔøΩ\u000eÔøΩT\nÔøΩÔøΩÔøΩP\u0016k3ÔøΩ/ÔøΩaM:ÔøΩ\u0015\u001aÔøΩBT1—¨ÔøΩÔøΩ3f\u001bÔøΩqÔøΩ\u0016ÔøΩ\u0010ÔøΩ=hO\u0002ÔøΩÔøΩÔøΩÔøΩÔøΩ\u000eÔøΩÔøΩ\u0001>ÔøΩÔøΩ€ÉfÔøΩÔøΩtÔøΩ&I)ÔøΩ=#ÔøΩ~=jÔøΩÔøΩy5ÔøΩ<ÔøΩ\u001bÔøΩY5hÔøΩ\u0011ÔøΩ\u001f=8L⁄∑5ÔøΩ\u001fÔøΩÔøΩaÔøΩ}6B≈Ω\u0005_jÔøΩ≈≤ÔøΩÔøΩhÔøΩÔøΩÔøΩÔøΩ;LCÔøΩ\u0016ÔøΩÔøΩÔøΩ>\u0003|\u0017p\u001ff\u0007ÔøΩÔøΩÕ•nY‚øéYÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ~\u0005TQEzaÔøΩÔøΩ\u0004ÔøΩ\u0001ÔøΩw…éÔøΩÔøΩ\u000f_l\u000e,ÔøΩÔøΩ<\tÔøΩikS\"\u0012ÔøΩÔøΩ\u0018\u001aÔøΩÔøΩ\nŒπÔøΩ»£uÔøΩÔøΩﬁê9{2ÔøΩjÔøΩ\u000fÔøΩtÔøΩ;JÔøΩ%\\ÔøΩÔøΩfÔøΩDÔøΩ)EÔøΩ\u0010’§ÔøΩ\u001fÔøΩ`ÔøΩÔøΩÔøΩ%ÔøΩÔøΩ\u0003ÔøΩÔøΩ-\nÔøΩ\u001f\u001fÔøΩz\u00020/ÔøΩÔøΩÔøΩ»ãÔøΩRÔøΩ\u001a\u0004\nZÔøΩsÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩhÔøΩn«éÔøΩbxÔøΩÔøΩÔøΩ∆Ä¬ÉÔøΩÔøΩ{\u000fÔøΩjÔøΩ<ÔøΩÔøΩ1%*9ÔøΩÔøΩÔøΩÔøΩuÔøΩ1tﬁûÔøΩKÔøΩ\\N\u0013\"\u0001J7ÔøΩ\t\u001aÔøΩU7Ôå¢\nKwdÔøΩ\nÔøΩÔøΩÔøΩÔøΩ@ÔøΩ\u0013ÔøΩ\njoÔøΩÔøΩwzÔøΩÔøΩI\\WyÔøΩÔøΩ9,dÔøΩÔøΩ=e57ÔøΩÔøΩ\u0019ÔøΩS\u00050ÔøΩM/ÔøΩÔøΩÔøΩ◊àÔøΩÔøΩÔøΩÔøΩ\u000fÔøΩÔøΩE\u001bÔøΩiÔøΩKohM.ÔøΩÔøΩ€ô\u0002AÔøΩÔøΩNVÔøΩ\u0014\u000fw\u000eÔøΩ\u001fŒÄÔøΩÔøΩ#ÔøΩ5ÔøΩÔøΩ\u0015mÔøΩIjÔøΩÔøΩ\u0006ÔøΩ\u0016fgÔøΩ\bZÔøΩWÔøΩhÔøΩÔøΩqÔøΩt1X“îÔøΩ≈ÑÔøΩÔøΩn^ÔøΩÔøΩ@ÔøΩÔøΩ\u0006ÔøΩÔøΩjÔøΩ\bÔøΩ⁄üzQ3\n<-ÔøΩeG7-ÔøΩÔøΩÔøΩ)ÔøΩ\u0017^ÔøΩWGÔøΩ\u000fÔøΩOÔøΩÔøΩÔøΩvc\bÔøΩÔøΩÔøΩÔøΩÔøΩBÔøΩ#ÔøΩÔøΩÔøΩ\u000eÔøΩ)ÔøΩ\u0003ÔøΩ\"ÔøΩÔøΩÔøΩÔøΩMÔøΩÔøΩT—Ç~\u00114ÔøΩÔøΩ\u000erh6\u0006ÔøΩ ÔøΩ2%ÔøΩlO4—≠6\\ÔøΩÔøΩB…âÔøΩoÔøΩ\u0012'_V\u0013m\u0018\u0010ÔøΩ&,$<1ÔøΩ4*ÔøΩ)ÔøΩ/ÔøΩÔøΩlŸç9ÔøΩÔøΩ|ÔøΩ6\u0001ÔøΩÔøΩÔøΩ\bÔøΩY€ûÔøΩ\u000eÔøΩ\u0006ÔøΩ.ÔøΩÔøΩÔøΩÔøΩK\u0000ÔøΩ6B\nÔøΩÔøΩÔøΩÔøΩhF\nÔøΩ8\u0019\nÔøΩNÔøΩÔøΩÔøΩN5)ÔøΩ\"UÔøΩy≈©<ÔøΩpÔøΩ%ÔøΩ5ÔøΩÔøΩYbÔøΩÔøΩÔøΩjsÔøΩÔøΩÔøΩ\u0012j{ÔøΩ&ÔøΩKlÔøΩÔøΩÔøΩ\n}ÔøΩ\u001a%ÔøΩÔøΩÿµÔøΩ&D‹ºÔøΩ2ÔøΩ\u001f=}ÔøΩÔøΩ:Rn\u0018#ÔøΩÔøΩ\u0017ÔøΩ\nÔøΩÔøΩÔøΩ=ÔøΩÔøΩLÔøΩYÔøΩ%cÔøΩÎ°ÅÔøΩÔøΩ(\u000f\u0010\\ ÅÔøΩÔøΩ\u000fÔøΩÔøΩ\u001aÔøΩRÔøΩYÔøΩ\u001fÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩSÔøΩ\u0017ÔøΩ\u000fÔøΩÔøΩlËºΩ'ÔøΩ›õÔøΩÔøΩÔøΩ\u0006ÔøΩÔøΩÔøΩÔøΩÔøΩ/ÔøΩrP‹íÔøΩ\u0013wÔøΩSÔøΩÔøΩ\n«åX\u0014ÔøΩÔøΩ}vÔøΩÔøΩÔøΩÔøΩ\n7ÔøΩ<ÔøΩÔøΩÔøΩÔøΩÔøΩ{/ÔøΩ<{ÔøΩ⁄£ÔøΩÔøΩKÔøΩÔøΩÔøΩgÔøΩÔøΩ\nÔøΩE\nÔøΩÔøΩ\nÔøΩWzÔøΩÔøΩÔøΩÔøΩ÷†ÔøΩÔøΩ!≈´d9pÔøΩ;\u000eÔøΩÔøΩ\u0005\u0017\u0015\u001fÔøΩvÔøΩÔøΩmÔøΩÔøΩÔøΩ0ÔøΩn\u000f.vÔøΩyXÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ]~ÔøΩÔøΩ3ÔøΩ#ÔøΩ$ÔøΩ–ôEÔøΩ2\u0017lÕòÔøΩ%ÔøΩ\u001fÔøΩCÔøΩW\u001fzÔøΩ*ÔøΩE\u0014ÔøΩ2q◊ï\u001bOÔøΩÔøΩJ\tÔøΩCÔøΩÔøΩ55ÔøΩatÿ∑ÔøΩÔøΩÔøΩÔøΩ\u0018\u001a\nÔøΩ\u0003ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ%ÔøΩ-ÔøΩÔøΩK\u0013_ÔøΩzG\nbÔøΩÔøΩMÂ®°ÔøΩ€°6ÔøΩ&ÔøΩÔøΩ∆ùÔøΩÔøΩ;Z\u0018]ÔøΩfÔøΩcÔøΩÔøΩiÔøΩ-IÔøΩŸê7 V\bRÔøΩPÔøΩ\n\bÔøΩIÔøΩÔøΩÔøΩr\u0010ÔøΩÔøΩ(ÔøΩgÔøΩÔøΩ\n>ÔøΩÔøΩÔøΩ\u0010\u0016MÔøΩrEÔøΩÔøΩ]{ÔøΩÔøΩÔøΩ\"ÔøΩÔøΩÔøΩ_\nÔøΩÔøΩlÔøΩÔøΩTÔøΩ6ÔøΩ“πuÔøΩKm(\u0001\nrÔøΩÔøΩJÔøΩc≈ê;ÔøΩ>ÔøΩÔøΩ{u:ÔøΩ|ÔøΩ√àÔøΩvÔøΩÔøΩ]\n^ÔøΩÔøΩÔøΩÔøΩQÔøΩ:ÔøΩHh\u0006ÔøΩ\u0019ÔøΩWrÔøΩÔøΩxÔøΩ\nÔøΩd{\u0012ÔøΩÔøΩ^ÔøΩTÔøΩÔøΩ\u001fÔøΩ_ÔøΩ<ÔøΩÔøΩNÔøΩ∆ë~\u001bÔøΩÔøΩYl6ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩnÔøΩÔøΩNÔøΩÔøΩwNRÔøΩvŒûÔøΩÔøΩrg\n_ÔøΩ\u0006ÔøΩHo$F~CÔøΩT_&pqcuÔøΩ+%[6sÔøΩD.nÔøΩÔøΩ›Ω)ÔøΩÔøΩ\u000eÔøΩ\n\u0004ÔøΩÔøΩ&(vÔøΩ;\u0016/ÔøΩÔøΩÔøΩ;\u0019.UÔøΩÔøΩ\u0018’¥\u001aÔøΩÔøΩŒú'ÔøΩÔøΩÔøΩ\u0018ÔøΩ4/ÔøΩVR7ÔøΩÔøΩ<\u0014ÔøΩ\u0019ÔøΩ\u00118ÔøΩ/\u0003oÔøΩ+ÔøΩÔøΩ §uÔøΩÔøΩ-)7JÔøΩRÔøΩ(◊©ÔøΩ\u0014ÔøΩ'<ÔøΩÔøΩÔøΩGÔøΩÔøΩÔøΩÔøΩY\u0016yDÔøΩaÔøΩÔøΩVyÔøΩ<xÔøΩÔøΩÔøΩÔøΩÔøΩZÔøΩÔøΩÔøΩÔøΩGÔøΩ\nÔøΩ\u0013ÔøΩÔøΩÔøΩ\u0003ÔøΩÔøΩ]~\u0010ÔøΩÔøΩ⁄åÔøΩÔøΩ\u0013\"cSOLXÔøΩÔøΩrfÔøΩ\u0016'd\nÔøΩÔøΩÔøΩ#\u001b~^ÔøΩ\u0015\u0018wTÔøΩÔøΩ\u0003ÔøΩÔøΩY|ÔøΩ^aÔøΩ\n|ÔøΩﬁÉÔøΩfoHKÔøΩ9\nÔøΩÔøΩÔøΩÔøΩwÔøΩÔøΩÔøΩuÔøΩ\u00189wÔøΩÔøΩ'ÔøΩ_?~ÔøΩz‘¢\u0004x|ÔøΩwÔøΩ\u0006AO[\u0003(>ÔøΩÔøΩÔøΩÔøΩ3wÔøΩÔøΩÔøΩÔøΩ;ÔøΩ€†ÔøΩÔøΩ#wÔøΩJÔøΩÔøΩk≈•ÔøΩÔøΩÔøΩ>uÔøΩgÔøΩÔøΩ9ÔøΩÔøΩVÔøΩÔøΩÔøΩhÔøΩÔøΩKÔøΩOLJÕ†9{:ÔøΩ\b]ÔøΩ5cÔøΩW^ÔøΩ\u0011ÔøΩÃÜÔøΩ:ÔøΩÔøΩLÔøΩ?ÔøΩÔøΩÔøΩ\u0000ÔøΩÔøΩÔøΩÔøΩÔøΩUÔøΩÔøΩYXqUxÔøΩY\u001b“û<}ÔøΩÔøΩ…´\nÔøΩ\nÔøΩ\nÔøΩÔøΩ\u0013]ÔøΩ—ø`ÔøΩÔøΩL\u0018&ÔøΩÔøΩÔøΩ\u0015_\"ÔøΩÔøΩÔøΩOÔøΩÔøΩ|ÔøΩ\u0017^ÔøΩﬂ´ÔøΩMÔøΩÔøΩ\u0005pj€µ~ÔøΩ\u0001h%ÔøΩÔøΩ1FÔøΩ\tWÔøΩÔøΩ\n2 ó6{\u0018S\nŸ™#WÔøΩ‹§ÔøΩJ)[\"gÔøΩQÔøΩTÔøΩ\u0002ÔøΩUÔøΩgÔøΩÔøΩÔøΩÔøΩ\nÔøΩ^c)ÔøΩÔøΩÔøΩÔøΩ\u0014€≥\u0019\u0012eÔøΩu”ûÔøΩÔøΩÔøΩ>.‘ÅÔøΩÔøΩ#kh\bÔøΩÔøΩÔøΩ[uDFÔøΩ\n\u0014\u0001a\u0000\u0010ÔøΩÔøΩÔøΩ\nÔøΩÔøΩtÔøΩÔøΩÔøΩ>ÔøΩ2+\u0007ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩuÔøΩ+ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0014ZQÔøΩÔøΩÔøΩÔøΩ\u0011\nÔøΩ%G)o\u001a\n\u0002ÔøΩ0z\u0007DDgÔøΩÔøΩ)ÔøΩÔøΩJ`\n\u001bXÔøΩf\nÔøΩ,p\u001aÔøΩ>ÔøΩ÷àIVÔøΩNÔøΩ\u0001jÔøΩqÔøΩGÔøΩÔøΩÔøΩ_2ÔøΩÔøΩÔøΩÔøΩ5utÔøΩ\u000e81hOr\bÔøΩ`ÔøΩÔøΩ”É∆µÔøΩ:=h\n0ÔøΩ3ÔøΩÔøΩD5\u0017ÔøΩÔøΩÔøΩ…ëÔøΩ\u000fÔøΩTDAiÔøΩ?cÔøΩ“•ÔøΩ÷∫)\nÔøΩSn2D-[{nrÔøΩÔøΩ$ÔøΩ\nn\b:>ÔøΩÔøΩ[8\u001bÔøΩ$\\EÔøΩp Z;i\u0012|wÔøΩ\u0014A+ÔøΩ&ka]'D<}ÔøΩÔøΩÔøΩ/ÔøΩxÔøΩHÔøΩE\nPÔøΩeÔøΩÔøΩÔøΩ&QÔøΩÔøΩ\u0011ÔøΩ7i\nÔøΩÔøΩÔøΩÔøΩÔøΩ_ÔøΩÔøΩÔøΩ%ÔøΩ\u000egXÔøΩ/ÔøΩÔøΩVxÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ[wÔøΩÔøΩÔøΩ)oÔøΩÔøΩK|~ÔøΩÔøΩ%\u0000!%O\u0001\u0017ÔøΩÔøΩ\"\tÔøΩÔøΩmpÔøΩjÔøΩ<8ÔøΩÔøΩ\u0006\u0007ÔøΩÔøΩ%\\nKP—ÄYÔøΩÔøΩÔøΩMÔøΩ\b\u001anÔøΩÔøΩ‘òÔøΩÔøΩÀÆ;\u000fÔøΩ&\nÔøΩuP ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩCÔøΩ+'.OÔøΩ\u0003\u0016mÀå9P\u0003h\nÔøΩ+ÔøΩ\u0019sÔøΩÔøΩÔøΩÔøΩWzYrÔøΩ`m{ÔøΩ\nÔøΩ9\u00077\t\u0001ÔøΩrÔøΩÔøΩ\u0011\nÔøΩ-pÔøΩÔøΩFSBÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ›ôpÔøΩÔøΩÔøΩÔøΩ\u000eÔøΩ9ÔøΩspÔøΩ3◊ßAÔøΩÔøΩI?ÔøΩ%ÔøΩ\u0010\u001a\u0004ÔøΩ<R}ÔøΩdÔøΩ=\u0017ÔøΩÔøΩ\u0017<ÔøΩÔøΩ\ni\u001f>ÔøΩ${x;ÔøΩ57$ÔøΩÔøΩÔøΩÔøΩKÔøΩyÔøΩÔøΩ-ÔøΩÔøΩÔøΩ]TÔøΩÔøΩÔøΩh\u0016ÔøΩlÃ©6ÔøΩo0ÔøΩÔøΩaÔøΩÔøΩ\u0011uÔøΩ}}\u0005ÔøΩÔøΩ\u0006ÔøΩÔøΩ(ÔøΩÔøΩÔøΩ{ÔøΩnÔøΩjvxÔøΩ&ÔøΩ&tÔøΩ‘¨qEÔøΩ\bdÔøΩ]ÔøΩÔøΩ4!TÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ0|ÔøΩQ\nÔøΩ1ÔøΩ\nÔøΩÔøΩÔøΩ\u000eÔøΩÛî¢†a\u0017eÔøΩ\u0010ÔøΩa1ÔøΩ\nÔøΩkÔøΩ}VÔøΩ,LÔøΩ\nÔøΩ?ÔøΩ!ÔøΩQÔøΩqÔøΩWmÔøΩZo öpÔøΩ\nBGÔøΩÔøΩ+RÔøΩ!ÔøΩÔøΩ6$/ÔøΩÔøΩKÔøΩ\u000eÔøΩ+Q\u0006x›òÔøΩÔøΩrQt\u0007ÔøΩTÔøΩkÔøΩpN\u0000ÔøΩÔøΩÔøΩ\n\u0014ÔøΩ.u1ÔøΩzÔøΩÊ≤®ÔøΩÔøΩwÔøΩ'ÔøΩÔøΩ[ÔøΩÔøΩÔøΩÔøΩÔøΩ`ÔøΩ\u000eÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\u000eÔøΩ\u0016ÔøΩÔøΩUÔøΩ2ÔøΩ2\u0005ÔøΩcÔøΩÔøΩÔøΩ\u00101ÔøΩeÔøΩ\u0015$}OÔøΩ(∆ºÔøΩk\u0013L$ÔøΩ#ÔøΩ\nÔøΩ{uÔøΩÔøΩÔøΩh\u001aWÔøΩ\u0018ÔøΩÔøΩÔøΩ◊∫ÔøΩ5—ÜÔøΩ4ÔøΩWÔøΩÔøΩXÔøΩÔøΩ2yÔøΩ?8\tÔøΩbÔøΩÔøΩÔøΩ\u0006i]MtÔøΩ\u0011ÔøΩÔøΩ\tÔøΩÔøΩ—à(ÔøΩÔøΩ÷ÑÔøΩÔøΩ\nÔøΩ1ÔøΩÔøΩJÔøΩÔøΩœø\u001f?~ÔøΩK;)ÔøΩgÔøΩrp-;~ÔøΩ'ÔøΩ\\ÔøΩ\u0006ÔøΩA\u0000’∂\u001f\u0011jÔøΩ\n€ÇÔøΩÔøΩ<3r!ÔøΩÔøΩÔøΩ:.bÔøΩgÔøΩxsjÔøΩÔøΩÔøΩqK\u0013iÔøΩns@ÔøΩ√ÇÔøΩÔøΩw\n\u0017ÔøΩÔøΩÔøΩ{ÔøΩ\t\u0011ÔøΩﬁß%\u0012ÔøΩ|ÔøΩÔøΩGÔøΩ ÉR\u0004\nÔøΩ\nj≈âÔøΩ\u0000ÔøΩ\u0005%ÔøΩaÔøΩÔøΩ\u0006ÔøΩU‹∏ÔøΩ%BcÔøΩÔøΩÔøΩÔøΩÔøΩwÔøΩÔøΩÔøΩÔøΩÔøΩewQÔøΩ6XJÓπ±pÔøΩ\nÔøΩ\u001aÔøΩx_p|%\u0001:ÔøΩ]ÔøΩÔøΩ\u0012opÔøΩQIÔøΩ\u0019\u0018ÔøΩh[∆ùÔøΩÔøΩÔøΩ,ÔøΩ'ŸøÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0011QBÔøΩÔøΩÔøΩhÔøΩÔøΩ3\n\u0005ÔøΩ_\u0006\u0006ÔøΩÔøΩyÔøΩ^ÔøΩ~?lÔøΩ^ÔøΩÔøΩ\n,ÔøΩeÔøΩÔøΩÔøΩ~ÔøΩrD\u0010ÔøΩ6Db\u0018n≈≤bÔøΩÔøΩ›ΩÔøΩÔøΩÔøΩÔøΩ;ÔøΩ*\u0019MÔøΩ ÔøΩjxÔøΩ8BxÔøΩ#\u0011!ÔøΩt\nÔøΩÃö!h/ÔøΩÔøΩ\n)ÔøΩ\u0018nuÔøΩAfÔøΩ\nÔøΩ-ÔøΩeÔøΩÔøΩ<ÔøΩ%ÔøΩijG\u0019ÔøΩ%ÔøΩ5ÔøΩ(ÔøΩrÔøΩi\nÔøΩÔøΩ\n\\uÔøΩfÔøΩÔøΩ–ív:ÔøΩÃÄ\u0007ÔøΩkÔøΩÔøΩBcoXÔøΩÔøΩÔøΩ\neÕâ÷ÄÔøΩÔøΩq\u0012ÔøΩs\b\u0014ÔøΩvÔøΩ\u0006gÔøΩJÔøΩ\u001am}ÔøΩ\u000f#\u0013ÔøΩÔøΩ\u0006ÔøΩ`ÔøΩZÔøΩÔøΩ3O]tIHÔøΩÔøΩhÔøΩÔøΩdÔøΩÔøΩÔøΩÔøΩ^ÔøΩ\nÔøΩÔøΩ1ÔøΩZTBÔøΩ'ÔøΩ\u0010.]z6ÔøΩ,\u0015ÔøΩÔøΩÔøΩÕö\u0015\u0004yÔøΩÔøΩÔøΩÔøΩ\nÔøΩ%=ÔøΩz#\u0004ÔøΩ\u000e;Z NÔøΩÔøΩ2ÔøΩM&ÔøΩKgÔøΩ&B=\u0005/ÔøΩ.ÔøΩÔøΩÔøΩÔøΩÔøΩAl\u0017ÔøΩÔøΩÔøΩÔøΩ^ÔøΩÔøΩ^\u001fz\nÔøΩ5ÔøΩÔøΩÔøΩ ÔøΩÔøΩ\u000e\u0016ÔøΩ\u000eÔøΩ,ÔøΩ\u0019z_ÔøΩÔøΩ\u0010ÔøΩÔøΩoÔøΩS\u00121ÔøΩF$\u0015\u0011CÔøΩÔøΩ\u0018z\u000e1ÔøΩ70ÔøΩŒçÔøΩ\u0015\u001bÔøΩÔøΩ+ÔøΩ%cÔøΩbÔøΩ%VÔøΩÔøΩÔøΩo{ÔøΩ÷úÔøΩKÔøΩÔøΩÔøΩ_ÔøΩ\u001b4'NÔøΩWxÔøΩg/=ÔøΩ19JNÔøΩÔøΩÔøΩÔøΩOtÔøΩÔøΩŸªÔøΩÔøΩÔøΩÔøΩ)ÔøΩxÔøΩQÔøΩÔøΩ}ÔøΩÔøΩ,qÔøΩ\u0012wrÔøΩÔøΩ;ÔøΩmÔøΩ üÔøΩÔøΩIÔøΩ3-FÔøΩ):ÔøΩÔøΩÔøΩÔøΩ&ÔøΩU\u001fÔøΩZrd8^ÔøΩÔøΩ<ÔøΩÔøΩÔøΩ*\"hpÔøΩÔøΩSkÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩ\u0011ÔøΩ:ÔøΩÔøΩÔøΩ:=wcÔøΩÔøΩVÔøΩ1ÔøΩ,M\nOÔøΩÔøΩÔøΩ_ÔøΩ\u0019ÔøΩ`{F\nIÔøΩ?ÔøΩxopÔøΩpÔøΩÔøΩ\u0015IÔøΩH\n\u001aÔøΩÔøΩ!ÔøΩ\u0001ÔøΩ\n\u0011\nÔøΩ{+ÔøΩÔøΩKÔøΩ>ÔøΩRÔøΩCnÔøΩ\n€ç\u0011IÔøΩw\u000e?ÔøΩﬁò\u0006ÔøΩÔøΩ{e2ÔøΩÔøΩÔøΩÔøΩ‰ûëGﬁΩÔøΩH\u0014ÔøΩÔøΩ9gÔøΩ ‘ÆÔøΩQ…™ÔøΩÔøΩ\u001as~KQ/rÔøΩÔøΩÔøΩÔøΩ|ÔøΩ^\u0018ÔøΩÔøΩ(ÔøΩ÷ä\u0013iuÔøΩ0ÔøΩÔøΩ.i#rÔøΩÔøΩÔøΩ=\neÔøΩ*ÔøΩ+6\u001aÔøΩoÔøΩ\u001bÔøΩBYÔøΩ!ÔøΩ\u0011-ÔøΩ\u000eÔøΩ\u0001ÔøΩÔøΩÔøΩÔøΩSFkÔøΩ^ÔøΩÔøΩ=\u0015Mk\u0016^ÔøΩ’ÉÔøΩ\u0001AÔøΩ!,mÔøΩ|ÔøΩÔøΩ ∏ÔøΩÔøΩÔøΩzÔøΩ\nDÔøΩÔøΩ!ÔøΩÔøΩÔøΩÔøΩÔøΩaV\u000eTdÔøΩÔøΩQÔøΩÔøΩ7ÔøΩÔøΩ\u0016\u0019~ÔøΩÔøΩ\u0010ÔøΩO3^ÔøΩÔøΩÔøΩ0ÔøΩÔøΩ\nÔøΩÔøΩ\u000eÔøΩÔøΩÔøΩÔøΩEÔøΩÔøΩt)3hF.’§ÔøΩpÔøΩÔøΩÔøΩÔøΩFÔøΩ^ÔøΩÔøΩ\u001bdÔøΩÔøΩÔøΩÔøΩ&m8ÔøΩjÔøΩ&ÔøΩ^TÔøΩÔøΩÔøΩÔøΩ‹ØÔøΩEXs%\nÔøΩ\u001bH\nÔøΩÔøΩ\\\u0007ÔøΩ\u000eÔøΩÔøΩeÔøΩDﬂÖÔøΩC\u0011ÔøΩxR&\nÔøΩÔøΩzÔøΩ{KÔøΩ\u0002ÔøΩÔøΩÔøΩfÔøΩ;ÔøΩÔøΩVÔøΩÔøΩ\u0010P32+ÔøΩÔøΩ^u1RÔøΩÔøΩQÔøΩW—¨ÔøΩÔøΩE[ÔøΩ>\nrÔøΩz]\u00124ÔøΩ}ÔøΩ\u0013ÔøΩ5ÔøΩQ96_ÔøΩÔøΩÔøΩWÔøΩ7$ÔøΩÔøΩ$ODÔøΩ%ÔøΩW%,wÔøΩÔøΩÔøΩÔøΩÔøΩ`GÔøΩÔøΩ\nÔøΩ\b\u00168c\nÔøΩ=ÔøΩylÔøΩ\nÔøΩnÔøΩJÔøΩRÔøΩ/[uÔøΩÔøΩ+ÔøΩ|ÔøΩÔøΩ}ÔøΩÔøΩ]ÔøΩÔøΩSV\nÔøΩÔøΩ:\u0005lÔøΩÍîÖ[3ÔøΩÔøΩ\u0014S6ÔøΩÔøΩÔøΩÔøΩ;ÔøΩ\u0014o`_ÔøΩiÔøΩÔøΩ#ÔøΩ]ÔøΩÔøΩÔøΩ\u0004ÔøΩÔøΩÔøΩÔøΩÔøΩ\\ÔøΩ\u0017rÔøΩÔøΩÔøΩ$ÔøΩoÔøΩn1CÔøΩÔøΩ\\&ÔøΩRÔøΩrÔøΩÔøΩ?hN\n<ÔøΩÔøΩÔøΩÔøΩ\u0017ÔøΩ=r\u0018ÔøΩSÔøΩ\u0016›ºÔøΩK.~ÔøΩÔøΩ\u000fÔøΩ\u0007ÔøΩyÔøΩÔøΩkÔøΩq\u0015\u0000{ehÔøΩQe'jÔøΩ{ÔøΩ11ÔøΩ4a+xSÔøΩ\u0016%\n>vÔøΩÔøΩÔøΩÔøΩW\u001fÔøΩﬂíA8\u001aÔøΩÔøΩÔøΩÔøΩÔøΩeÔøΩÔøΩÿ™Áî®{ÔøΩ^\u0012ÔøΩÿ≥ÔøΩo{MÔøΩFÔøΩ)ÔøΩw92ÔøΩÔøΩÔøΩ-ÔøΩÔøΩÔøΩ+M\u0010ZÔøΩÔøΩÔøΩÔøΩÔøΩ*\u0019ÔøΩP&\u0015ÔøΩVÃãÔøΩÔøΩÔøΩ\u0005»¨8Z\u0012OÔøΩÔøΩU\u0012ÔøΩÔøΩFSÔøΩLÔøΩÔøΩÔøΩC\u0013QT\nD7.ÔøΩdÔøΩÔøΩiN`ÔøΩœÇ„ã¶›ïÔøΩÔøΩXp%ÔøΩÔøΩÔøΩÔøΩ*{SÔøΩXÔøΩÔøΩ≈êÔøΩ\u0005ÔøΩzÔøΩhﬁûPpÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ_≈¢DÔøΩ&ÔøΩ\u001bR_\u0002ÔøΩ‹∂ÔøΩ/JÔøΩx5ÔøΩÔøΩÔøΩƒà\\\u0007\u001a\u001aÔøΩÔøΩÔøΩ\u0011ÔøΩÔøΩÔøΩÔøΩ\n\u0019aoÔøΩdÔøΩZÔøΩƒòÔøΩ-o.ÔøΩ>3ÔøΩEcÔøΩÔøΩr4ÔøΩÔøΩ>-ÔøΩ'\u001aÔøΩÔøΩÔøΩ:\nÔøΩfÔøΩwLeY_\nÔøΩÔøΩÔøΩw\bBÔøΩ–å+mvÔøΩ|*ÔøΩ%zEÔøΩa1j&\u0014#45\u0002ÔøΩ\u0006ÔøΩÔøΩ…ëÔøΩcÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩfÔøΩÔøΩÔøΩ\u0015ÔøΩÔøΩÔøΩ8ÔøΩMÔøΩoÔøΩÔøΩ}pÔøΩ$\u0018Z\u001bE_Z/8CkÔøΩÔøΩ[ÔøΩÔøΩ\u0017ÔøΩqÔøΩ[ÔøΩJKcÔøΩ ÔøΩ\u0010\u0005ÔøΩÔøΩÔøΩ\u0018)ÔøΩÔøΩ|ÔøΩGÔøΩÔøΩÔøΩ(\u0017=$ÔøΩ}ÔøΩ€ØÔøΩÔøΩmBÔøΩ\u001f?~ÔøΩÔøΩ8BÔøΩÃîÔøΩÔøΩ0:lÔøΩÔøΩ!ÔøΩ^<„óûÔøΩkÔøΩGÔøΩ\nœúÔøΩSÔøΩÔøΩÔøΩU{ÔøΩ?ÔøΩÔøΩ\u0014T\\\u0001ÔøΩËµ´t»ºÔøΩÔøΩÔøΩÔøΩ'\u0011ÔøΩÔøΩ2ÔøΩCÔøΩsÔøΩÔøΩÔøΩÔøΩÔøΩ\u0019EÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ‹≥ÔøΩÔøΩ\u0010\u001aIÔøΩD\u0002ÔøΩTW\"ÔøΩ\nÔøΩ\n\u001aÔøΩZÔøΩ\u0007+\"h-f\u0019sÔøΩÔøΩ»ÖÔøΩÔøΩ%ÔøΩhÔøΩ@ÔøΩ\u0000ÔøΩGÏØÇÔøΩÔøΩ<  .ÔøΩDVÔøΩ\u0005Áâª\u0004[ÔøΩÔøΩÔøΩÕáÔøΩÔøΩ\u001abÁÉ≥M[ÔøΩ2iE\u0012<fÔøΩÔøΩ;\u0017nÔøΩ<yÔøΩ^hÔøΩ1bÔøΩm7\"ÔøΩ◊î\u0018ÔøΩÔøΩ DÔøΩÔøΩ~ÔøΩÔøΩz8ÔøΩÔøΩÔøΩBJ{”ùo__ÔøΩÔøΩ[\ng/≈≤ +ÔøΩTYÔøΩÔøΩjÔøΩXÔøΩÔøΩÔøΩ\u0010\u00120ÔøΩV|ÔøΩDÔøΩmÔøΩÔøΩ…ç á\bFÔøΩAÔøΩÔøΩsÔøΩwÔøΩ\nAÔøΩ\u001aÔøΩÔøΩVÔøΩÔøΩÔøΩ}ÔøΩÔøΩ\tÔøΩÔøΩ[\n\u0017VwÔøΩ8ÔøΩÔøΩ@ÔøΩl/ÔøΩÔøΩ∆≠\u0001w0ÔøΩ]ÔøΩo\nPÔøΩÔøΩ*\nÔøΩÔøΩiÔøΩNtCÀÇÔøΩÔøΩqÔøΩÔøΩÔøΩÔøΩ5sÔøΩ7ÔøΩ>ÔøΩP\u00060 ÆÔøΩÔøΩkÔøΩ)aÔøΩ\nÔøΩ5U2vÔøΩ\nÔøΩeqwÔøΩ\u0006cÔøΩÔøΩ!~∆àÔøΩÔøΩ⁄†ÔøΩÂñßÔøΩÔøΩ8ÔøΩ&VÔøΩÔøΩ\u001b\nÔøΩﬂúÔøΩÔøΩÔøΩÔøΩÔøΩ\u0019ÔøΩnÔøΩÔøΩ\u0018 ÔøΩÔøΩj3ÔøΩÔøΩ!\u0012ÔøΩÔøΩkÔøΩuI=ÔøΩ\nÔøΩÔøΩIÔøΩ#ÔøΩUyÔøΩÔøΩ\u0006\u0017'\u0012ÔøΩÔøΩÔøΩÔøΩ\u000eÔøΩ<\u0005GÔøΩÔøΩUÔøΩ![pÔøΩÔøΩ\\GÔøΩu`1ÔøΩÔøΩvp’•&ÔøΩ`{ÔøΩÔøΩ\u0005c\u0014ÔøΩ\u0011ÔøΩfÔøΩ#ÔøΩ4ÔøΩÔøΩiÔøΩ%b\u00021Ìä•9ÔøΩÔøΩÔøΩdÔøΩlÔøΩzÔøΩÔøΩ8ÔøΩÔøΩ{ÔøΩÔøΩ\u000f[ÔøΩxÔøΩÔøΩÔøΩb=nUGÔøΩcvÔøΩ!LÔøΩÔøΩÔøΩwÔøΩÿº=«Æ÷ò\u0011ÔøΩe\u0007ÔøΩ h0@v/^ÔøΩ#\u0010ÔøΩÔøΩÔøΩÔøΩÔøΩ3\u0019ÔøΩÔøΩ%\"tp&ÔøΩÔøΩÔøΩŸôplÔøΩÔøΩ.ÔøΩ#ÔøΩÔøΩ\u0002BÔøΩÔøΩÔøΩ≈ëÔøΩÔøΩÔøΩ|ÔøΩÔøΩcÔøΩ'ÔøΩÔøΩhÔøΩÔøΩ\u001a\u0016rÔøΩÔøΩÔøΩ¬£WÔøΩÔøΩÔøΩKÔøΩ\t‘å;BÔøΩÔøΩ\u0013ÔøΩfoHÔøΩSÔøΩ\u0015ÔøΩÔøΩchÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩUÔøΩ\u0000ÔøΩ\n{4HÔøΩÔøΩÔøΩJYR\nÔøΩ[/ÔøΩÔøΩ\u001a.nÔøΩ\nÔøΩ5}mÔøΩÔøΩK\u000fÔøΩ.ITÔøΩW6pVÔøΩÔøΩM\u0007[uÔøΩÔøΩ9PÔøΩ`KÔøΩ{D\tÔøΩÔøΩ:ÔøΩR#ÔøΩ_ÔøΩÔøΩÔøΩ\u001fÔøΩÔøΩÔøΩ\u0001\u001bÔøΩÔøΩÔøΩÔøΩzOÔøΩyÔøΩÔøΩÔøΩÔøΩeÔøΩÔøΩ{\u0016ÔøΩ\u000emÔøΩÔøΩÔøΩg⁄ÄÔøΩ8q\u0013.A\n9Õ∫4ÔøΩj\u0002ÔøΩÔøΩ[vÔøΩÔøΩ\u0017{T~Y[ÔøΩÔøΩÔøΩDZQ*ÔøΩﬁâ\u0013imÔøΩ*\u0019u\nÔøΩcXr(ÔøΩÔøΩgMÔøΩ7GÔøΩ\u0003ÔøΩQÔøΩ\u0005ÔøΩ\u0019jXckÔøΩÔøΩYÔøΩZÔøΩÔøΩÔøΩ[ÔøΩ^CmnÔøΩ^\u0015ÔøΩÔøΩeÔøΩ+\nÔøΩ\n\u0005\u0005ÔøΩÔøΩÔøΩÔøΩ6sÔøΩÔøΩÔøΩÔøΩ’∞,k*ÔøΩÔøΩÔøΩ6\nUfÔøΩ!ÔøΩS4VSÔøΩt[ÔøΩs—©ÔøΩ&\u00151ÔøΩ2\"\\ÔøΩÔøΩÔøΩ\tÔøΩ\tNÔøΩÔøΩÔøΩi%ÔøΩÿπM~ÔøΩJ^\u001f\u0005\u000fF\nÔøΩÔøΩÔøΩÔøΩ≈íÔøΩÔøΩ/\nÔøΩÔøΩÔøΩ\u000eÔøΩ\bÔøΩÔøΩ?\u001bÔøΩ\u0010ÔøΩ+QÔøΩbÔøΩÔøΩ\u0005ÔøΩ;-LÔøΩ3\u0000ÔøΩSG~!ÔøΩHÔøΩ$”é_ÔøΩ8∆©6w ÔøΩÀÑÔøΩÔøΩÔøΩÔøΩÔøΩ[\u0013:SMÔøΩÔøΩÔøΩ0ÔøΩ\"w*bf\u0016ÔøΩDÔøΩ\n]3yA\u0004‹∫¬∫iÔøΩoCÔøΩ$ÔøΩhrDv\u0017JÔøΩlTJ”¥ÔøΩ`\n\u00057ÔøΩÔøΩÔøΩDÔøΩÔøΩ\u0001}ÔøΩÔøΩ2ÔøΩ\u001fÔøΩÔøΩÔøΩ-ÔøΩÔøΩÔøΩÔøΩpÔøΩÔøΩ\u000fÔøΩpÔøΩ.ÔøΩc)ÔøΩÔøΩ&ÔøΩbBUÔøΩ47ÔøΩ“•\nÔøΩÔøΩ\u0018ÔøΩÔøΩ^ÔøΩyÔøΩv\nÔøΩÔøΩ{ÔøΩÔøΩ>}ÔøΩ-ÔøΩ\u0004ÔøΩÔøΩ\u0012I~ÔøΩtfÔøΩF\u0000ÔøΩ&ÔøΩdÔøΩÔøΩKvd=ÔøΩvÔøΩÔøΩÔøΩ4{ÔøΩ3\u0001h<\nÔøΩ XÔøΩÔøΩGÔøΩo\u0014\u001fÔøΩ&$?CoÔøΩ\nRÔøΩcÔøΩÔøΩÔøΩ\u0019ÔøΩRÔøΩÔøΩy[\u000e*ÔøΩ\u0001ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩJÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ?\u001bÔøΩ,m\u000f\u0003NeÔøΩ\u0013A%ÔøΩ#ÔøΩÔøΩ3=ÔøΩÔøΩ\b\u001aNÔøΩiLXX¬±¬äÔøΩ='GÔøΩÔøΩÔøΩÔøΩÔøΩ5ﬁπ#\u0017&ÔøΩÔøΩ<<iERÔøΩ)QÔøΩ‹≥ÔøΩ\u0002zÔøΩ=ÔøΩn<NÔøΩ?ﬂ∂G\n-XÔøΩÔøΩÔøΩDÔøΩeÔøΩ’≤ÔøΩ-AÔøΩÔøΩKÔøΩœÉfÔøΩyÔøΩÔøΩ\u0015ÔøΩcÔøΩÔøΩ\u0011NnÔøΩQHÔøΩÔøΩÔøΩÔøΩÔøΩ+Zs5ÔøΩuÔøΩHÔøΩ\bÔøΩJ\u000fÔøΩwe[x-ÔøΩÔøΩfGwÔøΩFS⁄¨A'ÔøΩd5ÔøΩÔøΩ5ÔøΩd0ÔøΩ\u0010ÔøΩÔøΩÔøΩ\n\u0017cYÔøΩ\u0003\u001a5yÔøΩÔøΩÔøΩ\\]\u0018{ÔøΩ6\\—¥\u0017LÔøΩmzxÔøΩÈ∏üb\u0012cÔøΩ_ÔøΩ&ÔøΩ ÔøΩDÀé\\\u0012ÔøΩÔøΩ–ãÔøΩÔøΩ6piÔøΩ\n\u0003B;ÔøΩ\u0013ÔøΩÔøΩnÔøΩoN,a-\u0010ÔøΩÔøΩ\n\u0019ÔøΩ!Hil)#◊¢=\u0011ÔøΩyÔøΩuÔøΩ\u001bÔøΩ\u0019ÔøΩÔøΩ\u0000\nÔøΩÃò+'(ÔøΩ&ÔøΩÔøΩ/mÔøΩÔøΩN’ê\u001a‘†c`ÔøΩÔøΩAzÔøΩÔøΩÔøΩjaÔøΩD\b+$V’∫&ÔøΩ\bÔøΩ\u001bÔøΩ\u0006ÔøΩ\u0015ÔøΩÔøΩ\u0005\u000eÔøΩ%W8ÔøΩŸÑÔøΩÔøΩDÔøΩKÔøΩÔøΩ;WÔøΩÔøΩhÔøΩ|MÔøΩp\tÔøΩÔøΩƒª.uÔøΩ\\ÔøΩ;ÔøΩaÔøΩÔøΩÔøΩ»¶\u0006ÔøΩ(ÔøΩÔøΩÔøΩÔøΩ\nÔøΩQDxvÔøΩOÔøΩÔøΩDÔøΩ\u0016\u0016\u001aÔøΩÔøΩBeq\u0018\u0015ÔøΩÔøΩl»ëÔøΩ\nÔøΩfÀÉNÔøΩzYÔøΩ\u0014ÔøΩ>7ÔøΩÔøΩ6ÔøΩv\n!ÔøΩÔøΩÔøΩÔøΩhÔøΩ,ÔøΩ\nÔøΩtÔøΩÔøΩ\u0007ÔøΩÔøΩ\u0010ÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩ3w>1\u0001ÔøΩ/ÔøΩÔøΩ_LÔøΩ=I…ã\u001a\u0001ÔøΩEf—ÖÔøΩÔøΩÔøΩ-ÔøΩÔøΩFÔøΩK.6ÔøΩœ∫ÔøΩÔøΩÔøΩ>{\u0007NÔøΩe\\ÔøΩ\u0005Ú±ªáÔøΩÔøΩÔøΩ?\u0000\\ÔøΩwTÔøΩzÔøΩÔøΩ]IÔøΩ*ÔøΩoÔøΩÔøΩpÔøΩÔøΩ+!ÔøΩTŒëKÔøΩ=ÔøΩ\u0001ÔøΩÔøΩ;ÔøΩ#ÔøΩ\u0000ÔøΩcÔøΩTW\u0013ÔøΩ5ÔøΩÔøΩ{ÔøΩ=-&0ÔøΩhqÔøΩ5ÔøΩÔøΩÔøΩÔøΩÔøΩSs04x‘ÅÔøΩYÔøΩÔøΩÔøΩ\u001fjÔøΩÔøΩ}ÔøΩÔøΩÔøΩ\tÔøΩÔøΩC+\nÔøΩ\u001fÔøΩÔøΩ√¥ÔøΩÔøΩÔøΩÔøΩ3Ó¥∫ÔøΩÔøΩ\nÔøΩ_~ÔøΩÔøΩEÔøΩEÔøΩÀ∏ÔøΩ\u0015ÔøΩ9ÔøΩÔøΩ|ÔøΩÀ∑ÔøΩÔøΩ5ÔøΩÔøΩ~ÔøΩÔøΩS–ûÔøΩB+QÔøΩt#\nVTÔøΩ\u0002ÔøΩ\u0005ÔøΩÔøΩÔøΩÔøΩX\u000e\nÔøΩÔøΩPÔøΩÔøΩK+zRi3D_M\u0018ÔøΩÔøΩ\u001fDCÔøΩ2@FÔøΩÔøΩ\u0010ÔøΩ%h\u0019;ÔøΩÔøΩTsÔøΩÔøΩÔøΩÔøΩÔøΩ\u0006ÔøΩ\u0007ÔøΩÔøΩR<\u001aOÔøΩÔøΩ&ÔøΩ\n\u0001\nt\u0015\u0016*ÔøΩÔøΩÔøΩ\u0005¬°$ÔøΩÔøΩÔøΩ—ΩÔøΩÔøΩÔøΩ54ÔøΩ{B N\u0003ÔøΩ÷∫WÔøΩÔøΩLÔøΩÔøΩ+,;6@vÔøΩjÔøΩmÔøΩÔøΩgcgÔøΩja|%#/“äUÔøΩÔøΩjÔøΩÔøΩÔøΩy2BÔøΩBÔøΩs\u0012cÔøΩÔøΩ.ÔøΩÔøΩÔøΩÔøΩtW6ÔøΩÔøΩhÔøΩÔøΩ\u0003ÔøΩÔøΩÔøΩTÔøΩ\u0014ÔøΩ_ÔøΩÔøΩƒ¥ÔøΩzÔøΩ\u0016\nd0ÔøΩÔøΩÔøΩ\u001fZKEÔøΩ#ÔøΩ0QÔøΩh]W\u0013ÔøΩÔøΩ\u001bÔøΩ5ÔøΩFÔøΩÔøΩU?nLCJPmÔøΩÔøΩ\u0011h0ÔøΩÔøΩNÔøΩz\\\u00154ÔøΩD :ÔøΩbe\nÔøΩdÔøΩ\n“å\\ÔøΩ\u0001SB\nÔøΩ'ÔøΩ{ZÔøΩ/ÔøΩ⁄ú+ÔøΩFÔøΩiÔøΩ\u00109:\u0011ÔøΩTÔøΩ+ÔøΩ6ÔøΩ3\u0017ÔøΩj\u001aGÔøΩ1ÔøΩ\u0015lIÔøΩ[gÔøΩÔøΩ\"4,4khÔøΩmÔøΩqÔøΩeÔøΩ7@ÔøΩÔøΩÔøΩF6ÔøΩÔøΩMu7ÔøΩÔøΩN(ÔøΩ\u000eÔøΩIÏµî(ÔøΩÔøΩ6ÔøΩÔøΩjXÔøΩ>ÔøΩÔøΩÔøΩÔøΩ\u0001E>zÔøΩÔøΩxÔøΩ_ÔøΩz7c]JsÔøΩÔøΩÔøΩÔøΩ0|ÔøΩ1IÔøΩ\u000fÔøΩÔøΩdÔøΩÔøΩV›ºÔøΩÔøΩ\u0007\bÔøΩSzÔøΩÔøΩÔøΩÔøΩ}gÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩW-\\dÔøΩÔøΩÔøΩÔøΩ*ÔøΩAÔøΩSÔøΩzpXÔøΩÔøΩ1{\u000fÔøΩÔøΩrÔøΩ…æÔøΩÔøΩÍµÆPÔøΩÕ≥ÔøΩHÔøΩA\bÔøΩÔøΩ~\u0001;BÔøΩOÔøΩÔøΩ_{ÔøΩÔøΩ_UÔøΩÔøΩÔøΩGÔøΩÔøΩÔøΩÔøΩQÔøΩyg4$\u0010BH(\"ÔøΩ(vÔøΩ:ÔøΩeÔøΩ6ÔøΩÔøΩ2ÔøΩŒ®ÔøΩ›õÔøΩÔøΩ\u0005ÔøΩ\u0002ÔøΩ\u0004EÔøΩ.\nTÔøΩÔøΩ&ÔøΩYÔøΩÔøΩÔøΩ[{=ÔøΩÔøΩ{sÔøΩÔøΩÔøΩdÔøΩÔøΩ[ÔøΩœÅÔøΩÔøΩsÔøΩÔøΩgÔøΩ’ûÔøΩ9ÔøΩnÀ°ÔøΩÔøΩÃ∏ÔøΩ{ÔøΩÔøΩlNÔøΩk–∫ÕáÔøΩ\nÃπÔøΩ«ÄÔøΩ;◊ø\nÔøΩ{ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩWÔøΩÔøΩ8;fÔøΩÔøΩV\u0015ÔøΩÔøΩÔøΩlÔøΩÔøΩÔøΩÔøΩ\nN>y√∂#ÔøΩ∆Ø\u000e\u000eÔøΩ\u0014ÔøΩ}ÔøΩr6ÔøΩcÔøΩÔøΩ6ÔøΩYyÔøΩÔøΩÔøΩ~ÔøΩÔøΩÔøΩWÔøΩÔøΩ\\ÔøΩ/ÔøΩÔøΩÔøΩA8\u000fVÔøΩÔøΩÔøΩ\u0006\nIÔøΩÔøΩ`ÔøΩ/ÔøΩdMÔøΩÔøΩt@ÔøΩÔøΩÔøΩ\nÔøΩ\u000f8v<^ÔøΩÔøΩe)ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0019ÔøΩÔøΩ\n\u0018@ÔøΩvÔøΩÔøΩcÔøΩddÔøΩDÔøΩÔøΩDÔøΩ\nÔøΩ\u0006ÔøΩ/P\u0005ÔøΩÔøΩ&ÔøΩ\u0012DÔøΩÔøΩÔøΩ\nÔøΩÔøΩ\nÔøΩÔøΩNhIÔøΩÔøΩÔøΩBÔøΩ;\u0004ÔøΩ*ÔøΩfÔøΩ3ÔøΩﬁ∂\u0003ÔøΩM,y\u0002/ÔøΩ/(ÔøΩZÔøΩÔøΩ\u000e!ÔøΩTq\bÃó\u0012kÔøΩ(bÔøΩÔøΩ\"ÔøΩPÔøΩ$TÔøΩÔøΩSÔøΩ?KÔøΩ\n9ÔøΩAÔøΩsÔøΩ&7ÔøΩHÔøΩÔøΩrÔøΩ\u0013ÔøΩsÔøΩ@@ÔøΩ9ÔøΩ}\u000e^ÔøΩÕºÔøΩL^ÔøΩÔøΩ\nÔøΩ\u0018ÔøΩ\u0011ÔøΩ@ÔøΩx]ÔøΩ\u0014ÔøΩHR:;\u0004\u0001m.\nÔøΩ\u0005ÔøΩ\u0018ÔøΩ6ÔøΩ7pÔøΩJÔøΩÔøΩhÔøΩ\u0014gÔøΩÔøΩa\t;ÔøΩ\u001fÔøΩÔøΩ\u0017lrÔøΩ‘©ÔøΩ/ÔøΩÔøΩRuÔøΩAFÔøΩÔøΩ\u00124MÔøΩÔøΩÔøΩ(P\u001beÔøΩÔøΩÔøΩ\n0,N.WÔøΩMÔøΩÔøΩÔøΩQÔøΩÔøΩj\nÔøΩWÔøΩoÔøΩÔøΩ1\tÔøΩPÔøΩÔøΩÔøΩVÔøΩÔøΩÔøΩ6$ÔøΩ\u0005ÔøΩ=\nÔøΩÔøΩ(\u0012ÔøΩWÔøΩcÔøΩC@ÔøΩÔøΩÔøΩ\u0010\u00058sÔøΩ…Å1+ﬁ£ÔøΩ6UÔøΩOÔøΩ]ÔøΩt\u0005ÔøΩÔøΩ;ÔøΩÔøΩ\\ÔøΩÔøΩÔøΩPWFÔøΩÔøΩÔøΩ~ÔøΩCÔøΩÔøΩÔøΩÔøΩ1+\u0016ÔøΩÔøΩ%FÔøΩÔøΩU_ÔøΩÔøΩ_oÔøΩÔøΩÔøΩÔøΩyÔøΩ\u000f\n=ÔøΩvÔøΩ!ÔøΩ\u0007g-ÔøΩ>tÔøΩ\u001fOÔøΩnÔøΩÔøΩ53\u0017mÔøΩÔøΩÔøΩ\u0006≈æ\u0006A\nVÔøΩÔøΩ√•\u000f\u000f\u000fÔøΩXÔøΩlÔøΩÔøΩ/\u0017nÔøΩÔøΩ0ÔøΩÔøΩ\nÔøΩÔøΩÔøΩŒÅtÔøΩaÔøΩÔøΩ\nnÔøΩÔøΩÔøΩÔøΩ3N\u0006\nQÔøΩ[z\u000eÔøΩÔøΩÔøΩ◊´fOÔøΩr√íÔøΩ~\n?s√£ÔøΩÔøΩj\u0014ÔøΩÔøΩOÔøΩÔøΩ]ÔøΩj7eƒÉ>]5`ÔøΩÔøΩ\u001fÔøΩ\u001f9ÔøΩÔøΩ„øú\u0019<aÔøΩ*ÔøΩÔøΩÔøΩÔøΩ\u0006~4ÔøΩÔøΩÔøΩ\nX‘ªÔøΩkzÔøΩÔøΩÔøΩÔøΩÔøΩM;ÔøΩÔøΩÔøΩÔøΩÔøΩOÔøΩÔøΩrÔøΩÔøΩzOyÔøΩrÔøΩÔøΩ=ÔøΩ?^ÔøΩÔøΩLÔøΩÔøΩÔøΩÔøΩ\u001f9~ÔøΩÔøΩÔøΩ?\nH09ÔøΩÔøΩÔøΩsÔøΩÔøΩ$%ÔøΩoU^◊∫S=3<ÔøΩ ÔøΩÔøΩ\u0018ÔøΩÔøΩP,\u0015~\tÔøΩWÔøΩMÔøΩÔøΩÔøΩ&+\u0004ÔøΩÔøΩÔøΩÔøΩ)jy3ÔøΩoT\u0006ÔøΩÔøΩÔøΩÔøΩ\u0007ÔøΩÔøΩÔøΩ5ÔøΩ|BcÔøΩ(\bÔøΩÔøΩ\u001b\u0001ÔøΩ7ÔøΩOÔøΩÔøΩ?ÔøΩÔøΩÔøΩÔøΩÔøΩ6ÔøΩ\u0007ÔøΩÔøΩÔøΩDgi+ÔøΩKÔøΩÔøΩÿá\nÔøΩvÔøΩÔøΩÔøΩ@ÔøΩaBHÔøΩzÔøΩp-aÔøΩÔøΩRÔøΩ9ÔøΩ\u0001⁄™\u0004\u0015ÔøΩÔøΩhÔøΩhÔøΩÔøΩaÔøΩ-ÔøΩÔøΩ\u0019:ÔøΩÔøΩD{go\bÔøΩ\nÔøΩÔøΩqÔøΩÔøΩÔøΩar:XÔøΩ\nÔøΩe)m$)ÔøΩÔøΩ8(ÔøΩ\u0002|\u0012ÔøΩÔøΩÔøΩ3ÃøÔøΩÔøΩy\n\u0010ÔøΩÔøΩBÔøΩhÔøΩmÔøΩ\u0011ÔøΩz]I4ÔøΩ\nV\u0005ÔøΩ\u001a-ÔøΩ\u0010\n+SÔøΩ\u0004\u0015ÔøΩ\u001bB\u0000\u0011Íá∫\nÔøΩ\u0001ÔøΩÔøΩ}vÔøΩÔøΩ9ÔøΩ€öÔøΩ*ÔøΩ[\u0015,>\u0012ÔøΩlÔøΩ\nÔøΩ\u0002ÔøΩl_ÔøΩÔøΩS\u000eÔøΩdFÔøΩ“É√∫ÔøΩ3\u0011\u0010mÔøΩB{\u000fG…ÆÔøΩIL+4\bÔøΩKÔøΩ\u000eÔøΩÔøΩ\u0002ÔøΩ\u0013ÔøΩÔøΩpÔøΩÔøΩYv>ÔøΩjÔøΩÔøΩÔøΩÔøΩ≈§$AGÔøΩÔøΩÔøΩÔøΩÔøΩhÔøΩÔøΩÔøΩ\u000eÔøΩÔøΩSÔøΩ*x\u0001—¨cÔøΩ6k%ÔøΩÔøΩ.f.‹Ü\nÔøΩÔøΩ+ÔøΩÔøΩjUﬁ§_ÔøΩÔøΩÔøΩ\u001fÔøΩ=5I)ÔøΩÔøΩi\\pÔøΩ»âÔøΩÔøΩ\n\u0015izÔøΩ\u0007ÔøΩ_UÔøΩÔøΩ\byaeÔøΩÔøΩaKŒûÔøΩÔøΩaÔøΩÔøΩﬁµsP@ÔøΩÔøΩÔøΩ>ÔøΩÔøΩ\u0017'DÔøΩÔøΩ\u0010ÔøΩÔøΩ≈ó\ne@\u0002;<ÔøΩÔøΩÔøΩ\u001f~ÔøΩ|ÔøΩﬁù{ÔøΩ\u0004ÔøΩÔøΩÔøΩÔøΩ6ÔøΩ\u0005EÔøΩDÔøΩÔøΩF\u0012ÔøΩR`V\n#ÔøΩÔøΩÔøΩ{ÔøΩ|4kÔøΩMÔøΩv\u001f€µÔøΩÔøΩÔøΩ\u0007,ÔøΩÔøΩÔøΩ1ÔøΩÔøΩÔøΩ+ÔøΩ\u0014UÔøΩR9ÔøΩwÔøΩ\nÔøΩnqÔøΩÔøΩÔøΩ5ÔøΩÔøΩÔøΩÔøΩÔøΩgT`x\u0012œÑÔøΩ\u001bÔøΩÔøΩÔøΩ\n\u0003ÔøΩÔøΩÔøΩlÔøΩÔøΩ\u001a0yÔøΩ\u000f\nÔøΩWÔøΩxvlÔøΩÔøΩÔøΩx|ÔøΩÔøΩÔøΩ?#ÔøΩÔøΩpÔøΩ“øÔøΩÊÖ±ÔøΩ!JÔøΩ)ÔøΩÔøΩÔøΩi∆êÔøΩÔøΩ-_”•ﬂµ]ÔøΩÔøΩÔøΩ‹†&cÔøΩÔøΩ6{&ÔøΩ5f»ÇÔøΩvÔøΩa\u0014\u0018ÔøΩÔøΩQK`ÔøΩ\nÔøΩKÔøΩÔøΩÔøΩÔøΩÔøΩQÔøΩ,0ÔøΩÔøΩyÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩJELÔøΩ9ÔøΩKÔøΩk\u0015ÔøΩ\u0016∆ú<VÔøΩ\u0015ÔøΩh‹ß»â(ÔøΩEZ\nkgÔøΩƒúPB\u0003Z\nÔøΩÔøΩÔøΩÔøΩ{NÔøΩ9%ÔøΩ\nÔøΩ/`ÔøΩÔøΩÔøΩ\u0015ÔøΩC0ÔøΩdD\u0014GÔøΩ%r›òÔøΩÔøΩ\u0016BÔøΩ\naQH.ÔøΩ2m\u000e05ÔøΩ{bÔøΩ5YvÔøΩ\u0002\u0013]ÔøΩI\u0018L%)ÔøΩ$ÔøΩÔøΩ>ÔøΩÔøΩ ÷≥YV\n\u0019ÔøΩÔøΩl9\n&]\u0013&WÔøΩÔøΩÔøΩ.\u0005ÔøΩ[()\u0002\b7ÔøΩÔøΩEÔøΩC@ÔøΩdÔøΩ5@ÔøΩ5ÔøΩÔøΩÔøΩ*ÔøΩ\nÔøΩaÔøΩccÔøΩ$ÔøΩÔøΩNÔøΩI\u0000—ßÔøΩÔøΩ`\u0012ÔøΩcÔøΩz8BiÔøΩV\u0002ÔøΩ;(ÔøΩiÔøΩÔøΩQN«•;ÔøΩqÔøΩÔøΩÔøΩ1‘úÔøΩ\u0007ÔøΩ◊§rÔøΩÔøΩWÔøΩÔøΩÔøΩ8^ÔøΩzÔøΩÔøΩÔøΩÔøΩÔøΩ4ÔøΩÔøΩÔøΩ8ÔøΩÔøΩ\u000eÔøΩ\u0013ÔøΩ/ÔøΩÔøΩ.!ÔøΩÔøΩÔøΩLU$ÔøΩ\nÔøΩÔøΩOÔøΩ\u000eÔøΩÔøΩÔøΩ!ÔøΩjÔøΩÔøΩ?ÔøΩ=hÔøΩÔøΩ√óÔøΩ@ÔøΩ\u0010ÔøΩ\u001a6ÔøΩÔøΩÔøΩÔøΩÔøΩ,ÔøΩÔøΩÔøΩ\u001f}ÔøΩÔøΩE}\u001aÔøΩÔøΩYÔøΩcÔøΩcOÔøΩ3\nIÔøΩÔøΩ=\u0007|ÔøΩÔøΩÔøΩÔøΩik‘àÔøΩVÔøΩÔøΩwgÔøΩ9ÔøΩÀ≤5?\u0016ÔøΩ?\u00143ÃîbÔøΩ^5;r}‰ÜåFBÔøΩÔøΩtÔøΩÔøΩw??ÔøΩÔøΩw>\u001f;cÔøΩÔøΩuÔøΩÔøΩ^ÔøΩÔøΩÔøΩ9\n\u0001ƒôÔøΩSÔøΩ\u0019ÔøΩT-ÔøΩ–üÔøΩÔøΩÔøΩ{ÔøΩÔøΩIcÔøΩÔøΩ]ÔøΩÔøΩÔøΩÔøΩfmÔøΩÔøΩG_ÔøΩ\u0005ÔøΩZÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\u0017ÔøΩÔøΩÔøΩÔøΩÔøΩ..yh8ÔøΩ#%ÔøΩ7\nÿ¥ÔøΩÔøΩ]\u001f\u001fÔøΩÔøΩ<ÔøΩ\u000fA2\u0016ÔøΩÔøΩWuÔøΩÔøΩbÔøΩohIÔøΩÔøΩÔøΩÔøΩ!ÔøΩ\n\u0006ÔøΩÔøΩÔøΩÔøΩ+_.ÔøΩ\u001aÔøΩ\nÔøΩZ\u001aÔøΩÔøΩ.\u000eÔøΩ\u0005ÔøΩ\u0019ÔøΩfÔøΩÔøΩÔøΩ.ÔøΩÔøΩVÔøΩiÔøΩÔøΩÔøΩÔøΩ@ÔøΩÔøΩmzÔøΩÔøΩSÔøΩÔøΩ>ÔøΩÔøΩ5SÔøΩ5ypÔøΩUÔøΩ+ÔøΩ4ÔøΩ>DIÔøΩÔøΩ4&&jr$&ÔøΩ\u0019,ÔøΩ\u0000kÔøΩ\u0005\u0012\u000e~pWÔøΩJLÔøΩ1n:[ÔøΩÔøΩ3hÔøΩÔøΩ)ÔøΩ<!ÔøΩNÔøΩ8ÔøΩÔøΩÔøΩxawÔøΩÔøΩÔøΩ<Q@\u000eÔøΩ%ÔøΩ\u0010ÔøΩÔøΩ\u0003YÔøΩ{mÔøΩÔøΩÔøΩ\u0017ÔøΩ\u0014–ÇÔøΩV\u0004OÔøΩHkÔøΩ\u001a\u000eÔøΩÔøΩÔøΩuÔøΩÔøΩÔøΩÔøΩ2»ø-ÔøΩÔøΩO_ÔøΩÔøΩOOÔøΩÔøΩJÔøΩ/ÔøΩÔøΩÔøΩÔøΩaÔøΩÔøΩÔøΩNRÔøΩÔøΩÔøΩÔøΩ>GZZÔøΩÔøΩ“•ÔøΩÔøΩ/ÔøΩK.ÔøΩkzÔøΩﬂ¨ÔøΩ8PB>mÔøΩÔøΩHf\u0007ÔøΩ\\\u0007\b\u0012jsTÔøΩÔøΩ√ä\u000eÔøΩcI*ÔøΩÔøΩxÔøΩWÔøΩ\bD[@ÔøΩBÔøΩ\u0017ÔøΩwAlÔøΩÔøΩ,%&\u0018ÔøΩÔøΩCÔøΩÔøΩK\u0015ÔøΩ4\u000e«∏ÔøΩ\u0012\u001a-E\u0014\u0016ÔøΩÔøΩÔøΩ€∑\u000e\bÔøΩÔøΩÔøΩMAc>A{ÔøΩBÔøΩ.9ÔøΩ3ÔøΩu…ä\n9WuÔøΩ?ÔøΩ\n\u001bÔøΩÔøΩHL4ÔøΩiÔøΩ\u0019ÔøΩŸÑ\u0010ÔøΩÔøΩ;ÔøΩÔøΩ,ÔøΩ!ÔøΩÔøΩTp\n8ÔøΩ\u0010BÔøΩg‰¨úaÔøΩ-ÔøΩƒõÔøΩNÔøΩÔøΩ \u0015ÔøΩ/_ÔøΩÔøΩxIÔøΩŒºÔøΩ«Ä'ﬂúÔøΩÔøΩ+ÔøΩÔøΩ–≠\u0001ÔøΩÔøΩÔøΩÔøΩiÔøΩÔøΩqÔøΩ\u000e\n9ÔøΩjÔøΩYaÔøΩÔøΩÔøΩÔøΩÔøΩn‹øÔøΩÔøΩÔøΩÔøΩﬁûFkÔøΩÔøΩ;\u001a\u0007ÔøΩ[ÔøΩÔøΩ4xV\u0010mÔøΩ+ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ1lÔøΩwIJÔøΩ6^Áøå\u001a2ÔøΩ[ÔøΩÔøΩ\u0017ÔøΩÔøΩMY<\u0010OÔøΩ\tÔøΩ\u0011ÔøΩ\u0000À¢?yÔøΩrÔøΩÔøΩGÔøΩ,ÔøΩÔøΩÔøΩ€ü_’π\nÔøΩŒîÔøΩ*ÔøΩÔøΩÔøΩ|ÔøΩNo‘Å\u0013ÔøΩnÔøΩwÔøΩÔøΩÔøΩﬂ∏ÔøΩÔøΩÔøΩgÔøΩÔøΩÔøΩtCÔøΩ∆∞LÔøΩÔøΩ5_\u0007>Y\u0016ÔøΩÔøΩÔøΩÔøΩ1ÔøΩ«≥ÔøΩÔøΩNszqÔøΩ\\ÔøΩMÔøΩﬂ¶ÔøΩÔøΩÔøΩÔøΩÔøΩQÔøΩÔøΩÔøΩÔøΩ\nÔøΩ\u0018\nÔøΩqÔøΩ\u0000F2V)ÔøΩ%?ÔøΩÔøΩ\u00060?ÔøΩ\n\\ÔøΩNÔøΩÔøΩ<ÔøΩÔøΩ_ÔøΩAc(\"OÔøΩÔøΩÔøΩj\bÔøΩ4zÔøΩ\nÔøΩÔøΩ\u0006ÔøΩ>ÔøΩi ?ÔøΩ)LhÔøΩÔøΩeE0ÔøΩÔøΩ]MÓëªÔøΩ<p\"ÔøΩJÔøΩ~ÔøΩ\u0000ÔøΩ3\u000fÔøΩqÔøΩ#ÔøΩ+\u0010ÔøΩ ÔøΩ\u0003(#ÔøΩP\nÔøΩyÔøΩ\n\u001a\u0001s^ÔøΩ2ÔøΩÔøΩÔøΩÔøΩgÔøΩ8\u0004f\u0001\n}(ÔøΩÔøΩ6ÔøΩÔøΩ'€∫ÔøΩ.N\u001f\u0004ÔøΩÔøΩÔøΩGÔøΩ¬öÔøΩÔøΩ3ÔøΩdf ÔøΩ4ÔøΩ`ÔøΩÔøΩ6hU.ÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩ\\ÔøΩÔøΩÔøΩVBOÔøΩÔøΩÔøΩWÔøΩs^\u0018pÔøΩ4IBÔøΩÔøΩdÔøΩ@ÔøΩÔøΩoN\u0011\u0017ÔøΩ6\nc$cÔøΩÔøΩ\u0010ÔøΩDn\u0015yRÔøΩgÔøΩÔøΩﬂ£ÔøΩ\u0012\\ÔøΩ\\\u0001xÔøΩ\u0018ÔøΩ_\u0006GÔøΩ;€Ω\u0011)\u0010ÔøΩ5UÔøΩÔøΩUÔøΩÔøΩÔøΩC16ÔøΩÔøΩ*ÔøΩÔøΩ\u0002ÔøΩl@ÔøΩÔøΩÔøΩ{ÔøΩÔøΩÔøΩFÔøΩ\u0019ÔøΩ<\u001aÔøΩ+ÔøΩÔøΩm\u001f}ÔøΩN”ØÔøΩ )ÔøΩ.ÔøΩ-MhÔøΩdÔøΩX\u0001\u0013\nÔøΩÔøΩ@ÔøΩÒ§Ñö5d\u0005\nBÔøΩÔøΩ(ÔøΩÔøΩdÔøΩ\tim\u0015ÔøΩVÔøΩOÔøΩÔøΩ3g/@ÔøΩÔøΩÔøΩÔøΩSÔøΩÔøΩÔøΩÀ®ÔøΩ\n\u0015ÔøΩ6ÔøΩ\nyÔøΩÔøΩÔøΩ\u0017?ÔøΩÔøΩÔøΩ‡Ø∑ÔøΩ;\u0018ÔøΩÔøΩCÔøΩÔøΩÔøΩzÔøΩÔøΩÔøΩdÔøΩŒΩ?ÔøΩÔøΩbÔøΩÔøΩ\n=bÔøΩ\u001aJBÔøΩ\u000fÔøΩÔøΩ\u001fÔøΩ\u001bÔøΩÔøΩ\u0014ÔøΩfÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩSÔøΩÔøΩ⁄ØmÔøΩ`4ÔøΩÔøΩÔøΩ=ÔøΩ?ÔøΩ{ÔøΩ'ÔøΩÔøΩ|ÔøΩdÔøΩÔøΩÔøΩÔøΩy\nÔøΩÔøΩÔøΩÔøΩoÔøΩÔøΩÔøΩÔøΩÔøΩY\nWÔøΩ\n>ÔøΩÔøΩ^/ÔøΩ/PÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩAÔøΩÔøΩ\u000eÔøΩÔøΩv\bÔøΩF#gBÔøΩ=ÔøΩ.ÔøΩÔøΩmÔøΩÔøΩÔøΩÔøΩÔøΩC\u0001CÔøΩÔøΩÔøΩik7\u001fÔøΩÔøΩ{5ÔøΩÔøΩÔøΩ}ÔøΩÔøΩÔøΩœΩ7#ÔøΩÔøΩ(\n;}ÔøΩ\u0005ÔøΩ\n&\u0016yÔøΩÔøΩ\u0013=x\u0018>ÔøΩAÔøΩÔøΩ…ìÔøΩwÔøΩÔøΩ1ÔøΩL;ÔøΩÔøΩÔøΩ@-ÔøΩÔøΩM;ÔøΩÔøΩUÔøΩÔøΩÔøΩDÔøΩFÔøΩÔøΩ\u0019ÔøΩGÔøΩ\u000e3ÔøΩ\\\bNÔøΩ&\u0010?]&ÔøΩ\u0002KÔøΩÔøΩÔøΩÔøΩ\u00182ÔøΩ,ÔøΩr\u0006…íÔøΩÔøΩÔøΩhÔøΩhÔøΩ\u0000MÔøΩ\u0002\nc<]Q\u0015feÔøΩY ÔøΩ!\u0000;ÔøΩÔøΩÔøΩ ÔøΩ\\ÔøΩÔøΩÔøΩO$ÔøΩ$ÔøΩYÔøΩÔøΩ\u0014ÔøΩW*5VÔøΩJÔøΩ\b\bÔøΩÔøΩÔøΩÔøΩPÔøΩ\nÔøΩpÔøΩ\u0018ÔøΩÔøΩÔøΩi\u0005ÔøΩÔøΩA6)ÔøΩ\u0010ÔøΩ@ÔøΩ1\u001a\"=.ÔøΩxr\u001bÔøΩDcÔøΩÔøΩÔøΩ≈ârÔøΩÔøΩ√£ÔøΩT[hÔøΩÔøΩ\u001aI<ÔøΩ_:ÔøΩ[S\u0006ÔøΩE<◊ã\u0018`ÔøΩÔøΩ\u0018JÔøΩzÔøΩÔøΩINÔøΩ.ÔøΩÔøΩÔøΩÔøΩ1ÔøΩÔøΩ2ÔøΩÔøΩ&!R\n\u0002}#2ÔøΩ|ÔøΩÔøΩ\u0000HÔøΩdÔøΩ!ÔøΩ\tMÔøΩÔøΩ)›±ÔøΩ?\tÔøΩ\u001bÔøΩÔøΩÔøΩA',W\u0002ÔøΩÔøΩÔøΩÔøΩ3ÔøΩpLÔøΩQYhÔøΩ&ÔøΩ3ÔøΩÔøΩc*cZÔøΩÔøΩÔøΩÔøΩSÔøΩÔøΩ\nÔøΩAÔøΩeNcÔøΩ$ÔøΩÔøΩ\n9^ÔøΩÔøΩU1ÔøΩNÔøΩÔøΩ>«≤d\u0001\u000f\u0011\nÔøΩÔøΩ;ÔøΩcZ\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ_ÔøΩCÔøΩj<0feÔøΩ\u001bÔøΩÔøΩ'SÔøΩgÔøΩÔøΩÔøΩÔøΩ\nÔøΩoÔøΩcÔøΩ\u0000`ÔøΩxGÔøΩ\u001b\u000fR>X?jÔøΩÔøΩy[\u0000i)(ÔøΩÔøΩ4ÔøΩÔøΩ1ÔøΩ◊ÖÔøΩ\nÔøΩÔøΩ\u0018yÔøΩ»â\u0013ÔøΩœØÔøΩzÔøΩÔøΩÔøΩÔøΩÔøΩ\u0005uXi-:»±VÔøΩ\u0012ÔøΩ=ÔøΩÔøΩ\u0004ÔøΩ.fÔøΩÔøΩÔøΩaÔøΩXÔøΩ÷µ]ÔøΩ-[ÔøΩÔøΩ€çÔøΩÔøΩ\\ÔøΩm›ñCÔøΩv\nÔøΩÔøΩÔøΩÔøΩa \\ÔøΩÔøΩ\u0003«≠ÔøΩpÔøΩrÕ∞ÔøΩ\u0013gnÔøΩÔøΩÔøΩhÔøΩ\u0011ÔøΩÔøΩ·ìøÔøΩkXÔøΩÔøΩ ›µÔøΩÔøΩÔøΩZÔøΩO^ﬂ≠ÔøΩ_ÔøΩÔøΩ\nÔøΩ\u0018ÔøΩrÔøΩ|ÔøΩÔøΩÔøΩ`ÔøΩ7a\nÔøΩ\u0011ÔøΩ/]ÔøΩÔøΩmdOÔøΩ9-N9\nÔøΩÔøΩÏúùpeJÔøΩKl~z4 ÔøΩLcndÔøΩ26ÔøΩ„ëÄlÔøΩ\u0014\u0003q\u0010XvÔøΩu'\u001a'oÔøΩÔøΩÔøΩ\u0018ÔøΩ\u0011!ÔøΩ%ÔøΩÔøΩ\u001a\nÔøΩÔøΩ(A\u0018~ äzÔøΩ?\u000enÔøΩu\u0013ÔøΩ=N!ÔøΩÔøΩÔøΩQÔøΩÔøΩÔøΩ\u0004c\u0015=ÔøΩÔøΩ \u001f\u0005+ÔøΩ8ÔøΩ\u0015ÔøΩÔøΩ@$ÔøΩaÔøΩ3ÔøΩÔøΩoÔøΩÔøΩd}ÔøΩ8\u0003EÔøΩ}ÔøΩ[\nÔøΩÔøΩbÔøΩÔøΩÔøΩN\nÔøΩÔøΩÔøΩ p)ÔøΩCÔøΩÔøΩ€ô\bB\u0003ÔøΩ\u001bDÔøΩpÔøΩÔøΩ&qÔøΩOÔøΩÔøΩÔøΩ|LÔøΩÔøΩzÔøΩÔøΩwÔøΩ$ÔøΩA\u0005\u0002\nsÔøΩÔøΩÔøΩ0ÔøΩZ|QÔøΩÔøΩÔøΩ\u00072QÔøΩWÔøΩÔøΩxÔøΩVÔøΩÔøΩ\"c\u001bÔøΩ|UcÔøΩÔøΩÔøΩ\u00171ÔøΩÔøΩÔøΩ:(\u0017S\u0007ÔøΩJÔøΩÔøΩ:\u0004ÔøΩ\nÔøΩÔøΩ7ÔøΩÔøΩ7a_>IÔøΩBc9ÔøΩTEÔøΩ&ÔøΩMÔøΩÔøΩaLÔøΩpÔøΩ)ÔøΩÔøΩ\u0000ÔøΩÔøΩ\u0001\u0011OÔøΩÔøΩÔøΩ`\u0014ÔøΩÔøΩBF√ác|ÔøΩPÔøΩ\"ÔøΩÔøΩÔøΩ\n:IÔøΩÃ≥Nw@ÔøΩ*hÔøΩÔøΩ\u0002\nÔøΩÔøΩÔøΩ@:` ÔøΩ\u0016ÔøΩa%<ÔøΩÔøΩ^ÔøΩtÔøΩÔøΩ\nl0ÔøΩ\u0006^w{√µ]ÔøΩ!kÔøΩ;ÔøΩÔøΩ@aÔøΩÔøΩy4\"ÔøΩÔøΩÔøΩÔøΩÔøΩ#ÔøΩÔøΩÔøΩ’ùÔøΩ'ÔøΩ‹Ä&)ÔøΩÔøΩÔøΩÔøΩÔøΩ{^\nÔøΩ\u0016ÔøΩÔøΩyÔøΩÔøΩÔøΩsÔøΩÔøΩÔøΩÔøΩ›ÉÔøΩÔøΩTÔøΩÔøΩÔøΩ\u001f{>7ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ0j≈Ø'ÔøΩ\u000e\u0018ÔøΩÍ©∑?ÔøΩ\b!ÔøΩÔøΩÔøΩÔøΩÔøΩaK~<ÔøΩÀíÔøΩ~|ÔøΩÔøΩÔøΩÔøΩ;ÔøΩÔøΩ.ÔøΩÔøΩwÔøΩ\n\u000eÔøΩyÔøΩ'GÔøΩzÔøΩÔøΩ¬ïÔøΩ\n~mÔøΩÔøΩ9?ÔøΩ*\u0015ÔøΩÔøΩ{ÔøΩ?gÔøΩÔøΩoÔøΩÕ£ÔøΩ;ÔøΩ,ÔøΩWÔøΩÔøΩdÔøΩÔøΩ\nÔøΩcVÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩS}ÔøΩ|ÔøΩqAhWÔøΩÔøΩÔøΩW+ÔøΩÔøΩ€ºÔøΩÔøΩ[ÔøΩÔøΩÔøΩÔøΩjÂèçÔøΩ‘ûÔøΩPmÔøΩÔøΩ\nÔøΩÔøΩÔøΩ+ÔøΩÔøΩÔøΩJÔøΩÔøΩÔøΩyÔøΩ√ôÔøΩkÔøΩÔøΩÔøΩÔøΩ+tÔøΩ4ÔøΩÔøΩ\u0013'œ°ÔøΩ|ÔøΩÔøΩÔøΩ◊™ÔøΩÔøΩ\u0001xÔøΩrÔøΩ^ÔøΩa\n)\"ÔøΩ5P\nÔøΩQ–¶\u0012ÔøΩÔøΩ#@1ÔøΩPÔøΩÔøΩURÔøΩtu@ÔøΩyÔøΩ\b@ÔøΩEÔøΩl\bÔøΩ\n|ÔøΩ\\ÔøΩÔøΩ’ÑÔøΩ20\u0016ÔøΩÔøΩÔøΩBJ:ÔøΩÔøΩDÔøΩÔøΩRÔøΩm9ÔøΩ\u001fTÔøΩ\nÔøΩÔøΩÔøΩÔøΩ0EO3ÔøΩÔøΩ\u0006ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0018\u0013]\nÔøΩÔøΩXÔøΩ\u000fjÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩeÔøΩÔøΩ\u0007ÔøΩkÔøΩa:ÔøΩÔøΩ@\u0007ÔøΩ@2ÔøΩ\u0013\u0013\u0011ÔøΩ57A2ÔøΩÔøΩŸπÔøΩ%»© ¥ÔøΩeÔøΩÔøΩQÔøΩÔøΩv'\u0014·ú≠jGÔøΩ\nÔøΩÔøΩ!GrÔøΩÔøΩ»í]\u001agmÔøΩ\u001bÔøΩ\u0007ÔøΩÔøΩ$ÔøΩÔøΩ48ÔøΩ8AÔøΩÔøΩÔøΩlN\u0001ÔøΩÔøΩ4ÔøΩB1ÔøΩaÔøΩ\nÔøΩ=ÔøΩ\u0018ÔøΩYa?3ÔøΩ1ÔøΩC\u0010ÔøΩÔøΩ{H∆®ÔøΩ`c5\tÔøΩ*ÔøΩÔøΩÔøΩ_U\nÔøΩÔøΩB—≤ÔøΩÔøΩÔøΩw\u0011ÔøΩ\u0019ÔøΩ0\u0014\nÔøΩm>\nk\u0014ÔøΩt-ÔøΩtÔøΩ\bÔøΩ24ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u000fÔøΩÔøΩÔøΩÔøΩÔøΩ\u000fÔøΩÔøΩe7ÔøΩÔøΩUw%\nZÔøΩÔøΩ$DÔøΩk‡®ë\u000eAÔøΩ\u001aÔøΩx\nÔøΩÔøΩ:J\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩb\u0018\u0018ÔøΩ\u0000√ÇÔøΩÔøΩ+ÔøΩT\u0015ÔøΩ~◊£ÔøΩÔøΩmÔøΩÔøΩ>ÔøΩOÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩPÔøΩ@ÔøΩÔøΩ_ﬁûVÔøΩÔøΩ'ÔøΩ3nÔøΩzÔøΩÔøΩÔøΩ\nﬂ™ÔøΩ{kÔøΩÔøΩ3\u0017nÔøΩ8kcnGÔøΩÔøΩÔøΩ·ÆÅ{FV{%ÔøΩlSYÔøΩÔøΩÔøΩSÔøΩÔøΩtÔøΩÔøΩÔøΩ9Kvt}jt~ÔøΩÔøΩXÔøΩÔøΩ^ÔøΩÔøΩr›æÔøΩ{ÔøΩdÔøΩÔøΩÔøΩ:gÔøΩ\u001fÔøΩ\nÔøΩÔøΩ\u0006”îÔøΩÔøΩP?÷∫ÔøΩÔøΩ2ÔøΩ€ü\u001aM78x¬∑ÔøΩ\u0017ÔøΩHÔøΩ\u001aÔøΩÔøΩ\u001f|ÔøΩU\u001bÔøΩSÔøΩ;ÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩb\u0016ÔøΩ7ÔøΩaÔøΩ\nzÔøΩnÔøΩs@ÔøΩwÔøΩÔøΩ{ÔøΩ3ÔøΩŒ†ÔøΩÔøΩÔøΩ\u0017'ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩk\\xÔøΩÔøΩÔøΩ._ÔøΩrÔøΩÔøΩÔøΩÔøΩQÀ¥ÔøΩÔøΩÔøΩrSÔøΩ-[ÔøΩEÔøΩ.\bmÔøΩÔøΩÔøΩr>#hXÔøΩÔøΩJcÔøΩ)E=ÔøΩÔøΩBz\u0010\u0016ÔøΩÃíÔøΩ\u0014ÔøΩœÜfÔøΩÔøΩLÔøΩÔøΩÔøΩ\u0014;H-NÔøΩvÔøΩ≈ÑÔøΩÔøΩ^ÔøΩÔøΩÔøΩ8F[ÔøΩUEÔøΩÔøΩemTrÔøΩ\u0017VqaÔøΩ<[ÔøΩÔøΩ(wNmÔøΩjÔøΩÔøΩV„•àzÔøΩÔøΩÔøΩÔøΩÔøΩ\u0017ÔøΩÔøΩÔøΩfÔøΩÔøΩ6ÔøΩ\u0003ÔøΩ\u0017ÔøΩÔøΩGÔøΩC\n\u0002BJÔøΩCÔøΩKÔøΩnBTÔøΩ1\u00016<ÔøΩ(ÔøΩ\u0006\nÔøΩÔøΩ!ÔøΩÔøΩÔøΩZdÔøΩ\n1ÔøΩ=nÔøΩc$ÔøΩT\thÔøΩÔøΩ\nj'ÔøΩ0}ÔøΩIÔøΩÔøΩwÔøΩ~ÔøΩÔøΩÔøΩÔøΩÔøΩiÔøΩ\u0006ÔøΩÔøΩÔøΩÔøΩ&#\u000fV\u0005“µÔøΩl@>IÔøΩ6W\u0003…ï[*\n\u0002ÔøΩrÔøΩÔøΩ\\\u0007\u0016\u001aÔøΩÔøΩ~ÔøΩP\u0017\u0001ÔøΩÔøΩÔøΩC%ÔøΩÔøΩÀ¥ÔøΩÔøΩP\"ÔøΩÔøΩ\u0019ÔøΩ*√õ\u001bTÔøΩÔøΩ\u0007ÔøΩÔøΩÔøΩSÔøΩÔøΩ|ÔøΩÔøΩ\u001bÔøΩÔøΩ\u0016\u001bÔøΩTÔøΩ<ÔøΩÔøΩ\u0004`\"ÔøΩQ\u000eÔøΩÔøΩÔøΩ.ÔøΩ:ÔøΩÔøΩ9sw\n#ÔøΩ\u0018i@ÔøΩÔøΩn6dÔøΩpMF(ÔøΩÔøΩQÔøΩ@3ÔøΩMÔøΩÔøΩÔøΩ_@5O%\u0003ÔøΩ]ÔøΩÔøΩ+ÔøΩÔøΩcfÔøΩÔøΩÔøΩrÔøΩÔøΩÔøΩÔøΩ?w~rTrÔøΩ!^ÔøΩk\nÔøΩX6r⁄öÔøΩ\u000f\u000fÔøΩPgÔøΩÔøΩ+ÔøΩÔøΩ.ÔøΩ\u0014ÔøΩÔøΩjÔøΩÔøΩ\nÔøΩÔøΩÔøΩŒÑ2ÔøΩ\u0007^ÔøΩlÔøΩÔøΩÔøΩÔøΩ~>ÔøΩ≈Ç-ÔøΩÔøΩ8Ahr≈∏ÔøΩÔøΩooÔøΩÔøΩÔøΩÔøΩÔøΩ!\u000e\u0018ÔøΩÔøΩÔøΩ>3ÔøΩ\nKÔøΩÔøΩIÔøΩ \\L<*ÔøΩÔøΩ[z\u000eÔøΩOhÔøΩÔøΩ0ÔøΩÔøΩ<ÔøΩÔøΩÔøΩ.ÔøΩÔøΩÔøΩqÔøΩ<\u0003ÔøΩÔøΩUÔøΩÔøΩ}wÔøΩÔøΩoÔøΩ\n<zrÔøΩ‘µ\nÔøΩÔøΩfÔøΩ\nÔøΩ\nÔøΩÔøΩÔøΩÔøΩ'ÔøΩÔøΩÿ™ÔøΩNÔøΩÔøΩÔøΩ<_ÔøΩÔøΩqFÔøΩÔøΩ~,-ÔøΩ{ÔøΩ\nNÔøΩ>ÔøΩÔøΩ3eÔøΩ#ÔøΩ|ÔøΩÔøΩOIƒ¥eÔøΩ!WÔøΩÔøΩ ÔøΩ9ÔøΩH&\u0017\u0012#ÔøΩ\tÔøΩJ#ÔøΩ/ÔøΩ\u0013\u0012ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩEÔøΩÔøΩIt3\u001aÔøΩÔøΩÔøΩ4ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩŒ¥\u0002ÔøΩÔøΩÔøΩ\u0014ÔøΩf ¥ÔøΩa_ÔøΩ[ÔøΩk2ÔøΩ]ÕëÔøΩ0lf\u0016!ÔøΩÔøΩ\n\u0001ÔøΩ`\u0004\ns»ÄÔøΩYÔøΩÔøΩÔøΩÔøΩtBÔøΩÔøΩÔøΩDCSÔøΩÔøΩÔøΩÔøΩÔøΩ?DÔøΩ:ÔøΩÔøΩTF;k_%ÔøΩuÔøΩn*ÔøΩÔøΩ\u001aÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ-ÔøΩ\"ÔøΩCp1\u0014D\u0006ÔøΩÔøΩ>,«πÔøΩ[smÔøΩXÔøΩWÔøΩÔøΩfÔøΩ<%+\u0002ÔøΩÔøΩhÔøΩv)ÔøΩÔøΩÔøΩ+ItNÔøΩ\\ÔøΩ-ÔøΩÔøΩt\u0014Nx*ÔøΩH\tMÔøΩÔøΩK≈ÄS\\\u001f\u0002\u0014-ÔøΩÔøΩ \bÔøΩg\\\nÔøΩ\u001fÔøΩ—¨8\u0004/ÔøΩoÔøΩÔøΩUÔøΩ`=m>ÔøΩÔøΩÔøΩÔøΩÔøΩ\u00172ÔøΩzÔøΩÔøΩÔøΩ\n\u0004ÔøΩ9\nÔøΩ $À°ÔøΩBPÔøΩÔøΩz~ÔøΩgK%ÔøΩ,ÔøΩÔøΩ\u0012ÔøΩ(:'\u0000ÔøΩÔøΩÔøΩ]ÔøΩÔøΩÔøΩO@\u0010ÔøΩ\u001b(x6ÔøΩÔøΩT\n\u0002ÔøΩdT\u0000ÔøΩY\u0003ÔøΩ\na<\u001b{I2“∑ÔøΩ\u0012ÔøΩbÔøΩ\u0014m.I4ZÔøΩÔøΩ\nÔøΩ\"ÔøΩ‡πéÔøΩ3ÔøΩÔøΩ\u0003\u001fÔøΩÔøΩrÔøΩÔøΩ\u0015\tÔøΩ={ÔøΩÔøΩ#ÔøΩO\u000eÔøΩ\u0006‰µÆÔøΩ\\O{Ô∫ÆÔøΩ^ÔøΩ`ÔøΩÔøΩ#'ÔøΩxfÔøΩÔøΩÔøΩÔøΩ^\u0018ÔøΩmÔøΩÔøΩ·ìøWÔøΩ\u001bEÔøΩÔøΩ\u0015B*ÔøΩÔøΩÔøΩÔøΩW&}ÔøΩÔøΩ»£ÔøΩ-ÔøΩÔøΩÔøΩ;ÔøΩoÔøΩ90ÔøΩ]ÔøΩÔøΩ\u0010ÔøΩ9eÔøΩ\nVÔøΩ\u001a:ÔøΩo7ÏøéÔøΩÔøΩ8\u0006ÔøΩ?ÔøΩÔøΩÔøΩG-_ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩL?NiZDÔøΩÔøΩk\nÔøΩ\u0019ÔøΩzÔøΩÔøΩÔøΩ\u001b\u000f\n8rÔøΩÔøΩ/7T<>ÔøΩ…≤jÔøΩ\u0016ÔøΩYR}ÔøΩÔøΩÔøΩÔøΩÔøΩV<>\u0002ÔøΩ_KÔøΩ€ª`ÔøΩÓµõ\u000e-XÔøΩÔøΩÔøΩÔøΩÔøΩK]'ÔøΩÔøΩÔøΩÔøΩ\nŒùÔøΩÔøΩaÔøΩÔøΩ_ÔøΩ:wsASpt|ÔøΩÏ∏±\u00170EÔøΩ.ÔøΩÔøΩnFnÔøΩ^ÔøΩÔøΩ\u0018\u0017\u0019D-AÔøΩ'r$5I\u0001\"ÔøΩZ!ÔøΩÔøΩqÔøΩ'ÔøΩjÔøΩD}ÔøΩÔøΩÔøΩK7ÔøΩcVCÔøΩBf\u0003cÔøΩ\nNÔøΩ⁄†WÔøΩ\u001fÔøΩj\"\u0019t|K$3ÔøΩ<\n\u0004ÔøΩ\nÔøΩAÔøΩ*ÔøΩEÔøΩ@ÔøΩ[ŸïVÔøΩÔøΩmeÔøΩ◊°ÔøΩ\u0005JÔøΩÔøΩÔøΩÔøΩ\u0018>…∂:\u0018S\t\u0000ÔøΩ\u0003QNÔøΩ(ÔøΩÔøΩJÔøΩ…†ÔøΩÔøΩ&ÔøΩÔøΩÔøΩNX)ÔøΩtÔøΩ\"ÔøΩ3WÔøΩ\n\u0007[\u0006ÔøΩ|*MÔøΩKÔøΩÔøΩt\u000eÔøΩÔøΩÔøΩsÔøΩRXÔøΩÔøΩ\u0013ÔøΩ0;ÔøΩ\u001aÔøΩ9ÔøΩuÔøΩÔøΩÔøΩÔøΩbEÔøΩ\nl2ÔøΩÔøΩQb V\u0015ÔøΩ…ú%ZV%fÔøΩPÔøΩÔøΩÔøΩq\bÔøΩ\u0005ÔøΩQÔøΩKÔøΩ\u0013‹π\tÔøΩÔøΩ*ÔøΩÔøΩv/DI\\NÔøΩC`|\nÔøΩNÔøΩÔøΩHÔøΩdVÔøΩ\u0016ÔøΩ\u0017ÔøΩ+ÔøΩD.ÔøΩÔøΩÔøΩ]6ÔøΩ\u001bQMLÔøΩÔøΩTÔøΩ√ôÔøΩÔøΩÔøΩpÔøΩÔøΩtÔøΩÔøΩÔøΩÔøΩZ6ÔøΩXtÔøΩÔøΩ\u001b≈Æ2ÔøΩPÔøΩÔøΩ\nRÔøΩLÔøΩkHÔøΩ\u0010!}\"ÔøΩÔøΩÔøΩ8\u0016ÔøΩÔøΩYÔøΩrÔøΩÔøΩ\u0003\u0006ÔøΩCÔøΩR\u0004cUx›ûÔøΩÔøΩÔøΩCÔøΩÔøΩm>ÔøΩÔøΩbBwÔøΩÔøΩÔøΩ\u0013ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩIkEÔøΩÔøΩfÔøΩ\u0010ÔøΩÔøΩÔøΩhÔøΩumÔøΩ];gŸöÔøΩÔøΩkÔøΩNRRÔøΩ^œèÔøΩÔøΩ[ÔøΩÔøΩVÔøΩu]ÔøΩ\u001aEIÔøΩÔøΩ=ÔøΩ7ÔøΩ<ÔøΩÔøΩ\u0017ÔøΩ\u001fzuÔøΩZC\u000e{FEÔøΩ\u000fÔøΩwÔøΩƒº\u0015;g}ÔøΩÔøΩÔøΩ3c\u0013vFÔøΩ=ÔøΩÔøΩ!ﬂ¨ÔøΩ3b öÔøΩÔøΩN=ÔøΩ⁄§XÔøΩ\u0014]ÔøΩÔøΩÔøΩÔøΩbÔøΩÔøΩÕªÔøΩÔøΩÔøΩnÀ°ÔøΩ\u001a\u0017ÔøΩ<4L=ÔøΩUÔøΩ[ÔøΩÔøΩK\n\u001aÔøΩoÔøΩÔøΩ_ÔøΩÔøΩ<ÔøΩC\u001fcÔøΩcÔøΩÔøΩ>ÔøΩÔøΩÔøΩ7kÔøΩnÔøΩ~$ÔøΩ\"\u0015ÔøΩÔøΩ«ìÔøΩ;ÔøΩÔøΩÔøΩÔøΩW»ÆÔøΩÔøΩÔøΩ}ÔøΩÔøΩÔøΩ9ÔøΩÔøΩnÔøΩ`ÔøΩÔøΩ&\"ÔøΩ\u0012ÔøΩÔøΩ…ìL\u0015ÔøΩdÔøΩÔøΩBÔøΩB9\u001bÔøΩÔøΩ]ÔøΩDgÔøΩJPÔøΩÔøΩF$|ÔøΩÔøΩ\u0005ÔøΩÔøΩ»ûÔøΩ*QÔøΩÔøΩDÔøΩNXÔøΩZR1ÔøΩ&ÔøΩ\u0012-4\u0014\u0016(Ot7ÔøΩGÔøΩ ∆ïÔøΩÔøΩÔøΩ\b1M”ûÔøΩÔøΩ\u0005ÔøΩ\u0010ÔøΩ\n{A2ÔøΩÔøΩ?bÔøΩ ÔøΩoÔøΩ\tEÔøΩ\\ÔøΩ3ÔøΩÔøΩ\"o\u0019HÔøΩl\u0005ÔøΩÔøΩ\bqÔøΩ\u0010“ΩÔøΩdÔøΩa\u0014?>ÔøΩy,c\nÔøΩ\u001aÔøΩÔøΩŒáÔøΩÔøΩÔøΩ\u0002<HÔøΩÔøΩÔøΩ√™ÔøΩÔøΩin4ÔøΩ\u0000\u0003ÔøΩMÔøΩÔøΩÃ•rÔøΩ\u0006ÔøΩXÔøΩ%KÔøΩ\u000eÔøΩÔøΩÔøΩ\"8ÔøΩ\\fÔøΩ\u0000ÔøΩÔøΩ\u0018\nÔøΩÔøΩÔøΩ\u000eÔøΩ>ÔøΩOÔøΩÔøΩ\u0010ÔøΩÔøΩSÔøΩDÔøΩ’õ\u000e!ﬁÉ–†ÔøΩ\nuÔøΩÔøΩÔøΩ3A\u0006\nÔøΩÔøΩ.?ÔøΩ$ZƒçÕÅÔøΩ+xÔøΩPÔøΩ\u0015ÔøΩÔøΩÔøΩVÔøΩ\u0001ÔøΩrÔøΩ ÔøΩ\nÔøΩ!\u0019V(MÔøΩ~\u0017!ÔøΩÔøΩ∆´ÔøΩÔøΩ\u0014ÔøΩÔøΩ\u0016ÔøΩcÔøΩÔøΩeÔøΩÔøΩQ\nÔøΩsÔøΩmÔøΩTkMq\bRÔøΩYÔøΩÔøΩHÔøΩ :nÔøΩÔøΩ6ÔøΩÔøΩÔøΩbÔøΩÔøΩÔøΩÔøΩÔøΩ-ÔøΩÔøΩZ0ÔøΩEwZ\u0010c0ÏèΩ\u0006oÔøΩs\n\u0000^ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÀ∞\u0016\u001b-⁄´ÔøΩÔøΩKÔøΩ<\nÔøΩ-mÔøΩaÔøΩÔøΩÔøΩÔøΩ‹∏\n?ÔøΩ‘ìGÔøΩÔøΩÔøΩTÔøΩV`.%ÔøΩÔøΩgÔøΩp}ÔøΩÔøΩÔøΩA\bz[ÔøΩÔøΩ7ÔøΩÔøΩÔøΩc?ÔøΩﬁ≤ÔøΩXÔøΩÔøΩK:=1ÔøΩÔøΩÔøΩÔøΩ\u0012UÔøΩEÔøΩÔøΩwÔøΩÔøΩ\u001bÔøΩ5>ÔøΩÔøΩ4 µÔøΩ·∂π(ÔøΩLÔøΩ\u0005”ÉkÔøΩÔøΩÔøΩy\nÔøΩoÔøΩÕõ:wÔøΩ’ùÔøΩÔøΩÔøΩÔøΩ/oÔøΩs\u0000ÔøΩMÔøΩÔøΩÔøΩE]Ëñû\u0003z<;vÔøΩÔøΩÔøΩÔøΩ\u000eÔøΩzÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ_ÔøΩÔøΩÔøΩÔøΩÔøΩiÔøΩ–∞?ÔøΩ=(ÔøΩ/\u0018{cÔøΩUÔøΩ ÔøΩ\u0013ÔøΩÔøΩÔøΩÊ¢≠ÔøΩ{Pÿï\\”•ÔøΩ'S÷úÔøΩÔøΩÔøΩÔøΩÔøΩ~ÔøΩÔøΩÔøΩÔøΩÕ∂ÔøΩr}ÔøΩsÔøΩ\u0012\u001bÔøΩa(\u0002ÔøΩÔøΩÔøΩ+]ÔøΩ`UÔøΩÔøΩ\u0003ÔøΩÔøΩ\u0010∆§1UcÔøΩÔøΩ\n\u0007\u0018\u0018\nÔøΩÔøΩ9\"ÔøΩÔøΩeÔøΩ\u0018ÔøΩtY2ÔøΩ-\ne\u0004\u0003ÔøΩkÔøΩd'ÔøΩÔøΩIÔøΩD\nÔøΩ^%\u000fÔøΩkÔøΩÔøΩL-ÔøΩiÔøΩUÔøΩ4ÔøΩJ’¶ÔøΩ4ÔøΩf\bLt(ÔøΩ8ÔøΩÔøΩ3TÔøΩ\np{ÔøΩ\u000e? ÔøΩÔøΩ.1ÔøΩÔøΩÔøΩÔøΩWÔøΩVPÔøΩÔøΩÔøΩ\u0010:ÔøΩÔøΩfOlÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩj\nÔøΩn«ã6ÔøΩÔøΩÔøΩÔøΩ3ÔøΩ\u0018ÔøΩÔøΩ4ÔøΩ\u0014`ÔøΩÔøΩm^KÔøΩ^ÔøΩ'ÔøΩÔøΩ2ÔøΩ@:&ÔøΩ3ÔøΩQ*Qv/\bE_ÔøΩÔøΩ+V1ÔøΩlÔøΩ–ØÔøΩCÔøΩVc(ÔøΩQÔøΩÔøΩÔøΩ<uÔøΩÔøΩNÔøΩ+qÔøΩÔøΩÔøΩ\u0014ÔøΩmÔøΩÔøΩÔøΩP[ÔøΩÔøΩ9ÔøΩBIt\u0001zÔøΩÔøΩO\"\"ÔøΩÔøΩ\nÔøΩÔøΩ8AXÔøΩ0G\u0006ÔøΩÔøΩPÔøΩi\u0001ÔøΩ.ÔøΩ(Õ≤2\u0006)\"1ÔøΩI\\*g@q<\u001bÔøΩQ5BÔøΩÔøΩmÔøΩfÔøΩÔøΩX\t:ÔøΩÔøΩPhÔøΩ?K\u000fÔøΩ?EÔøΩ√´C?\\ÔøΩ\\ÔøΩÔøΩXÔøΩ\n'ÔøΩyÔøΩÔøΩ\u0014ÔøΩÔøΩ\u0010sÔøΩÔøΩÔøΩkÔøΩUÔøΩUbÔøΩÔøΩÔøΩÔøΩ\u000f\u0019ÔøΩÔøΩÔøΩÔøΩ–üÔøΩGp]ÔøΩ~ÔøΩÔøΩÔøΩ\u001bÔøΩÔøΩÔøΩhÔøΩ^/LÔøΩÔøΩÔøΩÔøΩ{~ÔøΩÔøΩ!ÔøΩ\nﬁßaÔøΩÔøΩY\u001bÔøΩÔøΩuÔøΩ=ÔøΩÔøΩ;ÔøΩÔøΩÔøΩÔøΩÁèø1ÔøΩÔøΩ€õ|BAy\nÔøΩ\nÔøΩÔøΩ_/Ÿ±mÔøΩqJ€øXÔøΩÔøΩÔøΩAÔøΩ\n|eRÔøΩCr\u0018'HÔøΩPÔøΩÔøΩoVÔøΩ\u0019ÔøΩŸ∑ÔøΩ7\nXÔøΩrWÔøΩ&ÔøΩÔøΩÔøΩ–´ÔøΩ-Ÿ¥ÔøΩ(ÔøΩÔøΩtÕ∑?5ÔøΩ\u0015ÔøΩÔøΩÔøΩ&ÔøΩhÔøΩÔøΩÔøΩÔøΩ\ncÔøΩÔøΩÔøΩeÔøΩÔøΩ‹ΩÔøΩgÔøΩÔøΩÔøΩ_ozÈÉômÔøΩ\u001b\u001a5ÔøΩyÔøΩ_ÔøΩÔøΩXÔøΩÔøΩÔøΩG(—æÔ•âÔøΩ]ÔøΩ7ÔøΩ~~<ÔøΩÔøΩ=(ÔøΩosÔøΩ\u0010ÔøΩÔøΩ7ÔøΩsÔøΩ\\qÔøΩ\u0007ÔøΩ.\\ÔøΩÔøΩÔøΩÔøΩÔøΩXÔøΩ\n\u0017.ÔøΩ\u001fÔøΩ*ÔøΩ\u0010JÔøΩ7d8\nRh22ÔøΩG\u001a\u0017ÔøΩiUÔøΩ\u0006ÔøΩ9HÔøΩ\u0003ÔøΩ^\u0018#2ËÑ©qÔøΩÔøΩ&y—çÔøΩÔøΩ7ÔøΩÔøΩ\u0018ÔøΩÔøΩ\\\u0006ÔøΩÔøΩÔøΩﬂ≠\u0004ÔøΩÃ™ÔøΩÔøΩÔøΩaÔøΩ\u0010PÔøΩq}\u0012ÔøΩGÔøΩÔøΩ\"ÔøΩ\u0012eÔøΩ\nÂôÇyÔøΩ\u0002ÔøΩ>PƒõÔøΩ_ÔøΩÔøΩxÔøΩtÔøΩMÔøΩ,ÔøΩÔøΩÔøΩ*ÔøΩFjÔøΩ\u0002PEÔøΩÔøΩ2e\u0010ÔøΩÔøΩÔøΩ6ÔøΩ\u0014ÔøΩÔøΩt`ÔøΩ\u0010ÔøΩv\u001aÛßπãkÔøΩeÔøΩ'rUF\u0002c\u0015-ÔøΩÔøΩ/ÔøΩ'ÔøΩÔøΩÔøΩÔøΩ2ÔøΩ|ÔøΩ\u0003ÔøΩ0]\t#\n,ÔøΩ\u0019ÔøΩ^&\u0013pZK\nYÔøΩ7ÔøΩÔøΩDy_HÔøΩ\niÔøΩ÷ùÔøΩ5\u0006ÔøΩ?ÔøΩj5ÔøΩ√∏fÔøΩ ÔøΩqÔøΩÔøΩ:\u001f\u0001GÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩŒß0ÔøΩÔøΩÔøΩ\u0016ÔøΩW@ÔøΩT\u0003+ÔøΩw,R0\u0002ÔøΩ$ÔøΩÔøΩÔøΩÔøΩ\u0014\u0000ÔøΩrÔøΩÔøΩ@ÔøΩ\nJÔøΩÔøΩÔøΩHÔøΩÔøΩF÷ØÔøΩ$MI$\u0014\"KÔøΩ\u001aÔøΩÔøΩ|ÔøΩ)MÔøΩ\u000eÔøΩÔøΩÔøΩ/EÔøΩsDÔøΩÔøΩÔøΩÔøΩÔøΩ(s:`\\ÔøΩÔøΩÔøΩ ÔøΩ\u000fŸöH1zÔøΩ\u0010ÔøΩjÔøΩÔøΩ\u0000p9ÔøΩ…ºyÔøΩ?DA\u0001ÔøΩ+ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ9\u001fÔøΩNÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩ?w\u000e\n{knÔøΩÔøΩÔøΩÔøΩﬂåÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩSÔøΩÔøΩ9N9ÔøΩ\nÔøΩ==f”é#\u000fÔøΩÔøΩ\u0019~ÔøΩÔøΩÔøΩ'ÔøΩ8uÔøΩÔøΩ\u0005ÔøΩum\u0010ÔøΩƒêÔøΩ0\u0006ÔøΩÔøΩ)ÔøΩ}ÔøΩœåÔøΩ'}ÔøΩjÔøΩ~ ¶wÔøΩÔøΩiÔøΩÔøΩÔøΩ\u0013fnÔøΩ\nÔøΩÔøΩÔøΩ\u000ffR>{ÔøΩÏÖ™ÔøΩÔøΩPÔøΩÔøΩoÔøΩÔøΩP\bÔøΩ\u0003ÔøΩ|ÔøΩsÔøΩÔøΩÔøΩÔøΩÔøΩwÔøΩÃÖÔøΩÔøΩÔøΩ>ÔøΩÔøΩÔøΩÔøΩ_7ÔøΩY9cÔøΩÔøΩÔøΩ[\u000fÔøΩÔøΩÔøΩÔøΩÔøΩ]«ñ~ÔøΩc√òÔøΩÔøΩÔøΩ>ÔøΩÔøΩ\u001a\u0018ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ/CÔøΩyÔøΩÔøΩ\nÔøΩ^\u0018?jÔøΩÔøΩq_ÔøΩ_ÔøΩÔøΩ–™uÔøΩÔøΩÔøΩ-}ÔøΩ\u001f\u0013?\u0018ÔøΩHÔøΩK\u0015ÔøΩ}ÔøΩÔøΩÔøΩoÔøΩ{iBÔøΩÔøΩÔøΩ>ÔøΩÔøΩÔøΩ\t?ÔøΩ8zYÔøΩ~ÔøΩÔøΩ‰πæÔøΩÔøΩÔøΩÔøΩ∆•ÔøΩÔøΩCzÔøΩÔøΩdÔøΩQÔøΩ1ÔøΩÔøΩ\nr$UÔøΩÔøΩfWÔøΩÔøΩ7ÔøΩÔøΩ&4ÔøΩs<F–ßÔøΩœµÔøΩÔøΩ\u0019BÔøΩ◊òN+ÔøΩÔøΩjÔøΩÔøΩ\niÔøΩdEÔøΩÔøΩÔøΩkÔøΩÔøΩÔøΩW\u0007ÔøΩ\u0013\nÔøΩM*M\u0017K1ÔøΩ,ÔøΩ!ÔøΩÔøΩfÔøΩVÔøΩÔøΩ.ÔøΩ\u0015ÔøΩ7ÔøΩ2ÔøΩo1’õÔøΩDÔøΩÔøΩ5ÔøΩÔøΩÔøΩÔøΩ8ÔøΩ<JÔøΩÔøΩ+3\u0014ÔøΩ\u00174ÔøΩÔøΩ\u0010ÔøΩh2ÔøΩÔøΩÔøΩzÕòÔøΩfÔøΩÔøΩ+ÔøΩhÔøΩÔøΩhÔøΩX<ÔøΩÔøΩÔøΩ ÔøΩ@ÔøΩÔøΩ_ÔøΩÔøΩÔøΩÔøΩdÔøΩÔøΩvk\u0000ÔøΩÔøΩ›ß\u000fgÔøΩsG\u0005ÔøΩ}–öÔøΩÔøΩÔøΩ:raÔøΩÔøΩB‹ßÔøΩUyÔøΩ5]ÔøΩ]ÔøΩUÔøΩfÔøΩÔøΩ\\MÔøΩ*ÔøΩÔøΩÔøΩuÔøΩÔøΩÔøΩÔøΩDÔøΩ\u0000ÔøΩÔøΩÔøΩÔøΩRÔøΩ\na\b\u0003ÔøΩ0ÔøΩJPÔøΩÔøΩÔøΩ\nÔøΩ\\ÔøΩBÔøΩxÔøΩÔøΩÔøΩÔøΩ(R\u0015d\u0014i/\u000fGÔøΩÔøΩgÔøΩ\u0004ÔøΩwÔøΩ%ÔøΩÔøΩÔøΩy\n\u0012GÔøΩÔøΩpÔøΩaÔøΩ\u0015}ÔøΩÔøΩ2ÔøΩNSccÔøΩ{ﬁâxÔøΩÔøΩ∆îÔøΩÔøΩÔøΩÔøΩÔøΩLÔøΩiÔøΩÔøΩ,i3ÔøΩÔøΩ.ÔøΩÔøΩqÔøΩ\\WÔøΩœÑÔøΩBÔøΩÔøΩ÷ïÔøΩÔøΩbÔøΩƒæMÔøΩ\u001aÔøΩMÔøΩÔøΩÔøΩÔøΩÔøΩ\u0018ÔøΩ|ÔøΩÔøΩÔøΩÔøΩ«ªÔøΩuLÔøΩ+ÔøΩ÷∂ÔøΩÔøΩÔøΩ&ÔøΩÔøΩsÔøΩÔøΩÔøΩ\u0017\n:zÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ[ÔøΩÔøΩ9ÔøΩÔøΩ.€Å\u0014ÔøΩ>j»ÑÔøΩÔøΩÔøΩoÔøΩÔøΩMÔøΩ?ÔøΩ\u001fÔøΩÔøΩÔøΩÔøΩÔøΩ#ÔøΩ\u0018–ªÔøΩ|ÔøΩÀìÔøΩﬂ®ÔøΩzÔøΩÔøΩ\u001b(\nÔøΩÔøΩÔøΩÔøΩÔøΩNÔøΩ>ÔøΩÔøΩS?ÔøΩ8KÔøΩÔøΩÔøΩÔøΩÔøΩ\u0012€ûNÔøΩÔøΩ;ÔøΩÔøΩ=wÔøΩ–±SgÔøΩ^ÔøΩÔøΩ◊≥\u0007ÔøΩÔøΩÔøΩwÔøΩÔøΩo7\nÔøΩ<ÔøΩwÕúÔøΩ'F^◊µÔøΩhÔøΩGsÔøΩÔøΩÔøΩ\u000eÔøΩÔøΩÔøΩfÔøΩÔøΩÔøΩ+vÕòÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩtÔøΩÔøΩS?/ÔøΩÔøΩ“û|s⁄¢UÔøΩ\u000f\n9I\u0019ÔøΩSÔøΩÔøΩ\u000eÔøΩﬂ∞ÔøΩÔøΩÔøΩvÔøΩ/}ÔøΩ’±ÔøΩN_ÔøΩÔøΩÔøΩÔøΩwÔøΩÔøΩÔøΩW+giDOÔøΩZ|ÔøΩÔøΩÔøΩ2ÔøΩÔøΩ\nmf0c5ÔøΩ\u0001\n\u0005.I\u001aRmÔøΩ√ÄÔøΩ\u0000lÔøΩÔøΩ\bs_ÔøΩe\u0016ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0006ÔøΩ5\u0016ÔøΩZÔøΩoÔøΩJgÔøΩÔøΩÔøΩ\u0003l\nÔøΩ\\ÔøΩÔøΩÔøΩ\u0005e\bÔøΩ7A,ÔøΩ‘ª*h\u001aÔøΩlÔøΩ\u0003\u0006PÔøΩÔøΩr'\u0014ÔøΩ‹†ÔøΩMgÔøΩ!ÔøΩiY\u0019+ÔøΩÔøΩaÔøΩÔøΩ:ÔøΩG2JÔøΩ \u0019ÔøΩJj\u0014ÔøΩ\u0003ÔøΩ\nÔøΩÂñÇYÔøΩhÔøΩÔøΩÔøΩ?ÔøΩÔøΩh&ÔøΩuRÔøΩ\nsq\t\u001fÔøΩ.ÔøΩIÔøΩ\u0015GÔøΩÔøΩÔøΩ>◊ÇÔøΩÔøΩÔøΩÔøΩÔøΩDÔøΩBÔøΩaÔøΩE&ÔøΩÔøΩ\tCÔøΩÔøΩ0dÔøΩaÔøΩP{BC_)ÔøΩ\tÔøΩÔøΩ6ÔøΩÔøΩ&ÔøΩ< ÔøΩÔøΩ\u0015uÔøΩ\u001aÔøΩÔøΩÔøΩÔøΩÔøΩ\u0005’ÄqÔøΩ\u00072hÔøΩ8e\nP\u0019xÔøΩƒ©\\:ÔøΩ/ZÔøΩ\n.ÔøΩ\n.e-?)ÔøΩ\nÔøΩV\u0005\u000fGÔøΩg\u0015\bmF4ÔøΩiÔøΩoÔøΩÔøΩÔøΩhÔøΩAÔøΩÔøΩOÔøΩ,\u0016ÔøΩÔøΩX>ÔøΩÔøΩUy\u0002iÔøΩ tÔøΩ\u0003ÔøΩÔøΩÔøΩÔøΩ\\\u0007 £QÔøΩ\u000eÔøΩ-\"ÔøΩÔøΩoÔøΩÔøΩ(÷¢%ÔøΩJ^ÔøΩÔøΩ/HÔøΩÔøΩÔøΩ=&SÔøΩM-ÔøΩY>1h\nÔøΩ\nÔøΩÔøΩÔøΩ_ŒÄÔøΩÔøΩÔøΩƒÉGNÔøΩ\u0015ÔøΩ€™ÔøΩ.ÔøΩÔøΩ]ÔøΩÔøΩÔøΩÔøΩÔøΩyÔøΩwÔøΩÔøΩÔøΩÔøΩÔøΩ\u001f\u000fÔøΩJÔøΩÔøΩ_ÔøΩ‹£ÔøΩÔøΩ5{\nÔøΩ\u001bÔøΩ,rÔøΩÔøΩ’≥ﬂÆÔøΩÔøΩÔøΩÔøΩ\u0007=ÔøΩ{JÔøΩgÔøΩE}ÔøΩ0BCÔøΩÔøΩÔøΩÔøΩ)ÔøΩÔøΩÔøΩÔøΩÿá^ÔøΩÔøΩÔøΩ{3ﬁ®ÔøΩ3kÔøΩÔøΩÔøΩœéÔøΩcÔøΩÔøΩÔøΩÔøΩÔøΩ6ÔøΩÔøΩÔøΩ/}0ÔøΩÔøΩLÔøΩÔøΩÔøΩwÔøΩuLÔøΩ\u0007ÔøΩQ\u0002\u000e!ÔøΩÔøΩÔøΩÔøΩ…îI\u0013ÔøΩ\u0015ZwÔøΩCSÔøΩÔøΩÔøΩc\u001a«ÆÔøΩx\u0016ÔøΩnÔøΩOÔøΩÔøΩ\\ÔøΩÔøΩÔøΩÔøΩÔøΩim\u001fÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩYÔøΩÔøΩ…ëÔøΩ\nÔøΩÔøΩ\u0012ÔøΩÔøΩDÔøΩ>ÔøΩÔøΩ{\u00062ÔøΩÔøΩ+Ãπ}EPÔøΩ]ÔøΩÔøΩÔøΩÔøΩÔøΩS\n<\u0014\n6PÔøΩcÔøΩxÔøΩÔøΩÔøΩÔøΩ\u0010ÔøΩ`>ÔøΩgÔøΩÔøΩrÔøΩ;ÔøΩmÔøΩE'5ÔøΩ1ÔøΩ42=ÔøΩIÔøΩÔøΩÔøΩyeZ\n\u000fWÔøΩ ÔøΩÔøΩ\u0011sÔøΩÔøΩKÔøΩbÔøΩFÔøΩRÔøΩÔøΩ^ÔøΩ0ÔøΩlÔøΩÔøΩÔøΩ]ÔøΩ)%ÔøΩ<fÔøΩÔøΩaH1o@oh\u001ac\nimÔøΩÔøΩ\u0011j':ÔøΩ1ÔøΩRÔøΩÔøΩÔøΩ`-`yhqÔøΩÔøΩDÔøΩ,f?+ÔøΩ!d\u0005ÔøΩ\u000eÔøΩw\b\u0016ÔøΩÔøΩÔøΩ⁄≥*!ÔøΩÔøΩU~GG\u0007nS%ÔøΩÔøΩÔøΩ\u0019ÔøΩÔøΩÔøΩÔøΩ,ÔøΩ?3ﬁÉPÔøΩÔøΩÔøΩÔøΩÔøΩfÔøΩDÔøΩÔøΩ\u0014ÔøΩ\u0018\u001bÔøΩÔøΩP\u0014ÔøΩEÔøΩÔøΩq~rÔøΩ\\ÔøΩ }ÔøΩj…ÖAÔøΩÔøΩÔøΩ ÔøΩ-ÔøΩp\bÔøΩRÔøΩ\b\nÔøΩ|prÔøΩ\u0011LÔøΩ\u0003ÔøΩÔøΩ>\u001bsz\u0012ÔøΩÔøΩQkkÔøΩs+Hu\u0018IÔøΩSÔøΩ+/m\u0006`s:eÔøΩ\u0012)|EÔøΩM.ÔøΩb\u0013*ÔøΩÔøΩÔøΩ\u0014ÔøΩÔøΩ8\nEÔøΩ”ùÔøΩÔøΩuÔøΩÔøΩOf$HTYI\nÔøΩ\t\u0011ÔøΩÔøΩ\u001b\u0000BÔøΩrÔøΩœçlQL\b\u0004\nÔøΩÔøΩÔøΩ…ëÔøΩÔøΩ\u0000ZoÔøΩjÔøΩÔøΩD\u0006]^\u0017'ÔøΩS∆π\u0011|ZÔøΩ,ÔøΩ\nVeÔøΩ\u001aÔøΩvÔøΩCÔøΩ~ÔøΩÀâÔøΩW$ÔøΩÔøΩogÔøΩ_ZÔøΩÔøΩÔøΩ[ÔøΩ\nÔøΩda\u0005ÔøΩaÔøΩÔøΩ]ÔøΩU\n]ÔøΩÔøΩÔøΩ/\u001fOÔøΩ\u0016œéÔøΩÔøΩÔøΩŒå\u0005[‘ØÔøΩÿ¶ÔøΩÔøΩ%9\nÔøΩÔøΩ\nzÔøΩÔøΩÔøΩoÔøΩ!?*ÔøΩÔøΩ:ÔøΩÔøΩÔøΩÔøΩÔøΩ\u000fÔøΩ:iﬂ°_ÔøΩo9\\7b9ÔøΩ%ÔøΩÔøΩÔøΩbQ-ÔøΩÔøΩ+zÔøΩ=ÔøΩ\u001bÔøΩ0zEÔøΩÔøΩÔøΩ\nÔøΩMÔøΩÔøΩÔøΩÔøΩrÔøΩ\u0017ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0003ÔøΩC\u0017ÔøΩÔøΩ;oŸÆÔøΩ{ÔøΩW;hÔøΩfÔøΩ>ÔøΩÔøΩsÔøΩÔøΩ\n<ÔøΩÔøΩÔøΩiyÔøΩl:xÔøΩ}CtÔøΩ\n\u001bÔøΩÔøΩtÔøΩtÔøΩÔøΩÔøΩ\u0003\u0000ÔøΩÔøΩ/ÔøΩW;2oÔøΩ)ÔøΩÔøΩÔøΩrÔøΩ!ÔøΩ)]dc\u0006o\u0000ÔøΩyÔøΩÔøΩ»å:ÔøΩ\nÔøΩ4 7-_\"\u001f\u000fK\bnÔøΩR\u0017$~ÔøΩ\nÔøΩ'YG\nÔøΩÔøΩÔøΩE'\bÔøΩÔøΩ\u0005\\ÔøΩJÔøΩ1ÔøΩÔøΩVÔøΩHÔøΩÔøΩÔøΩ ÔøΩ}uÔøΩÔøΩÔøΩÔøΩSZ\u0003ÔøΩ\u0015g[5\u0005\nÔøΩÔøΩ[}RÔøΩÔøΩ/ÔøΩ{\nÔøΩT2ÔøΩÔøΩw\u0005v\u0018ÔøΩ \\XÔøΩ,ÔøΩdÔøΩÔøΩZcÔøΩÔøΩÔøΩoÔøΩÔøΩb+\u0007ÔøΩEaÔøΩÔøΩY\u000fÔøΩÔøΩÔøΩ\u0018ÔøΩbÔøΩ\\&ÔøΩ\u0014p6Z~ÔøΩÔøΩtÔøΩﬁÖ6ÔøΩ+\u0019ÔøΩ/ÔøΩÔøΩU@\u000eA\u0004\tuÔøΩÔøΩÔøΩÔøΩ+ÔøΩÔøΩ\u001a`B)$sK\u0004ÔøΩBKfÔøΩf3\nMÔøΩÔøΩÔøΩ\n\\kÔøΩ[‘ÑÔøΩ¬ÄÔøΩ\u0014V\"ÔøΩÔøΩrÔøΩS\nÔøΩ3ÔøΩÔøΩÔøΩy'\u0018EÔøΩÔøΩ\nÔøΩÔøΩ9Wj8(bÔøΩ\nQÔøΩÿ∑ÔøΩT.ÔøΩÔøΩÔøΩ\tÔøΩ^ÔøΩ>ÔøΩÔøΩÔøΩÔøΩ\u000fN)\naÔøΩ[WÔøΩÔøΩDÔøΩÔøΩÿ∏ÔøΩÔøΩS–¢ÔøΩ%rLPkZÔøΩ8ÔøΩ\u0019ZÔøΩŒ¨pÔøΩÔøΩ<-Ã¥cZÔøΩ\u0015ÔøΩÔøΩÔøΩËà•ÔøΩÔøΩxÔøΩÔøΩe5ÔøΩﬁ∂ÔøΩÔøΩs}fÔøΩÔøΩNÔøΩÔøΩÔøΩÏπàv…ìoMÔøΩwÔøΩÔøΩÔøΩYÔøΩÔøΩÔøΩZÔøΩÔøΩ’™ÔøΩÔøΩV|‰üì{ÔøΩÔøΩ4eÔøΩÔøΩUÕ¢ÔøΩÔøΩ7j9.ÔøΩÔøΩÔøΩ'~ÔøΩÔøΩÔøΩ'ﬂúJ\u001bÔøΩÔøΩSÔøΩÔøΩÔøΩ7\u0014\nÔøΩ–ãIaÔøΩÔøΩ√ØMÔøΩp–¢ÔøΩÔøΩ\u0010ZÔøΩÔøΩ;ÔøΩÔøΩÔøΩ{ÔøΩÔøΩ^/ÔøΩÔøΩGÔøΩY#ÔøΩÔøΩ)~ÔøΩÔøΩW>ÔøΩjÔøΩÔøΩ_J\nN?CÔøΩmÔøΩÔøΩ}ÔøΩÔøΩ\nÔøΩÔøΩ\u0016yÔøΩÔøΩÔøΩ_8ÔøΩmÔøΩÔøΩ*\nAÔøΩÔøΩ\nœåÔøΩJ\nqÀù\u0003jÔøΩ/ÔøΩÔøΩÔøΩ9,\u0007\u0003ÔøΩœéÔøΩÔøΩ.ÔøΩÔøΩUÔøΩ%ÔøΩ\u0004e.ÔøΩJd{\n\u0007\u001f\u0019ZT(txa`\n\u0010?\u0003\u001bXÔøΩÔøΩ;ÔøΩÔøΩSÃôÔøΩ¬ºmÔøΩÔøΩÔøΩÔøΩIÔøΩÔøΩ$&.ÔøΩ6ÔøΩd\u0019tÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0018ÔøΩÔøΩÔøΩnÔøΩÔøΩÔøΩ4MÔøΩ\u0014\u0017\u0016`ÔøΩQ\u0013ÔøΩ8E\\ﬁØb1ÔøΩÔøΩÔøΩ\u0017cN“Å+ÔøΩ\u0005WnÔøΩÔøΩ\u000eA–∂pÔøΩÔøΩ\u0018\u000eÔøΩ\u0001ÔøΩ\u0018sUÔøΩÔøΩmÔøΩÔøΩkÔøΩÔøΩvdÔøΩÔøΩÔøΩUÔøΩ*ÔøΩÔøΩ7P78ÔøΩÔøΩ\"ÔøΩuÔøΩ-Y.L\u0002ÔøΩÔøΩ^ÔøΩÔøΩÔøΩZÔøΩ ¥ÔøΩ5fÔøΩÔøΩ7ÔøΩÔøΩÔøΩÔøΩhÔøΩ\u0019ÔøΩXÔøΩÔøΩÔøΩ\u0010ÔøΩÔøΩ\u0010ÔøΩ1ÔøΩÔøΩ(:ÔøΩÔøΩW(ÔøΩ}ÔøΩA@?\u001aÔøΩÔøΩÊÖ¶\tÔøΩ4ÔøΩ\u0013ÔøΩUK\u0000k“µa ,bÔøΩÔøΩÔøΩ:›∫9√ùÔøΩ\u0014ÔøΩxÔøΩa\u0018ÔøΩ\u000e?\n;2ÔøΩ%CÔøΩÔøΩÔøΩ\u0018ÔøΩ\u000e-vIÔøΩÔøΩÔøΩ\u0002ÔøΩ\u0018ÔøΩ ÔøΩ\u0010\n\u0012…º\u000eÔøΩ\u0002\u0015}JaÔøΩÔøΩÔøΩp\b2ÔøΩ\u0010LÔøΩ2khc;SzjÔøΩ,]&ÔøΩkÔøΩ\"1fÔøΩÔøΩÔøΩbgÔøΩÔøΩNÔøΩÔøΩÔøΩxÔøΩ\u0006>0ÔøΩ\u0007ÔøΩrJu\u0007\u0015ÔøΩ*\u0002ÔøΩ«Ø>{ÔøΩÔøΩeÔøΩÔøΩÔøΩÔøΩÔøΩ/\u0017mÔøΩÔøΩÔøΩ<MD\u0007eÔøΩ\u0005ÔøΩÔøΩÔøΩ\nÔøΩÔøΩ,ÔøΩ^ÔøΩÔøΩÔøΩÔøΩœèÔøΩ◊ëÔøΩÔøΩWÔøΩ<ÔøΩ\u0007ÔøΩ\u0000oÔøΩÕΩÔøΩÔøΩ\tGÔøΩÔøΩ*{ÔøΩ\u0013ÔøΩÔøΩkÔøΩÔøΩ\u001bÔøΩÔøΩ|ÔøΩÔøΩZ9ÔøΩRTJToÔøΩ&\u0006ÔøΩÔøΩNÔøΩ\u001f\u001bÔøΩÔøΩ\u00073_ÔøΩ`&eÔøΩ%\u000f}\nÔøΩÔøΩ6ÔøΩÔøΩÔøΩ\u0018\u0005\u0019\u001arÔøΩÔøΩ/\u000fÔøΩÔøΩÔøΩ?\u0003\u0017\n:vÍÆøzkÔøΩÔøΩÔøΩ'ÔøΩÔøΩcÔøΩÔøΩ,ÔøΩA\u001fÔøΩÔøΩÔøΩÔøΩ\u0015kÔøΩ=ÔøΩÔøΩÔøΩÔøΩ\u0002ÔøΩÔøΩ>ÔøΩÔøΩÔøΩÔøΩmÔøΩqÔøΩÔøΩÔøΩ'◊ç\\ÔøΩÔøΩÔøΩ}v^ÔøΩÔøΩÔøΩ^ÔøΩÔøΩa€ë+2w\u0016ÔøΩÔøΩ'OÔøΩÔøΩÔøΩ_ÔøΩÔøΩÔøΩÃ°]ÔøΩÔøΩ +Yo¬ØÔøΩ\u0014ÔøΩp\u0018ÔøΩD\\\n\u0018\nÔøΩr`ÔøΩÔøΩEAÔøΩÔøΩ\u0000Q\u0000G\u0003\n9ÔøΩ»≤⁄∂L\u0012ÔøΩ&'\tMÔøΩ\u0017ÔøΩ8ÔøΩÔøΩÔøΩÔøΩÔøΩÀ∏#ÔøΩwLbÔøΩÔøΩÔóì:ÔøΩnYJ\u001bqLÔøΩÀõÔøΩ&8*\u0015A\u0019ÔøΩ6\u0002|‘∂ZÔøΩ\u0000\u0014ÔøΩÔøΩ’ùÔøΩÔøΩ\u0004YÔøΩÔøΩvÔøΩJk+eÔøΩÔøΩ\u0010p\n\\3;\u0004OÔøΩÔøΩÿªÔøΩEBpÔøΩÔøΩÔøΩÔøΩÔøΩ6ÔøΩÔøΩÔøΩÔøΩ\bÔøΩÔøΩJ\u0019g’ª\nV\u0016\u0007ÔøΩEPÔøΩÔøΩÔøΩƒêÔøΩ_)ÔøΩ[9NO[EÔøΩÔøΩ\\JL\u0000»µÔøΩ,ÔøΩÔøΩÔøΩ\nÔøΩÔøΩRÔøΩÔøΩÔøΩa8ÔøΩL8ÔøΩÔøΩÔøΩ\blxhÔøΩÔøΩ*%ÔøΩ\u0002ÔøΩ\u0012ÔøΩ8c>ÔøΩ\u0015ÔøΩtÔøΩÔøΩŸæ&_gÔøΩ\u0010zÔøΩ&ÔøΩÔøΩÔøΩVÔøΩÔøΩnÔøΩÔøΩFÔøΩÔøΩÔøΩ\u0016@,ÔøΩW9\nÔøΩÔøΩ'7x82hÔøΩ~m,5ÔøΩÔøΩÔøΩq:ÔøΩÔøΩ7\u0013ÔøΩÔøΩÔøΩÔøΩÔøΩvÔøΩF\u0018x^ÔøΩÔøΩ\u0010\nPÔøΩ e‹É\u0011ÔøΩÔøΩÔøΩ\niÔøΩÔøΩ\u0005ÔøΩÔøΩ÷ªfÔøΩÔøΩ\u001aÔøΩÔøΩ$%\u0007…µn\nLSÔøΩ-PÔøΩcÔøΩhg\nqÔøΩURÔøΩƒõSÔøΩÔøΩ:ÔøΩ$ÔøΩÔøΩÔøΩtÔøΩÔøΩﬁÉÔøΩ6ÔøΩ^qÔøΩÔøΩÔøΩÔøΩ^\n]ÔøΩsÔøΩÔøΩ\bÔøΩX\u0006\bÔøΩ3ÔøΩÔøΩÔøΩÔøΩQÔøΩÔøΩÔøΩ{ÔøΩÔøΩÔøΩ\u0019ÔøΩ%ÔøΩÔøΩvÔøΩ–âÔøΩÔøΩ%U<>ÔøΩoÔøΩÔøΩÔøΩÔøΩ'ﬁòzÔøΩÔøΩIÔøΩIÔøΩ\u000f4xQ√òÔøΩÔøΩ\u000f||mÔøΩ~ÔøΩÔøΩsÔøΩÔøΩuÔøΩ‘ØÔøΩÔøΩ~—™›ÖÔøΩ\nÔøΩ\nÔøΩÔøΩÔøΩÔøΩ)CÔøΩH√ÜmÔøΩÔøΩ\u0003ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u001brKÔøΩÔøΩ\u0014OÔøΩÔøΩ\u0011\u0000ÔøΩ~ÔøΩW,ÔøΩÔøΩGUÔøΩ9ÔøΩmÔøΩ\u0007\u0003\u0017ÔøΩÔøΩÔøΩB0ÔøΩÔøΩrÁÄ∞%ÔøΩÔøΩhÔøΩÔøΩIÔøΩ~ÔøΩÔøΩ◊≥W\u0014j5ZÔøΩ/\u0017nÔøΩÔøΩÔøΩ'QwÔøΩÔøΩXÔøΩ\u0004\u001bÔøΩÔøΩ8\u0019ÔøΩ4ÔøΩÔøΩHk\nÔøΩ*’éÔøΩ\u0015ÔøΩ4\u0004\nÔøΩ\u0005JÔøΩÔøΩ.\u000eÔøΩ,\u0007ÔøΩÔøΩSÔøΩÔøΩ\nÔøΩM'WÔøΩ:ÔøΩeÔøΩRÔøΩÔøΩWÔøΩ4ÔøΩÔøΩ(ÔøΩwÔøΩxJ6ÔøΩÔøΩÔøΩÔøΩÔøΩV2ÔøΩYÔøΩÔøΩ6ÔøΩÔøΩÔøΩÔøΩ8\u0003mÔøΩÔøΩKd\u0010WuÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ%ÔøΩEI4ÔøΩ*\nTÔøΩÔøΩ\u0002ÔøΩ(\u0012ÔøΩ,EÔøΩÔøΩÔøΩ{qtÔøΩ\u0018Og_ 'ÔøΩ8ÔøΩ ÔøΩOÔøΩÔøΩÔøΩ]ÔøΩLÔøΩÔøΩ\u0015nwÔøΩÔøΩFaÔøΩ\n\u0013ÔøΩwÔøΩeÔøΩ7TC–™$ÔøΩÔøΩÔøΩVÔøΩunÔøΩ«ÇÔøΩ7ÔøΩ\nÔøΩ\u0001ÔøΩÔøΩÔøΩfqÔøΩÔøΩB\u0000ÔøΩÔøΩX{ÔøΩÔøΩ\u0012ÔøΩA')ƒ≥ÔøΩÔøΩ\u0015iÔøΩ3)Í°ÅW9ÔøΩeÔøΩ\nZÔøΩÔøΩ\"ÔøΩÔøΩÔøΩGÔøΩXÔøΩP{-ÔøΩÔøΩKÔøΩÔøΩ»ÅY\u000e8\u0019ÔøΩsÔøΩ√ëÔøΩhÔøΩG0hÔøΩ“¶\nÔøΩÔøΩÔøΩÔøΩÔøΩ7ÔøΩ%ÔøΩÔøΩÔøΩÔøΩmhÔøΩÔøΩG\u0007ÔøΩÔøΩÔøΩ)U\nÔøΩÔøΩlÔøΩ\u001bT\n\u0002\nÔøΩÔøΩ!tÔøΩdÔøΩÔøΩ,ÔøΩÔøΩ$ÔøΩÔøΩ'/ÔøΩÔøΩQœì\u0000ÔøΩyÔøΩ41ÔøΩDÔøΩÔøΩFÔøΩ\u0019ÔøΩÔøΩÔøΩ{aÔøΩÔøΩÔøΩÔøΩÔøΩmÔøΩÔøΩ~ÔøΩ\nÔøΩÔøΩÔøΩ<uÔøΩÔøΩÔøΩÀóÔøΩÔøΩÔøΩDrÔøΩÔøΩ\u0013\u000fÔøΩ:)\u0016CÔøΩ_ÔøΩÔøΩ|ÔøΩÔøΩ-ÔøΩ&?ÔøΩÔøΩtÔøΩ$ÔøΩ-ÔøΩwÔøΩc#},ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩs2ÔøΩÔøΩIÔøΩ<ÔøΩ]7wÔøΩ›úMWÔøΩÔøΩÔøΩ/ÔøΩÔøΩ<v}ÔøΩ\u0006ÔøΩ\u0005ÔøΩÔøΩ0_\u0005MÔøΩ?6bÔøΩÔøΩ}ÔøΩfoÔøΩÔøΩ\u0013BwÔøΩ≈ôÔøΩÔøΩÔøΩÔøΩÔøΩ\u0019ÔøΩÔøΩZW\u0004ÔøΩ|sÔøΩÔøΩÔøΩnÔøΩÔøΩÔøΩ]ÔøΩÔøΩÔøΩÔøΩ@ÔøΩÔøΩÔøΩ\u0014ÔøΩfÔøΩÔøΩÔøΩ\u000fÔøΩÔøΩÔøΩ\u000f7vkÔøΩ\\\u0019ÔøΩÔøΩÔøΩ6ÔøΩ\u0015ÔøΩ{ÔøΩÃÖ+\nÔøΩÔøΩÔøΩÔøΩO3ÔøΩ:ÔøΩ-ÔøΩ)ÔøΩ^ÔøΩÔøΩ\u0015u\u0014?ÔøΩ*\u000fÔøΩÔøΩ\u0001ÔøΩ/ÔøΩÔøΩTÔøΩ√ÜrÔøΩyN\u0000zÔøΩ\nx\u0005\nhÔøΩÔøΩÔøΩ ÔøΩ\u001fÔøΩLÔøΩPÔøΩÔøΩd@\u000eÔøΩ5ÔøΩ\nd\"Ia(ÔøΩÔøΩA\u0014ÔøΩÔøΩNVKÔøΩiÔøΩF2yÔøΩÔøΩÔøΩ}JÔøΩ)wÔøΩÔøΩÔøΩÔøΩÔøΩ5^{ÔøΩ5]ÔøΩ]’©ÔøΩÔøΩÔøΩÔøΩÔøΩzÔøΩÔøΩ\u0012K)ÔøΩÔøΩÔøΩÔøΩ\ni\n8\u0019l]m`s\nÔøΩXÔøΩ]3jÔøΩv\u00108ÔøΩ\u001a\u0006ÔøΩÔøΩtŒ∏ÍªóÔøΩdÔøΩiÔøΩÔøΩ\u0014ÔøΩÔøΩÔøΩmÔøΩÔøΩÔøΩnÔøΩ\u0018^ÔøΩ[/8|YÔøΩrÔøΩ\"dD\bÔøΩoÔøΩÔøΩ#<ÔøΩÔøΩ\b\u0013lQQÔøΩ ≥&\u0006ÔøΩ,ÔøΩ\u0011'ÔøΩÔøΩ\nÔøΩ@ÔøΩÔøΩÔøΩÔøΩ;ÔøΩs\u0002ÔøΩÔøΩ3ÔøΩÔøΩÔøΩÔøΩi1ÔøΩÔøΩÔøΩ<ÔøΩ\u0019ÔøΩ$m»¶;ÔøΩ.ÔøΩ\n\u0016`ÔøΩÔøΩ\u001aÔøΩÔøΩd∆∞ÔøΩm&ÔøΩ’¢ÔøΩ2ÔøΩÔøΩÔøΩÃüM+\u0003ÔøΩ<J\u0016ƒ≤tÔøΩ\u0013ÔøΩÔøΩ\u0012[ÔøΩ\u0018ÔøΩYFIuÔøΩ~ÔøΩÔøΩ^\\\u0005ÔøΩ3ÔøΩÔøΩoJÔøΩ\u0013mÔøΩÔøΩÕêzÔøΩÔøΩÔøΩ\nFÔøΩM\u0006ÔøΩÔøΩ\tÔøΩÔøΩÔøΩbÔøΩÔøΩ√òdÔøΩÔøΩrÔøΩ‹¨ÔøΩÔøΩ\u001fcjNDÔøΩÔøΩÔøΩÔøΩ_ÔøΩzÔøΩ!ÔøΩÔøΩm\u001f\u0018ÔøΩÔøΩÔøΩmÔøΩtÔøΩÔøΩ\nDÔøΩg_m|ÔøΩqÔøΩ;ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u001aFÔøΩ \u00039ÔøΩÔøΩE[ÔøΩ}ÔøΩ>-ÔøΩMWÔøΩ2ÔøΩÔøΩ}\u001a\u0016<ÔøΩg∆Å#'ÔøΩÔøΩ\u001f\u001aÔøΩÔøΩÔøΩ\n\u000fÔøΩtÔøΩzÔøΩÔøΩVÔøΩÔøΩ\u000eŸµÔøΩÔøΩ\nœé≈ûÔøΩcÔøΩÔøΩÔøΩÔøΩÔøΩ7.xÔøΩÔøΩÔøΩ\u001f|ÔøΩ[{\n\u000es&ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ~\nÔøΩÔøΩB\u001aÔøΩÔøΩ,\u0017ÔøΩ>waÔøΩ◊õÔøΩ?ÔøΩDÿã5\u0019EÃ£ÔøΩÔøΩ\u0003c\u0002ÔøΩ\u00024ÔøΩu-ÔøΩ ∑ÔøΩÔøΩ 7\u000fÔøΩ[3INÔøΩÔøΩqÔøΩÔøΩÔøΩÔøΩhÔøΩ\u000fÔøΩÔøΩ\u001aÔøΩ\u0013-ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0019ÔøΩJÔøΩ»¨e6.\nÔøΩ?ÔøΩÔøΩf\u0014ÔøΩŸ∏\u0014L\u000fÔøΩIÔøΩÃ†o\u0010\u0019t#eÔøΩjÔøΩÔøΩÔøΩÔøΩ-KÔøΩ(ÔøΩKÔøΩ6~\nÔøΩÔøΩ◊Ä ÔøΩÔøΩ$ÔøΩÔøΩ\\ÔøΩh]ÔøΩÔøΩÔøΩ+1ÔøΩkÔøΩrÔøΩÔøΩ\u0013ÔøΩlVCÔøΩÔøΩÔøΩV&\u0001ÔøΩ_3ÔøΩÔøΩ\\Ï®ÇÔøΩÔøΩÔøΩAÔøΩÔøΩ\bC\u0014ÔøΩÔøΩ\u0015ÔøΩ\u0007ÔøΩbc\u0015ÔøΩ]ÔøΩp\n0ÔøΩÔøΩÔøΩÔøΩ\u0012\"\u0004ÔøΩBSÔøΩ9ÔøΩs\nÔøΩ\u0011ÔøΩwYÔøΩÔøΩe`ÔøΩNg\nÔøΩ:,U@ÔøΩ\u0006ÔøΩÔøΩ\u0003ÔøΩÂëíÔøΩyÔøΩ43∆úÔøΩÔøΩÔøΩÔøΩb\nÔøΩDÔøΩÔøΩ!rÔøΩ\\r]%yÔøΩ\nÔøΩ\"3h0lÔøΩÔøΩ.%Q$7ÔøΩ-l\u0003ÔøΩ(d\nÔøΩ\u0010ÍêπÔøΩKjÔøΩ!Xn)\u0012ÔøΩÔøΩ;ÔøΩV\u001bÔøΩÔøΩ\u0003fÔøΩÔøΩÔøΩ|ÔøΩÔøΩ0O1\tÔøΩÔøΩiÔøΩkÔøΩ|+J\u0014ÔøΩ:ÔøΩÔøΩ‘£ÔøΩÔøΩqÔøΩ\u0004ÔøΩÔøΩÔøΩ+RÔøΩÔøΩÔøΩogÔøΩ]ÔøΩÔøΩbWÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩy\nWÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ'ÔøΩÔøΩsÔøΩÔøΩcÔøΩ>\nÔøΩHÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ{BRÔøΩ.xÔøΩw?ÔøΩ\nÔøΩ8ÔøΩG\u0015t\nÔøΩÔøΩ=ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩiÔøΩ\u001fÔøΩWOÔøΩ;ÔøΩÔøΩ;rBjÔøΩÔøΩ*\u0002_,ÔøΩZ9d1ÔøΩÔøΩ) £ÔøΩÔøΩÔøΩ*8ÔøΩ|ÔøΩZ$ÔøΩ\nh›©ÔøΩÔøΩ\u000ffÔøΩÔøΩÔøΩÔøΩÔøΩKWTÔøΩÛÖãó\u000f\n=ÔøΩÔøΩÔøΩLgT;ÔøΩÔøΩ\u0014\u0012ÔøΩS\n*ÔøΩÔøΩŸÆ&ÔøΩÔøΩ\u0005rÔøΩ(ÔøΩÊòÆÔøΩ\u00186zÔøΩÔøΩ,VÔøΩÔøΩ\u0002\u0013\nn\n{ÔøΩvjÔøΩÔøΩ\nr 7ÔøΩj_ÔøΩw0,ÔøΩeÔøΩÔøΩÔøΩNh\nÔøΩ&\u00171l\nÔøΩÈç∏ÔøΩÔøΩ\u0013ÔøΩa\n4ÔøΩÔøΩd\nÔøΩ\u001aÔøΩ!ÔøΩÔøΩ]{\u0015ÔøΩÔøΩ]\u001bÔøΩÔøΩÔøΩÔøΩX01hÔøΩ\nÔøΩÔøΩ2kÔøΩ}&\u0007ÔøΩÔøΩÔøΩsÔøΩÔøΩÔøΩ\u0019ÔøΩ9\nÔøΩÔøΩTÔøΩÔøΩÔøΩnÔøΩ`ÔøΩ7EÔøΩRÔøΩrÔøΩÔøΩDÔøΩ+5H7jjÔøΩ^ÔøΩ»µÔøΩÔøΩÔøΩ\u0018hxÔøΩdÔøΩ–óÔøΩCÔøΩn\u001a!ÔøΩÔøΩ\nÔøΩ@À∏qÔøΩeÔøΩuIÔøΩE *\nÔøΩ[GÔøΩcÔøΩÔøΩ\u0018\u0004$ÔøΩ\u0016\u0011pA/ÔøΩÔøΩLTJ\u001b+\t=ÔøΩÔøΩÔøΩÔøΩÔøΩ\ni\u0011^ÔøΩ\nAat\u0019/ÔøΩÔøΩ4ÔøΩÔøΩ\nÔøΩ\u0000MÔøΩÔøΩ\ndLÔøΩÔøΩ<\u0019m\u0005\n6U)!&ÔøΩÔøΩgf\u0018)hÔøΩ,6fÔøΩ\nÔøΩÔøΩW/ÔøΩUÔøΩBÔøΩ\u0006\n&\u0001ÔøΩBÔøΩÔøΩÔøΩÔøΩjÔøΩE5v$ÔøΩÔøΩ\u00077k2ÔøΩ\u0014ÔøΩ(ÔøΩp3Ip\u0004i\u0015ÔøΩÔøΩÔøΩLÔøΩ$ÔøΩÔøΩ&n!Yrl2\u0011\\quÔøΩ'F}>gÔøΩÔøΩSÁÅåF?ÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩ_ÔøΩCp9ÔøΩ\u0013i[MÔøΩ$ÔøΩÔøΩÔøΩcÔøΩÔøΩÔøΩÔøΩÔøΩ:\u0006ÔøΩÔøΩ>ÔøΩÔøΩÔøΩ\u000eÔøΩ|ÔøΩÔøΩ3\u0000ÔøΩfÔøΩ0uŒ¶ÔøΩ¬æZÔøΩ|ÔøΩÔøΩÔøΩÔøΩ\u001aÔøΩ\u0017ÔøΩ;‰æó'|ÔøΩz\u000fEÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u001bÔøΩÔøΩÔøΩ{ÔøΩO:>:bÔøΩÔøΩ5}ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0018AÔøΩÔøΩÔøΩ\u0011OÔøΩP\u0016\u0003ÔøΩÔøΩ6ÔøΩ\u000eyÔøΩqÔøΩÔøΩ-ÔøΩÔøΩÔøΩ,nÔøΩ\n#ÔøΩÔøΩ\u001f>Q?rÔøΩ\u001f{\nÔøΩÔøΩQÔøΩÔøΩÔøΩÔøΩ≈íryÔøΩ(ÏΩ≥ÔøΩP:sÔøΩÔøΩÔøΩÔøΩÔøΩY\u0016ÔøΩ\u0001ÔøΩ+ÔøΩlÔøΩ\u0013ÔøΩ)‘£ÔøΩLC2ÔøΩzo\u0004ÔøΩÔøΩ÷êÔøΩ\u0002>iÔøΩÔøΩ/:ÔøΩÔøΩDÔøΩ4ÔøΩE\u001f€∫ÔøΩÔøΩE‹ÅÔøΩ\u0006ÔøΩÔøΩp\bÔøΩ  ÔøΩ\u0005ÔøΩ6\n\u0002‘ü\u0005\n7ÔøΩe]#@ÔøΩÔøΩÔøΩÔøΩÔøΩkUb\u0004PÔøΩ1ÔøΩÔøΩÔøΩÔøΩTÔøΩÔøΩÔøΩBÔøΩÂÄëoiUnM%*ÔøΩÔøΩI4\nÔøΩ6%\u0011\nu\u00072LÔøΩ/\u0000ÔøΩjS\u0006ÔøΩÔøΩSU!Ztn\\$EÔøΩÔøΩÔøΩÔøΩ$qÔøΩÔøΩ:vÔøΩQ-U“±FÔøΩÔøΩIFbÔøΩ\u0010ÔøΩX6≈èÔøΩ\n8ÔøΩÔøΩ\u0005!ÔøΩÔøΩ{/M\u000e%ÔøΩsÔøΩ),N!ÔøΩﬂìÔøΩ%ÔøΩÔøΩÔøΩZLGÔøΩ\u0002C3ÔøΩ<InÔøΩwcÔøΩ?ÔøΩ\b/ÔøΩDgÔøΩ!:ÔøΩÔøΩk\"JDÔøΩQ\u0012\u0002ÔøΩÔøΩ\u0004EÔøΩÕ†i\u0010ÔøΩ\u001bÔøΩÔøΩ\u0010^ÔøΩ&„¨°ÔøΩ\u0015(ÔøΩÔøΩVÔøΩÔøΩUÔøΩÔøΩmÔøΩ\nt\bÔøΩ6ÔøΩÁæüÔøΩiÔøΩ\u0007e(ÔøΩÔøΩÔøΩÔøΩﬂ¢\u0013ÔøΩU>ÔøΩ\u0010?nÔøΩZpx\t9ÔøΩ2\n\u0003ÔøΩÔøΩ\u0013@ÔøΩÔøΩn\"ÔøΩ\u0015(ÔøΩ\u0012\u0011ÔøΩ\nÔøΩÔøΩ'\u000fÔøΩ:iÔøΩÔøΩ\n'OÔøΩG\u0012-;ÔøΩ\u0017.}ÔøΩ√ÅÔøΩÔøΩ/(yhXÔøΩR^ÔøΩÔøΩ#08qÔøΩÔøΩ\nÔøΩWÔøΩÔøΩvÔøΩÔøΩGÔøΩ9\u0005Õ¨;ÔøΩ:fÕ¶ÔøΩÔøΩÔøΩ\u0018IÔøΩ\u0011ÔøΩÔøΩÔøΩÔøΩFÔøΩÔøΩÔøΩÔøΩÔøΩ\u0000\u001f0nÔøΩ\u0018\u000eg?ÔøΩÔøΩÔøΩÔøΩ/~0\u0013\nÔøΩ~ÔøΩÔøΩ>3ÔøΩn>ÔøΩv`nÔøΩÔøΩ\u0003[ÔøΩvjÔøΩÔøΩÔøΩÔøΩ\u0005ÔøΩ&ÔøΩ?G}ÔøΩrÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩwÔøΩÔøΩY\nM\u0007ÔøΩÔøΩÔøΩÔøΩNÔøΩÔøΩÔøΩÔøΩ‚âëb\u0014ÔøΩÔøΩ\u000fU\u001a)ÔøΩjÔøΩÔøΩÔøΩ1-fK,ÔøΩÔøΩ4ÔøΩ\n,ÔøΩÔøΩAVnÔøΩÔøΩ\u0003CWÔøΩÔøΩvÔøΩÔøΩÔøΩ\u0016cz\u00041\u0018fÔøΩ\u0012\u0015ÔøΩÔøΩÔøΩJÔøΩ\u0013ÔøΩÔøΩ+ORÔøΩO\n&ÔøΩÔøΩ\u000f\u0007M%WÔøΩÔøΩÔøΩÔøΩ`f\naÔøΩmd%?_q\bR\u0004ÔøΩ\n~ÔøΩ\u0012)\u0003ÔøΩÃµÔøΩ}ÔøΩÔøΩÔøΩÔøΩ8ÔøΩ\u0018\u0006SÔøΩ2iÔøΩ\u0010ÔøΩ\u0014ÔøΩ\nÔøΩW1∆¶d\u0019ÔøΩ‹æeÔøΩVÔøΩ\n2m\u0018ÔøΩÔøΩ\u0005ÔøΩhÔøΩŸágÔøΩÔøΩÔøΩ\tÔøΩCÔøΩ|ÔøΩÔøΩÔøΩÔøΩ]È≠ä}v^,ÔøΩWÔøΩW\u0012ÔøΩ\u0019ÔøΩ5ÔøΩÔøΩG#ÔøΩqÔøΩ.ÔøΩ0ÔøΩÔøΩ$EÔøΩY8ÔøΩBÔøΩÔøΩÔøΩ*\u000eÔøΩ.ÔøΩEZ\nÔøΩ+ÔøΩ\nÔøΩ,ÔøΩÔøΩÔøΩÔøΩÔøΩ âFK-ÔøΩDÔøΩÔøΩÔøΩÔøΩ ÔøΩh\u001f\u0019ÔøΩEÔøΩÔøΩ2B@ÔøΩ<\u0001ÔøΩy\u0013'N\u0007ÔøΩJÔøΩfÔøΩE!YÔøΩÔøΩÔøΩPÔøΩ\u0006ÔøΩÔøΩÔøΩ>tÔøΩÔøΩ\nÔøΩR'ÔøΩÔøΩ_ÔøΩcÔøΩ\n}ÔøΩT%I\u0000~\u0013H\nÔøΩÔøΩ<ÔøΩÔøΩ\u00125H`1\u001ar\bÔøΩV8ÔøΩÔøΩ$6ÔøΩŸùÔøΩÿ∑&?ÔøΩÔøΩ*ÔøΩ\u000eÔøΩ…çÔøΩÔøΩ\"ÔøΩ\u0014ÔøΩÔøΩBÔøΩÔøΩ6jÔøΩÔøΩÔøΩ#spÔøΩ\u000ftjt~rÔøΩÔøΩÔøΩ{0Œ≠ÔøΩÔøΩÔøΩÔøΩ_:|ÔøΩ‘∑\u001b\u000e<ÔøΩÔøΩt`ÔøΩÔøΩ\nÔøΩÔøΩ;ÔøΩÔøΩÔøΩÔøΩ?ÔøΩ\u001aÔøΩÔøΩaÔøΩE;=1jÔøΩÔøΩ\u001f>\u0018ÔøΩÔøΩ\u000fÔøΩÔøΩrÔøΩÔøΩ\t3ÔøΩÔøΩ∆¶\u000fÔøΩÔøΩÔøΩOÔøΩ'ÔøΩ!ÔøΩz~ÔøΩÔøΩe;ÔøΩL_ÔøΩÔøΩÎìì8HÔøΩÔøΩ]ÔøΩnÔøΩÔøΩX\u0010ÔøΩÔøΩÔøΩblÔøΩ2ÔøΩtÀù\u0003jÔøΩ/Ÿ¥ÔøΩÔøΩÔøΩ'Œ¢ÔøΩ6ÔøΩÔøΩÔøΩ_lÔøΩÔøΩÔøΩQÔøΩÔøΩﬁêO//shÔøΩ\u0016WÔøΩÔøΩ\u0011ÔøΩ#\u0007ÔøΩ\u000fÔøΩS/jb6QÔøΩ\u0007ÔøΩo\u000fZS%\bÔøΩÔøΩ\u0014ÔøΩ$ÔøΩb2lÔøΩfÔøΩÔøΩ\noÔøΩUÔøΩSÔøΩÔøΩÔøΩ\u0012ÔøΩ\n-ÔøΩﬁÆ\n\u0005^k?ÔøΩÂøôÔøΩÔøΩÔøΩÔøΩa\u0013√êÔøΩÔøΩÔøΩV\n(`ÔøΩKÔøΩ.ÔøΩ8\u0004ÔøΩ=ÔøΩ`IÔøΩÔøΩ3ÔøΩÔøΩÔøΩ\nrdÔøΩjG/5'\u0002ÔøΩB\u0001\u0017ÔøΩÔøΩ0—É\u000eÔøΩF\u0005ÔøΩ(ÔøΩzÔøΩÔøΩY\u0002ÔøΩ\u0016ÔøΩIÁñ¨ÔøΩs\u001bVÔøΩchÔøΩÔøΩ8ÔøΩÔøΩ+ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ%,CÔøΩ·∏£ÔøΩÔøΩÔøΩ?\u0007?%ÔøΩ\u000fÔøΩÔøΩbQÔøΩÔøΩR^zÔøΩÔøΩTÔøΩ”ÅÔøΩÔøΩÔøΩPÔøΩÔøΩ\u0015ÔøΩÔøΩ\u0004ÔøΩ\u00115ÔøΩ<mÔøΩÔøΩ2ÔøΩÔøΩPÔøΩ;ÔøΩ:MÔøΩÔøΩ,VOFÔøΩaÔøΩÔøΩÔøΩ'Õ†\nlÔøΩ\u0002LzÔøΩEÔøΩÔøΩ1ÔøΩÔøΩUCt\u000eÔøΩ<\nY–á_ÔøΩÔøΩ\u0002ÔøΩnHÔøΩuÔøΩy@`ÔøΩ\nÔøΩ1H-ÔøΩRÔøΩ!ÔøΩÔøΩÔøΩo\u0000ÔøΩÔøΩ:ÔøΩk7ÔøΩÔøΩÔøΩÔøΩ\nÔøΩ1B$ÔøΩXÔøΩ\u0015\u0019tÔøΩÔøΩÔøΩ\u0012ÔøΩoÔøΩÔøΩ^rJdÔøΩ3\u0018ÔøΩÔøΩsÔøΩ\u0012:ÔøΩÔøΩ\u0010=ÔøΩÔøΩ\u0015uÔøΩkÔøΩLi(&+ÔøΩÔøΩÔøΩÔøΩÔøΩy{⁄¨ÔøΩÔøΩ\u000f\n;uÔøΩÔøΩeÔøΩÔøΩ\nÔøΩ+Jx\u001b«¨|ÔøΩÔøΩIÔøΩÔøΩ\u001aÔøΩÔøΩÔøΩ@ÔøΩÔøΩÔøΩÔøΩÔøΩ{<;ÔøΩw\u000fbÔøΩÔøΩÔøΩ\u000eÔøΩ\nÔøΩÔøΩGÔøΩÔøΩÔøΩ\u000fÔøΩÔøΩÔøΩÔøΩOÔøΩÔøΩÔøΩÔøΩ[c◊ßFÔøΩÔøΩÔøΩWÔøΩ}ÔøΩÔøΩ◊ìÔøΩpÔøΩHÔøΩÔøΩÔøΩ\u0017.ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u001bz>7\u000eÔøΩÔøΩ\u0010nNs\nÔøΩÔøΩ}ÔøΩ\nÔøΩuÔøΩS\noÔøΩÔøΩ\u0011ÔøΩgÔøΩÁ°å}ÔøΩÔøΩzÎé∏&ÔøΩgYÔøΩÔøΩÔøΩtÔøΩ\u0001ÔøΩ\"CÔøΩs[\"ÔøΩ\u0019j ÔøΩtXÔøΩÔøΩn\nj2ÔøΩÔøΩ0FÔøΩ\u001f@ÔøΩU8ÔøΩÔøΩ\nÔøΩLÔøΩ\nÔøΩÔøΩFp(ÔøΩ1K$|ÔøΩÔøΩ2k0ÔøΩ\u0016uÔøΩÔøΩÔøΩF)ÔøΩÔøΩ\u001a\tÔøΩ‰Ä°YÔøΩ}%4\u0004ÔøΩÔøΩÔøΩ4\u0002\u0003%%ÔøΩÔøΩ\nÔøΩ\u0011m/&ÔøΩ1ÔøΩ\tÔøΩÔøΩ2ÔøΩ.]bÔøΩ$W(ÔøΩÔøΩmÔøΩb\u0013$U!ÔøΩ—ãÔøΩ\u0016\u0018IÔøΩYÔøΩÔøΩÔøΩ@2ÔøΩ\u0015ÔøΩÔøΩÔøΩ\u0015ÔøΩÔøΩ*ÔøΩÔøΩ\b\u0001HFÔøΩ!\u0018ÔøΩsÔøΩÔøΩ≈ÑÔøΩGÔøΩhÔøΩ\u0003ÔøΩÔøΩÔøΩÔøΩfÔøΩ^,ÔøΩ1CKÔøΩZÔøΩÔøΩlVk3ÔøΩÔøΩ8\nÔøΩ\\ÔøΩ/\u0004#ÔøΩ\u0017*Q\u0006LHƒ∂UdÔøΩKÔøΩ\u0018d\u0014ÔøΩÔøΩtÔøΩ€Ø\u0006ÔøΩ:ÔøΩÔøΩÔøΩÔøΩ\u0002ÔøΩ\u0012ÔøΩ~w-ÔøΩk\tÔøΩmA\nÔøΩÔøΩ|M/ÔøΩ\u0000q3|;ÔøΩnVÔøΩÔøΩ\u0019ÔøΩÔøΩÔøΩÔøΩ[fÔøΩ“ä+ÔøΩÔøΩ4VÔøΩÔøΩÔøΩ\u0013\"ÔøΩ*X$c~L\u000eÔøΩiV⁄¢ÔøΩÔøΩÔøΩÔøΩ\n:9\u0004AÔøΩxÔøΩx‘™ÔøΩÔøΩ\ncÔøΩ\u001f<zÔøΩ[ÔøΩW$ÔøΩÔøΩ\u0015ÔøΩGÔøΩ:sÔøΩÔøΩ\u0003ÔøΩÔøΩ^ÔøΩÔøΩÔøΩÔøΩ\u0017v~rT(ÔøΩ ÔøΩÔøΩÔøΩK‘øNÔøΩÔøΩÔøΩ+ÔøΩÔøΩ{iÔøΩqÔøΩÔøΩÔøΩÔøΩ{ÔøΩÔøΩÔøΩÔøΩ.\\BÔøΩÔøΩlÔøΩmÔøΩzÔøΩÔøΩÔøΩÔøΩÔøΩ\u000f\u000fÔøΩÔøΩSÔøΩ}ÔøΩQÔøΩÔøΩ-ÔøΩ`ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩtÔøΩ\nLicÔøΩÔøΩKL4CÔøΩ\np)EÔøΩJ\u0019 πHÔøΩÔøΩlWÔøΩ ùÔøΩÔøΩ$≈èq\tÔøΩ2ÔøΩ_ÔøΩZ:v\nÔøΩÔøΩ\nÔøΩMÔøΩY∆ùÔøΩ<ÔøΩÔøΩ\u0002Z9ÔøΩHÔøΩÔøΩ\u0018ÔøΩÔøΩ<-ÔøΩCOÔøΩ=SÔøΩ\u0010ÔøΩÔøΩZ&qÔøΩ\u0018ÔøΩ7“ÄÔøΩ\u000e^F-ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nGÔøΩ`ÔøΩ*rÔøΩÔøΩ!ÔøΩÔøΩnWÔøΩbiÔøΩ\u0007ÔøΩÔøΩ\n\u0013X]F\u0015%ÔøΩÔøΩÔøΩ*…•ÔøΩ\t^v›µÔøΩÔøΩhEZÔøΩÔøΩyETÔøΩ!ÔøΩÔøΩÔøΩÔøΩZ\nÔøΩ\u00103ÔøΩ\u0014NÔøΩÔøΩu[b\nÔøΩÔøΩÔøΩÔøΩÔøΩY\u001aÔøΩo\tÔøΩÔøΩ\u0019ÔøΩ)ÔøΩ(LÔøΩdÔøΩ\u001arÔøΩ›üaÔøΩÔøΩ\u0000^¬Ü1ÔøΩ*ÔøΩf\\)\u0007eÔøΩ7UXÔøΩÔøΩq\u0018YÔøΩrÔøΩjYÔøΩÔøΩÔøΩU\nÔøΩÔøΩÔøΩ\u000e9ÔøΩ\u0010}ÔøΩÔøΩ^\nÔøΩeÔøΩ\nÔøΩÔøΩAÔøΩ.:tÔøΩ,/⁄∂L.ÔøΩC`hÔøΩ DÔøΩ\u0017ÔøΩ√µSA}ÔøΩ_QÔøΩV\u0017\u0019\u0005|ÔøΩ8'ÔøΩÔøΩ\u001aÔøΩ+ÔøΩcÔøΩ\u0001*7ÔøΩ\u0002ÔøΩkÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ,ÔøΩÔøΩ\u0000ÔøΩC\u0017ÔøΩFÔøΩÔøΩÔøΩÔøΩzÔøΩÔøΩ\u0015ÔøΩNÔøΩ>AÔøΩÔøΩEÔøΩ\u0017cÔøΩ\u0017/^ÔøΩÔøΩ‘πm{~ÔøΩÔøΩÔøΩ\u000fÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩbSLkÔøΩJ”™^”•_ÔøΩG>yÔøΩÔøΩÔøΩÔøΩÔøΩKÔøΩ-ﬂπÔøΩÔøΩÔøΩÔøΩg/\\V:Œ™ÔøΩÔøΩ\n8rbÔøΩMOÔøΩkÔøΩ\nÔøΩ\u001aRxÔøΩÔøΩ#\bDzp\u0005ÔøΩFÔøΩÔøΩÃòÔøΩaÔøΩ\u001a\u001b\u0000ÔøΩbÔøΩxSÔøΩ;<\u0019ÔøΩiÔøΩÔøΩGÔøΩÔøΩ,ÔøΩ⁄âkPÔøΩÔøΩCÔøΩAdÔøΩO2LÔøΩ9(KÔøΩh\nÔøΩjÔøΩ\u0019tÔøΩ\u0010u%ÔøΩ*KÔøΩKe\n\u0004ÔøΩ*ÔøΩ)ÔøΩGah$T\u0014\u0018\u0007t‰êòÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\t\u001bÔøΩÍ∑ßpÔøΩ\u0016\u0012Y^G?@ÔøΩrBÔøΩKÔøΩ\u0017\n!ÔøΩ[ﬂû\nÔøΩÔøΩmaÔøΩÔøΩcÔøΩÔøΩÔøΩ\u000eÔøΩÔøΩÔøΩÔøΩL&\u0010ÔøΩ0\u0006ÔøΩ)}mÔøΩ1y&lNVÔøΩ=3ÔøΩÔøΩÔøΩÔøΩ“äZÔøΩÔøΩ\n^(ÔøΩÔøΩ÷®\u000eÔøΩ6-zÔøΩ\u0018c0<ÔøΩgÔøΩe\u001bÔøΩÔøΩDEÔøΩ\n&ÔøΩÔøΩ\"ÔøΩÔøΩÔøΩ@ÔøΩÔøΩÔøΩÔøΩ0ÔøΩ[,\u000fÔøΩ\nÔøΩÔøΩ\"ÔøΩ=ÔøΩ[)\\ƒ†ÔøΩÔøΩyÔøΩM\u0004]^\bn-ÔøΩ \u0011\u0001ÔøΩnÔøΩ0JÔøΩ\u000fÔøΩ@\u000eÔøΩ\bÔøΩÔøΩ!\u0015\u0010ÔøΩ\u00103$<\u0015ÔøΩÔøΩq\nÔøΩÔøΩÔøΩjÔøΩ2√ßÔøΩÔøΩÔøΩrÔøΩ:\u0004FÔøΩÔøΩ\u000eÔøΩÔøΩÔøΩ~UÔøΩ%BhvFÔøΩP8ÔøΩvVÔøΩk)lÔøΩ\u0016ÔøΩ\np\u0012m\u0010\u0013\nÔøΩÔøΩÔøΩkPi\u0000ÔøΩÔøΩÔøΩ\\ÔøΩÔøΩ.ÔøΩÔøΩÔøΩÔøΩ@ÔøΩÔøΩ;\u0007T\u000f[ÔøΩnÀ°ÔøΩg.\\\u000eÔøΩvÔøΩ.K%ÔøΩ+\u0017.^:}ÔøΩ¬ÅÔøΩ'ÔøΩ.ÔøΩ9`‹™◊™fSN]ÔøΩÿàÔøΩÔøΩ\n|CÔøΩÔøΩVÔøΩ\u0001ÔøΩ\u0017\u001a\u0010t;0ÔøΩrÔøΩ[pUEÔøΩÔøΩ=\u0007\u0014ÔøΩ?ÔøΩÔøΩ_F=ÔøΩ⁄§ÔøΩÔøΩ\u001b1ÔøΩÔøΩ\u0015ÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩ\u0017/]ÔøΩÔøΩ\u000eiÔøΩ\u0015ÔøΩÔøΩ3ÔøΩÔøΩÔøΩ_Œ¨\\ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩGÔøΩA\u0016\\ÔøΩA;ÔøΩfÔøΩGÔøΩVÔøΩ.ÔøΩ\u0017ÔøΩÔøΩTÔøΩÔøΩ\u0004eÔøΩRzrÔøΩÔøΩ@ﬂïÔøΩ.ÔøΩÔøΩ>ÔøΩOÔøΩ9Õ∂t8\\ÔøΩÔøΩHÔøΩ\nÔøΩ(l\u00183ÔøΩ7WÃß\n7ÔøΩÔøΩÔøΩ21)ÔøΩ÷ñÔøΩLÔøΩÔøΩ…®uÔøΩÔøΩm…ãÔøΩÔøΩÔøΩÔøΩqÔøΩ\u0012h\u0012ÔøΩÔøΩPÔøΩ\u0012ÔøΩ^ÔøΩÔøΩÔøΩÔøΩQÔøΩ,ÔøΩ…æƒÖ\u00014ÔøΩQÔøΩÔøΩÔøΩ+n8c)ÔøΩvÔøΩÔøΩÔøΩGÔøΩQHÔøΩUR+ÔøΩIÔøΩÔøΩ>ÔøΩ$ÔøΩÔøΩ\u0015ÔøΩ%ÔøΩ\nÔøΩ\nÔøΩ2(ÔøΩÔøΩMÔøΩyBÔøΩÔøΩ,ÔøΩÔøΩÔøΩÔøΩ^ÔøΩ|mSp&M\nÔøΩoÔøΩdÔøΩN\nq\bd\\ÔøΩ\nd8E^\u0011ÔøΩÔøΩpndÔøΩÔøΩ;8\u0010dwÔøΩÔøΩ\u0001oÔøΩf\n;>ﬂí\n\u001a;ÔøΩÔøΩÔøΩbÔøΩÔøΩ\u0018s\u001a\u0013ÔøΩ_f\nfÔøΩe\n*M+ÔøΩiÔøΩÔøΩÔøΩÔøΩ1ÔøΩ8ÔøΩ\u0014ÔøΩ1\u0002LM/ÔøΩ ÔøΩÔøΩÔøΩÔøΩÔøΩAÔøΩ\\M\u0018ÔøΩÔøΩ]ÔøΩ\nÔøΩÔøΩÔøΩÔøΩ9\u0004WÔøΩ~\n3ÔøΩAZ\u0019ÔøΩ\u0012ÔøΩÔøΩOJÔøΩoÔøΩoÔøΩ\u0000\u0016!ÔøΩ1uvÔøΩÔøΩÔøΩ(\n}¬ü\u0004\u0007ÔøΩ»ÇÔøΩ\\XÔøΩ\"\\e…î,}⁄µ]ÔøΩu{zÔøΩ[ÔøΩÔøΩÔøΩÔøΩlÔøΩÔøΩÔøΩOÔøΩ/\\RÔøΩÔøΩW(ÔøΩÔøΩÔøΩ\u0014d*ÔøΩ\u0017ÔøΩÔøΩgÔøΩ^ÔøΩ\nwÔøΩÔøΩ_ÔøΩÔøΩ>ÔøΩ~ÔøΩÔøΩo7\nXÔøΩvﬂ¢UÔøΩg/ÔøΩÔøΩÔøΩÔøΩ\ndKÔøΩÔøΩKÔøΩÔøΩÔøΩÔøΩ\u001bÔøΩ\nÔøΩ\u000f<xÔøΩÔøΩ'œù;wQm1ÔøΩÔøΩv9dTÔøΩÔøΩÔøΩ\nk7\u001f\u001a9mÔøΩ?>ÔøΩÔøΩÔøΩ3cwGÔøΩœãÔøΩÔøΩÔøΩ\tÔøΩÔøΩ∆µÔøΩZÔøΩ\u0014XÔøΩZfÔøΩÔøΩ\u0013ÔøΩÔøΩÔøΩ\tÔøΩDk{ÔøΩÔøΩÔøΩL~ÔøΩ\u0001ÔøΩÔøΩ0\n*OY}ÔøΩH\\ÔøΩ^ÔøΩG\nÔøΩWÔøΩDÔøΩÔøΩDÔøΩ|ÔøΩ':2ZnÔøΩÔøΩ?ÔøΩlÔøΩÔøΩfÔøΩZjg\n…àÔøΩHNÔøΩÔøΩn\u0016ÔøΩ\u0005ÔøΩÔøΩEÔøΩ\nÔøΩ[ÔøΩÔøΩÔøΩÔøΩ\bZ}ÔøΩÔøΩÔøΩÔøΩÔøΩDbÔøΩUBQÔøΩYVÔøΩ8Q\u0006ÔøΩ\u0016\nÔøΩÔøΩ{%*ÔøΩ:ÔøΩÔøΩÔøΩkÔøΩÔøΩW@))ÔøΩÔøΩ2ÔøΩ%7ej-'DÔøΩnÔøΩÔøΩÔøΩ\\ÔøΩÔøΩÔøΩG5\tÔøΩg\u0004=ÔøΩÔøΩÔøΩÔøΩbÔøΩ”≠xÔøΩÔøΩuÔøΩÔøΩH∆®ÔøΩ\u0007V;rÔøΩ'\nw\u0001+ÔøΩ\u001bÔøΩ0ÔøΩPÔøΩÔøΩÔøΩ…£ÔøΩ&ÔøΩj>EcÔøΩÔøΩÔøΩcÔøΩÔøΩGÔøΩÔøΩ:ÔøΩbd%Õ©ÔøΩOÔøΩTÔøΩÔøΩÔøΩ&\u0000YmÔøΩp86Qan/ÔøΩ\nÔøΩ;S0Ÿ∫ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\\/ÔøΩJ\u0018ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0016TÔøΩÔøΩÔøΩ\u000ehÔøΩÔøΩ5ÔøΩ≈±ÔøΩÔøΩÔøΩeÔøΩ5\nÔøΩÔøΩ\u0014Ÿâ\u000eÔøΩÔøΩÔøΩÔøΩÔøΩ1MÔøΩzÔøΩÔøΩ\u0005:'rS8ÔøΩÔøΩ!+[ÔøΩ\nZÔøΩÔøΩ\u0007ÔøΩ\u000e\u0018ÔøΩjÔøΩÔøΩ_/…ÑWÔøΩÔøΩ/ÔøΩwÔøΩÔøΩ5&ÔøΩÔøΩÔøΩÔøΩÔøΩirÔøΩ\u000fÔøΩÔøΩ]ÔøΩtÔøΩÔøΩÔøΩÀøÔøΩÔøΩÔøΩ+ÔøΩ/^⁄¥ÔøΩÔøΩ;ÔøΩÔøΩÔøΩ‘£?ÔøΩF(#ÔøΩÔøΩsÔøΩÔøΩÔøΩjÔøΩhOSÔøΩBÔøΩoÔøΩ*\u0018ÔøΩ\u0016\u0013sk\u000eÔøΩPÔøΩb w`\u0013ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0018ÔøΩvÔøΩYÔøΩÔøΩLÔøΩZnÔøΩÔøΩ√ùÔøΩ~*,%ÔøΩ\u0004ÔøΩ\u001fÔøΩDoÔøΩLBXÔøΩ^\u001f\u0005ÔøΩX\n6TÔøΩ)\u0016ZÔøΩÔøΩJÔøΩÔøΩ\nÔøΩÔøΩt\u0013ÔøΩÔøΩ\u0006ÔøΩi\u00071ÔøΩ<ÔøΩ_\u0013\u0006ÔøΩIVtÔøΩÔøΩÔøΩÔøΩIG.ÔøΩÔøΩ*\u0013ÔøΩÔøΩÔøΩ:paÔøΩ\u0002)ÔøΩ(ÔøΩ3ÔøΩ?ÔøΩÔøΩ\nÔøΩÔøΩ6BÔøΩQ&ÔøΩVq\u0002\u0018ÔøΩƒ∂\u0010ÔøΩÔøΩLÔøΩÔøΩ=ÔøΩ>ÔøΩÎüùHÔøΩÔøΩÔøΩSÔøΩV)CÔøΩQEÔøΩÔøΩÔøΩ@ÔøΩ1OÔøΩ&ÔøΩ!-ÔøΩ$m]I\u0006ÔøΩÔøΩ\n\u0006ÔøΩÔøΩ^:ÔøΩ0ÔøΩƒí,bÔøΩ/\nÔøΩÔøΩÔøΩVQ'|E,ÔøΩFÔøΩFÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩTÔøΩsÀ©\u0015ÔøΩ\u000fP\u0016\u0010ÔøΩÔøΩ`ÔøΩY\u0018ÔøΩÔøΩÔøΩJÔøΩMÔøΩ&ÔøΩÔøΩÔøΩÔøΩsÔøΩu]\u001bÔøΩÔøΩÔøΩÔøΩÔøΩMI4j; èÔøΩÔøΩ`ÔøΩÔøΩ\nÔøΩ\u000eXÔøΩÔøΩsÔøΩI\nÔøΩÔøΩRÔøΩtÔøΩÔøΩÔøΩ\u0004ÔøΩÔøΩÔøΩÔøΩÔøΩsÔøΩÔøΩ\u0007ÔøΩÔøΩVÔøΩÔøΩ»†ÔøΩ2ÔøΩÔøΩÔøΩTlÔøΩÔøΩhKÔøΩÔøΩkÔøΩÔøΩœèÔøΩ];g–ßÔøΩgÔøΩÔøΩÔøΩÔøΩÔøΩ\u0003ÔøΩÔøΩ\n?rÔøΩÔøΩÔøΩ3\u0017(ÔøΩV\u0013gÔøΩDÔøΩÔøΩÔøΩÔøΩÔøΩ-ÔøΩÔøΩr ÔøΩÔøΩÔøΩK'NÔøΩ?xÔøΩÔøΩÔøΩ]ÔøΩVÔøΩÔøΩ7mÔøΩÔøΩ\u0001cWÔøΩÔøΩwVÔøΩgÔøΩÔøΩÔøΩs`XÔøΩ\nL+ÔøΩ ÔøΩÔøΩxÔøΩ<HÔøΩÔøΩÔøΩISÔøΩgz\nÔøΩaÔøΩ}ÔøΩO!÷ã_ÔøΩVÔøΩÔøΩ]IÔøΩÔøΩÔøΩAwÔøΩkÔøΩÔøΩ{a.PÔøΩ%\u0005ÔøΩÔøΩ\u0001ÔøΩ\u0007\nÔøΩxÔøΩ<ÔøΩ\u0018wÔøΩÔøΩ:36ÔøΩCtÔøΩFÔøΩ\u0005P\u0015ÔøΩ\u000emNhÔøΩ>GÔøΩÔøΩÔøΩÔøΩ\u0016ÔøΩxÔøΩÔøΩ\u0011N^ÔøΩSÔøΩÔøΩ-ÔøΩMÔøΩ(VWfHXw ÔøΩ-\u001blÔøΩÔøΩ,ÔøΩ\u0006‘≠5ÔøΩÔøΩÔøΩrS“äÔøΩZÔøΩPÔøΩÔøΩfm2kÔøΩÔøΩÔøΩÔøΩ\u0005ÔøΩ2ÔøΩ\nÔøΩ\u0013ÔøΩaÔøΩSÔøΩÔøΩDUÔøΩÔøΩÔøΩ2\u000eﬂº\u000e11ÔøΩiÃókÔøΩÔøΩ\u0013\nmNÔøΩÔøΩÔøΩ?ÔøΩLE2ÔøΩ(ÔøΩjÔøΩ\nUÔøΩ(ÕΩÔøΩ\u001bÔøΩ\u0006PPÔøΩÔøΩ|ÔøΩNÔøΩÔøΩ\u0017juqÔøΩ\u0018ÔøΩÔøΩÔøΩÔøΩm[O:ÔøΩÔøΩ`ÔøΩÔøΩO+6ÔøΩ ÔøΩÔøΩ 1ÔøΩÔøΩt~ÔøΩ\u000e\u0013sÔøΩqKUÔøΩÔøΩÔøΩS@ÔøΩ<ÔøΩ#ÔøΩ2ÔøΩ;ÔøΩ]”•\u001fÔøΩÔøΩ7tkÔøΩÔøΩÔøΩÔøΩkÔøΩ‘∑\u0012ÔøΩ”í\u0010ÔøΩ\u0012⁄¨ÔøΩÔøΩ[ÔøΩnÔøΩ<6\nÔøΩÔøΩSÔøΩÔøΩA\u0018\"ÔøΩÔøΩ\u0018ÔøΩ~9ÔøΩZI,yxÔøΩÔøΩ-tLÔøΩ8LÔøΩ7ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩwÔøΩÔøΩ”ßﬂùÔøΩNÔøΩÔøΩÔøΩ\u0011ÔøΩGN]3eŒ¶ÔøΩÔøΩvÔøΩ[ÔøΩkŒ≤ÔøΩKÔøΩÔøΩ3wÔøΩÔøΩÔøΩKwLÔøΩÔøΩ\u0003ÔøΩmÔøΩÔøΩÔøΩ}\u001a\u0017ÔøΩÔøΩÔøΩ\u0019@OÔøΩ“≥\nÔøΩÔøΩÔøΩ %\n\u0019\nÔøΩRo\n\u0001LÔøΩÔøΩqwÔøΩ7\u0006\u000fÔøΩ860FÔøΩ1ÔøΩ\t\u000eÔøΩ&ÔøΩÔøΩXÔøΩ ÔøΩ\u0011ÔøΩÔøΩ≈ùÔøΩÔøΩPoÔøΩUÔøΩS\u0006ÔøΩ\\sVbÔøΩÔøΩÔøΩ\nDÔøΩÔøΩ\u0019\n\u0013t[ÔøΩHkÔøΩÔøΩÔøΩJÔøΩ?ÔøΩRÔøΩ-ÔøΩNÔøΩÔøΩÔøΩ(ÔøΩÔøΩ\tÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ@VDÔøΩÔøΩ'ÔøΩzÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0005‹∏⁄ûÔøΩ,d,ÔøΩÔøΩÔøΩ@\u001a]LAÔøΩ-ÔøΩa&ÔøΩÔøΩÔøΩÔøΩ-\u0019ÔøΩÔøΩÔøΩHHÔøΩ\nihyÔøΩ\"iÃ´ÔøΩÔøΩÔøΩÔøΩÔøΩ’¥ÔøΩmÔøΩÔøΩ\u0002ÁÉëÔøΩYEamÔøΩrÔøΩ\u0016\u0003ÔøΩ\u0001\b\u0007TÔøΩÔøΩÔøΩÔøΩf&\nÔøΩÔøΩdtÔøΩÔøΩÔøΩ…≠\u0015ÔøΩÔøΩ/ÔøΩfÔøΩ\\ÔøΩ\u0004ÔøΩOÔøΩ¬ßÔøΩÔøΩ-\u0019ÔøΩÔøΩoÔøΩÔøΩDÔøΩ»ä\u001bÔøΩÔøΩÔøΩ›ç\u0015DÔøΩ&ÔøΩDÔøΩÔøΩHLÔøΩzÔøΩIÔøΩ%KÔøΩ ÔøΩÔøΩWq\u0006}mÔøΩ\u0006JÔøΩÔøΩ\u0013\u0019t?ZÔøΩ\u0002ÔøΩ\nt`*ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩo@ÔøΩÔøΩÔøΩ`ÔøΩÔøΩÔøΩ1ÔøΩÔøΩ\u0017ÔøΩ]VÔøΩ\u0007\u0006ÔøΩÔøΩ^ÔøΩ<ÔøΩÔøΩÔøΩÔøΩc\u0016\u0005kÔøΩyeÔøΩ+ÔøΩfK0#DZ\u0013?q\bÔøΩÔøΩMÔøΩÔøΩx\u0013m}ÔøΩ$ÔøΩt\u0010ÔøΩZÔøΩ\u0019\u001bÔøΩ2ÔøΩjÔøΩxÔøΩÔøΩ|M0ÔøΩMÔøΩ|\u001a\nÔøΩÔøΩ3!ÔøΩ[ÔøΩÔøΩÔøΩ0ÔøΩj(GÔøΩ|ÔøΩÔøΩjVÔøΩÔøΩBhVÔøΩ\u0003\u0013ÔøΩÔøΩ*ÔøΩ]$ÔøΩ\u0011\u0003ZmÔøΩ\u0000tÔøΩ(` \u0019ÔøΩ\tÔøΩ\t\u000eÔøΩm\tKÔøΩ;ÔøΩÔøΩh\u000fÔøΩAÔøΩM2bdÔøΩQÔøΩ\\»øÔøΩ\u0013mÔøΩ\nÔøΩÔøΩBÔøΩ\u0014ÔøΩlWÔøΩURÔøΩdÔøΩPdÔøΩÔøΩaÔøΩÔøΩÔøΩR\u000feÔøΩIÔøΩÔøΩlWÔøΩÔøΩsÔøΩÔøΩÔøΩÔøΩÔøΩ\tÔøΩÔøΩÔøΩ…ÖRKÔøΩÔøΩ^UKaÔøΩ+\u0002ÔøΩÔøΩWÔøΩ2\"ÔøΩÔøΩdO_s⁄ÖÔøΩ$+ÔøΩ\u0016@BÔøΩÔøΩÔøΩÔøΩ!XB\u0019\n&4ÔøΩÔøΩn-ÔøΩÔøΩÔøΩÔøΩXÔøΩ<ÔøΩÔøΩÔøΩ8dYÔøΩÔøΩÔøΩ>ÔøΩP\u0013\u0019tGÔøΩ2hV\nÔøΩÔøΩ;ÔøΩ/ÔøΩ\u0001&›∂$ÔøΩÔøΩÔøΩ\u0012ÔøΩÔøΩ?3ÔøΩTÔøΩ\u001a@{\"\nB{ÔøΩZÔøΩÔøΩÔøΩTÔøΩ$ÔøΩZÔøΩÔøΩ3=ÔøΩ}mÔøΩOÔøΩÔøΩÔøΩKÔøΩÔøΩ.ÔøΩÔøΩÔøΩÔøΩ\u0007LB!\n6ÔøΩN\\ÍáÑÔøΩÔøΩÔøΩEÔøΩUE]ÔøΩÔøΩ\u0011ÔøΩh-Gy2wÔøΩ\u00018r}\nBMA2RÔøΩ'%ÔøΩ\u00189\":ÔøΩÔøΩÔøΩ~ÔøΩÔøΩÔøΩÔøΩmL\u0013E.r*GÔøΩ,ÔøΩ\u0015ÔøΩ\u0000ÔøΩ'ÔøΩÔøΩÔøΩ0ÔøΩÔøΩIÔøΩsÔøΩ&0ÔøΩeM∆πSÔøΩ$e+ÔøΩ\n.ÔøΩ5ÔøΩ]⁄äkÔøΩÔøΩÔøΩ@ÔøΩ;ÔøΩÔøΩÔøΩVÔøΩÔøΩÔøΩ_Q\u0004pT9]UmfÔøΩP\u0005Z* s\u0014ÔøΩ`wÔøΩ%ÔøΩHÔøΩ!≈™ÔøΩAv*lV\u0019\u0003ÔøΩÔøΩÔøΩÔøΩ vÔøΩ3ÔøΩÔøΩÔøΩÔøΩÔøΩ'x<ÔøΩÔøΩÔøΩ(M3hZÔøΩÔøΩÔøΩ3ÔøΩ\nÔøΩ\nÔøΩ*Q&ÔøΩ\u0004ÔøΩYÔøΩÔøΩ\n*AÔøΩÔøΩ*\u0001fÔøΩLÔøΩ.ÔøΩ)YL!ÔøΩÔøΩÔøΩz\nEÔøΩÔøΩÔøΩ&,ÔøΩ ÔøΩ\u000e\u0015ÔøΩÔøΩÔøΩz \u0004ÔøΩNÔøΩp\bÔøΩUhÔøΩ?ÔøΩ6ÔøΩV\\ÔøΩ+E]TÔøΩÔøΩ,eW\u0013ŒºTÔøΩÔøΩ[ÔøΩÔøΩKÔøΩs\u0007ÔøΩÔøΩ^Z\nÔøΩÔøΩÔøΩ\u0016\u0011ÔøΩc:\"Àë\u0003.ÔøΩÔøΩ\nÔøΩÔøΩ}ÔøΩShÂ∏∞ÔøΩ[&ÔøΩIÔøΩ\u000eA\u0012ÔøΩ@>ÔøΩVÔøΩr\n0pÔøΩ—î2ÔøΩÔøΩ\u0000ÔøΩÔøΩd\u0005\n\u0003\\LÔøΩV@ÔøΩ~ÔøΩ\u0016ÔøΩÔøΩÔøΩÔøΩ\bÔøΩiÔøΩ_ÔøΩ/ ÔøΩ\u0012E#\n…ÉÔøΩ\u001bÔøΩ\nÔøΩ`g$ÔøΩ\n\u0016\u0001ÔøΩ\u0006hÔøΩÔøΩJ*ÔøΩi^hÔøΩÔøΩ\"ÔøΩÔøΩÔøΩ,ÔøΩÔøΩÔøΩX&\u0006ÔøΩSu\u0002ÔøΩÔøΩ,ÔøΩÔøΩ-QÔøΩ\"÷üÔøΩÔøΩ;ÔøΩ:pWÔøΩ+ÔøΩÔøΩw—≤ÔøΩ;ÔøΩÔøΩ$Z@ÃäÔøΩÔøΩ\u0010ÔøΩÔøΩ\u0002\u000eÔøΩ>GZ&ÔøΩ\nÔøΩ\u001aÔøΩÔøΩÔøΩÔøΩr73huÔøΩ\u0019ÔøΩÔøΩÔøΩXÔøΩ\n\u0010\nÔøΩÔøΩÔøΩÀä5q\bMEZAKeÔøΩ!ÔøΩ2ZÔøΩlFQÔøΩB|ÔøΩnÔøΩÔøΩ\nÔøΩÔøΩlV\\.\u0017ÔøΩTÔøΩ\u0005ÔøΩÔøΩ9\u001bXU\u0018ÔøΩ\u0001ÔøΩJÔøΩÔøΩ\u0005ÔøΩAÔøΩ!ÔøΩ›¢UUJÔøΩ^ÔøΩOÔøΩ\u0019%bÔøΩ\u000eÔøΩÔøΩdÔøΩ\nSXÔøΩ[{vÔøΩ\nhÔøΩÔøΩÔøΩÔøΩCYÔøΩE»°zGÔøΩÔøΩÔøΩÔøΩ+\u0014ÔøΩrÔøΩ1DÔøΩ–∂&+ÔøΩ\u000eÔøΩo\bÔøΩÔøΩ\u001aÔøΩ.60TÔøΩÔøΩ\u0007lu\bÔøΩÔøΩÔøΩRm\u0006ÔøΩÔøΩÔøΩÔøΩÔøΩ6R'ZÔøΩ4L,ÔøΩHrbÔøΩ\u0001\u000eÔøΩ\t\u0010K\u0002\u0006ÔøΩuÔøΩ|ÔøΩ\\ÔøΩ'ÔøΩÔøΩ[\nrIQ\u001adÔøΩpÔøΩ8`ÔøΩ€ÇÔøΩÕ¥”ÄÔøΩ\t:\u0004:sÔøΩÔøΩTÔøΩÔøΩ\neRNÔøΩAÔøΩ.‘¨ÔøΩÔøΩ2ÔøΩ101ÔøΩ\u0012ÔøΩ4ÔøΩ\u001fÔøΩG\t$\bFÔøΩÔøΩ*√îÔøΩÔøΩÔøΩ\n\u001bFÔøΩ‘¶wLrkhÔøΩ\nÔøΩ28ÔøΩqÔøΩÔøΩ\u0000ÔøΩÔøΩÔøΩ;\u0013^ÔøΩZ4ÔøΩÔøΩ%ÔøΩÔøΩIÔøΩs\u0019∆Ä\u0003'ÔøΩ79ÔøΩ\u0007ÔøΩEÔøΩNÔøΩ\u0015PÔøΩX\u00194\u0016\u0004t|ÔøΩÔøΩ30ÔøΩ\u001fÔøΩ)ÔøΩÔøΩ_ÔøΩÔøΩrQÔøΩpÔøΩ~ÔøΩ\n‘™\\ÔøΩGÔøΩ\nWE\u001aÔøΩ%[/\u0003Xo/\u0010ÔøΩ+tÔøΩÔøΩ+\u0006ÔøΩÔøΩZÔøΩ9\u0018\u0016t\bÔøΩÔøΩU\bÔøΩÔøΩh\u0016”ò,ÔøΩh{ÔøΩ9ÔøΩ${j‹ô\u0010_aLÔøΩcÔøΩÔøΩqÔøΩ)\u0014LÔøΩÔøΩU)ÔøΩÔøΩ/ÔøΩ\u001f)ÔøΩÔøΩÔøΩ_SÔøΩ2ÔøΩ(1ÔøΩc2.EÔøΩ›É\nÔøΩÔøΩ:ÔøΩÔøΩÔøΩÔøΩ)H2ÔøΩ \u0014ÔøΩÔøΩÔøΩ|AGSÔøΩj ÔøΩÔøΩsÔøΩÔøΩgÔøΩÔøΩW2ÔøΩ\u0015ÔøΩÔøΩR\u0013D\no|*ÔøΩ\u00191ÔøΩÔøΩ\u0010\bqÔøΩÔøΩCpdÔøΩIƒì\u001bÔøΩLÔøΩÔøΩÔøΩ1]ÔøΩÔøΩÔøΩ`ÔøΩ6}\u0012ÔøΩPÔøΩÔøΩ8ÔøΩÔøΩÔøΩ\t\u0016\u0017ÔøΩÔøΩÔøΩ@9ÔøΩX[A)ÔøΩ\u0006ÔøΩrk\u00186ÔøΩ\u0001ÔøΩÔøΩ+hÔøΩ|!\"ÔøΩy\nÔøΩ\u0013¬£jÔøΩ\u0010ÔøΩÔøΩJNÔøΩ\u000e2ÔøΩÔøΩaÔøΩ\u0014ÔøΩ$d0??ŒæÔøΩ-ÔøΩ\nJÔøΩUÔøΩ0RÔøΩ\u0003ÔøΩ#ÔøΩ7Xr8z îÔøΩKOÔøΩÔøΩ\u0018ÔøΩÔøΩÔøΩ»âÔøΩHKVfÔøΩÔøΩÔøΩÔøΩÔøΩ\n\u0014ÔøΩÀ∏ÔøΩ~j\u001bÔøΩÔøΩÎÑúATÔøΩ\n\u001bÔøΩ3ÔøΩÔøΩI…∂vBÔøΩÔøΩÔøΩ&ÔøΩ<)\u0014ÔøΩ\u001b\u0005r1ÔøΩI\u001a\u0013ÔøΩÔøΩÔøΩÔøΩ^ÔøΩAnÔøΩÔøΩÔøΩÔøΩEÔøΩ48ÔøΩsÔøΩÔøΩÔøΩÔøΩBÔøΩO\u0010D{ &ÔøΩSÔøΩ5ÔøΩAj&&x=ÔøΩ_ÔøΩÔøΩÔøΩA+'ŒèÔøΩkb\u00079\tB/ÔøΩÔøΩÔøΩ\\\nÔøΩ=ÔøΩd\u0014ÔøΩ]KÔøΩÔøΩ\u0006\n(ÔøΩ\n.h«π\u0000ÔøΩÔøΩFÕ†)ÔøΩÔøΩ\u0012\nÔøΩÔøΩ8Œñ~&ÔøΩÔøΩNlÔøΩ|ÔøΩÔøΩÔøΩÔøΩV\n1MÔøΩ,(c*\nRiÔøΩ–ÖÔøΩq÷™ÔøΩÔøΩÔøΩ#pÔøΩ1GÔøΩÔøΩÿ•ÔøΩÔøΩ\u001a=\u0006ÔøΩÔøΩÔøΩ,ÔøΩ\u0015ÔøΩ”ä\u0005\n]nÔøΩ;ÔøΩÔøΩÔøΩ\u001bÔøΩXÔøΩ\n]\u0000\u0015AÔøΩG\u000e!#ÔøΩ! HÔøΩ,ÔøΩCÔøΩ6ÔøΩÔøΩfÔøΩuf!ÔøΩv|?\n~ÔøΩÔøΩ:ÔøΩÔøΩ%jÔøΩÔøΩd'\u001arÔøΩ\u000ePÔøΩÔøΩÔøΩJy~kÔøΩÔøΩÔøΩ3ÔøΩ\nÃ†ÔøΩ$rÔøΩ\u0006ÔøΩ:V}f&ÔøΩ65ÔøΩ~bÔøΩUÔøΩw_\u0005,3ÔøΩÔøΩt\bÔøΩ(ÔøΩÔøΩÔøΩÔøΩCÔøΩg1ÔøΩÔøΩÔøΩ‹´ÔøΩ0b\n\bÔøΩÔøΩ8Er\nÔøΩ—∑@lÔøΩ)hZdÔøΩÕà}+ÔøΩÔøΩÔøΩÔøΩ\u0005ÔøΩIÔøΩ&Ÿâ\u0013ÔøΩs”èÔøΩ}\nÔøΩÁúÜ‘†{\u001bÔøΩw\u0019ÔøΩ'ÔøΩgÔøΩÔøΩD \u0019ÔøΩÔøΩ4\u0019ÔøΩÔøΩ9ÔøΩ&ÔøΩ}QVZÔøΩjp\nÔøΩ\u000foTÔøΩ /`ÔøΩÔøΩ\u0003ÔøΩÔøΩÔøΩNV»≤\\ÔøΩcÔøΩ\u0017ÔøΩ\u0007ÔøΩÔøΩÔøΩ\u0007ÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩ2ÔøΩÔøΩÔøΩ970ÔøΩÔøΩÔøΩdiÔøΩ]«ï(ﬁñVÔøΩ\u0017ÔøΩ\u000e\u0001ÔøΩfÔøΩ\u0013ÔøΩC\u0000ÔøΩÔøΩ\tÔøΩÔøΩÔøΩÔøΩÔøΩLÔøΩSÔøΩ\u0012ÔøΩ!ÔøΩ\u0015W¬©FÔøΩ[Z4&ÔøΩw;HÔøΩ\u0004ÔøΩ\"Q\u0011ÔøΩ\u0001\u0003ÔøΩ\u0014LwÔøΩeA)mEiZ\u00144ÔøΩÔøΩÔøΩÔøΩÔøΩrÔøΩÔøΩÔøΩÔøΩÔøΩtÔøΩm$pÔøΩXÔøΩ\u0011jG\u0018\nÔøΩÔøΩÔøΩiEBÔøΩÔøΩ@ÔøΩÔøΩSÔøΩÔøΩA\nÔøΩÔøΩÔøΩYeÔøΩÔøΩ-ÔøΩSÔøΩxuÔøΩ:ÔøΩ}&ÔøΩ‘öPÔøΩÔøΩÔøΩÔøΩ`\nÔøΩZKgÔøΩ3Á±•ÔøΩ»†msÔøΩÔøΩr\nÔøΩÔøΩ8w\u001aSÔøΩÔøΩ:[\u0003L\u0016ÔøΩ\u001f\u0016gÔøΩ>ÔøΩÔøΩ\u0016,ÔøΩ\u000eR*1\nƒü[jÔøΩÔøΩ'ÔøΩqÔøΩ6ÔøΩÔøΩÔøΩÔøΩaÔøΩÔøΩÔøΩÔøΩÔøΩABFÔøΩj\u0004ÔøΩÔøΩÔøΩ5\"\u0013\u0016jTÔøΩT&ÔøΩ\u0010?,g9ÔøΩ\u0005{!ÔøΩÔøΩ8mS\u0014!ÔøΩƒüÔøΩ–îÔøΩpÔøΩÔøΩ\u001bB>ÔøΩ`&5ÔøΩ\nÔøΩÔøΩ-ÔøΩÔøΩ_ÔøΩIÔøΩRv\u0000ItÔøΩÔøΩ…è6\u0000ÔøΩÔøΩY$ÔøΩ(ÔøΩ\u00179/sHÔøΩSÔøΩbÔøΩ\n\u001bbQ:}\nÔøΩRÔøΩ\\ÔøΩ2KvÔøΩ\u0013\nÔøΩVÔøΩÔøΩ^HÔøΩƒ§\u0001OÔøΩ€óÔøΩ+ÔøΩÔøΩ\u000eÔøΩ~«π\n@\u0018D7VÃÄ\u0005\u0018ÔøΩÔøΩZÔøΩ\u0004&ÔøΩÔøΩR'ÔøΩaÔøΩÔøΩÔøΩ\u000eÔøΩ\u001fÔøΩÔøΩÔøΩh\u00123I ∞ÔøΩÔøΩ>]RÔøΩCÔøΩÔøΩp\bÔøΩÔøΩ!\u0000ÔøΩÔøΩÔøΩK\u0017ÔøΩhÔøΩÔøΩJÔøΩ\nÔøΩUÔøΩÔøΩ\u0018DRHÔøΩ\u0013\u0016\u0004|ÔøΩÔøΩÔøΩ\u0006\u0018.O\nÔøΩLÔøΩ$ÔøΩ\nÔøΩÔøΩ\u0002\u0001ÔøΩ\bÔøΩ8ÔøΩÔøΩBÔøΩ”πtiÔøΩnÔøΩ\u0019ÔøΩ)ÔøΩbYÔøΩ\u0014ÔøΩÔøΩÔøΩ ∑#ÔøΩOÔøΩZZ\u00066|Zq%ÔøΩ~^ÔøΩ67ÔøΩÔøΩAÔøΩPZ{UÔøΩÔøΩÔøΩÔøΩcÔøΩÔøΩ:ÔøΩaÔøΩÔøΩÔøΩ9ÔøΩÔøΩAD„¨ßbÔøΩ ZÔøΩÔøΩ\u0016\u0015ÔøΩ9BÔøΩXÔøΩÔøΩa6ÔøΩu8ÔøΩ)@;ÔøΩZÔøΩOÔøΩ\u0001\u0001\u0013pvÔøΩÔøΩÔøΩÔøΩBhÔøΩÔøΩÔøΩ5nÔøΩ>ÔøΩÔøΩÔøΩfÔøΩo\n:\u00041ÔøΩ\"fÔøΩ\u0005ÔøΩÔøΩ\u0005ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0015ÔøΩ({ÔøΩ0aÔøΩÃ†9ÔøΩfÔøΩ\nÔøΩ\ts\u0007\nÔøΩ\u0018ÔøΩÔøΩZ\u0006ÔøΩ (ÔøΩ\n\u0010+ÔøΩÔøΩÔøΩ1ÔøΩÔøΩLzsÔøΩÔøΩÔøΩgÔøΩIÔøΩNqÔøΩÔøΩI\u0019\u0018\nÔøΩÔøΩ‰∏ÉaJÔøΩ\u001apÔøΩTR\u00195i_Â≥∞ÔøΩ ÔøΩÔøΩHÔøΩJÔøΩ]ÔøΩÔøΩmÔøΩ\nnÔøΩkÔøΩÔøΩ\u0014ÔøΩhÔøΩ%ÔøΩÔøΩrkÔøΩÔøΩ\u0018ÔøΩ`ÔøΩ:w\u001b^2}ÔøΩ ÔøΩ\nÔøΩ&ÔøΩÔøΩÔøΩuaxÔøΩÔøΩÔøΩqK.ÔøΩÔøΩÔøΩÔøΩb‹±\n<09h%DÔøΩ#ÔøΩ\u0001ÔøΩÔøΩo,}fÔøΩV \u0019ÔøΩ\nuÔøΩ\u0018\u001a‰Ü¶ÔøΩ\u0010<ÔøΩgÔøΩÔøΩÔøΩ\u0004\u0011ÔøΩÔøΩ\n\u0007ÔøΩaq\u0006ÔøΩxIÔøΩÔøΩIÔøΩÔøΩÔøΩRÔøΩRÔøΩ9nÔøΩÔøΩ\nAÔøΩ\"ÔøΩ\u00051ÔøΩ!i[ÔøΩ$miZDÔøΩ€≤b\u0014ÔøΩÔøΩÔøΩÔøΩvÔøΩ‹∂FÔøΩLÔøΩÔøΩ]ÔøΩ0%ÔøΩI‘©2ÔøΩÔøΩ.AÔøΩÔøΩ\"ÔøΩÔøΩOIÔøΩ_jA\u000eœ∑\u0003ÔøΩÔøΩ'8rÔøΩÔøΩ\\OÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩuF<;!'ÔøΩpj\u000f\u0016}ÔøΩdwÔøΩ\nÔøΩ\nS'œ¨5ÔøΩ\tÔøΩ}vÔøΩ\u0004ÔøΩ\u0011\u0017;ÔøΩWt|Jcj(ÔøΩgÔøΩ\u0010ÔøΩÔøΩ;¬õ\u0015\u0015UÔøΩYÔøΩH%ÔøΩ\nH\u0019@0ÔøΩÔøΩ'ÔøΩakÔøΩ…ÇÔøΩÔøΩ\u0013ÔøΩPÔøΩNÔøΩ0\"+\u0007NPÔøΩÔøΩ öPÔøΩÔøΩNÔøΩ“òÔøΩÔøΩ]\nÔøΩ\u0016ÔøΩÔøΩi\u00024]ÔøΩ\nÔøΩÔøΩ\u0004HÔøΩ_+ƒ§ÔøΩHrÔøΩRÔøΩB\nÔøΩDGÔøΩÔøΩ…Ö\n|ÔøΩÔøΩSÔøΩ\u0003ÔøΩJQÔøΩÔøΩtjÔøΩ\u0013ÔøΩAmÔøΩÔøΩg\u0013ÔøΩÔøΩ\u0004)ÔøΩXÔøΩÔøΩhÔøΩ0ÔøΩÔøΩÔøΩÔøΩ\u0011\"ÔøΩqnÔøΩÔøΩ@ÔøΩ~YLÔøΩFnÔøΩÔøΩNÔøΩ.ÔøΩ äÔøΩdÔøΩ\u000eRÔøΩ\u0014ÔøΩ^ÔøΩ[IÔøΩ\ns*ÔøΩÔøΩÔøΩÔøΩÔøΩbEÔøΩmdGÔøΩsÔøΩ~PÔøΩ\u001bÔøΩ∆ÑÔøΩAÔøΩR{\u0005ÔøΩX\"ÔøΩÔøΩÔøΩ\u0004›§\u0014IÔøΩ0ÔøΩ\"#ÔøΩ\nÔøΩÔøΩjTÔøΩO^\u0011ÔøΩÔøΩJ\u0012]ÔøΩ$Z5ÔøΩÔøΩ \u000eÔøΩ…†iÔøΩÔøΩsÔøΩ\u0016\u0014ÔøΩ\u0004ÔøΩÔøΩS{[ÔøΩÔøΩÔøΩJÔøΩÔøΩÔøΩ\nÔøΩÔøΩ%W\u001bÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩcÔøΩ^ÔøΩSÔøΩ\u001aÔøΩLÔøΩÔøΩÔøΩJb=ÔøΩ\u0010JÔøΩ\u000eAÔøΩÔøΩu\nÔøΩÔøΩ0ÔøΩÔøΩ\nÔøΩ]Zn\u0001ÔøΩÔøΩÔøΩÔøΩ&&@ÔøΩiÔøΩ*@?ÔøΩ\u0012ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ`)E\u0010ÔøΩ÷Ü\u0006\u000fVÔøΩÔøΩPF&ÔøΩÔøΩ.\u0011|ÔøΩNÔøΩÔøΩ+fhÔøΩÔøΩaÔøΩÔøΩH\u0010\"5=AÔøΩ\u00100\u0014ÔøΩbÔøΩÔøΩÔøΩ\u0007\u001f+e7ÔøΩ{#ÔøΩÔøΩ6Cq,L\u0019ÔøΩÔøΩ\b!>BÔøΩÔøΩÔøΩÔøΩ◊çÔøΩ y8ÚçÑä‘´ÔøΩÔøΩÔøΩ+ÔøΩdÔøΩ\u0000AjodÔøΩƒ≥ÔøΩtÔøΩHÔøΩaÔøΩ\u001fÔøΩÔøΩÔøΩ\u0018]^VÔøΩÔøΩÔøΩÔøΩÔøΩI\u0001ÔøΩ\"ÔøΩVÔøΩ\u0006\u0013F&ÔøΩ`ÔøΩ\u0004ÔøΩ\n' ÔøΩÔøΩl\nÔøΩÔøΩÔøΩÔøΩw\n\u0013ÔøΩÔøΩÔøΩÔøΩ6ÔøΩÔøΩÔøΩ—µÔøΩH\u0000œ¥ÔøΩT)ÔøΩRÔøΩﬁú(kÔøΩÔøΩÔøΩ\nÔøΩC\u0017&*ÔøΩ~k\u0007ÔøΩb>ÔøΩÔøΩÔøΩ\u0000ÔøΩÔøΩ|\u0017ÔøΩ# Vj\"ÔøΩJ\u0019t|v;ÔøΩ5TÔøΩ\u0000ÔøΩ\u0013t\bÔøΩ;;\"-ÔøΩg?ÔøΩ9ﬁòÔøΩÔøΩ\u0019tÔøΩÔøΩZL\u001bjÔøΩÔøΩE\u0012ÔøΩ2ÔøΩÔøΩ`ÔøΩÔøΩW[ÔøΩ(HÃÄ)ÔøΩi?ÔøΩBV\u0003ÔøΩ#D@ÔøΩa\u0012ÔøΩÔøΩQÔøΩ&ÔøΩÔøΩÔøΩÔøΩL?ÔøΩÔøΩ~]ÔøΩ7UIÔøΩ=¬ôÔøΩuMB\nBz[8\u0004\u0005ÔøΩX*\u001bLÔøΩ\u000fÔøΩ\u0003ÔøΩÔøΩÔøΩ\u0014ÔøΩ\u0013ÔøΩÔøΩÔøΩÔøΩ1K.TYÔøΩÔøΩ{\u0010X\nÔøΩÔøΩSÔøΩL<ÔøΩÔøΩÔøΩ\u0007ÔøΩ”∂ÔøΩÔøΩ\n!<zÔøΩ6Ââ¥WEÔøΩÿç\u001a/ÔøΩÔøΩPÔøΩ5ÔøΩ\tÔøΩÔøΩÔøΩ`ÔøΩ\u0012BÔøΩÔøΩÔøΩC(ÔøΩ\\\u0007880ÔøΩdÔøΩ+ÔøΩ-KÔøΩÔøΩÔøΩ|ÔøΩÔøΩÔøΩ^EÔøΩ\bbÔøΩÔøΩÔøΩ_…¨ÔøΩÔøΩhÔøΩÔøΩJÔøΩÔøΩ√±$ÔøΩ(ÔøΩÔøΩi\"ÔøΩÒ±û†'ÔøΩ\nÔøΩEÔøΩÔøΩOEÔøΩDÔøΩÔøΩ>OÔøΩÔøΩ>ÔøΩÔøΩhm#ÔøΩ,\u0016tÔøΩÔøΩÔøΩHÔøΩ\u0013ÔøΩ\u001b\u0014\u0013\n\u0010ÔøΩ\u0010ÔøΩj\u0015u\u0005bÔøΩ«îÔøΩÔøΩMÔøΩ\u0002ÔøΩ\u0016\u0003\u0019\ncÔøΩmÔøΩÔøΩÔøΩh\nÔøΩÔøΩuRÔøΩ)\u0004\nÔøΩÔøΩÔøΩÔøΩÔøΩ\u0001ÔøΩ\u0010ÔøΩgÔøΩÔøΩ\u0018ÔøΩÔøΩ\n^>\tcÔøΩ%’π\u000eÔøΩÔøΩiJÔøΩÔøΩÔøΩSÔøΩoÔøΩ9ÔøΩÔøΩjÔøΩÔøΩ¬ÜTC\nÔøΩÔøΩ'ÿ™<–∫ÔøΩÔøΩÔøΩ7HÔøΩt\u0019”â\bhÔøΩÔøΩ-ÔøΩÔøΩ\u0010E\nÔøΩÔøΩÔøΩdV/ÔøΩÔøΩI|<ÔøΩ\u0002%JovÔøΩc.\b3ÔøΩÔøΩuÔøΩÔøΩ=pMÔøΩÔøΩ÷ùÔøΩr\u0019ÔøΩÔøΩ›ÑJÔøΩ\u000eG\u0007,ÔøΩÔøΩ≈≤ÔøΩÃádÔøΩu|ÔøΩ\n(ÔøΩÔøΩ\n6%4fÔøΩÔøΩs[ÔøΩ\u0019ÔøΩ\u0004ÔøΩ!ÔøΩÔøΩ\u0017\u0005p\b\u0015uÔøΩqÔøΩ:ÔøΩÔøΩÔøΩÔøΩÔøΩd3ÔøΩ√ôY ]ÔøΩÔøΩ;ÔøΩÔøΩŸé\u001f\u0010>%ÔøΩ\u0014ÔøΩ9ÔøΩÔøΩ0ÔøΩÔøΩÔøΩÔøΩ%<QÔøΩÔøΩWÔøΩ2vbÔøΩTÔøΩÔøΩ8ÔøΩC\u0018 ÔøΩ$ÔøΩÔøΩÔøΩGhÔøΩ\u0012QÔøΩ:\u00044ÔøΩÔøΩ($ÔøΩ@ÔøΩÔøΩÔøΩ6ÔøΩ»ÉÔøΩÔøΩÔøΩ(ÔøΩÔøΩuÔøΩ&ÔøΩK@ÔøΩ#ÔøΩÔøΩÔøΩÔøΩ—áhÔøΩÔøΩ‘ª\u0014P!DÔøΩÔøΩÔøΩ6SÔøΩlÔøΩv8JiEoÔøΩRÔøΩ$s\nÔøΩ\u000f,ÔøΩ5ÔøΩÔøΩYÔøΩ3|ÔøΩaÔøΩAÔøΩÔøΩ\u0000PÔøΩ}iÔøΩÔøΩÔøΩÔøΩÔøΩ\u001aÔøΩ\u0005ÔøΩ\u0006ÔøΩ)n-\nwQHÔøΩ\u001aÔøΩÔøΩlÔøΩHFÔøΩÔøΩ?ÔøΩÔøΩq\u0018ÔøΩ}ÔøΩÔøΩÔøΩJsÔøΩÔøΩ\nzXÔøΩ4ÔøΩÔøΩyÔøΩÔøΩRÔøΩ\u0000$V<ÔøΩDÔøΩ}\bÔøΩ;)ÔøΩÔøΩ\nÔøΩÔøΩ'\nZ\nÔøΩ\u0001{0PÔøΩÔøΩÔøΩbÔøΩ\b1LÔøΩ^\\ÔøΩ…∞ÔøΩÔøΩ+ÔøΩÔøΩ\u0003\u0017r\u0019ÔøΩÔøΩÔøΩ4ÔøΩÔøΩ\nÔøΩ\u0000.5\u001b\u001aÔøΩ$ŒÖÔøΩÔøΩt\u0005ÔøΩÔøΩÔøΩ\tÔøΩ5ÔøΩÔøΩÔøΩÔøΩ\u000f„¥ÖZÔøΩGÔøΩÔøΩ⁄Ø<EÔøΩ(‚ú¢ÔøΩ!ÔøΩÔøΩ/#z*ÔøΩGÔøΩÔøΩk\u0007ÔøΩÔøΩA\u0019IQ\u0012j\u0018\\ÔøΩ\n7ÔøΩQ'\u0012ÔøΩÔøΩACÔøΩM>nÔøΩÔøΩÔøΩÔøΩ7ÔøΩ!ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0014ÔøΩ\u0018ÔøΩÔøΩÔøΩÔøΩÔøΩÿ≤ÔøΩdnRÔøΩjÔøΩÔøΩ;ÔøΩaÔøΩÔøΩ^ÔøΩ{FÔøΩ?xZ2JÔøΩ\ttÔøΩ<\u0005lÔøΩ\u0018ÔøΩÔøΩÔøΩ\nÔøΩGÔøΩÔøΩÔøΩ—ºÔøΩÔøΩÔøΩÔøΩ\b]◊†\u0006ÔøΩÔøΩ\\vÔøΩ]ÔøΩHsfIHÔøΩÔøΩlÔøΩ\nÔøΩÔøΩ#ÔøΩ\u001b\n\u0000ÔøΩÔøΩÔøΩ&ÔøΩÔøΩ\u0018\u0002ÔøΩÔøΩVÔøΩÔøΩx-\u0017WÔøΩÔøΩ+ÔøΩ^|ÔøΩz8fÔøΩÔøΩ\u0003w\u0015ÔøΩ\u0005WÔøΩ{[ÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩÔøΩZÔøΩ≈ºiÔøΩ\u000fÔøΩAÔøΩÔøΩ\nendstream\nendobj\n33 0 obj\n108070\nendobj\n23 0 obj\n<</Length1 34 0 R/Length 35 0 R/Filter/FlateDecode>>stream\nxÔøΩÔøΩ9\nt\u001bWÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ⁄éÔøΩÔøΩPÔøΩ|dÔøΩÔøΩ'ÔøΩ4ÔøΩI[\u0013«±ÔøΩÔøΩJÔøΩÔøΩcilÔøΩÔøΩ%ÔøΩ$ÔøΩ5M!ÔøΩÔøΩÔøΩÔøΩÔøΩ\nlSÔøΩn\u001bÔøΩ-ÔøΩaÔøΩ8i@fÔøΩ&ÔøΩB{ÔøΩÔøΩ]\u0016Ns84@[J),ÔøΩ,ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩﬁåm9\t=\nÔøΩÔøΩÔøΩÃªÔøΩwÔøΩo4#\u0001\u0001ÔøΩ\n8\u0002\u0012tÔøΩÔøΩ_ÔøΩÔøΩÔøΩ\u001fv#ÔøΩ\"ÔøΩÔøΩ|#j\u0014FÔøΩ\u0011ÔøΩlD|ÔøΩo,ÔøΩÔøΩ<ÔøΩÔøΩ]ÔøΩ\u0007\u0000\u0014y(:<rÔøΩÔøΩM\tÔøΩ\u0012BÔøΩÔøΩÔøΩÔøΩƒêÔøΩcÔøΩÔøΩ\u0006ÔøΩÔøΩ\u0006ÔøΩÔøΩ\u0001MÔøΩoÔøΩÔøΩ\u0001rÔøΩqÔøΩÔøΩ\u0000\u0012ÔøΩNÔøΩÔøΩ\u0011?ÔøΩÔøΩÔøΩÔøΩHÔøΩ6{+ÔøΩ\nÔøΩ#n\u000fE|*-'ÔøΩÔøΩ?ÔøΩxÔøΩz[ÔøΩ\nÔøΩlAÔøΩ\u0005ÔøΩYX\nÔøΩzÔøΩ\u001aÔøΩ#ÔøΩoÔøΩÔøΩÔøΩh$ÔøΩxÔøΩÔøΩÔøΩG\u0001ÔøΩ\u0018ÔøΩ∆¥ÔøΩÔøΩ\nŸéÔøΩ\u0000ÔøΩ\u0014\u0007\"ÔøΩHÔøΩ@\u0001ÔøΩÔøΩ\u0001ÔøΩÔøΩ6fÔøΩ$\n—ÇlIQ\b%V\nUÔøΩpÔøΩ—∑ÔøΩÔøΩÔøΩÔøΩ\u0012ÔøΩ4ÔøΩÔøΩ3ÔøΩÔøΩlÔøΩeÔøΩgÔøΩÔøΩ\n\u0010ÔøΩ\u0017?ÔøΩAÔøΩU8[p&@ÔøΩAK*ÔøΩ}ÔøΩsÔøΩÔøΩy ÔøΩ\u0002ÔøΩ ÔøΩS>\nUxVHÔøΩÔøΩrÔøΩ‘´ÔøΩÔøΩÔøΩÔøΩ7ÔøΩÔøΩr\bÔøΩsÔøΩÔøΩ.ÔøΩÔøΩCÔøΩ'ÔøΩÔøΩ8TÔøΩÔøΩÔøΩa5ÔøΩ\u0000ÔøΩÔøΩ%xÔøΩÔøΩÔøΩn8ÔøΩÔøΩ!ÔøΩÔøΩÔøΩÔøΩC\nÔøΩ?\u0003ﬂÇÔøΩÔøΩsh\u0007?ÔøΩPF\u000e\u0003KÔøΩ3ÔøΩ\u00035p'ÔøΩÔøΩ\u0006ÔøΩ,u\u0016>\noÔøΩÔøΩ\u0018V@#ÔøΩÔøΩÔøΩE0\n\u000fÔøΩÔøΩp=tÔøΩÔøΩ&ÔøΩ\tÔøΩÔøΩ\u0018^ÔøΩ ÔøΩ/d+r\bdÔøΩAÔøΩÔøΩyÔøΩ\"<\u0003ÔøΩ\u0001ÔøΩÔøΩRÔøΩÔøΩ\nÔøΩJÔøΩÔøΩÔøΩ7hÔøΩ^ÔøΩÔøΩvÔøΩÔøΩÔøΩ+ÔøΩÔøΩÔøΩp\nÔøΩ#|\u0015\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ<L~+ÔøΩ>u6ÔøΩÔøΩÔøΩÔøΩ’∞\u000e6ÔøΩM0ÔøΩÔøΩÔøΩ%ÔøΩÔøΩ*ÔøΩ;uJ_IÔøΩÔøΩnO}-ÔøΩ\u0002TÔøΩÔøΩÔøΩ0ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ;ÔøΩÔøΩ}ÔøΩG\u001fÔøΩ&ÔøΩÔøΩKÔøΩSÔøΩDÔøΩÔøΩÔøΩÔøΩ8Z0ÔøΩ.HÔøΩ#(y\u0001ÔøΩ'\u00198>I\u0019ÔøΩN}sÔøΩÔøΩe`ÔøΩ*ÔøΩÔøΩ\nÔøΩÔøΩ\n#ÔøΩq8\u0006ÔøΩb\u0014\u000fÔøΩCÔøΩ\u0004ÔøΩIÔøΩÔøΩ\u0000yÔøΩÔøΩÔøΩÔøΩÔøΩ#ÔøΩÔøΩÔøΩmÔøΩÔøΩveÔøΩÔøΩ})ÔøΩ3ÔøΩ\u000eÔøΩÔøΩ\u0006\u0007z{#\nÔøΩÔøΩpÔøΩgÔøΩsp\u001fÔøΩÔøΩ\u0012ÔøΩz\u000eÔøΩ%ÔøΩ%ÔøΩI\u0013ÔøΩFÔøΩ'=ÔøΩ3ÔøΩSÔøΩ\u0011ÔøΩÔøΩE_ÔøΩÔøΩKÔøΩRÔøΩT+yÔøΩ\u0001ÔøΩÔøΩÔøΩÔøΩÔøΩMÔøΩÔøΩ=wbÓá©ÔøΩÔøΩmÔøΩKÔøΩ9ÔøΩÔøΩJÔøΩaÔøΩ}p\u0000ÔøΩ\u0010ÔøΩq8ÔøΩ;ÔøΩ\u0018\nÔøΩ1ÔøΩÔøΩ;ÔøΩCÔøΩ|ÔøΩÔøΩÔøΩ]ÔøΩ\u0019ÔøΩ\u0012ÔøΩ\u001bÔøΩ&ÔøΩ\u000eÔøΩTÔøΩ\u00183ÔøΩ\u001a\nkq4\u0011\u000fÔøΩÔøΩÔøΩ%ÔøΩÔøΩa\u0012''ÔøΩ7IÔøΩ<CÔøΩG~Kﬁ¶\u001bÔøΩfÔøΩ@wÔøΩ\n:LÔøΩ4AÔøΩÔøΩNÔøΩÔøΩ9ÔøΩ:ÔøΩ\u0013zÔøΩ(uHqÔøΩ\u0013ÔøΩ)ÔøΩYÔøΩ\u0005ÔøΩGÔøΩOeÔøΩoÔøΩU9(ÔøΩ üÔøΩuÔøΩ%ÔøΩÔøΩÔøΩÔøΩ<ÔøΩÔøΩÔøΩÔøΩÔøΩVTÔøΩÔøΩÔøΩÔøΩ:ÔøΩnJ’§ÔøΩRÔøΩÔøΩ{SS8ÔøΩÔøΩ\nWb45ÔøΩ\u0012ÔøΩÔøΩ∆™ÔøΩ`\b;'ÔøΩÔøΩ\u001fpL`ÔøΩÔøΩ¬àÔøΩ\u00071w<{ﬂÑ$<ÔøΩ]ÔøΩ,ÔøΩÔøΩÔøΩÔøΩCÔøΩ)ÔøΩÔøΩ3x\nÔøΩ\u0002ÔøΩbrx|EÔøΩAÔøΩÔøΩ:ÔøΩÔøΩd'ÔøΩÔøΩXÔøΩ1rÔøΩ\n!ÔøΩÔøΩ\u0003ÔøΩÔøΩir\u0016ÔøΩyr\u0011ÔøΩÔøΩÔøΩ\bÔøΩQ/ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ^zÔøΩ~ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0002V\"%YÔøΩ\u0012%ÔøΩNÔøΩSÔøΩQÔøΩIÔøΩYJHÔøΩIÔøΩK_ÔøΩ\nÔøΩ\nÔøΩÔøΩÔøΩyÔøΩÔøΩ2ÔøΩ\u001bÔøΩn9&ÔøΩ)OÔøΩ_ÔøΩÔøΩÔøΩÔøΩÔøΩ,_TÔøΩ*MÔøΩ$\u000e]9ÔøΩ<ÔøΩÔøΩa)ÔøΩÔøΩ[6Zz-IÔøΩÔøΩ6aÔøΩÔøΩm\u000eÔøΩÔøΩÔøΩ0\ng/ÔøΩ_ÔøΩcÔøΩNÔøΩÔøΩ_…Ø%Y:B@ÔøΩi\u0016ÔøΩ@>)ÔøΩ'YÔøΩ\u0015h&ÔøΩ\nÔøΩ0ÔøΩ\u0011=\\N~DÔøΩÔøΩ\u001b%\u001fŸèÔøΩÔøΩ$\u0019\"7ÔøΩÔøΩH\u0015“óÔøΩ\u001bÔøΩ\u0007JÔøΩÔøΩJÔøΩÔøΩ\u000fÔøΩÔøΩ\tÔøΩ@ÔøΩ.ÔøΩÔøΩ$=-QeRÔøΩ%ÔøΩÔøΩS\u0010ÔøΩÔøΩÔøΩÔøΩÔøΩc)/…Ö^ÔøΩ0}\u0014;ÔøΩ\u000ehÔøΩÔøΩr\u0019\\ÔøΩ\nÔøΩ\nÔøΩÔøΩÔøΩÔøΩ9ÔøΩ7H\u0012ÔøΩY-RÔøΩÔøΩhÔøΩCÔøΩaÔøΩÔøΩfÔøΩ-ÔøΩÔøΩ\u0016TÔøΩ5ÔøΩ?ÔøΩÔøΩÔøΩÍ°èÔøΩ=ÔøΩ\nr—∫\u001bÔøΩÔøΩÔøΩÔøΩ@ÔøΩ;`\u001byx.\nSÔøΩtÔøΩT–áÔøΩGgÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩCÔøΩÔøΩÔøΩ\u00060ÔøΩ?ÔøΩB€∞ÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\u001fÔøΩÔøΩ‹ªÔøΩ+ÔøΩ\n}\u0019ÔøΩÔøΩ]ÔøΩ'vÔøΩ\u001fqÔøΩÔøΩf\u001f|@sp?ÔøΩÔøΩ}$ÔøΩÔøΩÔøΩmÔøΩvmsScÔøΩÔøΩ-ÔøΩ6nXÔøΩnmÔøΩÔøΩÔøΩ÷µfÔøΩÔøΩÔøΩ5ÔøΩ+ÔøΩ\u001fqÔøΩÔøΩÔøΩÔøΩ\u0015ÔøΩeÔøΩ%ÀäÔøΩ\nÔøΩ)»∑ÔøΩÔøΩÔøΩdgefÿ¨\u0016EÔøΩ(ÔøΩÔøΩ\u000eÔøΩ\u0001ÔøΩÔøΩ\nÔøΩrÔøΩÔøΩÔøΩÔøΩ8ÔøΩTÔøΩÔøΩÔøΩ\u0011\u0006tÔøΩÔøΩ\nKet6 ÔøΩÔøΩRI\u000fJ\u000e]&ÔøΩ1$=\nÔøΩÔøΩŒöÔøΩÔøΩÔøΩÔøΩu8ÔøΩÔøΩbÔøΩÔøΩ%ÔøΩÔøΩ=ÔøΩ\bÔøΩÔøΩÔøΩÔøΩ2ÔøΩ-\u0001ÔøΩ\u0012ÔøΩ\\#ÔøΩ\nD\n\u000e\\ÔøΩ:J\u0002ÔøΩL'\u0003ÔøΩCÔøΩ1\u0016ÔøΩÔøΩ\u0018hG}ÔøΩYÔøΩmÔøΩ6-ÔøΩÔøΩ\u0016ÔøΩ3ÔøΩ\u0010ÔøΩBH_ÔøΩNÔøΩe€à\u0000Ë≤éÔøΩi\nÔøΩ\nÔøΩJ/sÔøΩwÔøΩÔøΩvÔøΩ.UwÔøΩ~ÔøΩ{OG{ÔøΩÔøΩ·≠´ÔøΩIÔøΩÔøΩ9ÔøΩÔøΩÔøΩUÔøΩs\t\u0011h\u0013ftKÔøΩn\u0015fXÔøΩÔøΩ\u0003wÔøΩÔøΩÔøΩÔøΩÔøΩ$ÔøΩ08ÔøΩÔøΩÔøΩ;ÔøΩÔøΩ~]RÔøΩÔøΩFÔøΩ\nÔøΩÔøΩÔøΩÔøΩ>ÔøΩzÔøΩ\"ÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩsÀ•…éÔøΩ ÔøΩÔøΩÔøΩÔøΩQÔøΩÔøΩÔøΩ”üÔøΩuÔøΩ◊ã:p-ÔøΩÔøΩ10ÔøΩ\u0003MﬂÉYÔøΩÔøΩehÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ]hÔøΩÔøΩHxTF|ÔøΩÔøΩÔøΩS\u0006nezÔøΩÔøΩÔøΩ\u0019ÔøΩÔøΩu\u0000kS6ÔøΩCœÑÔøΩtYÔøΩg&ÔøΩ\nÔøΩuÔøΩ…æ~ÔøΩCÔøΩ^ÔøΩÔøΩÔøΩÔøΩ\u0015”Ö0ÔøΩ3qÔøΩÔøΩÔøΩJÔøΩrÔøΩjÔøΩÔøΩÔøΩFbÔøΩsÔøΩL ;'\nÔøΩ\u0016x\u0002\u0012ÔøΩ\nÔøΩÔøΩYÔøΩ,ÔøΩ\n9ÔøΩ«é–ôÔøΩÔøΩ'ÔøΩNÔøΩi+ÔøΩh[a“∑\u0015ÔøΩÔøΩÔøΩ\u0012\\ÔøΩÔøΩÔøΩ\"A=ÔøΩm`ÔøΩÔøΩÔøΩÔøΩ|ÔøΩÔøΩT€ùlÔøΩœÄ\nÔøΩ|ÔøΩwK)ÔøΩIÔøΩTÔøΩÔøΩ\nÔøΩ}ÔøΩÔøΩk»üÔøΩuÔøΩK_ÔøΩÔøΩÔøΩÔøΩÔøΩ\nkÔøΩ>n\u0013ÔøΩÔøΩÔøΩ⁄±$mqFÔøΩ\n'L\u001ftcnUoc=ÔøΩÔøΩÔøΩÔøΩ\u0005ÔøΩ;ÔøΩADÔøΩ#{ÔøΩ\nÔøΩÔøΩ`ÔøΩiÔøΩ‘ªÔøΩ:\nÔøΩÔøΩÔøΩÛú¢ΩÔøΩsdÔøΩÔøΩÔøΩ|ÔøΩÔøΩÔøΩÔøΩÔøΩxÔøΩ)ÔøΩm5\nÔøΩ<{ÔøΩ5\nÔøΩFÔøΩ\u0014\b[3ÔøΩÔøΩÔøΩÔøΩÔøΩ=ÔøΩÔøΩYÔøΩ‰ÄôÔøΩŒæ%ÔøΩÔøΩﬂ∫ÔøΩ3!ÔøΩÔøΩÔøΩ~ÔøΩÔøΩÔøΩ\u0010-ÔøΩ\u0004\u0017ÔøΩÔøΩÔøΩÔøΩ0GÔøΩÔøΩuÔøΩ\u001a?\u0016ÔøΩÔøΩÔøΩÔøΩ’Ü])(ÔøΩÔøΩÔøΩÔøΩ\u0003ÔøΩ\u0019WoÔøΩÔøΩÔøΩ7.JÔøΩ.ÔøΩUbZ\\fÔøΩÔøΩ7ÔøΩÔøΩÔøΩMKÔøΩ%ÔøΩeOJÔøΩ\\C;ÔøΩÔøΩONf.ÔøΩÔøΩ\u0006oÔøΩvÔøΩc{ÔøΩ=ÔøΩXÔøΩÔøΩÔøΩ\u0019|TfÔøΩÔøΩÔøΩOSBÔøΩ\u0006ZÔøΩÔøΩ+ÔøΩÔøΩ?ÔøΩ\u000fÔøΩ\nAÔøΩ\nTÔøΩ1ÔøΩA'ÔøΩÔøΩ=MmÔøΩU>ÔøΩ\u00018\"ÔøΩÔøΩ \b‹ó$ hÔøΩy\u001a\u0001_ÔøΩ\u001a4ÔøΩÔøΩÔøΩQÔøΩÔøΩÔøΩÔøΩ\n\u000eÚ∞ÇèÔøΩÔøΩÔøΩÔøΩ‰©¥X}xÔøΩUdÔøΩ\u0004ÔøΩ\u0016ÔøΩ'IÔøΩ,ÔøΩ*ÔøΩ\bÔøΩÔøΩVo-quÔøΩÔøΩnÔøΩ5ÔøΩÔøΩeÔøΩyÔøΩ}ÔøΩ\u0019ÔøΩ7ÔøΩ6ÔøΩsÔøΩÔøΩ\nÔøΩÔøΩÔøΩjGÔøΩcXÔøΩ\u000fÔøΩtÔøΩ\u0003ÔøΩ\u0002ÔøΩ\u0003ÔøΩÔøΩs\u001bÔøΩiÔøΩ$>ÔøΩV@\u0015aÔøΩÔøΩ+\n\u001b2ÔøΩ\u0015^OÔøΩÔøΩÔøΩ[ÔøΩ/\nÔøΩ\u001fŒøcÔøΩÔøΩÔøΩÔøΩ\u001f[~ÔøΩ~~^U%!UÔøΩ\u0014ÔøΩWVVdÔøΩfVf\u0014W,ÔøΩ,ÔøΩQkÔøΩÔøΩ(ÔøΩÔøΩÔøΩ\bÔøΩ\u0019ÔøΩÔøΩÔøΩ~[eÔøΩÔøΩÔøΩÔøΩÔøΩ&IÔøΩSC+\n)ÔøΩÔøΩÔøΩÁØ¨ÔøΩ,ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0019ÔøΩ\u0015EÔøΩ\u0015ÔøΩÔøΩT&≈ïy@Í™™ÔøΩ'ÔøΩSÔøΩ<ÔøΩ«û'\u0015\u0017eÔøΩlÔøΩeÔøΩ äÔøΩÔøΩÔøΩ[ÔøΩ,ÔøΩ%{ÔøΩÔøΩ2IÔøΩxÔøΩeÔøΩ2GÔøΩ\n<ÔøΩhÔøΩÔøΩÔøΩÔøΩÔøΩzÔøΩÔøΩtÔøΩlYÔøΩÔøΩ\u001beÔøΩÔøΩŸíÔøΩ\u000eÔøΩÔøΩ\nÔøΩ\u0019ÔøΩ.ÔøΩ)ÔøΩOAC~ÔøΩÔøΩ\u0006ÔøΩ_ÔøΩ–ÄÔøΩÔøΩÔøΩnÔøΩrÔøΩÔøΩ9ÔøΩK\u0004ÔøΩÔøΩ\n~ÔøΩÔøΩÔøΩ-b?ÔøΩÔøΩjonÔøΩÔøΩœ∫ÔøΩÔøΩ qX-EÔøΩÔøΩÀä\nÔøΩ6oŸºÔøΩl ÔøΩ\u0006ÔøΩyÔøΩ∆ïYTÔøΩÔøΩÔøΩ\u000e|ÔøΩÔøΩ7ÔøΩwoÔøΩmÔøΩgNÔøΩ7ÔøΩÔøΩÔøΩÔøΩwÔøΩÔøΩÔøΩ\u001bÔøΩ#ﬂøÔøΩÔøΩÔøΩÔøΩ[ÔøΩÔøΩÔøΩJÔøΩ~N>ÔøΩÔøΩÔøΩÔøΩ{ÔøΩÔøΩjÔøΩÔøΩ^qÔøΩdÔøΩ\u0015ÔøΩ\u0017AÔøΩÔøΩ\u001f”óÔøΩo@\u00168f@\"Ozr3ÔøΩPÔøΩc)ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\u0016:Thq~ÔøΩfÔøΩÔøΩÔøΩ\u001bÔøΩ\u0017”ó/ÔøΩÔøΩÔøΩ\nÔøΩO\\ÔøΩ-ÔøΩ|\u0001\n}ÔøΩ@ÔøΩÔøΩL\nÔøΩ%ÔøΩÔøΩœ∂ÔøΩ6ÔøΩÔøΩÔøΩÔøΩjÔøΩÔøΩwv6(ÔøΩÔøΩÔøΩ6|ÔøΩÔøΩB4kÔøΩÔøΩÔøΩœ∂ÔøΩÔøΩmxÔøΩ7wÔøΩ\u0003ÔøΩÔøΩÔøΩ\nÔøΩÔøΩCÔøΩ_ÔøΩaÔøΩy(ÔøΩ-ÔøΩÔøΩﬁäÔøΩzx\u000fÔøΩ#\u000e”ÆÔøΩÔøΩÔøΩ\bY\nvÔøΩ«ßÔøΩÔøΩ+\u000fÔøΩÔøΩ7“±\u0005ÔøΩ\u0007ÔøΩ\u0015 ÔøΩÔøΩ\u000fÔøΩ0≈Æ\u001f4a\tÔøΩZÔøΩ&,ÔøΩÔøΩ(ÔøΩÔøΩÔøΩY\u0013ÔøΩÔøΩ\nÔøΩo\nÿÜÔøΩ?ÔøΩÔøΩp\u0006ÔøΩCÃòp&aÔøΩÔøΩkÔøΩYÔøΩÔøΩÔøΩgÔøΩŸ∞ÔøΩ÷öp\u000eyÔøΩÔøΩb¬πÔøΩÔøΩÔøΩÔøΩÔøΩRÔøΩ–ülyÔøΩÔøΩ\u0015\nÔøΩ|PÔøΩ\u0016A\u001f\u0011ÔøΩUÔøΩ'\u0004l\u0013ÔøΩ\u0005ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0017LÔøΩ@ÔøΩÔøΩ«Ñ)ÔøΩ*\u0007MXÔøΩ\n‹á\u0006,ÔøΩÔøΩ(PÔøΩ<mÔøΩ\u0016ÔøΩ+/ÔøΩÔøΩ\njÔøΩ_ÔøΩp\u0006ÔøΩZÔøΩL8ÔøΩz,ÔøΩL8\nÔøΩÔøΩ{M8\u001bÔøΩlÔøΩcÔøΩ9“±ÔøΩ\u001bM8\u0017\u000edÔøΩBÔøΩÔøΩi1fqÔøΩsÔøΩ\u0004ÔøΩÔøΩFÔøΩÔøΩpNÔøΩÔøΩÔøΩÔøΩÔøΩ\nCÔøΩ5\b\u0017ÔøΩh\u0002.LÔøΩ/\u0012zÔøΩ\u0005\\ÔøΩF/\u0015kÔøΩ\nÔøΩ\\ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩTÔøΩÔøΩ+ÔøΩÔøΩ#\u0002ÔøΩ\u0013ÔøΩ\u0019\u000eÔøΩÔøΩ|ÔøΩÔøΩÔøΩÔøΩNÔøΩgÔøΩÔøΩÔøΩMDÔøΩ!’ßÔøΩÔøΩX_@cÔøΩ\"ÔøΩH\u0002IÔøΩ-\u0012ÔøΩFbj\"\u0018\tÔøΩhÔøΩÔøΩfÔøΩjBÔøΩ0ÔøΩÔøΩPÔøΩÔøΩ\u0004ÔøΩ\u0003ÔøΩ8ÔøΩÔøΩÔøΩZlLÔøΩÔøΩNÔøΩ\nFB=ÔøΩÔøΩhHÔøΩÔøΩ/j4ÔøΩÔøΩqLÔøΩÔøΩÔøΩÔøΩuÔøΩ-kŸ™]A_,\u0012ÔøΩ\n%VÔøΩ3ÔøΩÔøΩkÔøΩ\u001aÔøΩÔøΩÔøΩ\u0016,ÔøΩÔøΩÔøΩÔøΩ\u001a\nLÔøΩÔøΩ4ÔøΩ\nS«ÉÔøΩaÔøΩ{h(ÔøΩÔøΩÔøΩkÔøΩÔøΩÔøΩ\u0017\bÔøΩÔøΩP$ÔøΩ`>ÔøΩÔøΩÔøΩpÔøΩÔøΩ\u0005GÔøΩ8ÔøΩÔøΩÔøΩYOdD\nÔøΩÔøΩ1M;ÔøΩ|j4ÔøΩPCqÔøΩÔøΩÔøΩ,\u0014\u0019ÔøΩb>5ÔøΩ’≤ÔøΩÔøΩÔøΩhL3»Éj<ÔøΩcÔøΩ—∞/1jƒõÔøΩ\nkÔøΩÔøΩ\u0016cÔøΩÔøΩD\u0000\u001fÔøΩ}ÔøΩPHÔøΩ\tVdÔøΩÔøΩÔøΩÔøΩÔøΩK–ßÔøΩX<8\n6ÔøΩ\nka-ÔøΩÔøΩÔøΩ(&.ÔøΩÔøΩÔøΩ ÔøΩ\u0005‘òÔøΩK`ÔøΩnÔøΩÔøΩ\"m(\u0012cq-ÔøΩÔøΩÔøΩ,QÔøΩ\u0015ÔøΩ}A-ÔøΩ\bbÔøΩl<\u0012;$hj\\ÔøΩ\u001fÔøΩÔøΩ0<\n7\u0011aÔøΩÔøΩÔøΩEÔøΩx\nFQ(\u0018fÔøΩ\u0004JÔøΩ1ÔøΩHJÔøΩ\nH$ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u00113ÔøΩnÔøΩR\u001fHÔøΩÔøΩÔøΩG\u0012ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ-ÔøΩ\u001a7ÔøΩÔøΩÔøΩ+∆µ\u0010R5ÔøΩdÔøΩÔøΩ,>\u001aE\u00075√∫ÔøΩy#ÔøΩ\u0018ÔøΩ\u0004\u001b≈à\u0013\nÔøΩBÔøΩ}1MM`ÔøΩÔøΩÔøΩx4ÔøΩN‘ä\u0010ÔøΩÔøΩÔøΩYG\n\u0006\nÔøΩb#ÔøΩD\u0002ÔøΩ\nNÔøΩhCXÔøΩ0◊ÖÔøΩ8ÔøΩ\u0014ÔøΩÔøΩ\u0010ÔøΩPÔøΩg.ÔøΩÔøΩN4\u0016ÔøΩÔøΩ\u0012ÔøΩÔøΩÔøΩ*ÔøΩÔøΩÔøΩkÔøΩ\n`ÔøΩÔøΩ\u0003A_ Õ≥q4\u001a\nÔøΩBÔøΩ~ÕøÔøΩ}$\nÔøΩ`ÔøΩÔøΩÔøΩÔøΩ62ÔøΩÔøΩ,ÔøΩÔøΩÔøΩ\u000fÔøΩVÔøΩÔøΩyÔøΩÔøΩÔøΩ\u0012ÔøΩÔøΩÔøΩE\u0003|ÔøΩÔøΩÔøΩ&ÔøΩÔøΩUAÔøΩÔøΩÔøΩFÔøΩ~ÔøΩ\u0005—™?2\n\u000eETÔøΩÔøΩÔøΩFÔøΩÔøΩ-1ÔøΩÔøΩÔøΩÔøΩÔøΩh\":ÔøΩ`~ÔøΩÔøΩÔøΩe\u0002Z(ÔøΩ4ÔøΩnÔøΩ\u0012ÔøΩ0ÔøΩyAP!ÔøΩ'\u0010\nÔøΩÔøΩ\u001fÔøΩ(\nÔøΩ7ÔøΩ/*\u000e]\u0010ÔøΩ\u0018ÔøΩÔøΩÔøΩt\bÔøΩAÔøΩ 9ÔøΩÔøΩWa\u0018~ÔøΩÔøΩ\"ÔøΩ\u0017\u00128ÔøΩÔøΩÔøΩÔøΩ\u0018ÔøΩÔøΩ\u0007ÔøΩiÔøΩiÔøΩ\nÔøΩ3“∑ÔøΩÔøΩC\u001fL@\u0014W\u000e!ﬂá3ÔøΩÔøΩÔøΩÏÉÄÔøΩwÔøΩ&ÔøΩ-aJ1h\u0013ÔøΩÔøΩÔøΩ\"=($\u0018RBÔøΩﬁçPÔøΩÔøΩÔøΩÔøΩÔøΩ\u0016ÔøΩ\u0014¬π\u0007)√∏:\u0001qÔøΩi8k(;ÔøΩW?ÔøΩ5ÔøΩ\u0011\u000e‚öêÔøΩ\nÔøΩ(B<ÔøΩÔøΩ-5.ÔøΩeÔøΩÔøΩ\t=ÔøΩ\u0005ÔøΩÔøΩÔøΩÔøΩ-ÔøΩ\u0016ÔøΩUÔøΩ:ÔøΩqƒê\u0013ÔøΩs\bÔøΩÔøΩÔøΩ}ÔøΩÔøΩoBÔøΩÔøΩ8ÔøΩÔøΩÔøΩÔøΩHÔøΩÔøΩÔøΩÔøΩn\u0004ÔøΩÔøΩ1F1ÔøΩÔøΩÔøΩ*ÔøΩÔøΩÔøΩqÔøΩqÔøΩ\u0014ÔøΩ5\nvÔøΩ\u0007CÔøΩ#MÔøΩÔøΩ\u0000[ÔøΩÔøΩ9\nÔøΩÔøΩ\n]\tÔøΩ|&ÔøΩÔøΩÔøΩqÔøΩ5ÔøΩÔøΩi\u0002ÔøΩÔøΩy\\d1\"ÔøΩÔøΩGÔøΩ\u0013mi8\u000eÔøΩÔøΩ‹ªÔøΩX\u001f\u0012+ÔøΩÔøΩaÔøΩÔøΩÔøΩEÔøΩ\\ÔøΩWÔøΩVÔøΩ\nÔøΩÔøΩÔøΩLÔøΩÔøΩ“ÉBÔøΩÔøΩÔøΩ;b\u0014ÔøΩ>ÔøΩ9ÔøΩÔøΩÔøΩ\tÔøΩ\u000f\nÔøΩÔøΩÔøΩDÔøΩ\ncÔøΩk|\"ÔøΩ!ÔøΩÔøΩÔøΩVÔøΩZ0·ª±nÔøΩÔøΩÔøΩ\u0013\n3auÿå|ÔøΩ\u001bn%,l\u00182QÔøΩqT‘ñÔøΩ\u001bÔøΩp{\u0001ÔøΩ7ÔøΩÔøΩgTÔøΩÔøΩ1ÔøΩÔøΩFCnHÔøΩ(\u0013XBX5ÔøΩÔøΩ◊ΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩÔøΩsÔøΩÔøΩÃå\n}ÔøΩÔøΩÔøΩTÔøΩo#ÔøΩ\u0011ÔøΩÔøΩÔøΩÔøΩ\u0019ÔøΩÔøΩ9cÔøΩ-ÔøΩuÔøΩÔøΩÔøΩ`ÔøΩÔøΩ\u0014\u0014Ÿä/ÔøΩÔøΩiÔøΩÔøΩc\nÔøΩ(ÔøΩÔøΩ\u0013ÔøΩqÔøΩÔøΩÔøΩFÔøΩKÔøΩÔøΩmÔøΩR/ÔøΩGÔøΩV=^\u0013(ÔøΩ\nÔøΩ8\u0016ÔøΩ[ÔøΩxÔøΩ^ÔøΩÔøΩÔøΩÔøΩÔøΩ;0dÔøΩjiVÔøΩ‹≠<ÔøΩQÔøΩÔøΩsHd>=v^UÔøΩ»ïQÔøΩ\tÔøΩGÔøΩ\u001a'ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩDÔøΩÔøΩÔøΩ7xÔøΩÔøΩÔøΩ\\TÔøΩm&\u0004eÔøΩÔøΩQÔøΩ\n^ÔøΩ\u001fÔøΩ+hT8*ÔøΩkDÔøΩ{ÔøΩÔøΩnPÔøΩ1_€êÔøΩÔøΩÔøΩ\n~\u0019+ÔøΩb\u000fƒÆÔøΩ\n-ÔøΩPÔøΩÔøΩ'\u0016ÔøΩWf'*p?ÔøΩÔøΩÔøΩ^kv5ÔøΩÔøΩ\u001avk\u0017ÔøΩ\\\nÔøΩÔøΩYÔøΩ\"O>ÔøΩ[ÔøΩÔøΩÔøΩq3“†ÔøΩÔøΩ!ÔøΩÔøΩ\u0017q^-ÔøΩ|MH@ÔøΩP~5ŒöÔøΩ$#/WÔøΩnÔøΩÔøΩÔøΩÔøΩvQÔøΩa\u000f\u001bwÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩj\u0011ÔøΩ[ÔøΩ“ØÔøΩÔøΩ\nÔøΩÔøΩ\u0018ÔøΩ$ÔøΩÔøΩÔøΩÔøΩ7ÔøΩﬂàÔøΩ/ÔøΩaqgUÔøΩjÔøΩFÔøΩKÔøΩ ∏[FÔøΩÔøΩÔøΩÔøΩÔøΩg5!ÔøΩn\tÔøΩ_[ÔøΩÊºûÔøΩÔøΩ?ÔøΩ\u000fÔøΩQÔøΩÔøΩÔøΩ\nÔøΩÔøΩYÔøΩ>ÔøΩCÔøΩfÔøΩyÔøΩp\u0007EÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ}ÔøΩWYÔøΩ/ÔøΩÔøΩÔøΩ'\nUÔøΩÔøΩÔøΩÔøΩ“ß m…≥ÔøΩqÔøΩKÔøΩcdrÔøΩÔøΩNÔøΩwÔøΩÔøΩÔøΩaÔøΩ&ÔøΩ=—ÖrcÔøΩ&ÔøΩÔøΩ\u0016ÔøΩÔøΩÔøΩ}ÔøΩk5~ÔøΩ\u0002H9ÔøΩ}ÔøΩC\u0006 \u0012ÔøΩDVÔøΩQ|ÔøΩSZB—Äj\u0019\nÔøΩÔøΩÔøΩ\u0004ÔøΩÔøΩÔøΩ\nZ\nÔøΩJ\n5ÔøΩ\u0006\"ÔøΩ\u0018_ÔøΩo\u0011ÔøΩÔøΩÔøΩ]\\ÔøΩÔøΩÔøΩ÷üÔøΩÔøΩÔøΩJRÔøΩÔøΩ|ÔøΩ=ÔøΩ|5N+<YÔøΩÔøΩUÔøΩ[YP’ºÔøΩÔøΩÔøΩ<MÔøΩÔøΩUÔøΩ<^ZÔøΩ*ÔøΩÔøΩVÔøΩÔøΩ:÷ºÔøΩÔøΩN<ÔøΩÔøΩ\nCÔøΩÀ≠||uUded$ÔøΩÔøΩQy\n\u0014\u0017ÔøΩ+\u0005ÔøΩ6OÔøΩÔøΩÔøΩ{\n3\n3ÔøΩL%ÔøΩ9OÔøΩuÔøΩiÔøΩÔøΩ\u0019ÔøΩ‘∞uÔøΩoÔøΩÔøΩÔøΩ:ÔøΩÔøΩ:ÔøΩÔøΩ:ÔøΩNÔøΩÔøΩSÔøΩ÷©\u0015ÔøΩB[ÔøΩÔøΩnÀµeÔøΩ2m6ÔøΩÔøΩ&€®\nlÔøΩÔøΩÔøΩ+\n\u0017ÔøΩÔøΩÔøΩÔøΩb\u0017?ÔøΩÔøΩ<ÔøΩÔøΩsÔøΩÔøΩ)ÔøΩ?ÔøΩÔøΩO,ÔøΩÔøΩ(ÔøΩ\u0000ÔøΩ5R'ÔøΩÔøΩm%ÔøΩÔøΩy\u001ft\u000e2ÔøΩÔøΩ^gÔøΩdÔøΩŸØ+ÔøΩVÔøΩ\u0017tBg_kÔøΩÔøΩÔøΩ’ôÔøΩÔøΩzÔøΩ-ÔøΩNÔøΩÔøΩ}SÔøΩ4!«ΩHÔøΩÔøΩ$ÔøΩÔøΩÔøΩ$IqÔøΩ]ÔøΩÔøΩÔøΩÔøΩ\u0019 $u◊ΩÔøΩÔøΩÔøΩÔøΩBÔøΩÔøΩÔøΩÔøΩÔøΩ\u0005ÔøΩÔøΩ\u001bvÔøΩ_ÔøΩ2`^]ÔøΩGÔøΩ+ÔøΩÔøΩÔøΩx\nÔøΩÔøΩ(ÔøΩ\u0007ÔøΩ$ÔøΩXÔøΩ>gÔøΩÔøΩ^ÔøΩN\tÔøΩ\u0014ÔøΩN\tjÔøΩrÔøΩÔøΩÔøΩÔøΩ~ÔøΩÔøΩÔøΩ^}=\u0007RÀΩÔøΩLÔøΩYÔøΩÔøΩÔøΩÔøΩÔøΩ\u0001gÔøΩÔøΩÔøΩ~ÔøΩXÔøΩD?2ÔøΩÿ¥ÔøΩÔøΩ'GÔøΩÔøΩ ÔøΩÔøΩÔøΩjÔøΩYÔøΩ÷Æ{ÔøΩÔøΩlÔøΩÔøΩÔøΩÔøΩÔøΩoÔøΩÔøΩ\u0016gÔøΩ4ÔøΩÔøΩÔøΩÔøΩ?}ÔøΩGk?ÔøΩÔøΩiÔøΩpÔøΩÔøΩÔøΩ\u0019ÔøΩ\"ÔøΩÔøΩkÔøΩ/1ÔøΩÔøΩys3ÔøΩÔøΩ\n^ÔøΩ1I\u0006ÔøΩÔøΩ5ÔøΩbÔøΩÔøΩX<ÔøΩÔøΩ]ÔøΩÔøΩqnÔøΩ8ÔøΩÔøΩÔøΩÔøΩ\u0012\u0016;ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ6hÔøΩÔøΩ\n0ÔøΩ34+\u0013k1PÔøΩÔøΩ\u0016€£ÔøΩDaÔøΩ\n%\u001f/ÔøΩÔøΩ\nÔøΩkÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩV=\u0007OŒ™kÔøΩkÔøΩ,l\u0018ÔøΩÔøΩÔøΩ[ÔøΩÔøΩÔøΩÔøΩ79 øMÔøΩfÔøΩÔøΩHÔøΩwÔøΩÔøΩkÔøΩuÔøΩ\u0011ÔøΩ\u0007ÔøΩt\u0004ÔøΩÔøΩÔøΩÔøΩÃ§ÔøΩÔøΩ#ÔøΩ\nÔøΩ÷ªÔøΩ.ÔøΩ?1ÔøΩ@w\nendstream\nendobj\n35 0 obj\n5166\nendobj\n34 0 obj\n8916\nendobj\n7 0 obj\n<</Length1 36 0 R/Length 37 0 R/Filter/FlateDecode>>stream\nxÔøΩÔøΩÔøΩy`\u001bÔøΩÔøΩ?>3ÔøΩ]ÔøΩÔøΩÍæØ]\nÔøΩd\nÔøΩÔøΩ€±6ÔøΩ\u0011ÔøΩv\nÔøΩ\u0002\u0011ÔøΩ$$@l\u0003\t!ÔøΩ\u0004\b\u0004¬ëpÔøΩ#ÔøΩ%PnZbÔøΩ\nN(%ÔøΩÔøΩ-mÔøΩÔøΩ#=ﬁíÔøΩ)Â®ÅÔøΩ5)ÔøΩXÔøΩ>ÔøΩÔøΩCÔøΩ}ÔøΩÔøΩÔøΩ__ÔøΩ⁄ôŸùÔøΩ›ùÔøΩÔøΩÔøΩyÔøΩÔøΩ\u0015ÔøΩ\b!=⁄å\u0018ÔøΩ;ÔøΩLÔøΩeSV\u0001{~\u000fÔøΩyÔøΩ.]“Ø=ÔøΩ\u001bA\bÔøΩÔøΩÔøΩ⁄≤uW\u0004ÔøΩ\u0018ÔøΩ;ÔøΩ\u0010ÔøΩ\n!ÔøΩjeÔøΩEÔøΩÔøΩÔøΩÔøΩÔøΩ\u0015\biÔøΩ:ÔøΩsÔøΩ%\u001bVz\u001at/ ÔøΩÔøΩ tÔøΩGÔøΩV,YÔøΩkÔøΩ\u0003O#ÔøΩÔøΩ\t*4ÔøΩÔøΩ\nÔøΩiÔøΩÔøΩ@ÔøΩ\u001fÔøΩÔøΩUÔøΩ^qÔøΩ_ÔøΩÔøΩ_\u0002ÔøΩ{ÔøΩÔøΩÔøΩKÔøΩ.[ÔøΩ‘™G\u0011ZÔøΩ\nÔøΩÔøΩ/]rUÔøΩÔøΩ<K\u0007B\u0017IP\u000e^ÔøΩÔøΩÔøΩ\u0015ÔøΩ?ÔøΩﬂÉr\u001f\u0014mÔøΩk/ÔøΩBÔøΩÔøΩK#ÔøΩÔøΩ*8~[ÔøΩÔøΩÔøΩÔøΩs?{e\nÔøΩÔøΩÔøΩ\u0019LÔøΩ\u0011fVÔøΩÔøΩ!\u0016ÔøΩÔøΩAÔøΩ\u000eÔøΩ RNÔøΩÔøΩh%1cÔøΩ\u0010\u0015ÔøΩdYÔøΩ@ÔøΩÔøΩÔøΩnFÔøΩnfÔøΩÔøΩl-\nÔøΩ`ÔøΩ4ÔøΩÔøΩÿçÔøΩ!ÔøΩÔøΩÔøΩpÔøΩT*WB\u0018ÔøΩÔøΩÔøΩÔøΩ\u000e)0ÔøΩ\u000fZÔøΩ$ÔøΩN\u0002Œ©BqÔøΩBiÔøΩAYTÔøΩjQ\u000e’£&‘ÇÔøΩPc2ÔøΩÔøΩ:P\u0017ÔøΩFSQ\u000fÔøΩÔøΩfÔøΩYh6ÔøΩ\u0006:\u0007ÔøΩÔøΩÔøΩCÔøΩÔøΩ\u0012ÔøΩ\n-G+ÔøΩJt\u0011ZÔøΩ.FkÔøΩ%ÔøΩRt\u0019ZÔøΩÔøΩÔøΩ ÔøΩ\n]ÔøΩÔøΩDÔøΩÔøΩzÔøΩAÔøΩÔøΩÔøΩgÔøΩ*ÔøΩ\tÔøΩMYÔøΩ\n>ÔøΩpe\\:MÔøΩÔøΩ\u0017ÔøΩÔøΩÔøΩÔøΩÔøΩ>,\n)ÔøΩwÔøΩÔøΩ“´ÔøΩ\u0017KÔøΩ*ÔøΩTÔøΩIÔøΩÔøΩÔøΩÔøΩKÔøΩ.\n/\nÔøΩÔøΩ/ÔøΩ/ÔøΩUÔøΩ\u000elÔøΩ*=SzÔøΩÔøΩÔøΩ4\u0002ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ.=ZÔøΩUzÔøΩÔøΩPÔøΩÔøΩÔøΩÔøΩoÔøΩÔøΩ,}WÔøΩÔøΩÔøΩGÔøΩ\n(ÔøΩŒîÔøΩ^ÔøΩ\nÔøΩAÔøΩÔøΩGÔøΩÔøΩnÔøΩÔøΩ+NÔøΩrGbquÔøΩ\u0004c\u0001aÔøΩÔøΩÔøΩÔøΩ_\u0004mAaÔøΩ\n⁄â^B\u0005ÔøΩc¬†.ÔøΩF\n@\u0013ÔøΩ»Ö\bnF30ÔøΩ\nÔøΩÔøΩ\u001aÔøΩÔøΩÔøΩfÔøΩ^dCÔøΩÔøΩ_ÔøΩ\nÔøΩ\u00019}ÔøΩÔøΩÔøΩu8ÔøΩÔøΩo!\u0001db\u0007Y›âv„©•ÔøΩÔøΩuÔøΩm|1z\u0016ÔøΩ~\nK(ÔøΩfÔøΩÔøΩ;h.ÔøΩ-\nÔøΩk ‘äÔøΩC\u000fb\u0003\nÔøΩ\u0011\n\u0016KÔøΩ\u0016.G[ÔøΩaÔøΩ+TBÔøΩÔøΩÔøΩÔøΩnhÔøΩ\u0017ÔøΩ|YÔøΩ ÔøΩ\u0000ÔøΩ\n/ÔøΩÁóº ÔøΩÔøΩ–µÔøΩ~ÔøΩ\bz\u0011ÔøΩÔøΩ7ÔøΩ#\nÔøΩÔøΩ\u0007⁄≤\u0014\nb%ÔøΩÔøΩ*ÔøΩÔøΩÔøΩSÔøΩÔøΩ=ÔøΩÔøΩ_zÔøΩt\nÔøΩPÔøΩ\u0011hÔøΩo$ÔøΩÔøΩ.}\u0004ÔøΩÔøΩÔøΩ\u0002ÔøΩVÔøΩÔøΩ[P\n|.CÔøΩÔøΩ\u0003ÔøΩÔøΩÿâÔøΩ\u000ed\u0000Õª\u0000ÔøΩÔøΩh\u000fS\u0005ÔøΩÿÉnÔøΩg;ÔøΩ7ÔøΩ=ÔøΩÔøΩÔøΩ\u0018<M#hÔøΩ&t\u0002_ÔøΩÔøΩÔøΩ\u0010{ÔøΩÔøΩÔøΩt52ÔøΩÔøΩÔøΩÔøΩNÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩUÔøΩ!ÔøΩ÷çÔøΩe.-ÔøΩKÔøΩA\u0017T(\tÔøΩ{\u0019ÔøΩÔøΩMÔøΩ9ÔøΩWÔøΩÔøΩ\u001a6ÔøΩ\u0010ÔøΩ\u0006-ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\\∆º\n-?ÔøΩFÔøΩ)ÔøΩ\u0019ÔøΩÔøΩ\u0017ÔøΩkIÔøΩ\\ÔøΩ÷é_W⁄èÔøΩÔøΩÔøΩ\u0012ÔøΩ1\nÔøΩÔøΩ\u0012ÔøΩ\nÔøΩ\u0012>\u001fÔøΩÔøΩ\u0016YOÔøΩ%ÔøΩÔøΩ\u0003ÔøΩÔøΩ\u0015UÔøΩÔøΩKMÔøΩW\u0011\u0007ÔøΩHB◊£gÔøΩ~ÔøΩÔøΩFÔøΩ\u0006yuÔøΩYÔøΩWÔøΩZfÔøΩÔøΩÔøΩÔøΩ\u0011ÔøΩ7\u0003#e\u0013ÔøΩÔøΩÔøΩÔøΩ\u0010ÔøΩ\u0014ÔøΩXÔøΩuÿäÔøΩÔøΩ\u000e7¬ìmÔøΩGÔøΩ\u001fÔøΩÔøΩÔøΩd\u0001ÔøΩÔøΩÔøΩÔøΩÔøΩVÔøΩPÔøΩ\nÔøΩ@W\n0ÔøΩV\u0001F‹ÄnD\u0007ÔøΩQÔøΩ'ÔøΩ!\u001aÔøΩn83\u0003gÔøΩq/ÔøΩ\no«ØÔøΩÔøΩÔøΩ|ÔøΩ\u0002fÔøΩBRÔøΩT<ÔøΩxEqÔøΩ5ÔøΩÔøΩ\u0014V<\u0001ÔøΩNÔøΩÔøΩÔøΩ\u0018ÔøΩ\u0005ÔøΩÔøΩDWC_ÔøΩÔøΩÔøΩUÔøΩ[ÔøΩ`\u000fÔøΩCKÔøΩÔøΩthi1^ÔøΩÔøΩÔøΩwÔøΩ{ÔøΩÔøΩ\u0013ÔøΩ\u0000ÔøΩ\u0001>ÔøΩÔøΩÔøΩ\u001fÔøΩ\u0011'ÔøΩÔøΩÔøΩC^ ÔøΩÔøΩÔøΩÔøΩ\u0018ÔøΩc\u0012L'ÔøΩ0ÔøΩ\"ÔøΩÔøΩÔøΩÔøΩ\nÔøΩq_ÔøΩÔøΩÔøΩ%m)YÔøΩ+ÔøΩ\u0011ÔøΩÔøΩ“®,\u0005/h|\nÔøΩd\u0011`ÔøΩfxÔøΩ\nÔøΩ^ÔøΩ\u0010ÔøΩÔøΩ>ÔøΩ\u0013ÔøΩK–ªwÔøΩÔøΩIÔøΩ\tÔøΩÔøΩ\nÃÅ6ÔøΩÔøΩÔøΩ\u0004,ÔøΩ\u0018ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0017ÔøΩÔøΩx\u001bÔøΩ\u001b?ÔøΩÔøΩÔøΩÔøΩ'ÔøΩiÔøΩÔøΩÔøΩ\bÔøΩIÔøΩ\u00062ÔøΩ\\@ÔøΩ'#ÔøΩ\u0019\n#2ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ3ÔøΩ+6ÔøΩÔøΩÔøΩyÔøΩÔøΩÔøΩ~¬ùTFToÔøΩÔøΩ5ÔøΩÔøΩ\"*^\\ÔøΩYÔøΩUÔøΩ\u0007]ÔøΩ@ÔøΩ,0ÔøΩrÔøΩu=0ÔøΩÔøΩ\u0003ÔøΩ\nÔøΩgPFÔøΩÔøΩÔøΩs\u0003ÔøΩÔøΩ\nÔøΩÔøΩE/ÔøΩ7ÔøΩ[ÔøΩÔøΩGÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩA\u0012ch\n\u00151\u0001yÔøΩX\u0005ÔøΩÔøΩgA2\nÔøΩ-}x\u0005»∂ÔøΩŸàÔøΩ«∑ÔøΩÔøΩÔøΩ\nÔøΩ\u0017~\u0004ÔøΩÔøΩgÔøΩÔøΩÔøΩmÔøΩ\u000eÔøΩ3ÔøΩ\u0014ÔøΩ\tÔøΩ\u0014ÔøΩLÔøΩÔøΩ\u0013ÔøΩÔøΩÔøΩI\u0001>ÔøΩÔøΩ2r\nÔøΩÔøΩÔøΩÔøΩO…ØÔøΩÔøΩ»üÔøΩÔøΩ\nœòÔøΩ\u0000\u0013cÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩmÔøΩ\u0010ÔøΩÔøΩÔøΩ\u0005ÔøΩKET1Y—£XÔøΩÔøΩÔøΩÔøΩgÔøΩÔøΩ=ÔøΩ4v1ÔøΩÔøΩÔøΩÔøΩ}ÔøΩÔøΩ6ÔøΩ\nÔøΩ#ÔøΩ$[ÔøΩÔøΩÔøΩ\nÔøΩFÔøΩÔøΩÔøΩ\u001aeÔøΩÔøΩWyÔøΩÔøΩ\u0016ÔøΩÔøΩÔøΩ\u0017ÔøΩÔøΩWÔøΩT1–ßYpÔøΩÔøΩ`ÔøΩn|ÔøΩ\"CvÔøΩ\u0012\u0019ÔøΩÔøΩÔøΩ>ÔøΩÔøΩÔøΩ1ÔøΩ\u0007?ÔøΩ\u0015$ÔøΩ\u0006wÔøΩ\n-&#ÃãÔøΩoÔøΩ`ÔøΩÔøΩ|ÔøΩ\\ÔøΩÔøΩÔøΩS><\tPÔøΩ-ÔøΩ=ÔøΩ\u0016ÔøΩÔøΩÔøΩ∆æÔøΩ~@ÔøΩÔøΩ#ÔøΩÔøΩ{ÔøΩ%ÔøΩÔøΩÔøΩ\u0001ÔøΩÔøΩ\nLÔøΩÔøΩFÔøΩ[ÔøΩ:\u001bÔøΩ>ÔøΩMÔøΩ!JÔøΩ\u0007j|\bÔøΩXÔøΩÔøΩÔøΩ.ÔøΩ?ÔøΩyÔøΩcÔøΩÔøΩÔøΩÔøΩ6ÔøΩÔøΩnÔøΩ\u0007ÔøΩ,yÔøΩL\u0007M>ÔøΩ\n#/ÔøΩ\u0007ÔøΩnÔøΩ\u00027ÔøΩÔøΩ-GÔøΩÔøΩÔøΩÔøΩN|ÔøΩ\tÔøΩ\u0003ÔøΩwÔøΩÔøΩ1ÔøΩ7tÔøΩÀªUd∆ßÔøΩ<ÔøΩ$ÔøΩ\u0016ÔøΩÔøΩ!<ÔøΩÔøΩ\u0003\u0012/}\bÔøΩÔøΩÔøΩÔøΩFÔøΩ;ÔøΩsÔøΩÔøΩyx6Œ†'–üAÔøΩÔøΩ9\nP\u0014\u0015\nÔøΩ3@>?ÔøΩ\u0005ZÔøΩW4\ncÔøΩGÔøΩ0ÔøΩÔøΩOÔøΩ!&ÔøΩ\u0016)NÔøΩÔøΩ3ÔøΩ?,vÔøΩW07ÔøΩSd2ÔøΩÔøΩ!#ÔøΩ\nÔøΩ∆ÄÔøΩÔøΩ\u0003VQ\n5ÔøΩ=ÔøΩ\tÔøΩ\"ÔøΩÔøΩ\u0010ÔøΩ\u0004\n–ãosÔøΩE\u000fÔøΩÔøΩÔøΩ0cC\u0011ÔøΩqÔøΩÔøΩÔøΩÔøΩ7\u0015At\u0017:ÔøΩÃÑÔøΩ^\u0003ÔøΩÔøΩ\u0005\nÔøΩ\u0014XŸãÔøΩ9ÔøΩÔøΩwÔøΩÔøΩA\nÔøΩÔøΩb7ÔøΩx\u0011ÔøΩ#=ÔøΩ_ÔøΩ\u0014ÔøΩÔøΩ\tÔøΩ\"ÔøΩtAÔøΩ\u0001v!ÔøΩD?ÔøΩ3ÔøΩ\nÔøΩ\u0004ÔøΩÔøΩ^ÔøΩ…™ÔøΩÔøΩPs\u001fÔøΩÔøΩﬂ°\n|+\u001a..GGÔøΩÔøΩ8q\u0004◊Ç6ÔøΩÔøΩÔøΩÔøΩ\nÔøΩ3ÔøΩ>ÔøΩÔøΩÔøΩOÔøΩ\u001at\u0015ÔøΩÔøΩ] ÔøΩ?ÔøΩ1ÔøΩ\u001aAÔøΩ\nÔøΩÔøΩ\u0003ÔøΩOÔøΩÔøΩ)0zÔøΩaÔøΩLÔøΩÔøΩÔøΩ\u0001\u001bv\tY»ºÔøΩ:ÔøΩ\u001bXÔøΩÔøΩ`ÔøΩ\u001aadÔøΩÔøΩÔøΩ_\u000eVÔøΩ*@ÔøΩÔøΩ`<=\u000e6ÔøΩÔøΩ\u0013ÔøΩÔøΩ\nÔøΩÔøΩÔøΩq\u00189\u000e\u0018ÔøΩÔøΩÔøΩÔøΩ*hg\u0006ÔøΩÔøΩÔøΩPÔøΩ\t@ÔøΩ\u001bÔøΩ0ÔøΩYÔøΩÔøΩÔøΩÔøΩ>DÔøΩc\u0003n\"WÔøΩÔøΩ(ÔøΩÔøΩ\u0004ÔøΩ=\u0002ÔøΩÔøΩ{ÔøΩ. GIÔøΩÔøΩj‹ä;AzÔøΩÔøΩ?ÔøΩXÔøΩ+4ÔøΩ^ÔøΩ<ÔøΩ.\n\u0000MÔøΩÔøΩ:ÔøΩÔøΩÔøΩ_P\u0018ÔøΩÔøΩ\u0014\u0018ÔøΩÔøΩÔøΩy}ÔøΩ\u001b\u0006ÔøΩCÔøΩÔøΩ1AÔøΩÔøΩŸ•&r1ÔøΩ\"ÔøΩÔøΩ54ÔøΩVÔøΩ\nÔøΩ}\u0012\nÔøΩÔøΩ0ÔøΩsÔøΩ#\u001bÔøΩÔøΩÔøΩSQ3ÔøΩÔøΩÕ®ÔøΩ}\\ÔøΩÔøΩ|ÔøΩÔøΩÔøΩ÷ñÔøΩÔøΩÔøΩ\\]mM6ÔøΩNU'\u0013ÔøΩX4\u0012\u0016ÔøΩP0ÔøΩÔøΩy=nÔøΩÔøΩaÔøΩY-f\u0013o4ÔøΩuZÔøΩZÔøΩÔøΩX\u0005C0ÔøΩÔøΩ\u0012ÔøΩÔøΩÔøΩC—æ!ETÔøΩÔøΩI—≤ÔøΩ\u0004v,9kGÔøΩP\u0010vuÔøΩÔøΩPÔøΩOÔøΩ\u0016ÔøΩjM\tjÔøΩÔøΩZMÔøΩ\\S:S\u0013ÔøΩÔøΩ6‘ñÔøΩ\u000evÔøΩÔøΩÔøΩÔøΩtÔøΩÔøΩ\u0011ÔøΩhÔøΩ\u0002ÔøΩÔøΩÔøΩ).\n\u000eÔøΩÔøΩÔøΩYr~ÔøΩÔøΩÔøΩC>\u0014ÔøΩ\u0013ÔøΩ]ÔøΩUÔøΩÔøΩ!ÔøΩ\u0017ÔøΩ\u001aÔøΩ^ÔøΩj[W_'4ÔøΩÔøΩVÔøΩ!vÔøΩ–§ÔøΩÔøΩÔøΩ\u001a-dÔøΩÔøΩ\u001brÔøΩÔøΩÔøΩcG;ÔøΩ3ÔøΩÔøΩÔøΩÔøΩ<A*=ÔøΩ‘ê[ÔøΩÔøΩ\u001arÔøΩÔøΩÔøΩ\u000eÔøΩÔøΩH◊íÔøΩCÔøΩs\u0017tuzBÔøΩÔøΩÔøΩÔøΩ!‹±L\\:ÔøΩÔøΩ)C∆§\\\u0005u»ó\u0019ÔøΩ:ÔøΩÔøΩÔøΩeÔøΩ\u0017”ßAÔøΩ\u0006ÔøΩÔøΩ>ÔøΩÔøΩÔøΩ\u0011\n-ÔøΩKÍñãÀó\\ÔøΩ`ÔøΩYÔøΩÔøΩ^√îÔøΩÔøΩv\u000e9ÔøΩ>ÔøΩÔøΩÔøΩ\bÔøΩÔøΩ;\u0016l=ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ8HÔøΩ€∂m\n\u000eÌûªÔøΩÔøΩ!ÔøΩ]ÔøΩ\u0010⁄ÄsIÔøΩÔøΩo[7\\ÔøΩ6ÔøΩÔøΩ\u0019ÔøΩ\u0004ÔøΩjÔøΩ∆Ö\nÔøΩÔøΩÔøΩpÔøΩ }\u0012ÔøΩTÔøΩÔøΩ[!vÔøΩ=}ÔøΩÔøΩCjqÔøΩÔøΩjÔøΩÔøΩ>\u0010ÔøΩ{ÔøΩ\u0010ÔøΩ∆ÜÔøΩ^ÔøΩ[:T:ÔøΩÔøΩ]ÔøΩmÔøΩ.\u0010CCyÔøΩÔøΩpIÔøΩÔøΩy+ÔøΩÔøΩÔøΩ\nÔøΩ.)ÔøΩÔøΩÔøΩTÔøΩÔøΩ‹±ÔøΩ\u001bÔøΩÔøΩÔøΩNvf≈ôcrNÔøΩNs3ÔøΩqÔøΩg1ÔøΩ#q\u001a(ÔøΩPpY\u0010ÔøΩdÔøΩ\bÔøΩÔøΩD7ÔøΩdm[ÔøΩ\u0004ÔøΩÔøΩo!ÔøΩÔøΩÔøΩÔøΩÔøΩD.\nRwÔøΩmÔøΩ[ÔøΩ~zÔøΩ\u0010\u001bÔøΩÔøΩÔøΩO\u0011hÔøΩ8ÔøΩÔøΩÔøΩÔøΩYRÔøΩÔøΩEÔøΩO\u0011ÔøΩR=9ÔøΩjp|\"?ÔøΩL\u000e%\u0012TEÔøΩ\nSÔøΩÔøΩvÔøΩ\\ÔøΩÔøΩ^7B\n\u0016ÔøΩÔøΩ $ÔøΩ}ÔøΩ\u0017ÔøΩvÔøΩ¬ñ\nt(D\u0005|ÎàÑÔøΩBahÔøΩÔøΩ\u0005ÔøΩr\u0010-ÔøΩÔøΩER&ÔøΩpÔøΩÔøΩÔøΩ#G&ÔøΩÔøΩŒ£G6O\n9szÔøΩ\bÔøΩÔøΩOÔøΩ\u0017lCÔøΩÔøΩ#oÔøΩtÔøΩj\u0019ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0015ÔøΩÔøΩ3ÔøΩ\u0011gÔøΩ]ÔøΩ ÿµÔøΩÔøΩ“∑3ÔøΩÔøΩJÔøΩ|ÔøΩÔøΩÃ±JnÔøΩ“±ÔøΩÔøΩJÔøΩx\u0018ÔøΩ((ÔøΩ\u0005g*ÔøΩÔøΩ\u0002›ê\"\u0002ÔøΩÔøΩÔøΩÔøΩÔøΩGÔøΩ*ÔøΩJy\u000f\u000ev\u000fÔøΩ}=ÔøΩÔøΩBM(ÔøΩÔøΩÔøΩÔøΩ'ÔøΩ,9ÔøΩÔøΩÔøΩm\u000eÔøΩ$ÔøΩZnÔøΩJÔøΩ+ÔøΩÔøΩÔøΩÔøΩÔøΩ\n+ÔøΩd∆πÔøΩÔøΩmÔøΩ|ÔøΩX7 –∂mÔøΩbÔøΩ{[ﬂ∂%#ÔøΩÔøΩKÔøΩ /n;\u0004l%ÔøΩÔøΩÔøΩÔøΩoBÔøΩ#ÔøΩ√∑zÔøΩÔøΩo[\b\u000fÔøΩ\nÔøΩÔøΩÔøΩÔøΩ\\ÔøΩ:ÔøΩ\nRÔøΩ)ÔøΩ\b~ÔøΩSÔøΩ0*…ÇX≈´\nÔøΩ(\u0015ÔøΩbÔøΩRqÏ´ÑÔøΩ\nÔøΩÔøΩÔøΩ`ÔøΩÔøΩ!gÔøΩ?ÔøΩ6ÔøΩ6ÔøΩ\u001fkÔøΩ5ﬁÜÔøΩÔøΩO√¶&\u001b2ÔøΩL\u0011ÔøΩ`ÔøΩ@ÔøΩÔøΩÃëÔøΩ\u0012ÔøΩÔøΩ@AÔøΩ\u0011\u0004ÔøΩÔøΩÔøΩk\nÔøΩÔøΩ(+ÔøΩYÔøΩnÔøΩ◊öÔøΩ+ÔøΩuÔøΩzq+ÔøΩÔøΩÔøΩÔøΩ ÔøΩ‹©\u001fÔøΩ\u0013\n\u0016\t\u0012D1ÔøΩ1h}\u001aGÔøΩÔøΩshÔøΩXMT>ÔøΩÔøΩdÔøΩÔøΩqXÔøΩ\u0004ÔøΩÂ¢ë\u000fÔøΩ(ƒáHH$ÔøΩÔøΩÔøΩÔøΩÔøΩLÔøΩHÔøΩ\u0010ÔøΩ2\u0018ÔøΩ\u0006ÔøΩÔøΩÔøΩ3`ÔøΩÔøΩj\u0013\u000eÔøΩ\u0001PÔøΩ≈êÔøΩ@\u0014ÔøΩ!\u001aÔøΩp\u0015P\nÔøΩOÔøΩ\u0012od\nvÔøΩ\u0006LÔøΩ—éÔøΩÔøΩÔøΩHÔøΩiI\nj\\ÔøΩhtstwÔøΩXÔøΩD\u0014ÔøΩV4\u0018ÔøΩÔøΩÔøΩÔøΩgGt(ÔøΩÔøΩ~ÔøΩ39{ÔøΩ/ÔøΩÔøΩ‹≥ÔøΩG\n»ôoÔøΩÔøΩosÔøΩÔøΩ\u0005ÔøΩ\tSÔøΩÔøΩÔøΩhÔøΩtS07\u0017ÔøΩÔøΩ\u001aÔøΩIÔøΩ5ÔøΩkÔøΩ:iÔøΩÔøΩZÔøΩÔøΩÔøΩ\nÔøΩNƒèbÔøΩHy[8ÔøΩÔøΩÔøΩÔøΩ⁄îm–ßÔøΩÔøΩ\n8ÔøΩCJÔøΩfuÔøΩ\nÔøΩP}CC#ÔøΩ/ÔøΩrÔøΩÔøΩÔøΩÔøΩÔøΩ>\u0017ÔøΩÔøΩb\n√ú[\n5{”ûÔøΩÔøΩIÔøΩ.ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩSBÔøΩxÔøΩgNÔøΩÔøΩ\u0011ÔøΩÔøΩ\nÔøΩÔøΩoÔøΩÔøΩlÔøΩÔøΩHDÔøΩlÔøΩÔøΩÂãßÔøΩ+\n`#\u0011;ÔøΩ7[ÔøΩSÔøΩÔøΩÔøΩ.ÔøΩ`\u0010ÔøΩﬂ™x\u0002ÔøΩÔøΩj\nÔøΩf)\u0015jM5#hÔøΩkYÔøΩÔøΩ4Q\u0012eÔøΩÔøΩÔøΩ&ÔøΩÔøΩÔøΩÔøΩ0›ö9⁄ïÔøΩuÔøΩÔøΩ4ÔøΩÔøΩÔøΩ;ÔøΩÔøΩ\u0015ÔøΩ5o(ÔøΩ–ºÔøΩxWsÔøΩ=ÔøΩÔøΩ\u0018ÔøΩ>ÔøΩ ÔøΩÔøΩ>ÔøΩ DÔøΩVWÔøΩÔøΩ*iuÔøΩ\u00175\u0002mÔøΩÔøΩÔøΩÔøΩXÔøΩS!%VÔøΩ%ÔøΩ«úOÔøΩ\u000fÔøΩ¬ÇÔøΩR*ÔøΩD7GOÔøΩÔøΩÔøΩK\u0011\nq\u000fÔøΩÔøΩÔøΩÔøΩzÔøΩ!` ÔøΩvÔøΩ\u0011|iÔøΩÔøΩÔøΩ~ÔøΩ+eÔøΩUWÔøΩI\u0015ÔøΩÔøΩÔøΩÔøΩaÔøΩÔøΩÔøΩLwDPU$LlÔøΩTÔøΩ{@1B@'ÔøΩÔøΩÔøΩIÔøΩœ∑ÔøΩÔøΩ|ÔøΩXÔøΩÔøΩ‹úi\u001bÔøΩ\nXVd~ÔøΩ@eÔøΩ\u0006j]8UÔøΩÔøΩÔøΩÔøΩrÔøΩ$ÔøΩGÔøΩÔøΩÔøΩO\n_KÔøΩ…ö,.ÔøΩÔøΩMuÔøΩ ÔøΩ:ÔøΩ\u0018ÔøΩÔøΩÔøΩÔøΩjÔøΩÔøΩÔøΩU\u0004\u0003ÔøΩ1ÔøΩÔøΩÔøΩÔøΩ\u0010=\u001aÔøΩÔøΩÔøΩ.ÔøΩhpÔøΩN\u0014-OÔøΩÔøΩ9:ÔøΩS„≠©÷∞€†ÔøΩC^qUÔøΩpÔøΩÔøΩmÔøΩ*ÔøΩÔøΩo7lÔøΩt|ÔøΩÔøΩ\u001bÔøΩKVÔøΩbÔøΩÔøΩÔøΩ 8W2ﬂ§ÔøΩÔøΩÔøΩ[ÔøΩxÔøΩ ÔøΩÔøΩÔøΩ?ÔøΩÔøΩÔøΩ50ÔøΩ|ÔøΩÔøΩÔøΩÔøΩyXÔøΩÔøΩyGÔøΩ)WIÔøΩÔøΩJ\n!ÔøΩsÔøΩ=\nÔøΩYÔøΩÏ∑∫ÔøΩ{nÔøΩ÷ò÷ò7ÔøΩ6ÔøΩo1=ÔøΩ=ÔøΩÔøΩÔøΩ\u0003«è=\u001aŒéÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩ\u001b\n7yÔøΩx\u000f*^ÔøΩk2ÔøΩUÔøΩÔøΩÔøΩ:ÔøΩ:ÔøΩMÔøΩÔøΩFeÔøΩÔøΩd\u000eÔøΩÔøΩ\"ÔøΩÔøΩx\u0004[%»ÜÔøΩ6ÔøΩ\nÔøΩj\u001fcXmSÔøΩÔøΩ\u0019\u00136ÔøΩÔøΩÔøΩ8jÔøΩ\\v\bÔøΩBÔøΩÔøΩÔøΩ;\u0016HjÔøΩ&ÔøΩ!ÔøΩY.ÔøΩÿ¨ÔøΩ\nÔøΩÔøΩrntÔøΩÔøΩÔøΩNÔøΩÔøΩh~\u0014ÔøΩÔøΩÔøΩÔøΩ@ÔøΩGÔøΩF\u0011ÔøΩ√öÔøΩs6<_ÔøΩÔøΩÔøΩ ÔøΩÔøΩ^NÔøΩÔøΩ:\"*ÔøΩRM8OToÔøΩD\u0010ÁÖçÔøΩiÔøΩ ÔøΩÔøΩÔøΩÔøΩd\u0012ÔøΩÔøΩÔøΩdÔøΩÔøΩd\n\u0003ÔøΩÔøΩƒ≤`\u0004ÔøΩ\u000e ÔøΩ\ndÔøΩhÔøΩ8Q\bÔøΩÔøΩÔøΩ9\\WÔøΩwÔøΩÔøΩc’ü‹øÔøΩ\u00175ÔøΩ\n^ÔøΩÔøΩÔøΩ_ÔøΩ\u001bÔøΩÔøΩÔøΩ)ÔøΩ9ÔøΩcÔøΩÔøΩÔøΩ\u000f_ÔøΩ\nfÔøΩÔøΩbb‰µªÔøΩÔøΩwÔøΩ@ÔøΩ\u000fÔøΩÔøΩrÔøΩÔøΩ5ÔøΩÔøΩÔøΩWÔøΩ\u0005GÔøΩÔøΩ\u0019\u001aÔøΩ\u0004o\u0016ÔøΩ:\u00185AÔøΩÔøΩCH(\n\u0019vÔøΩsÔøΩHÔøΩÔøΩ õsAA\u0012zÔøΩ#ÔøΩ\"\n\u0019ÔøΩÔøΩ[ÔøΩ<\n]ÔøΩ\nÔøΩxAP\u0007}FA\nÔøΩÔøΩÔøΩ}ÔøΩÔøΩ\n(ÔøΩU(HxÔøΩ\nÔøΩcÔøΩÔøΩ\tIP\u0019ÔøΩ\u00015QÔøΩÔøΩx'\u000e:{ÔøΩ;ÔøΩÔøΩ3ÔøΩ\u0007p0ÔøΩ\u001bÔøΩ\u0014ÔøΩ\u0011P\u0004\u000eÔøΩ\u0004rÔøΩÔøΩCÔøΩ-ÔøΩÔøΩ<V\u0018hÔøΩÔøΩ\n*=VÔøΩ\u0018MQÔøΩÔøΩÔøΩÔøΩ…äJÔøΩÔøΩ\u0002\u000f\u0014*\n\u0014ÔøΩOÔøΩ+\nJN)ÔøΩX]88ÔøΩ3ÔøΩxÔøΩÔøΩÔøΩ%5ÔøΩB\u0015÷®]zKÔøΩ|GÔøΩÔøΩYÔøΩsÔøΩ⁄êÔøΩÔøΩ\u000fÔøΩÔøΩSÔøΩÔøΩeÔøΩNÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\tE\btÔøΩ\u0019cÔøΩŸôÔøΩ\u001f_\u001fb8\u0003V\u001bÔøΩI.ÔøΩ4:ÔøΩ)cÔøΩÔøΩÔøΩ2B0\u0019ÔøΩnH4$/ÔøΩÔøΩ\u0012ÔøΩ%ÔøΩtn$q8gi>ÔøΩfÔøΩ$\u001bZdl\b4ÔøΩÔøΩÔøΩk|>ÔøΩÔøΩÔøΩ/\u0010\nÔøΩÔøΩ\bÔøΩJÔøΩÔøΩ/BnÔøΩMÔøΩOÔøΩÔøΩI#0\u0006ÔøΩÔøΩÔøΩÔøΩzÔøΩÔøΩuÔøΩuÔøΩ]ÔøΩ«µÔøΩÔøΩÔøΩ\u0019ÔøΩd‹®UÔøΩl}\n#ÔøΩÔøΩÔøΩsÔøΩbÔøΩ\u0016oÔøΩÔøΩ1ÔøΩÔøΩ(\u000fÔøΩ6ÔøΩyÔøΩ`vÔøΩJZ}ÔøΩ’®\nÔøΩÔøΩ\nvÔøΩ\n‘§]-#ÔøΩÔøΩÔøΩ\u0005TÔøΩÔøΩfÔøΩ\n-ÔøΩÊìßfÔøΩ\u0016N\u0016ÔøΩ*\n\u0018\u000f\u0018`nnnFÔøΩÔøΩ\ncÔøΩÔøΩÔøΩÔøΩ“ºÔøΩÔøΩ\u0018>\u0000 n)ÔøΩ\nÔøΩÔøΩÔøΩfUÔøΩ\u0002ÔøΩyÔøΩÔøΩrÔøΩÔøΩjb=ÔøΩE,\nÔøΩÔøΩ@USÔøΩMÔøΩ\u0015mÔøΩÔøΩÔøΩ-+ÔøΩÔøΩ|RÊôèÔøΩÔøΩFÔøΩoIÔøΩÔøΩ\u0001wÔøΩ9\u0012ÔøΩÔøΩÔøΩÔøΩÔøΩ\u001bÔøΩVm9ÔøΩÔøΩÔøΩÔøΩÔøΩOiÔøΩlÔøΩÔøΩÔøΩzP÷∂gÔøΩ^:5%f“°sÔøΩ\\ÔøΩÔøΩg>uÔøΩÔøΩUqÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩEs'ÔøΩÔøΩÔøΩÔøΩ\u0016?zÔøΩÔøΩM\u000eNÔøΩÔøΩJ\"ÔøΩYAw;p^2w\b`YÔøΩ|–ß\u0012\u0004ÔøΩdÔøΩÔøΩTÔøΩ\nÔøΩÔøΩ<rÔøΩÔøΩÔøΩN&\u0018ÔøΩ9GÔøΩo\u000e\nÔøΩA_B\u0010ZÔøΩa\nÔøΩkÔøΩÔøΩÔøΩÔøΩ%–≤ÔøΩÔøΩÔøΩ\u0012ÔøΩÔøΩ@ÔøΩ\u0003ÔøΩÔøΩÔøΩÔøΩ<”ÇÔøΩWÔøΩV28ÔøΩSÔøΩ\u0016DSÔøΩÔøΩ\u0010ÔøΩÔøΩD\nqÔøΩÔøΩx ÔøΩ'ŒàA_\nÔøΩH\u0017ÔøΩuA_ÔøΩ\u0000ÔøΩ]U\u0007ÔøΩÔøΩÔøΩÔøΩÔøΩ%\u0011ÔøΩ;ÔøΩ\u000eÔøΩÔøΩ‹¨R)U\"ÔøΩÔøΩO!SÔøΩkÔøΩu\u0018ÔøΩ\u0017ÔøΩÔøΩkPW_\u0017ÔøΩÔøΩzÔøΩvw\nu)ÔøΩÔøΩF\n\u0000œ∑›Ñx\nÔøΩÔøΩ<ÔøΩÈú¥NÔøΩka`\u0010ÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0002ÔøΩ0ÔøΩtkn\nÔøΩÔøΩ\u0018ÔøΩÔøΩRÔøΩQÔøΩÔøΩÔøΩJÔøΩÔøΩÔøΩ6ÔøΩ\u0005\u0006\u001bÔøΩo&ÔøΩ&\u000f'ÔøΩ\u0006$ÔøΩÔøΩ{ÔøΩ~\u0006ÔøΩ~ÔøΩ6ÔøΩÔøΩÔøΩÔøΩ\u000eÔøΩcÔøΩlÔøΩ&ÔøΩUÔøΩÔøΩPÔøΩÔøΩÔøΩrÔøΩÔøΩVÔøΩÔøΩÔøΩ∆¢ÔøΩ/n∆õÔøΩ,ÔøΩÔøΩÔøΩe\nrÔøΩ}\nÔøΩÔøΩÔøΩ ÔøΩ\u001a<[\u001aÔøΩ+\\j\u0012ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ{ÔøΩÔøΩkÔøΩ\nÔøΩ:ÔøΩÔøΩPoÔøΩ\\aÔøΩfÔøΩ\u0016ÔøΩ.+{ÔøΩÔøΩ~ÔøΩ~ÔøΩ›∂ÔøΩÔøΩÔøΩ4\\ÔøΩÔøΩeÔøΩÔøΩzÔøΩ60ÔøΩÔøΩÔøΩhHÔøΩdÔøΩÔøΩÔøΩQ|KÔøΩÔøΩÔøΩ9ÔøΩ\u0013VVÔøΩÔøΩÔøΩ\u001azÔøΩOIÔøΩF\u0010ÔøΩskjÔøΩXÔøΩKrs1ÔøΩ1ÔøΩÔøΩB eAÔøΩ\nÔøΩ)ÔøΩÔøΩÔøΩ€àÕûÔøΩÔøΩÔøΩCÔøΩ\u000fÔøΩÔøΩŒöÔøΩ8NÔøΩtÔøΩ8qÔøΩ8ÔøΩQ9GIÔøΩŸÆ‹£<ÔøΩ|GÔøΩ\u0019ÔøΩkÔøΩDY[ÔøΩ'ÔøΩRÔøΩdÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩIUÔøΩ\u0006ÔøΩÔøΩoÔøΩagÔøΩn\tt\u0002\u0019ÔøΩ\u0001=—∑ÔøΩÔøΩÔøΩÔøΩIÔøΩeÔøΩÀ¢ÔøΩ\u0018ÔøΩÔøΩ\u0000\nd\u0010\u0006dÔøΩ\u0007ÔøΩO\tÔøΩÔøΩh[ÔøΩÔøΩ5\u0017ÔøΩ\nÔøΩ\u0013ÔøΩ9ÔøΩÔøΩÔøΩTV\u0001ÔøΩÔøΩ%\u0007\nÔøΩWÔøΩ\u0006ÔøΩ\u0019ÔøΩI\u0005\\iÔøΩ9K3b2ÔøΩ~I\u000f“ûÔøΩÔøΩQ›¨ÔøΩK,ÔøΩÔøΩÔøΩÔøΩÔøΩKmdÔøΩ\u0017ÔøΩÔøΩ’ÄÔøΩxÔøΩSRÔøΩÔøΩQÔøΩÔøΩ}M\u000f\u0002I8ÔøΩÔøΩÔøΩÔøΩ\\qO]ÔøΩÔøΩÔøΩÔøΩÔøΩ–æÔøΩÔøΩ\tÔøΩ_`\u0013-ÔøΩHqÔøΩ›õÔøΩf`J{J≈ßÔøΩWÔøΩ\u00169PÔøΩ!‰Ç°ÔøΩ2[rÔøΩ4ÔøΩÔøΩM3kÔøΩÔøΩ4uÔøΩK6ls9ÔøΩ\u001f-ÔøΩÔøΩYcÔøΩÔøΩ\u0003ÔøΩÔøΩ:'q- \u0015XÔøΩ(ÔøΩÔøΩÔøΩ<ÔøΩ◊µdI\u0017|;ÔøΩÔøΩ,ÔøΩ)ÔøΩ\\ÔøΩÔøΩÔøΩÔøΩÔøΩrn)’ØlÔøΩ\u0004ÔøΩ\u0018ÔøΩW\u001a3ÔøΩ#NÔøΩK NMLHÔøΩ\u001bÔøΩÔøΩ\nwÔøΩ{ƒüÔøΩ%Q\nÔøΩ\bb`L\u0012ÔøΩÔøΩÔøΩÔøΩdÔøΩ}ÔøΩÔøΩÔøΩÕ™ÔøΩUÔøΩU\u0019XÔøΩfÔøΩÔøΩ`(*÷Ñ\u0016\tÔøΩWBÔøΩÔøΩÔøΩ\tÔøΩ\u0001\u0003ÔøΩS)ÔøΩ>,\b\u0001YÔøΩ\u0012ÔøΩÔøΩ\u000f\tÔøΩ2ÔøΩ8\n\u000e\nmÔøΩVÔøΩ’ºÔøΩ*ÔøΩ)ÔøΩ\u0017\u0007J\u0001\u0012ÔøΩ&ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ,ÔøΩU\u0019ÔøΩ\u0001–ûÔøΩxÔøΩ7ÔøΩ\u0013ÔøΩd&ÔøΩ\u0000ÔøΩÔøΩÔøΩ\u0006\u0000\u0006e/gÔøΩ\n\u0003tÔøΩÔøΩdÔøΩÔøΩ\u0000\u0018\u0001ÔøΩÔøΩF#ÔøΩÔøΩ6ÔøΩ †XÔøΩ^ÔøΩ3X\bÔøΩcÔøΩ\n2\nÔøΩÔøΩÔøΩÔøΩ-Y>ÔøΩÔøΩÔøΩ\b\n\u0002-\u0019ÔøΩÔøΩaÔøΩU\u0006=0GÔøΩ\u000f z\u0019\nÔøΩ`@ÔøΩI,\u001aÔøΩÔøΩCÔøΩ%ÔøΩ\u0013Á¥∫\u001aÔøΩÔøΩÔøΩl◊¨ÔøΩ/ÔøΩ\nÔøΩKoÔøΩLÔøΩ>?ÔøΩÔøΩPÔøΩ\nÔøΩÔøΩÔøΩyÔøΩÔøΩHÔøΩÔøΩÔøΩU\u0007ÔøΩÔøΩ^ÔøΩÔøΩq\u0012?ÔøΩÔøΩ\u0011ÔøΩÔøΩÔøΩÔøΩsÔøΩÔøΩ?ÔøΩÔøΩﬁåÔøΩÔøΩÔøΩÔøΩÔøΩ+ÔøΩ\u0005LÔøΩÔøΩXqÔøΩÔøΩÕºÔøΩ\u0004ÔøΩÔøΩWÔøΩ\u001adTF\u000eÔøΩ?SÔøΩ„ïºÔøΩÔøΩdDÔøΩuÔøΩr\u0004$i\u0005\n\u000epÔøΩÔøΩÔøΩpÔøΩDÔøΩ\n1ÔøΩ≈°4ÔøΩ9ÔøΩÔøΩÔøΩD\u0003ZNiÔøΩÔøΩ8.yÔøΩÔøΩ\u001aÔøΩÔøΩ3ÔøΩi2ÔøΩ:)GS)kÔøΩÔøΩzkÔøΩ’êlÔøΩTÔøΩ[ÔøΩ_ÔøΩÔøΩ1ÔøΩ!8ÔøΩ7K:ÔøΩÔøΩIÔøΩ^ÔøΩ\u0011ÔøΩ1\nÔøΩseg\u000fPÔøΩR\u0018\u0000ÔøΩÔøΩ\u0010ÔøΩA3ÔøΩP^GÔøΩs\u0006ÔøΩtÔøΩ#ÔøΩÔøΩ~\\ÔøΩV\nÔøΩÔøΩÔøΩÔøΩÔøΩ÷îÔøΩÔøΩTÔøΩ÷úUÔøΩZiJ}F)ÔøΩ…èRÔøΩ6\u0000DÔøΩÔøΩÔøΩ,ÔøΩ`ÔøΩÔøΩÔøΩwEÔøΩQ_4\u0016ÔøΩvÔøΩc8ÔøΩM¬ùÔøΩÔøΩ*o$ÔøΩPÔøΩQÔøΩÔøΩ\u0003\u0006ÔøΩ\u001aÔøΩÔøΩSs\"ÔøΩlrnÔøΩoÔøΩnÔøΩV\\aÔøΩÔøΩÔøΩÔøΩ}SÔøΩmJÔøΩhÔøΩMÔøΩiÔøΩÔøΩÔøΩÔøΩÔøΩ\u0001aWÔøΩ\tÔøΩÔøΩÔøΩ3ÔøΩ\u0003ÔøΩÔøΩÕù6ÔøΩ\u0006ÔøΩÔøΩÔøΩÔøΩ\u0016FdÔøΩS7ÔøΩl\u001ab xÔøΩÔøΩÔøΩÔøΩÔøΩ»í%\u001fÔøΩOxu`\u001fÔøΩ\nGÔøΩ{ÔøΩÔøΩvÔøΩ5=\u0005ÔøΩ\\S7mÔøΩEO/8ÔøΩÔøΩÔøΩguÔøΩ6ÔøΩ[ÔøΩ Êö£“ä…ãÔøΩÔøΩÔøΩ‰úë\b\t9ÔøΩÔøΩÔøΩ-kTD6ÔøΩ\u00043ÔøΩÔøΩeÔøΩ\nn\u0014‹è]ÔøΩ|ÔøΩÔøΩÔøΩgaÔøΩ]T\u0003ÔøΩN*ÔøΩ`4ÔøΩQ\n_-ÔøΩÔøΩÔøΩÔøΩ\u001fÔøΩ\u001fÔøΩ+ÔøΩÔøΩF\u0015ÔøΩÔøΩ~ÔøΩ\bÔøΩÔøΩ*ÔøΩ\u0001ÔøΩÔøΩbytÔøΩÔøΩÔøΩÔøΩG9ÔøΩ\u0014ÔøΩ\u001fT%ÔøΩÔøΩt*-PÔøΩ≈íÀ±!\u0014ÔøΩ*ÔøΩ)ÔøΩT<0ÔøΩ\u0004ÔøΩ\u0007ƒôÔøΩÔøΩÔøΩÔøΩuÔøΩx\u000e&C@c›ôÔøΩÔøΩd*\u001aBSÔøΩ\u0014ÔøΩ\\oÔøΩXÔøΩÔøΩgvgH&\u0000>ÔøΩÔøΩÔøΩ\u00036z*ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩx\u0005ÔøΩJ7]W1ÔøΩÔøΩYÔøΩÔøΩÔøΩ\npÔøΩ\\\u001a-3\\ÔøΩZ0FGÔøΩOGOÔøΩO\n_ÔøΩuÔøΩÔøΩÔøΩ[ÔøΩ\u00111B8sÔøΩ*\u0016ÔøΩ\u0011ÔøΩ\u0010\u0011,ÔøΩ\u0018JÔøΩa\u00131ÔøΩb8fLÔøΩÔøΩy\"ÔøΩÔøΩeÔøΩÔøΩÔøΩÔøΩ[ÔøΩÔøΩÔøΩÔøΩPÔøΩHÔøΩÔøΩ7l2ÔøΩsl\u0012ÔøΩÔøΩ\u001bS79ÔøΩÔøΩÔøΩÔøΩÔøΩgÔøΩUÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ’ÜÔøΩÔøΩ[LDÔøΩÔøΩBY33eÔøΩÔøΩT43SÔøΩLÔøΩÔøΩB0\u0012&ÔøΩÔøΩÔøΩ¬≥ÔøΩeÔøΩ\u0003ÔøΩR%pPÔøΩ\u0000h`ÔøΩÔøΩ\u0007ÔøΩÔøΩ⁄©ÔøΩÔøΩÔøΩÔøΩÔøΩSÔøΩÔøΩÔøΩWN]ÔøΩ=ÔøΩÔøΩÔøΩUÔøΩWuÔøΩjUÔøΩSÔøΩN_\u0013qF2ÔøΩÔøΩÔøΩjÔøΩlvÔøΩ\u0017o]j\n\u0005\u0015ÔøΩYÔøΩÔøΩkÔøΩ}ÔøΩÔøΩÔøΩ}|un2vÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0006ÔøΩÔøΩÔøΩÔøΩÔøΩD-ÔøΩ@\n.(ÔøΩÔøΩÔøΩ\u0003ÔøΩ@\nj#ÔøΩÔøΩÔøΩÔøΩÔøΩfEÔøΩoÔøΩÔøΩÔøΩ:sÔøΩÔøΩﬂ≠ÔøΩUœ¥SI,ÔøΩQÔøΩ\u0019_ÔøΩ|\"ÔøΩÔøΩÔøΩÔøΩÔøΩ7RÔøΩCÔøΩN\nÔøΩ7ÔøΩÔøΩWv)ÔøΩ[ÔøΩ;ÔøΩÔøΩ/pÔøΩT›ãvÔøΩ?ÔøΩ\u000fÔøΩ\u0003*]ÔøΩ\u0012on@ÔøΩ`ÔøΩ[5\nÔøΩÔøΩ€æÔøΩÔøΩÔøΩ>ÔøΩÔøΩi€É\u001foy\tÔøΩh◊®ÔøΩÔøΩWÔøΩ2=*b3ÔøΩH+ÔøΩÔøΩkÔøΩÔøΩ[qmÔøΩJÔøΩR&ÔøΩÔøΩÔøΩ’ëduÔøΩÔøΩÔøΩŸ∫\u0017ÔøΩ\u0018E›§ÔøΩYuÔøΩÔøΩÔøΩ^ÔøΩpÔøΩwÔøΩ^ÔøΩÔøΩiÔøΩ◊çÔøΩiÔøΩÔøΩÔøΩÔøΩZU!ÔøΩ\n’ï*\u0005QÔøΩÔøΩfÔøΩÔøΩV›¢zXÔøΩÔøΩÔøΩMÔøΩoTjÔøΩ £ÔøΩW1VÔøΩÔøΩqÍ£Å$ÔøΩ\u0018_ÔøΩiÔøΩ!ÔøΩÔøΩÔøΩB&CÔøΩR<ÔøΩ3:\u0003ÔøΩÔøΩŒµŒáÔøΩ{ÔøΩ/9ÔøΩÔøΩ8ÔøΩÔøΩ<\nÔøΩÔøΩS2ÔøΩ9'\u0001ÔøΩÔøΩ\u001aÔøΩ\u0003’ôÔøΩ|ÔøΩÔøΩÔøΩ3ÔøΩaÔøΩ\u0004\"$ÔøΩ\u0001B\u0019u^ÔøΩIÔøΩÔøΩZ\u0011ÔøΩÔøΩ 5\u000fÔøΩÔøΩ\b~AÔøΩÔøΩÔøΩÔøΩDjÔøΩk'ÔøΩOÔøΩIÔøΩ–ßÔøΩÔøΩ ó<ÿìDÔøΩ|#iÔøΩe%1ÔøΩ[ÔøΩ~¬í,+ÔøΩÔøΩl\u001fÔøΩ`]ÔøΩÔøΩÔøΩsÔøΩÔøΩÔøΩ\u001bÔøΩ(ÔøΩÔøΩÔøΩ@ÔøΩÔøΩ\u0002ÔøΩ—±BaÔøΩ\nÔøΩÔøΩ)ÔøΩq\u0000+IfÔøΩ8ÿ¢1ÔøΩNÔøΩÔøΩNÔøΩe\u001fdPf.ÔøΩAÔøΩÔøΩ\bÔøΩC\u0015ÔøΩfhkCÔøΩ$\nÔøΩG@mSÔøΩWÔøΩÔøΩÔøΩÔøΩ\u0018ÔøΩEC\u0011mÔøΩ9jÔøΩÔøΩH\u0017TÔøΩÔøΩ ÔøΩ0ÔøΩ~ÔøΩ{ÔøΩ~ÔøΩ\u0011`”§hÔøΩ”±\u0000ÔøΩÔøΩlÔøΩd\u0010\u0004ÔøΩzpÔøΩÔøΩÔøΩÔøΩ\u0007ÔøΩh\u0000ÔøΩE*d B\n\u0017ÔøΩÔøΩ‘Ω9C\u0011dÔøΩÔøΩWL]ÔøΩÔøΩjz4fÔøΩ µÔøΩj…¥goÔøΩ]=ÔøΩÔøΩ\nRÔøΩÔøΩÔøΩ\u001bÔøΩ÷ö?oÔøΩÔøΩnÔøΩÔøΩ0hÔøΩzÔøΩÔøΩ_ÔøΩÔøΩÔøΩwÔøΩfCk,ÔøΩJÔøΩnÔøΩÔøΩÔøΩ9kÔøΩÔøΩÔøΩÔøΩ’çqÔøΩÔøΩi\n$ÔøΩjÔøΩfÔøΩÔøΩÔøΩÔøΩ=0%q_ÔøΩ^)ÔøΩGÔøΩÔøΩ;f‹ãÔøΩÔøΩÔøΩmhLÔøΩ\nÔøΩ\u0014ÔøΩÔøΩ?ÔøΩNÔøΩ8rÔøΩÔøΩ%ÔøΩvÔøΩv\nÔøΩ7Z\nrÔøΩÔøΩ\u0003ÔøΩ\n+ÔøΩVÔøΩv\u0003ÔøΩ\\PÔøΩÔøΩJZF;ÔøΩ^aÔøΩ\u0012f\u0004\u001b$\u001fÔøΩ=ÔøΩs{ÔøΩBÔøΩÔøΩlÔøΩ%lÔøΩbÔøΩmÔøΩZ-ÔøΩŒöÔøΩPÔøΩÔøΩ>!ÔøΩÔøΩ\nÔøΩ\nÔøΩ0\u0016ÔøΩ{ÔøΩyeÔøΩJ&ÔøΩ:ÔøΩÔøΩÔøΩ\u0001%\nÔøΩ\u0015\u0018\"(ÔøΩÔøΩÔøΩÔøΩ\u0002ÔøΩÔøΩ)ÔøΩ\u0018\u0003oÔøΩÔøΩÔøΩ\u0001ÔøΩYÔøΩM<ÔøΩEÔøΩG\bÔøΩÔøΩ\nÔøΩ\u001bÔøΩÔøΩ\u001fÔøΩQ~rÔøΩÓÅÖ\u001bMÔøΩÔøΩÔøΩ}~ÔøΩbÔøΩÔøΩÃ≤ÔøΩÔøΩf|ÔøΩÔøΩGÔøΩM\u0012vÔøΩÔøΩ\n_ÔøΩPAvÔøΩ\u0000c⁄ÜÔøΩÔøΩ^ZÔøΩÔøΩgX\n÷µÔøΩUÔøΩ\u0015ÔøΩ\nVeDÔøΩ$yÔøΩÔøΩÔøΩÔøΩ3ÔøΩ3ÔøΩÔøΩÔøΩÔøΩ\u001fÔøΩgzÔøΩ&\u001b\u0016$ÔøΩÔøΩÔøΩÔøΩZa=ÔøΩIÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ=ÔøΩ:ÔøΩ*Ÿ±JÔøΩNRh\u000fÔøΩ\u0018UÔøΩ\nÔøΩ\u0011Ó∂èÔøΩ}ÔøΩÔøΩE…é`ÔøΩÔøΩNÔøΩÔøΩ”éÔøΩ\u0002bÔøΩ%ÔøΩÔøΩÔøΩ_ÔøΩÔøΩ|ÔøΩ\u00028t?ÔøΩ>ÔøΩ!GSÔøΩiÔøΩGÓåê\u0017\u0016\n!\u0018/ÔøΩÔøΩZÔøΩÔøΩÔøΩP_NÔøΩÔøΩr\u001aÔøΩÔøΩdKÔøΩ\u0003\u0013q\nXÔøΩ+ÔøΩtÔøΩDÔøΩ&ÔøΩ@ÔøΩOR ûL“ë\u0002ÔøΩ-ÔøΩU ÔøΩÔøΩg\n'ÔøΩk\u0007dÔøΩ\u0007ÔøΩÔøΩEÔøΩ\u000eÔøΩÔøΩp^sÔøΩÔøΩÔøΩVÔøΩ\u001fÔøΩM\n?vÔøΩ`SÔøΩÔøΩDÔøΩ:pÔøΩ(p\u000fÔøΩÔøΩÔøΩ\n\u000frLÔøΩÔøΩ\u00194X)\u00134ÔøΩÔøΩ6ÔøΩ0^R/ÔøΩZ“∂ÔøΩIÔøΩ9ÔøΩÔøΩÿöyÔøΩÔøΩÔøΩÔøΩƒàMÃÖZÔøΩ/9ÔøΩcÔøΩ}ÔøΩuÔøΩÔøΩ{ÔøΩ\u0003l{ÔøΩÔøΩGÔøΩ\nÔøΩÔøΩ\u0016ÔøΩ\u0012ÔøΩxÔøΩIÔøΩ\nÔøΩN-ÔøΩ*]ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ«≠ÔøΩ/ÔøΩÔøΩvÔøΩ\u001bÔøΩ7ÔøΩÔøΩÔøΩ?ÔøΩÔøΩÔøΩ+ÔøΩ|$ÔøΩÔøΩ7$ÔøΩjÔøΩ\u0015ÔøΩ\n?ÔøΩÔøΩÔøΩ8ÔøΩÔøΩ'jY\nÔøΩWÔøΩmÔøΩÔøΩÔøΩ\u0019:NÔøΩJ:bÔøΩ\nÔøΩ]$ÔøΩ1ÔøΩÔøΩÔøΩÔøΩ5ÔøΩ\u001fÔøΩ#F]ÔøΩ\u0005ÔøΩ2g`G.ÔøΩ3ÔøΩÔøΩÔøΩ\tÔøΩÔøΩT,PeÔøΩ\u001bÔøΩGÔøΩ\u001aItÔøΩÔøΩÔøΩÔøΩlfÔøΩIÔøΩÔøΩ\u001f5k+ÔøΩL2ÔøΩrÔøΩ|s9ÔøΩ.KVkÃïKÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩ\u0001\u001bÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩ\u0016ÔøΩÔøΩÔøΩ6ÔøΩÔøΩ‰∏úÔøΩ\n6ÔøΩÔøΩÔøΩrNp«´&ÔøΩÔøΩ\n”¨ÔøΩ\u0006yVÔøΩÔøΩ\u0006daRÔøΩ\n-$)ÔøΩ€Ø\nzÔøΩÔøΩÔøΩVrÔøΩ\t*ÔøΩ\u001a ´ÔøΩl\"lÔøΩ\u0011(¬≠ÔøΩaÔøΩF\n\u0007N\nRÔøΩ_>1\u0014ÔøΩ«á{=!ÔøΩÔøΩÔøΩ–Öp6ÔøΩÔøΩah@NÔøΩ\n9ÔøΩfhÔøΩÔøΩLK…Ö'ÔøΩ\u0016\u0000\u0006ÔøΩ*'ÔøΩÔøΩÔøΩ\u0004\u001bÔøΩ\u0003\u001bÔøΩ\u00036z{ÔøΩÔøΩBP/z!ÔøΩÔøΩoÔøΩÔøΩGJ\u001aÔøΩÔøΩ/ÔøΩPÔøΩÔøΩ{ÔøΩÔøΩ\\QÔøΩw\bÔøΩÔøΩÔøΩÔøΩ\u0019ÔøΩ~ÔøΩÔøΩÔøΩÔøΩ\u0016kÔøΩÔøΩÔøΩÔøΩm\nn|\"HDIÔøΩdÔøΩH\u001aS>#ÔøΩÔøΩÔøΩÔøΩgÔøΩÔøΩhÔøΩr-zÔøΩH\nnM;R:6\\NÔøΩQ]ÔøΩ|$ÔøΩ0ÔøΩÔøΩ€í\u001a2ÔøΩÔøΩ]ÔøΩÔøΩÔøΩÔøΩÔøΩ>\n,\u0003“ì\u0007)uÔøΩ\u0002\u0005A…âÔøΩÔøΩ\b\u0006ÔøΩÔøΩQÔøΩÔøΩÔøΩ\nÔøΩRÔøΩaÔøΩÔøΩÔøΩ\"s&\u0018E\nÔøΩJ0ÔøΩÔøΩ\nrÔøΩQÔøΩtÔøΩÔøΩxÔøΩ5ÔøΩÔøΩÔøΩÔøΩwÔøΩÔøΩÔøΩÔøΩkCÔøΩ\u0010/ÔøΩ\nÔøΩÔøΩNj[ÔøΩ@j ΩÔøΩœúÔøΩ1ÔøΩÔøΩNÔøΩÔøΩÔøΩÔøΩwÔøΩj\n{\\ÔøΩ\u001fÔøΩ:oÔøΩÔøΩﬁÑÔøΩ\u0016ÔøΩnÔøΩ“öÔøΩvO]ÔøΩÔøΩeÔøΩÔøΩ\u0018ÔøΩ\" ÔøΩt@ÔøΩÔøΩ06ÔøΩ(ÔøΩœïÔøΩ?pÔøΩ\u000eÔøΩÔøΩ\nQ=FJGTÔøΩVi}ÔøΩbÔøΩU*ÔøΩ(ÔøΩn\u0005VÔøΩÔøΩ2ÔøΩÔøΩÔøΩÔøΩÔøΩ$/'ÔøΩÕìr4ÔøΩÔøΩUÔøΩÔøΩ\u0011ÔøΩHÔøΩ(ÔøΩ}\"ÕÇÔøΩ}X$ÔøΩÔøΩ\n0\u0013ÔøΩtLÔøΩÔøΩ\u0015\u0004ÔøΩShÔøΩÔøΩ\u0007@QÔøΩ.\u0001ÔøΩÿº/VOÔøΩUVÔøΩdÔøΩPÔøΩ\u0003ÔøΩrÔøΩÔøΩ\u0002#8k\u0014ÔøΩgÔøΩ(ÔøΩ ®”âÔøΩ\u0012ÔøΩHÔøΩ\u001fÔøΩ\u0013ÔøΩjÔøΩY\bÔøΩE=^ÔøΩÔøΩÔøΩe8ÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩ]mÔøΩ#ÔøΩÔøΩ\u0017ÔøΩ&ÔøΩ!ÔøΩÔøΩÔøΩÔøΩÔøΩ-\u001aÔøΩ\u001fyYGL\u0016ÔøΩÔøΩ|\u0000\u0013ÔøΩ!mÔøΩ5UÔøΩ\u0019OÔøΩÔøΩÔøΩ\n:ÔøΩÔøΩ€§ÔøΩÔøΩÔøΩÔøΩ6sÔøΩuÔøΩÔøΩÕÆ7ÔøΩ\u001b\u0001ÔøΩ&%ÔøΩUÔøΩ&ÔøΩvÔøΩfÔøΩfÔøΩvÔøΩ\n\\ÔøΩÔøΩÔøΩB\nÔøΩÔøΩ\n%\bÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩE\u001aÔøΩÔøΩ\nP\\ÔøΩÔøΩÁóÆÔøΩÔøΩÔøΩoÔøΩ|ÔøΩhÔøΩ4ÔøΩA€ìNÔøΩczk4\u0002ÔøΩÔøΩÔøΩmÔøΩÔøΩMÔøΩÔøΩWÔøΩÔøΩSgÔøΩÔøΩGk\nSÔøΩÔøΩÔøΩIÔøΩqÔøΩM>\u001bÔøΩÔøΩ- ÔøΩÔøΩÔøΩ'ÔøΩ\u0017oÔøΩÔøΩfÔøΩ\b6#3VdÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩ\u0013ÔøΩOÔøΩJ\u0018+ÔøΩÔøΩÔøΩÔøΩÔøΩT0Qo ÔøΩÔøΩœ±ÔøΩÔøΩ\u0018ÔøΩA\u0015P*\nÔøΩÔøΩq\u0006\nÔøΩÔøΩ9ÔøΩa6ÔøΩÔøΩy[\u0000ÔøΩÔøΩ-ÔøΩj#ÔøΩ\u0006=^oÔøΩÔøΩÔøΩ\u001aÔøΩ/@ÔøΩBÔøΩ\u0001r\u0006ÔøΩ\u0001#\u0005ÔøΩ\u0007AÔøΩy\n\u0002ÔøΩa ÔøΩÔøΩaÔøΩÔøΩF\u0003fÔøΩÔøΩÔøΩJ`ÔøΩ^ÔøΩÔøΩÔøΩÔøΩﬂê5JÔøΩ^#c,\u0018>ÔøΩ\u0012ÔøΩ\"ÔøΩf\u0010ÔøΩ∆ÑBÔøΩQÔøΩÔøΩ^zgÔøΩmsrÔøΩ\nzÔøΩXÔøΩ/ÔøΩ9ÔøΩÔøΩÔøΩÔøΩÔøΩ?ÔøΩWÔøΩ~<\u0004ÔøΩA|ÔøΩ\u0011ÔøΩ\nÔøΩ^\u0006{ÔøΩ,kÔøΩ\u0018ÔøΩÔøΩÔøΩÔøΩÔøΩ\ncÔøΩQ9ÔøΩ.ÔøΩ\nÔøΩaÔøΩf8\u0004Ÿ≠È§ÅN\u0002ÔøΩiÔøΩÔøΩIÔøΩÔøΩŸøÔøΩÔøΩ\u0006^ÔøΩÔøΩ\u0002E\u0007ÔøΩÔøΩﬁ¨ÔøΩÔøΩ,ÔøΩMÔøΩ<ÔøΩ\u001bÔøΩÔøΩ\u0013{ÔøΩÔøΩrbÔøΩÔøΩg{ÔøΩÔøΩ<ÔøΩ\u0018ÔøΩ,ÔøΩ#\u0017\\ÔøΩ2#ÔøΩXÔøΩ\neÔøΩSbÔøΩaÔøΩÔøΩÔøΩ#ÔøΩÔøΩÔøΩ19ÔøΩ\u001fÔøΩXÔøΩj∆µÔøΩMMÔøΩ7ÔøΩÔøΩ7ÔøΩnu$bÔøΩÔøΩ#+ÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩ\u0018ÔøΩDÔøΩÔøΩ-ÔøΩ\u0002ÔøΩÔøΩ\nÔøΩÔøΩÔøΩ`ÔøΩÔøΩqLÔøΩhÔøΩÔøΩfÔøΩU«ó\u0007ÔøΩ\u0010ÔøΩ\u0003ÔøΩ√û@.IÔøΩ59ÔøΩnÔøΩ\u001bÔøΩÔøΩÔøΩ>yÔøΩÔøΩÔøΩS)fÔøΩÔøΩÔøΩ$ﬁ©›ë$ZÔøΩﬁî3ÔøΩÔøΩÔøΩÔøΩ\u0007|ÔøΩÔøΩÔøΩsÔøΩfw8ÔøΩ\u0000^(ÔøΩ>\u0005ÔøΩ\u001b\u0001\u001fÕàb8\u0010ÔøΩÔøΩ\bÔøΩÔøΩÔøΩ5JÔøΩÔøΩÔøΩQÔøΩ6ÔøΩ\u0017ÔøΩÔøΩBÔøΩÔøΩﬂß1\u0016ÔøΩÔøΩ0^ÔøΩ\u0014xÔøΩÔøΩ\nÔøΩcÔøΩ\u0013J\u0006\nÔøΩ√í\u0016≈çÔøΩÔøΩÔøΩ8\u0012ÔøΩPF\nÔøΩ\ngsÔøΩ\nÕ∞'XÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ#\u0002ÔøΩ\u00170\u0012xÔøΩ\bÔøΩMPV«ü*\u0014(ÔøΩMÔøΩÔøΩ\u0017ÔøΩÔøΩ\nÔøΩÔøΩÔøΩIYÔøΩtÔøΩ'ÔøΩ4ÔøΩÔøΩÔøΩÔøΩMÔøΩm\u0002ÔøΩÔøΩÔøΩÔøΩ\u001fe\u00186ÔøΩ\u0006ÔøΩ9qÔøΩGP&ÔøΩ(ÔøΩt:WÔøΩ\u0003ÔøΩtÔøΩ\u000f\nÔøΩMnÍòúÔøΩÔøΩÔøΩÔøΩÔøΩ}ÔøΩ-ÔøΩÔøΩÔøΩLSQ9)ÔøΩÔøΩDÔøΩÃìÔøΩÔøΩsqWÔøΩczÔøΩÔøΩÔøΩ\nÔøΩ%WÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩpÔøΩmÔøΩÔøΩÔøΩÔøΩvÔøΩÔøΩÔøΩPŒã\u0010ÔøΩ\u0007\u000f?ÔøΩÔøΩK\nKÔøΩzÔøΩQÔøΩ)vÔøΩ;ÔøΩOÔøΩ\u0019MÔøΩG\u0004ÔøΩ\nÔøΩÔøΩg\n⁄†ÔøΩ$ÔøΩaQ`<b\u0018ÔøΩ\b\u001fÔøΩlÔøΩÏé∞ÔøΩ\u0011|H‚É°*b6\u0011\nÔøΩ<ÔøΩsÔøΩ\u0010\n!{$ÔøΩÔøΩÔøΩ\txÔøΩÔøΩ]ÔøΩqEÔøΩ1)ÔøΩ;ÔøΩLÔøΩs#ÔøΩJ$ull\nÔøΩ\u0014ÔøΩÔøΩq:ÔøΩDÔøΩ\u0011GÔøΩ\n3ÔøΩ'ÔøΩÔøΩÔøΩ=*.{ÔøΩÔøΩT*ÔøΩÔøΩÔøΩbÔøΩYÔøΩÔøΩ[ÔøΩ–∑StÔøΩ\n\\ÔøΩÔøΩÔøΩ\nÔøΩ}1ÔøΩÔøΩQÔøΩ”ªW5\u001bÔøΩ3\u001fÔøΩ\nÔøΩ$\u0001tXd/AZÔøΩEÔøΩÔøΩÔøΩÔøΩÔøΩ&GÔøΩhBfÔøΩ\nÔøΩx3ÔøΩÔøΩ\nÔøΩYVÔøΩJlkÔøΩÔøΩ\n\u0007ÔøΩ/ÔøΩÔøΩÔøΩÔøΩ8ÔøΩ\u0002SÔøΩ!ÔøΩÔøΩ√úÔøΩÔøΩÔøΩÀôM\u001a5ÔøΩ\n/ÔøΩ-ÔøΩz\u0012\u0013ÔøΩÔøΩ ±ÔøΩÔøΩ!AÔøΩ\u000eÔøΩS-ÔøΩ4\u0019\u000eÔøΩ99ÔøΩ8ÔøΩT §ÔøΩÔøΩ!\u0007ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0006nÔøΩÔøΩÔøΩ~\u0012ÔøΩQe»ØÔøΩÔøΩÔøΩÔøΩÔøΩ9\u0002\u0010ÔøΩÔøΩf\u001f\u0001u\u0003PÔøΩÔøΩ8ÔøΩrÔøΩÔøΩ\u0003ÔøΩZÔøΩcU&\nÔøΩ-4K{ÔøΩÔøΩ\u0012ÔøΩN^tÔøΩ$-ZÔøΩVÔøΩÔøΩÔøΩÔøΩ\u0003HÔøΩ%ÔøΩ\u000eI:ÔøΩÔøΩ:ÔøΩY÷®\bÔøΩÔøΩÔøΩXF\u0004\u001a1*ÔøΩ\u0004ÔøΩÔøΩÔøΩ\u001a{ÔøΩMJÔøΩÔøΩTÃóÔøΩ\"\b\n\u001aj\u0015ÔøΩJ\u0003z\u0001ÔøΩÔøΩ2ÔøΩ\u0004ÔøΩ[ÃüÔøΩÔøΩFÔøΩÔøΩkÔøΩh\u0014ÔøΩaC4ÔøΩÔøΩL#=\n_ÔøΩD\bÔøΩÔøΩÔøΩÔøΩH6$ÔøΩ\u0016ÔøΩ6ÔøΩÔøΩÔøΩÔøΩÔøΩ!W\nÔøΩÔøΩ,LrbÔøΩ%_\tÔøΩÔøΩdÔøΩÔøΩ2ÔøΩBÔøΩ&ÔøΩÔøΩÔøΩ(qt4GÔøΩÔøΩ\\ÔøΩ<ÔøΩ\u0001O⁄çÔøΩÔøΩ\u0011p,I\u0014ÔøΩ@ÔøΩÔøΩÔøΩÔøΩ6ÔøΩ9ÔøΩÔøΩwÔøΩ4ÔøΩÔøΩ7ÔøΩYÔøΩÔøΩ\"\nÔøΩ\u001bÔøΩ+ÔøΩnÔøΩÔøΩ’≥›ÑÔøΩ\u0015~ÔøΩCÔøΩF\u0003\nÔøΩdÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩKF\u0015ÔøΩxÔøΩÔøΩqÔøΩÔøΩN2\tÔøΩp $\nÔøΩpÔøΩ ÔøΩÔøΩp 8B~+ÔøΩÔøΩX8ÔøΩ\u0014ÔøΩP0\bVKÔøΩZIÔøΩÔøΩ3ÔøΩ\u0017\u000f\u001fÔøΩÔøΩ\nÔøΩÔøΩ<g=\u0006'.ÔøΩi\u0002jÔøΩNÔøΩlV)49oÔøΩ&ÔøΩ\u001bÔøΩ8h=j%VÔøΩkjÔøΩ*uÔøΩ[ÔøΩÔøΩ\u0016ÔøΩÔøΩ7ÔøΩ&[\u0003ÔøΩd\n6ÔøΩ*ÔøΩ\u0000\u0004[%\u00006ÔøΩ)ÔøΩ[ÔøΩÔøΩÔøΩ$ÔøΩÔøΩÔøΩPÔøΩdRÔøΩ)ÔøΩÔøΩÔøΩÔøΩS2;jÔøΩÔøΩ)4\"ÔøΩ–éÔøΩVÔøΩÔøΩ)ÔøΩ&ÔøΩ–ñÔøΩ'FÔøΩ\u0011SÔøΩÔøΩ¬ãD,\u0016ÔøΩÔøΩ\u0018ÔøΩÔøΩ'QÔøΩÔøΩ\nÔøΩ\nÔøΩ2tÔøΩpcKNN35r*ÔøΩ}ÔøΩ\\ÔøΩU=ÔøΩ\nÔøΩÔøΩÔøΩi Y\u000eÔøΩÔøΩ\u0013ÔøΩÔøΩÔøΩ7ÔøΩÔøΩSz<)ÔøΩÔøΩtÔøΩ\n‹ÉÔøΩ2ÔøΩÔøΩÔøΩÔøΩ'\u0019ÔøΩ@AV&ÔøΩAÔøΩÔøΩYÔøΩ\u000fÔøΩ\u0017\u0002ÔøΩhÔøΩÕπ<ÔøΩ\bÔøΩÔøΩ(\u0001%6JÔøΩÔøΩÔøΩ\u0012x\u0005F:<CÔøΩ\tÔøΩ\\\bÔøΩ4H=ÔøΩÔøΩ\ndT@ÔøΩ\nÔøΩ8k\u0014QÔøΩÔøΩÔøΩqÔøΩjœñÔøΩÔøΩ_eÔøΩcÔøΩÔøΩXÔøΩÔøΩ7'ÔøΩÔøΩÔøΩÍã±VÔøΩ)ÔøΩÔøΩ<ÔøΩaÔøΩÔøΩ\u00153ÔøΩ\u001f-ÔøΩsIÔøΩ2\nVF\\ÔøΩÔøΩÔøΩÔøΩÔøΩB\u001bÔøΩ⁄ãÔøΩ`\u0007\u0017ÔøΩ_BÔøΩÔøΩ9UÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0016ÔøΩ\nÔøΩÔøΩÔøΩ“•ÔøΩr8ÔøΩÔøΩ\nÔøΩOÔøΩ_'ÔøΩÔøΩÔøΩÔøΩ\u0007ÔøΩODÔøΩQÔøΩjÔøΩÔøΩÔøΩ\u000fÔøΩT_\u0014XÔøΩ^ÔøΩ\u0019ÔøΩÔøΩgÔøΩÔøΩÔøΩ;ÔøΩ\u0011rÔøΩzÔøΩX|ÔøΩ4bBÔøΩfAÔøΩÔøΩ{\nÔøΩ\u0000\u0011\nÔøΩ'0Q`+ÔøΩs!ÔøΩÔøΩÔøΩr~b¬¶\u000fÔøΩQÔøΩ2‘£ÔøΩ\u000252$qÔøΩÔøΩ °ZÔøΩ\u001aÔøΩ/\u0006&4\u0004gÔøΩÔøΩD>\u0006ÔøΩjÔøΩ\u0006ÔøΩÔøΩ[ÔøΩÔøΩ‘£ÔøΩ\u0001ÔøΩÔøΩ«îX9ÔøΩÔøΩ)]ÔøΩWÔøΩ,$+qY\u001aÔøΩ\u0005ÔøΩnPÔøΩÔøΩG\u0007ÔøΩ6yÔøΩSaÔøΩy@^\u000f%\u0007=\nÔøΩ\u0003\u0019\nÔøΩÔøΩ^6\u0010ÔøΩ∆≥\ndÔøΩÔøΩ.\u0007>\u001b\u0018)0ÔøΩÔøΩÔøΩ/ÔøΩXÔøΩÔøΩÔøΩÔøΩÔøΩv5ÔøΩÔøΩRs\u000eG +ÔøΩŒùÔøΩ8ÔøΩfÔøΩﬂùÔøΩ‹ÄÔøΩoÔøΩtÁûª\u00165wÔøΩ^ÔøΩwÔøΩÔøΩf=ÔøΩÔøΩÔøΩÔøΩteVÔøΩÔøΩ\u0001ÔøΩx\u0004ÔøΩmÔøΩVÔøΩœåÔøΩÔøΩÔøΩÔøΩ\"\u000eÔøΩ\u0001ÔøΩÔøΩxÔøΩ”Ü\u0003ÔøΩl\np8ÔøΩ ∂@\u0017\u000eÔøΩdÔøΩE8ÔøΩc5ÔøΩ2mÔøΩÔøΩRÔøΩ◊ñÔøΩ^ÔøΩ<ÔøΩÔøΩÔøΩ,PƒßÔøΩ$\u0002ÔøΩ\u000fiÔøΩv-FZ^KÔøΩ\u001b\u0003ÔøΩÔøΩÔøΩ!3ÔøΩ1ÔøΩÔøΩÔøΩÔøΩGÔøΩ'Ã¨ÔøΩ¬çhz ÔøΩŒôdÔøΩÔøΩÔøΩÔøΩ\u0015ƒüÔøΩ@\u0019ÔøΩa?ÔøΩ75\nÔøΩ\u0012ÔøΩg~ÔøΩÔøΩ\nÔøΩ3o.ÔøΩ`\u000eOÔøΩ/ÔøΩTÔøΩ\u0000'\u0017ÔøΩ]ÔøΩP\u0018ÔøΩÔøΩ\t@ÔøΩÔøΩ0QÔøΩ<ÔøΩÔøΩnÔøΩN—¨{ÔøΩÔøΩÔøΩwƒ´ÔøΩXÔøΩÔøΩÔøΩ\btÔøΩCÔøΩnÔøΩ,ÔøΩ;\u0016ÔøΩ;J\\\u0002ÔøΩ\b\n#ÔøΩÔøΩpÔøΩ\"ÔøΩÔøΩp@\u0010E\u0016z≈µBÔøΩÔøΩhÔøΩ XÔøΩ\u0016\u000eqÔøΩJ'ÔøΩ9\nT\n\u0017\u0007`ÔøΩQÔøΩÔøΩ(Nq\u0014ÔøΩ8\nQ\nE,ÔøΩÔøΩ\u0014GqÔøΩ(ÔøΩÔøΩ\n\u000erG9RÔøΩÔøΩ\u0014ÔøΩ4aÔøΩaÔøΩÔøΩp\u0005ÔøΩÔøΩ\u0015ÔøΩ\nWpÔøΩÔøΩ{\u0013ÔøΩÔøΩÔøΩrÔøΩ\u0002W4ÔøΩ\\\u0000WGÔøΩ8\u0010\nÔøΩLÔøΩ?LÔøΩ÷Ä\nÔøΩ\u0012F\u001aÔøΩ\u0018ÔøΩÔøΩ\n\u0015ÔøΩ2TÔøΩÔøΩPnL\u000epX\u0000ÔøΩ>1ÔøΩÔøΩÔøΩ·òÅ1ÔøΩÔøΩÔøΩgÔøΩ\ndÔøΩP\u0003uÔøΩÔøΩÔøΩcÔøΩÔøΩKTzÔøΩ_ÔøΩ\n\u0000ÔøΩÔøΩÔøΩÔøΩ.\u0001ÔøΩIÔøΩÔøΩ}\u000fÔøΩÔøΩÔøΩ\n\u0013ÔøΩVfÔøΩ*QÔøΩÔøΩFÔøΩ»ºU5ÔøΩxCÔøΩMÔøΩÔøΩŸòÔøΩÔøΩÔøΩk,qOÔøΩWÔøΩD1f<ÔøΩ\u0006ÔøΩÔøΩÔøΩiKÔøΩ\u0014_NÔøΩdÔøΩÔøΩÔøΩ-~ÔøΩ{\u000e\nÔøΩÔøΩÔøΩAÔøΩÔøΩÔøΩÔøΩÔøΩ\u0000YÔøΩh\u0012ÔøΩÔøΩ\u001fÔøΩÔøΩFxÔøΩ\u0010ÔøΩ#[ÔøΩ7ÔøΩÔøΩÔøΩW\nRÔøΩ\u001fH\u0012ÔøΩÔøΩSÔøΩÔøΩSÔøΩsÔøΩ\\kp6ÔøΩÔøΩ8#qu<ÔøΩ\u0014ÔøΩ\t‹¶ÔøΩ1ÔøΩKÔøΩdÔøΩÔøΩ\u000eÔøΩÔøΩ\u0013ÔøΩ\u0012ÔøΩ«ù<bZÔøΩak{\\ÔøΩÔøΩÔøΩÔøΩ\u0003nÔøΩ9ÔøΩ.ÔøΩÔøΩ€•h,ÔøΩNeÔøΩ.ŸúÔøΩl;ÔøΩÔøΩÔøΩÔøΩÔøΩ\\ÔøΩ\bÔøΩÔøΩlV+\n3Z}ÔøΩÔøΩÔøΩtÔøΩ\na$ÔøΩtfÔøΩÔøΩJ:\u0018ÔøΩ5ÔøΩeÔøΩ]QÔøΩÔøΩG\u000e\u0017ÔøΩÔøΩ4\u0005ÔøΩ¬ò\u000fÔøΩÔøΩÔøΩ\ngÔøΩHÔøΩ<%ÔøΩoÔøΩ-ÔøΩÔøΩA%ÔøΩÔøΩ\u0001%ÔøΩP$b8)^=ÔøΩ\u0006ÔøΩ`cÔøΩgÔøΩ`ÔøΩÔøΩ\u00142ÔøΩ'ÔøΩ”ùÔøΩÔøΩÔøΩÔøΩF>\u0000ÔøΩÔøΩ\bÔøΩJÔøΩh.\nMÔøΩ\n6ÔøΩ\u00029ÔøΩÔøΩBÔøΩd5ÔøΩ^\u0000ÔøΩVKUÔøΩ\\5\nÔøΩ\u001bÔøΩÔøΩVoÔøΩfzÔøΩÔøΩUÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ>\u0000’ÄÔøΩmÔøΩ4ÔøΩ>Z\u0000»´lÔøΩ\n\u0003ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩnÔøΩŒê\u0006#ÔøΩ∆ìÔøΩ2ÔøΩQ\u001aÔøΩ\u001fMÔøΩÔøΩ\nÔøΩ\u0006ÔøΩ+)ÔøΩÔøΩÔøΩen<@\u00032@lÔøΩuAr$ÔøΩ\u0006fÔøΩuÔøΩÔøΩÔøΩ\u000eeYMr\nÔøΩÔøΩM9(ÔøΩ,◊©ÔøΩ'\nÔøΩ2lVJQÔøΩ\u0010n\nÔøΩÔøΩ8◊æ4ÔøΩ\u001bLMjlÔøΩÔøΩÔøΩÔøΩ\nÔøΩ:Ô∫ßÔøΩ=ÔøΩÔøΩÔøΩÔøΩÔøΩW_~ÔøΩU'ÔøΩ\nÔøΩ[zÔøΩ4ÔøΩÔøΩÔøΩÔøΩWÔøΩ\n5ÔøΩÔøΩÔøΩÔøΩ\u000f\u001b=ÔøΩ2ﬂ∫ÔøΩÔøΩÔøΩÔøΩuÔøΩÔøΩÁ∞≠ÔøΩpÔøΩÔøΩÔøΩ\u001bœª5TS3?ÔøΩÔøΩÔøΩ\u0006ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u001fÔøΩ_9rÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩ@}ÔøΩ9ÔøΩ;\\&ÔøΩÔøΩ\"ÔøΩLÔøΩÔøΩ+AÔøΩÔøΩÔøΩl)0ÔøΩ`ÔøΩ9 iÔøΩÔøΩ*ÔøΩ\u0005ÔøΩÔøΩÔøΩ\u0005ÔøΩF7ÔøΩ\u001b\u001a]ÔøΩ\u0000ÔøΩÔøΩ_ÔøΩ\u0013RÔøΩ@\u0015d$ÔøΩ09\nh\u0013ÔøΩ2ÔøΩH1,ÔøΩ\u0003ÔøΩ\u0011rÔøΩÔøΩ(ÔøΩÔøΩÔøΩpÔøΩ\u0015ÔøΩRBÔøΩ\u0012\u000etÔøΩÔøΩRH5ÔøΩÔøΩXÔøΩoÔøΩ]ÔøΩÔøΩÔøΩhÔøΩ\u0005ÔøΩÔøΩÔøΩZÔøΩbVÔøΩÔøΩG\u0002y ätÔøΩ_»°ÔøΩÔøΩ=C=Gz\u0014=\u000eÔøΩÔøΩ`4\u0006ÔøΩƒòpÔøΩ\u0000ÔøΩ\\\u0014ÔøΩ\nvÔøΩÔøΩ:ÔøΩb$ÔøΩv\u0017qÔøΩ\u001f\u0012\u0012ÔøΩ\u0014\nJ…áR/ÔøΩÔøΩÔøΩ\u0018)ÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩ\u0018h$ÔøΩÔøΩ)ÔøΩeÔøΩÔøΩ\tÔøΩÔøΩÔøΩ'&ÔøΩ›ìÔøΩ&\u001fÔøΩÔøΩd`sl23ÔøΩ5ÔøΩgÔøΩÔøΩ3\nÔøΩÔøΩÔøΩÔøΩÔøΩ%ÔøΩÔøΩ\u0015ÔøΩm|\"-ÔøΩÔøΩÔøΩZÔøΩÔøΩÔøΩ\u0000ÔøΩ\u0018:U)ÔøΩÔøΩÔøΩËíìÔøΩ\u0005'ÔøΩÔøΩÔøΩ*ÔøΩ\nÔøΩ\u0003LÔøΩLÔøΩ«ß’≥\\6ÔøΩ÷∞i?ÔøΩ>ÔøΩ€èuÔøΩ\nWÔøΩÔøΩ\nÔøΩÔøΩ\n‹ûX~@gn–¥s7HÔøΩ@PÔøΩ\u000eÔøΩÔøΩ16ÔøΩ\u000eÔøΩP0ÔøΩRbÔøΩ\\ÔøΩÔøΩ\u0014wÔøΩÔøΩÔøΩD\u000fÔøΩta]N'ÔøΩÔøΩJÔøΩÔøΩaÔøΩfÔøΩÔøΩhÔøΩÔøΩÔøΩMd\u000e7GÔøΩ9ÔøΩÔøΩÔøΩÔøΩÔøΩ`yÔøΩÔøΩÔøΩÔøΩU}rG\u000fÔøΩ<7RÔøΩl\u0018hÔøΩÔøΩ\u00029”çÔøΩ>9ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩeÔøΩÔøΩ\\6VÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩ_\u0006.i,ÔøΩŒöVxZCyÔøΩÔøΩÔøΩÔøΩ\u001aÔøΩ4UVIÔøΩÔøΩ\tÔøΩÔøΩÔøΩ;ÔøΩÔøΩÔøΩ\u001bf/ÔøΩ:ÔøΩ{wÔøΩÔøΩS@ÔøΩ|ÔøΩ\nÔøΩ5ÔøΩK.HÔøΩ\nÔøΩÔøΩÔøΩXÔøΩhÔøΩxÔøΩBÔøΩz8ÔøΩ–∫ÔøΩÔøΩvÔøΩ;oQÔøΩÔøΩ[v\u0016ÔøΩÔøΩ$ÔøΩ\u0002ÔøΩ;ÔøΩYÔøΩÔ∫¶3ÔøΩÔøΩ\u00175+ÔøΩ\u0011j≈öoÔøΩ6Ia[`FQÔøΩ,ÔøΩ}ÔøΩÔøΩatNÔøΩ\u0004\u0017\u0006ÔøΩÔøΩ\u000e_\"ÔøΩ5<\u001bf\"ÔøΩÔøΩUÔøΩÔøΩ\u0003[ÔøΩ[\"ÔøΩÔøΩoNhÔøΩÔøΩÔøΩ]ÔøΩÔøΩVÔøΩ&ÔøΩJÔøΩ\u000e»¨RÔøΩ“Æ◊Æ\u000f\u001fbÔøΩÔøΩ\u0018ÔøΩ\u000eÔøΩ\u000fF\u000f&4ÔøΩbw\\JlÔøΩﬂî`\u001fÔøΩﬁóxÔøΩÔøΩÔøΩÔøΩ)ÔøΩÔøΩ\u001f«ïÔøΩ\nNJÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩyÔøΩ ÔøΩÔøΩVÿ≥…ÅMoÔøΩu2ug-ÔøΩ\u0015–¢ÔøΩÔøΩI\u0000ÔøΩ\u0001ÔøΩÔøΩÔøΩ\u0014ÔøΩÔøΩ$ÔøΩÔøΩ\u0017ÔøΩÔøΩƒõÔøΩÔøΩ\nÔøΩÔøΩÔøΩeÔøΩÔøΩÔøΩÔøΩÔøΩUÔøΩ\\ÔøΩÔøΩÔøΩ9ÔøΩ\n]ÔøΩO YÔøΩgu,8P^ÔøΩJÔøΩÔøΩ>ÔøΩÔøΩÔøΩÔøΩ&OÔøΩÔøΩ\n1\u0000\u0014ÔøΩj@=lÔøΩ\u0017KVÔøΩGÔøΩ\n\u000e_ÔøΩAÔøΩﬁ∑ÔøΩÔøΩÔøΩÔøΩ\n&8ÔøΩ6\u001aÔøΩEbÔøΩ\u0018ÔøΩ)#:Q\nAÔøΩ\u0010?\u0005\u0007\u0003FÔøΩK@I\u0013ÔøΩGÔøΩ1ÔøΩÔøΩÔøΩTqyÔøΩÔøΩ<W)ÔøΩ{B^o#ÔøΩnÔøΩ\u0006ÔøΩÔøΩp\nÔøΩrÔøΩÔøΩ\u0015ÔøΩgÔøΩHNÔøΩ\"9ÔøΩ\u000eÔøΩ\u0006ÔøΩy\u0014ÔøΩXuÔøΩÔøΩ\u0018ÔøΩ(ÔøΩ=|ÔøΩÔøΩŒΩÔøΩ'ÔøΩÔøΩ\\ÔøΩ`ÔøΩ\u0011ÔøΩÔøΩ3ZyÔøΩÔøΩ≈øÔøΩÔøΩrÔøΩÔøΩÔøΩnÔøΩ\nÔøΩ^1ÔøΩÔøΩuÔøΩ7keÔøΩ\u0014ÔøΩÔøΩÔøΩzÔøΩÔøΩÔøΩG^-ÔøΩvÔøΩÕä›ΩÔøΩh$ÔøΩ\u0006¬ñÔøΩÔøΩw[Z/~vÕ∂gq-~ÔøΩWÕà7SÔøΩ4ÔøΩNÔøΩ{ÔøΩÔøΩUÔøΩÔøΩÔøΩÔøΩ»ÑÔøΩ\b}&=iÔøΩÔøΩÔøΩv+ÔøΩvÔøΩÔøΩG<ÔøΩ+ÔøΩ\u0019k∆ûqÔøΩy{ÔøΩ1ÔøΩ:ÔøΩ>«±ÔøΩ]`ÔøΩÁøî]ÔøΩYÔøΩ]e^c_ÔøΩXÔøΩ_\u0019XÔøΩ_mÔøΩÔøΩÔøΩMÔøΩÔøΩÔøΩ\nÔøΩÔøΩbÔøΩÔøΩÔøΩ'ÔøΩsÔøΩw\nTÔøΩ>ÔøΩ|ÔøΩ=eÔøΩÔøΩ:ÔøΩi8-gPÔøΩIÔøΩ“ΩÈæ¥\u001acb6ÔøΩ,\u0016ÔøΩÔøΩ\u0001ÔøΩÔøΩÔøΩ\n8\u00151\nKV\u0005bÔøΩXÔøΩB\u0015P;,AÔøΩ3ÔøΩ=ÔøΩÔøΩ\u0006#ÔøΩÔøΩ4RZ7lb\bx[ÔøΩK\u0017\u0007Pu0Y]ÔøΩ\n\bZ\u0003ÔøΩÔøΩ\u0005ÔøΩ\u0011\u0017 ÔøΩ¬Ä\u001fÔøΩ~\u0005ÔøΩf0saÔøΩÔøΩ\nÔøΩÔøΩÔøΩtÔøΩÃê7ÔøΩ\nÔøΩ(ÔøΩÔøΩ~ÔøΩ\u0019#Œ§\nÔøΩ\u000fÔøΩ_\u0004I0\u0019\n$ÔøΩ\u0001?}mOÔøΩ5’±ÔøΩ”°QsÔøΩ\nAÔøΩ4ÔøΩ&V◊ßeÔøΩ÷îÔøΩÔøΩ`HN%ÔøΩÀùKKtÔøΩ\u0011ÔøΩÔøΩÔøΩIÔøΩ4\u0000}ÔøΩÔøΩ``\u0004ÔøΩ\u000eH}ÔøΩ~ÔøΩÔøΩ\u0017p\n\u0005ÔøΩzb}ÔøΩzÔøΩÔøΩÔøΩfÔøΩjI›´f‘ÆTzÔøΩÃìcÔøΩ#ÔøΩÔøΩÔøΩ2n'ÔøΩ.ÔøΩmÔøΩsÔøΩÔøΩ\u001awÔøΩ`](\u0013=\u0019ÔøΩÀ†}&ÔøΩ;\u00009ÔøΩÔøΩyÔøΩÔøΩÔøΩ\nÔøΩ÷¥39`ÔøΩÔøΩÔøΩ\nÔøΩMÔøΩœéÔøΩ&ÔøΩc\u0010ÔøΩÔøΩÔøΩ ´ÔøΩTmÔøΩZm:K)ÔøΩÔøΩÔøΩtÔøΩÔøΩ$~ÔøΩLÔøΩÔøΩÔøΩ15ÔøΩUÔøΩf\u0007|ÔøΩD\u0010ÔøΩ\u0011ÔøΩÔøΩ8p9\n,ÔøΩ\u0019ÔøΩÔøΩÔøΩ[PÔøΩ9YB_ÔøΩÔøΩÔøΩ\u0014sÔøΩÔøΩÔøΩÔøΩpÔøΩ-\u001a*ÔøΩÔøΩiÔøΩÔøΩÔøΩ\\ÔøΩ6b[ÔøΩ!YÔøΩÔøΩKÔøΩ\u001bÔøΩ\u0006ybW&hÔøΩDÔøΩfÔøΩy≈ªÔøΩÔøΩ\u0019qU$¬∏\nÔøΩ\u0019PÔøΩÔøΩqÔøΩ¬äHÔøΩÔøΩÔøΩÊ∏ãÔøΩ[\u0004^\u0018k\u0000/ÃÉ¬®\u0006ÔøΩ'ÔøΩyÔøΩ\u0003ÔøΩWx÷ëuÔøΩ'ÔøΩÔøΩV\u001fÔøΩ\u001fÔøΩ~KÔøΩÔøΩ‘ø2ÔøΩ*‹Ñ{ÔøΩ4ÔøΩydÔøΩg\u0005ÔøΩÔøΩlÔøΩ>ÔøΩPÔøΩÔøΩÔøΩwÔøΩÔøΩ\tÔøΩÔøΩÔøΩ\tÔøΩeM=ÔøΩhÔøΩ\u001b\u000eÔøΩ\nAÔøΩZ\u0010ÔøΩÔøΩÔøΩ%ŸàÔøΩ\tÔøΩt0[ÔøΩ@\u0011ÿãÔøΩÔøΩÔøΩMG\"jk8mÔøΩYI\"ÔøΩRÔøΩU(»É~ÔøΩÔøΩÔøΩÔøΩYÔøΩ\nÔøΩ\u0018cÔøΩ\u0018\u0001ÔøΩ7ÔøΩjÔøΩFÔøΩb8D√ò ÔøΩÔøΩÔøΩÔøΩÔøΩ)ÔøΩe(ÔøΩ\u0003ÔøΩÔøΩÔøΩ,\nX6&ÔøΩ'ÔøΩÔøΩeF2ÔøΩsÔøΩ-|ÔøΩ\u0013ÔøΩ\u000eT\u0000\u001aQR[ÔøΩV^uÔøΩLÔøΩDÔøΩÔøΩU:\"BÔøΩ\u0011ÔøΩÔøΩ\u0011ÔøΩ\nÔøΩ`ÔøΩnÔøΩÔøΩt\u0006ÔøΩÔøΩaÔøΩ\u0011aÔøΩÔøΩb\u0013\u0019ÔøΩdÔøΩ=\u0011ÔøΩ/ÔøΩÔøΩ\nÔøΩÔøΩWgÔøΩOÔøΩhu2ÔøΩ,,ÔøΩÔøΩÔøΩÔøΩWJÔøΩ\nÔøΩÔøΩQÔøΩJ.\u0014ÔøΩÔøΩÔøΩ)ÔøΩK\nÀãÔøΩXÔøΩÔøΩÔøΩ%4ÔøΩÔøΩÔøΩÔøΩC(1ÔøΩ_ÔøΩ›øÔøΩÔøΩÔøΩ9\u0017vx<ÔøΩ\u0005ÔøΩÔøΩÔøΩWÔøΩÔøΩ\u0018ÔøΩÔøΩ\n[{ÔøΩnÔøΩ\n76ÔøΩnÔøΩYÔøΩ 9ÔøΩÔøΩŒøÔøΩÔøΩ\u001b\"bÔøΩeLÔøΩeÔøΩBÔøΩÔøΩ\nK\u001f0KW,Zty\u001b\nÔøΩVq\u0016ÔøΩÔøΩ‘≠ÔøΩ,ÔøΩÔøΩ\nU\"kMÔøΩCÔøΩÔøΩÔøΩÔøΩ'{ÔøΩÔøΩÔøΩ\bÔøΩÃ®œ±›ÑÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩe9;\u0017ÔøΩ\u0014F=\u0012Pu@ÔøΩ\n|5gÔøΩcxÔøΩ@<\u0018YÔøΩ\u0001C9ÔøΩ\u0016ÔøΩÓ¶†\u000e\u0007ÔøΩr4-DÔøΩiÔøΩƒ™pÔøΩZ\u0014ÔøΩ\u0007NEŒï\nÔøΩ\u0010\n\u0019\nzÔøΩJÔøΩÔøΩY-ÔøΩuÔøΩP/ÔøΩ\u0002,ÔøΩ\u0002^ÔøΩÔøΩzÔøΩ\u0016ÔøΩZÔøΩ\u0017jÔøΩÔøΩÔøΩB]Q\nÔøΩX,ÔøΩ\u0017jÔøΩF\n\u000eZÔøΩZ\boÔøΩ\u0016ÍÉöÔøΩÔøΩq =ÔøΩ&ÔøΩt?eÔøΩÔøΩeËÅ¶ÔøΩ\u0015_4]q>ÔøΩ\u0015ÔøΩ3]ÔøΩMÔøΩ\u00076ÔøΩ√ìFÔøΩ0JÔøΩ*v&d\u0016√ôÿëÿ±\u0018\u0013ÔøΩ8ÔøΩÔøΩÔøΩ\u0013\u001aÔøΩÃ¢ÔøΩ °ÔøΩP.ÔøΩJ}\u0019:+ÔøΩÔøΩÔøΩ\nÔøΩ\u0001\u0007ÔøΩÔøΩÔøΩ9\u00118ÔøΩ=OÔøΩRSsÔøΩEÔøΩBÔøΩYh\"jfÔøΩ ìÔøΩÂ®ôÔøΩFÔøΩ\n4jfÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩY9\u001a1HÔøΩ8ÔøΩÔøΩbÔøΩÔøΩÔøΩagEÔøΩfmÔøΩZpM\n\\ÔøΩhÔøΩÔøΩlNzÔøΩfV\u001b-ÔøΩ≈®ÃØÿôÔøΩÔøΩ€±|ÔøΩÔøΩÔøΩÔøΩkÔøΩY»Ω\u0002?rEkÔøΩÔøΩÔøΩÔøΩ]ÔøΩ\u0012T2ÔøΩZÔøΩ\nÔøΩ0ÔøΩ)\u0000,SÔøΩ5(ÔøΩÔøΩÔøΩ\u0005R\u0006sÔøΩ@ÔøΩ–•ÔøΩt!ÔøΩ\"ÔøΩD\u0018ÔøΩMÔøΩ^ÔøΩ3#ÔøΩ!ÔøΩ\u001bÔøΩÔøΩÔøΩ+ÔøΩYiÔøΩ:ÔøΩaÔøΩxÔøΩÔøΩ6zÔøΩ\nÔøΩ\n7TC\u0015ÔøΩ\u0002ÔøΩÔøΩ\n5ÔøΩ\u00042ÔøΩdÔøΩ\nX\u0010,ÔøΩÔøΩÔøΩuyrN\\ÔøΩ \u0015vÔøΩ3ÔøΩ}'ÔøΩSÔøΩB(\nÔøΩ\b\n3b„Øé\u0019ÔøΩÔøΩWzÔøΩ9.ÔøΩ[ÔøΩÔøΩtÔøΩÔøΩÔøΩksAÔøΩ1\nAÔøΩÔøΩ.ÔøΩ€¨€°€≠„êéÔøΩÔøΩÔøΩÔøΩcÔøΩOtJÔøΩ+ÔøΩÔøΩfH:ÔøΩÔøΩaÔøΩ\nsÔøΩN\nÔøΩ=ÔøΩ‰¨ìÔøΩ[\u000eÔøΩ\n\u00007SŒΩÀüJÔøΩÔøΩ\nZ!€£ÔøΩ@\u001b]ÔøΩÔøΩ\u001fÔøΩÔøΩiÔøΩ\nÔøΩÔøΩ=(ÔøΩPy+OKÔøΩÔøΩ\u0017*ÔøΩÔøΩ\n;\u0016ÔøΩÔøΩÔøΩcÔøΩ/Co4\u000eW^tÔøΩÔøΩ\nÔøΩ:\u001b~ÔøΩ\u001aÔøΩ7ÔøΩÔøΩ|ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ6ÔøΩÔøΩ>)7ÔøΩSÔøΩxÔøΩ/FÔøΩ1]ÔøΩÔøΩ/tFÔøΩp\u0018{ÔøΩ3ÔøΩ-KÔøΩ2;ÔøΩ\\–îÔøΩÔøΩ\u0010R{MFÔøΩ∆òÔøΩﬂ≥^^ÔøΩÔøΩ\tÔøΩpÔøΩÔøΩÔøΩÔøΩ\tÔøΩ\u0000\t⁄ö\u000fÔøΩ\u0017\nÔøΩÔøΩÔøΩÔøΩs=ÔøΩSÔøΩÔøΩQÔøΩ \bbÔøΩÔøΩ<ÔøΩrÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ=wÔøΩw\u001awÔøΩOÔøΩÔøΩÔøΩÔøΩ\u0019ÔøΩÔøΩÔøΩÔøΩVbdxÔøΩÔøΩmÔøΩ2ÔøΩ`FÔøΩ|ÔøΩOgEÔøΩhÔøΩ}>ÔøΩÔøΩo\u0000\u0016\u0003€ÄÔøΩ\u0000\nÔøΩÔøΩu\u0004ÔøΩÔøΩp\u0000(ÔøΩÔøΩ\u0000&ÔøΩxkÔøΩ\u0016\b\u0007DQLÔøΩ\u0003iQdÿó\tÔøΩÔøΩ\bÔøΩÔøΩÔøΩ7XÔøΩDÔøΩhÔøΩÔøΩÔøΩB[FÔøΩÔøΩÔøΩÔøΩ\u0006ÔøΩUÔøΩ\u0000J# \u001fÔøΩAÔøΩ\bÔøΩÔøΩÔøΩÔøΩÔøΩ\u001fÔøΩUÔøΩ3\n\u0016ÔøΩ6ÔøΩ;ŸèÔøΩdS\u0016gAKlÔøΩ\u001aÔøΩ[ÔøΩ\bÔøΩ\u001fÔøΩkÔøΩf\u0004ÔøΩÔøΩk\u0018ÔøΩ\u000fc\u00032bÔøΩdÔøΩÔøΩ\u001a}\u0001\u001fÔøΩÔøΩÔøΩ\u0003ÔøΩ\bz\"\u0011/ÔøΩ\nÔøΩ2ÔøΩ#ÔøΩcÔøΩ\u0013qE‹ïÔøΩ~\u000f3@ÔøΩgÔøΩe:]ÔøΩ\u0004ÔøΩÔøΩÔøΩSÔøΩÔøΩÔøΩ'ÔøΩÔøΩ\nÔøΩÔøΩÔøΩceÔøΩÔøΩ S\u0010ÔøΩ,~lÔøΩ9~ÔøΩÔøΩ\n_UUfÔøΩ\u0015ÔøΩi\u0014hÔøΩÔøΩLÔøΩ&ÔøΩÔøΩ(ÔøΩÔøΩÔøΩÔøΩL1ÔøΩ\nÔøΩÔøΩijl8ÔøΩ\u001b(-ÔøΩÔøΩMÔøΩÔøΩ\u0002ÔøΩÔøΩ}›ùzÔøΩ7\nÔøΩj\tÔøΩmÔøΩ\n+ÔøΩKÔøΩ\u0015}A_ÔøΩP[\u0015xVÔøΩÔøΩ}zÔøΩ/'ÔøΩx<aÔøΩ\u0013ÔøΩ6//~ÔøΩ5ÔøΩ\u0000\u000eÔøΩÔøΩÔøΩhÔøΩÔøΩ÷ßÔøΩ1ÔøΩ\u0000\u0007ÔøΩ1ÔøΩÔøΩÔøΩÔøΩH~+ÔøΩ\u001fÔøΩÔøΩvÔøΩgq2ÔøΩyÔøΩÔøΩiÔøΩÔøΩMÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩf'VV;P\u00045ÔøΩ9ÔøΩÔøΩk{ÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩÔøΩoÔøΩÔøΩÔøΩmÔøΩÔøΩQÔøΩÔøΩnÔøΩNÔøΩ\n>ZÔøΩgÔøΩ\u000fTÔøΩe/W_Ó∫¢ÔøΩFÔøΩ\nÔøΩÔøΩÔøΩIÔøΩ\u0010z\u0015ÔøΩÔøΩÔøΩ8ÔøΩUeÔøΩ—¥`wÔøΩ \u001aÔøΩjÔøΩ\u0003ÔøΩmFXÔøΩr)ÔøΩjÔøΩÔøΩÔøΩtÔøΩUZ`/\u0004ÔøΩEÔøΩ}ÔøΩÔøΩ\n9ÔøΩ>SÔøΩ*ÔøΩ\n\"\nÔøΩ:ÔøΩÔøΩ\u000fÔøΩÔøΩÔøΩ@M\"Î´ë\u0014q\u0005“éÔøΩÔøΩ\n;ÔøΩ\u001aÔøΩÔøΩ\u001bÔøΩÔøΩÔøΩ*eÔøΩ]^#ÔøΩJ≈´ÔøΩÔøΩxÔøΩÔøΩ\n!÷¶ÔøΩ\u000eÔøΩÔøΩÔøΩPkÔøΩ*MÔøΩÔøΩ\u0005y\u0017tyU<\u0001ÔøΩ\u0012\u000eÔøΩVÔøΩÔøΩÔøΩ.ÔøΩÔøΩ\u0015NÔøΩ\u0004NÔøΩ\u0013ÔøΩ*ÔøΩÔøΩ\u0016:5ÔøΩÔøΩÔøΩ\u0004\u0003&\n\u0011ÔøΩFÔøΩT◊Å&ÔøΩÔøΩ\u001aÔøΩ\"ÔøΩÔøΩ8iC\u0012rÔøΩ<ÔøΩÔøΩ“ë\u0003t:ÔøΩÔøΩ\nrÔøΩphÔøΩÔøΩg\u0018ÔøΩÔøΩoœ¶ÔøΩ_ÔøΩÔøΩÔøΩAJiÔøΩeV{ÔøΩÔøΩÔøΩŒ¢ÔøΩÔøΩÔøΩrÔøΩÔøΩe\nÔøΩÔøΩ\u0015rÔøΩÔøΩ^o.\u0017>-»öwÔøΩk[ÔøΩ6ÔøΩ\u0001ÔøΩ\u00050h\u0003ÔøΩÔøΩRÔøΩ5Sb\u0011ÔøΩÔøΩYÔøΩ\u0015[bi\"{nÔøΩ\t\u0012[ÔøΩTÔøΩG—úÔøΩk.Œè\u0015ÔøΩÔøΩwDÔøΩS:\u001b$2sjÔøΩ\u0006k~ŸîÔøΩÔøΩÔøΩ'wvÔøΩmÔøΩÔøΩ?ÔøΩ[ÔøΩÔøΩ3\u0001ÔøΩ\"ÔøΩÔøΩ_<¬¨>ÔøΩSqŒìÔøΩ\\$BbÔøΩÔøΩÔøΩÔøΩÔøΩ\bŸ±nÔøΩ\u0010ÔøΩ`ÔøΩ2dsÔøΩ\u001bÔøΩÔøΩt-ÔøΩÔøΩgH\u00044ÔøΩÔøΩÔøΩ_ÔøΩÔøΩÔøΩÔøΩ\b\u0000\n[$ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩXÔøΩÔøΩ\u001bÔøΩb+ÔøΩ\u000f3.$ÔøΩÔøΩ\u0003q-z\u0005ÔøΩSÔøΩ;%ÔøΩÔøΩÔøΩ!ÔøΩ\u0004ÔøΩÔøΩÔøΩÔøΩÔøΩM\u0013aÔøΩNI\u001fÔøΩe|yÔøΩ\tÔøΩÔøΩ\u0007ÔøΩv\u0004ÔøΩÔøΩÔøΩ3ÔøΩÔøΩÔøΩ-(ÔøΩ~ÔøΩÔøΩÔøΩÔøΩ\u0007F\u0001\u0000F\n`\u000e∆ÅtÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩn\u000e\nV,F\u0001ÔøΩsÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩK ≥ÔøΩÔøΩy7N\\\u0017ÔøΩVMmÔøΩx-…é⁄¶ÔøΩo\n|ÔøΩ«´ÔøΩ]ÔøΩ7w\\xa\u0007|ÔøΩ·µóÔøΩ|ÔøΩ\u001bÔøΩÔøΩŸùÔøΩ0ÔøΩÔøΩf úÔøΩÔøΩÔøΩ\nÔøΩaÔøΩ#KÔøΩ\\t\u001aÔøΩ,^\nÔøΩ`|ÔøΩJdÔøΩÔøΩ\u0019ÔøΩ\u0003{ÔøΩÔøΩ\u0010]ÔøΩÔøΩoÔøΩÔøΩÔøΩ^ÔøΩÔøΩ{ÔøΩFÔøΩÔøΩÔøΩ\u0019ÔøΩ-ÔøΩ%B\u000f}#G ->ÔøΩ@_ÔøΩÔøΩœ¨ÔøΩÔøΩ\u000f0\u0011ÔøΩ\u000elÔøΩÔøΩÔøΩ\u0011ÔøΩ¬∞ÔøΩ\u0001ÔøΩ3\u0019\u0001ÔøΩ}ÔøΩÔøΩJÔøΩT\"‚∞´ÔøΩjLÔøΩÔøΩÔøΩÔøΩÔøΩzÔøΩÔøΩÔøΩWÔøΩ\"oÔøΩÔøΩ\"\u000fÔøΩ\u0005ÔøΩÔøΩYÔøΩ!ÔøΩkÔøΩ õ;\u0003\u0005ÔøΩÔøΩ^ÔøΩÕùÔøΩÔøΩ;gV„îóFÔøΩ\\ÔøΩÔøΩÔøΩÔøΩUqÔøΩk\bv:ÔøΩ\u001aIÔøΩ\nÔøΩ!UÔøΩ◊∞ÔøΩÔøΩ\u000fÔøΩ%T\u001aÔøΩÔøΩÔøΩzd‰ççh0ÔøΩ\u001fÔøΩ\nÔøΩ\nÔøΩ\u0013ÔøΩ0ÔøΩ\bÔøΩ\bÔøΩCÔøΩBzEP\u0011J(bZÔøΩÔøΩpsÔøΩÔøΩÔøΩÔøΩÔøΩR\u000fÔøΩ\u0013ÔøΩÔøΩ\\ÔøΩÔøΩÔøΩÔøΩbÔøΩﬂÅw{ÔøΩÔøΩ!ÔøΩ\nÔøΩHÔøΩ@!IcgÔøΩyÔøΩÔøΩ#ÔøΩpÔøΩÔøΩÔøΩÔøΩ#ÔøΩÔøΩÔøΩG#ÔøΩO*%ÔøΩÔøΩÔøΩYÔøΩu\u0018ÔøΩ@ÔøΩ[ÔøΩÔøΩ\u0010O,)YÔøΩ1]2ÔøΩÔøΩa` 6ÔøΩHÔøΩ\u0012ÔøΩÔøΩ\u0007)ÔøΩ//%)ÔøΩÔøΩÔøΩÔøΩ\u0000>2ÔøΩ5$ÔøΩ^ÔøΩÔøΩ+X\nÔøΩ/ÔøΩ%ÔøΩP(>HÔøΩ^ÔøΩl!ÔøΩÔøΩVÔøΩÔøΩÔøΩÔøΩ}ÔøΩÔøΩO?zÔøΩ7@ÔøΩ\u0017]¬º\u0018n\u0010\bÔøΩWÔøΩb73ÔøΩ<ÔøΩ2xÔøΩ~yÔøΩÔøΩÔøΩÔøΩÔøΩs\u0016[;ÔøΩVÔøΩÔøΩÔøΩÔøΩ{ÔøΩ\n›ã<\n“´›´=ÔøΩ“∑xF<oz\nUÔøΩ*k\u0013jrwÔøΩnÔøΩEÔøΩE ãtÔøΩgÔøΩBOÔøΩÔøΩ“ÉpÔøΩ\u0019ÔøΩ.cÔøΩt@ÔøΩl.{ÔøΩÔøΩÔøΩÔøΩÔøΩR\u0004XÔøΩÔøΩ\u0012ÔøΩ&lÔøΩÔøΩp“êÔøΩtÔøΩ]VÔøΩ€•ÔøΩÔøΩz{^!U\nÔøΩ\u0001\b](ÔøΩv\u0019ÔøΩ:ÔøΩÔøΩÔøΩ2(LÔøΩÔøΩeÔøΩÔøΩ\u000fÔøΩ;ÔøΩÔøΩÔøΩ\u0007~ÔøΩÔøΩÔøΩq,ÔøΩF⁄æÔøΩ\u00135ÔøΩÔøΩ0ÔøΩ[HzkUÔøΩ\u0006ÔøΩÔøΩhÔøΩÿàm\u00043ÔøΩÔøΩÔøΩ«ÉÔøΩ\\ÔøΩ\u0013\u0018ÔøΩ\u000fCIÔøΩ\nc\tÔøΩÔøΩÔøΩ|ÔøΩÔøΩkÔøΩÔøΩ\u0006\\ÔøΩ$ÔøΩ^\u0019\u0015ÔøΩÔøΩgÔøΩtÔøΩÔøΩÔøΩ&ÔøΩÔøΩeÔøΩ\u000fÔøΩÔøΩyÔøΩ*ÔøΩ,/ÔøΩ2TÔøΩ{Tq\u0007ÔøΩ/ÔøΩÔøΩÔøΩÔøΩ^=uÔøΩ\n,ÔøΩ_ÔøΩrÔøΩÔøΩr<;ÔøΩ/ÔøΩ44ÔøΩÔøΩ)ÔøΩÔøΩsr\u001aÔøΩÔøΩÔøΩÔøΩvÔøΩÔøΩÔøΩ5ÔøΩÔøΩ9ÔøΩÔøΩYÔøΩtEM◊™s.ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩJetzÔøΩ\u001aÔøΩdÔøΩC=ÔøΩ\u0013ÔøΩ\u0019ÔøΩ/ﬁÖ=ÔøΩÔøΩFÔøΩAÔøΩÔøΩÔøΩÔøΩg\u0014OÔøΩÔøΩÔøΩÔøΩÔøΩeÔøΩÔøΩ\u0011ÔøΩj}ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩd\u0003i\u0007ÔøΩÔøΩ-ÔøΩÔøΩÔøΩ\u0010rÔøΩÿ•ÔøΩÔøΩÔøΩtxÔøΩÔøΩÔøΩEÔøΩY\u0014yBÔøΩ!ÔøΩaÔøΩ\u0010ÔøΩÔøΩÔøΩ\nÔøΩg-*ÔøΩ\u0015?ÔøΩsHÔøΩÔøΩ}h\u001aÔøΩeFÔøΩEÔøΩ\u0006ÔøΩY5QÔøΩtkoÔøΩgÔøΩ.–ïÔøΩÔøΩÔøΩ\u0011\tUÔøΩ\nƒπÔøΩÔøΩ\u0019ÔøΩ#ÔøΩ\\\u0005ÔøΩ“ÄÔøΩHÔøΩ~ÔøΩ[!Z?VÔøΩ7ÔøΩIÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩIÔøΩ&b#ÔøΩ\u0006ÔøΩ\nÔøΩÔøΩÔøΩÔøΩ›ÑÔøΩzÔøΩÔøΩ\nÔøΩ=ÔøΩÔøΩÔøΩÔøΩÔøΩOÔøΩÔøΩtÔøΩÔøΩ\u0004ÔøΩ\u0001P+ ÔøΩ\u0011ÔøΩh6ZÔøΩVÔøΩÔøΩ\u0011u\u0003wÔøΩÔøΩÔøΩtÔøΩ/*TÔøΩA\u0005Z|\u0011AÔøΩ\u0004}ÔøΩtÔøΩÔøΩÔøΩR\u0007ÔøΩyÔøΩ\u0018ÔøΩ*ÔøΩÔøΩÔøΩÔøΩzPC\u0003B)ÔøΩ\u001fHÔøΩ\u001fÔøΩ\u0006ÔøΩÔøΩ\t`7jÔøΩ\u0007&\u0019ÔøΩÔøΩÔøΩfÔøΩ\nÔøΩÔøΩ&\u000fÔøΩÔøΩÔøΩvÔøΩQÔøΩ;`ÔøΩÔøΩMÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u001bjÔøΩ1\\ÔøΩÕûkÔøΩ\u0007ÔøΩ2ÔøΩÔøΩhÔøΩÔøΩÔøΩ'ÔøΩÔøΩ\bÔøΩÔøΩ”Æ∆¶\u0011|ÔøΩp\bÔøΩÔøΩL\u0010JÔøΩ;hÔøΩÔøΩ'\u0007ÔøΩÔøΩ)ÔøΩ\u000eÔøΩQ\u0007ÔøΩ9ÔøΩÔøΩÔøΩJÔøΩr?\u0016ÔøΩOE\n2+ÔøΩ0,ÔøΩmÔøΩÔøΩ\n“üDÔøΩK\nÔøΩÔøΩÔøΩÔøΩ1\u0016ÀØ%ÔøΩÔøΩÔøΩ\ng/ÔøΩaÔøΩÔøΩ~RUÔøΩ\u0016v}ÔøΩÔøΩ1ÕèÔøΩ\nÔøΩÔøΩÔøΩÔøΩ/\u0014ÔøΩÔøΩÔøΩÔøΩ*-ÔøΩÔøΩIÔøΩÔøΩ?eÔøΩÔøΩÔøΩ\n!ÔøΩÔøΩ◊üÔøΩVÔøΩÔøΩ\u0017IÔøΩ[ÔøΩÔøΩÔøΩ;jÔøΩ\u00008ÔøΩ\\F{>ÔøΩË¢∫XÔøΩÔøΩÔøΩ@◊≠ÔøΩ\u0006d.ÔøΩWwÔøΩÔøΩ?ÔøΩ=ÔøΩÔøΩ∆®DÔøΩ8ÔøΩ=ÔøΩ;UXÔøΩ]&(Õà\u0005ÔøΩÔøΩÔøΩ\"ÔøΩÔøΩ»≥ﬁ≥^ÔøΩY|ÔøΩÔøΩcÔøΩXFÔøΩÔøΩÔøΩ\u0017ÔøΩ9ÔøΩp}ÔøΩf3*ÔøΩ\bÔøΩÔøΩ>ÔøΩ7\u0002ÔøΩ3ÔøΩ ÔøΩ!#5ÔøΩÔøΩE\u0010ÔøΩÔøΩ\u0007ÔøΩÔøΩ\u0012\u0005ÔøΩ(\nÔøΩÔøΩ[E\"ÔøΩÔøΩ7ÔøΩÔøΩ,ÔøΩ\u0014ÔøΩ\u0004ÔøΩ\u0017zÔøΩc\u0002sLÔøΩÔøΩ#ÔøΩ\u0015ÔøΩh$ÔøΩ\u0019ÔøΩÔøΩ\n\"B\\8ÔøΩÔøΩXÔøΩ$ÔøΩÔøΩPH\u0010`ÔøΩÔøΩÔøΩÔøΩ\u0011ÔøΩJj\nJÔøΩ\tÔøΩÔøΩÔøΩcÔøΩ!ÔøΩ]\u0017ÔøΩÔøΩCÔøΩÔøΩ√îNÔøΩK-WzÔøΩ!ÔøΩ-zA^ÔøΩB\u0001ÔøΩ\u001fÔøΩÔøΩ<2ÔøΩÀ±ÔøΩÔøΩ6ÔøΩÔøΩIÔøΩÔøΩ*(20XhÔøΩ?)ÔøΩ\\ÔøΩfK&ÔøΩÔøΩdÔøΩ5ÔøΩ`ÔøΩÔøΩÔøΩE\\—™HÔøΩ5ÔøΩÔøΩ17lÔøΩÔøΩT\u0006«ùÔøΩ\nr{ÔøΩÔøΩ%ÔøΩÔøΩHÔøΩ(@\u0015ÔøΩiÔøΩÔøΩ9ÔøΩÔøΩ5{ÔøΩ\u0000ÔøΩlV\nÔøΩ{ieÔøΩ=ÔøΩDÔøΩXÔøΩ\u000fÔøΩÔøΩÔøΩ\u0012D\u0012IÔøΩÔøΩY\u0018ÔøΩ^\t ÔøΩ;urÔøΩ%]ÔøΩÔøΩ›í'ÔøΩP<ÔøΩ8caÔøΩÔøΩÔøΩÔøΩÔøΩ\bY]ÔøΩB\u0003G_ÔøΩÔøΩ:\u000fn‹πÔøΩ=PÔøΩ_h\u000f0\u0011ÔøΩÔøΩ<0ÔøΩ\\›çkvÔøΩCÔøΩÔøΩÔøΩbÔøΩÔøΩ\u00138V\u0007ÔøΩÔøΩÔøΩZYÔøΩ\u001by+ÔøΩÔøΩÔøΩÔøΩgS/ÔøΩT\u0001ÔøΩÔøΩÔøΩ\nfÔøΩÔøΩ~ÔøΩÔøΩÔøΩÔøΩYÔøΩÔøΩÔøΩ|G’ëÕ¢l\u001auÔøΩÔøΩÔøΩ\nÔøΩ+ÔøΩÔøΩËúÖ\u000e3&ÔøΩ\u0019ÔøΩdÔøΩ\n}@ÔøΩÔøΩ$ÔøΩÔøΩÔøΩÔøΩaÔøΩ&e_ÔøΩ5ÔøΩÔøΩ$„¨ª\u001a\u001a\u0002ÔøΩAÔøΩ)ÔøΩÔøΩsU\u001a\u001fÔøΩO\u0001ÔøΩ,ÔøΩ<ÔøΩÔøΩÔøΩ4NÔøΩÔøΩ/ÔøΩzNÔøΩ\\lÔøΩÀΩÔøΩnÔøΩœ¶ÔøΩÿºÔøΩ<ÔøΩ\u001bÔøΩ\u0007\\ÔøΩÔøΩ.+ÔøΩQÔøΩÔøΩ\u0010ÔøΩcÔøΩÔøΩÔøΩ1√±q90X\u0018\n\u001bÔøΩÔøΩÔøΩÔøΩÔøΩTÔøΩÔøΩÔøΩÔøΩ\nÔøΩtÔøΩÔøΩ\u001f;iÔøΩÔøΩ\u0010ÔøΩÔøΩÔøΩTÔøΩÔøΩ.\nÔøΩÔøΩkfcÔøΩMÔøΩLÔøΩ:ÔøΩ{\u001a√µÔøΩÔøΩÔøΩLjioQpÔøΩ:ÔøΩGBÔøΩHÔøΩ+?ÔøΩZ4ÔøΩqz\bq’ä\u0010R%ÔøΩ/bÔøΩ\u000fÔøΩgÔøΩ0ÔøΩ{ÔøΩn6\u001fÔøΩnÔøΩÔøΩi\u0014ÔøΩ>…áÔøΩaÔøΩ1ÔøΩÔøΩÔøΩZ<ÔøΩiV\bÔøΩ)e\biÔøΩÔøΩ/\"ÔøΩÔøΩ&ÔøΩ\u0005ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0006HÔøΩc]ÔøΩÔøΩ\">3ÔøΩ\u0006jsfÔøΩUÔøΩ/ÔøΩÔøΩÔøΩÔøΩ4ÔøΩÔøΩÔøΩT&ÔøΩÔøΩtÔøΩDP\u0010\u001b$AÔøΩXGhbÔøΩÔøΩÔøΩPÔøΩÔøΩlÔøΩ\u0012E9ÔøΩ@ÔøΩ|\u0019ÔøΩ€âÔøΩÔøΩÔøΩ\u0011+ÔøΩ\u0010mÔøΩÔøΩRÔøΩXTÔøΩkÔøΩ` #tÔøΩn{ÔøΩÔøΩO\u0015/ÔøΩ\u001b>ÔøΩÔøΩÔøΩ.ÔøΩÔøΩ¬§ÔøΩÔøΩÔøΩÔøΩcÔøΩMEÔøΩgÔøΩ\bÔøΩÔøΩ\u001bÔøΩxbÔøΩÔøΩÔøΩÔøΩV9ÔøΩIÔøΩ€´ÔøΩ.ÔøΩÔøΩÔøΩ\u0005ÔøΩÔøΩÔøΩiÔøΩ+ÔøΩÔøΩÔøΩÔøΩ\u0011ÔøΩ%\\wÔøΩtÔøΩÔøΩyH3~€™\\0ÔøΩ0ÔøΩ=\tÔøΩeÔøΩ_ÔøΩ=ÔøΩÔøΩO\u0014ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ-ÔøΩ\u0000ÔøΩ\n_ÔøΩ9ÔøΩÔøΩÔøΩ<_<ÔøΩÔøΩhÔøΩo/ÔøΩÔøΩ0[ÔøΩ‘êmÔøΩÔøΩÔøΩ_ÔøΩÔøΩÔøΩÔøΩÔøΩ=ÔøΩÔøΩDÔøΩ\u001fÔøΩYÔøΩÔøΩÔøΩ\u000eÔøΩÔøΩÔøΩ+ÔøΩ\u0016ÔøΩÔøΩÔøΩHÔøΩ\njÔøΩÔøΩÔøΩ8O\u0018+!\nb0&Zf\u000f5ÔøΩ{HÔøΩÔøΩ{]0ÔøΩÔøΩFÔøΩÔøΩd\u0016ÔøΩÔøΩM'a\u0004\u0003&\u0003⁄í|ÔøΩc3~ÔøΩ]ÔøΩÔøΩ:v\u001bmÔøΩ\u001b≈ïÔøΩ.h’åzÔøΩÔøΩÔøΩÔøΩÔøΩF“®ÔøΩÔøΩ‹£~ÔøΩ<ÔøΩfÔøΩ+ÔøΩ—ΩÔøΩÔøΩÔøΩu:ÔøΩÔøΩÔøΩÔøΩÔøΩs3ty3ÔøΩWRKÔøΩÔøΩŒ≥ÔøΩÔøΩ)ÔøΩ\n`ÔøΩ)\u0019P*oÔøΩÔøΩÔøΩ\u001fÔøΩ,\u0003\u0010\nÔøΩQD\n«ªVÔøΩtFÔøΩÔøΩgÔøΩ\n/>ÔøΩgÔøΩk“ùÔøΩ\u0017›æÔøΩÔøΩFÔøΩxqdEw}ÔøΩ\\ÔøΩ\u000f‹â%LWÔøΩ\u0016!ÔøΩ[ÔøΩ*ÔøΩEÔøΩIÔøΩÔøΩsÔøΩ;\n\u0013ÔøΩÔøΩÔøΩ\u0010ÔøΩVÔøΩ≈àÔøΩÔøΩwÔøΩÔøΩŸâÔøΩKÔøΩ\u0003kÔøΩ*ÔøΩ\bÔøΩ;`ÔøΩ8ÔøΩÔøΩÔøΩ\u0019ÔøΩÔøΩ^lVÔøΩ\n<ÔøΩ9ÔøΩ\b\u0013ÔøΩg`ÔøΩ\u000eh¬òÔøΩA,\u000f\u0014ÔøΩÔøΩÊ∑ç^ÔøΩÔøΩ\u0017{ÔøΩ~#ÔøΩ/aÔøΩ]ÔøΩÔøΩx\u0001ÔøΩ!ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0000]ÔøΩ!\u00133 ÔøΩÔøΩÔøΩ3\u0016\u0015]\u001bÔøΩÔøΩQ;\u0015ÔøΩZÔøΩ$ÔøΩ\nÔøΩ\\\bÔøΩ*\u0003ÔøΩÔøΩÔøΩOÔøΩÔøΩ’π4\"'ÔøΩÔøΩÔøΩÀ´uÔøΩÔøΩÔøΩ\u001b9'Mg\u0016e\u0014LÔøΩ?hÔøΩXÔøΩV\bÔøΩÍëπÔøΩ]ÔøΩ*ÔøΩN\u0000B\nÔøΩÔøΩÔøΩ=ÔøΩ'ÔøΩÔøΩÔøΩÔøΩ/|ÔøΩHÔøΩÔøΩw\\ÔøΩLÔøΩÔøΩ\u0015ÔøΩÔøΩÔøΩÔøΩ%ÔøΩ\u0013BÿéÔøΩÔøΩÔøΩo}PdÔøΩÔøΩÔøΩi7ÔøΩrÔøΩÔøΩﬂúÔøΩBÔøΩÔøΩ]ÔøΩC?ÔøΩQ\u0012OÔøΩ\\j\u000f\u0017ÔøΩ\"ÔøΩCÔøΩÔøΩÿÇÔøΩÔøΩ3ÔøΩVÔøΩÔøΩzÔøΩo\u0004kÔøΩÔøΩÔøΩ\u0018$√úÔøΩ\nfE#EÔøΩ\u0014ÔøΩÊêîL√¶ÔøΩ\u00016ÔøΩÔøΩr\u0012ÔøΩEÔøΩ)_KÔøΩÔøΩB@ \u0002ÔøΩiÔøΩ\u000e\u0016ÔøΩÔøΩÔøΩ—ªÔøΩ?ÔøΩ;ÔøΩÔøΩSÔøΩÔøΩYÔøΩÔøΩÔøΩ\u0005ÔøΩCÔøΩ¬±ÔøΩ@\u001b\u0011h#\u0002mdÔøΩÔøΩÔøΩÔøΩM\nÔøΩÔøΩÔøΩÔøΩ5ÔøΩQÔøΩ?ÔøΩ,ÔøΩ\u001b‘óS8ÔøΩÔøΩÔøΩÔøΩ>GÂ¨äÔøΩ\u0001ÔøΩ-≈â`(\u0010\"ÔøΩÔøΩÔøΩ\u001b\b\u0017\u0016#\"ÔøΩ:ÔøΩNÔøΩSÔøΩ\u0014ÔøΩÔøΩnÔøΩ\u0013ÔøΩÔøΩt;=NÔøΩ#ÔøΩÔøΩ\nÔøΩpÔøΩd<I8ÔøΩIXÔøΩÔøΩJÔøΩx-ÔøΩÔøΩÔøΩÔøΩ\u0014ÔøΩ-ÔøΩm\u0015gÔøΩsŒëmÔøΩ!K~$ÔøΩ#F'ÔøΩÔøΩ8ÔøΩÔøΩr\u0012;ÔøΩ8ÔøΩ+\u0010\n9NlBL)ÔøΩ,\nÔøΩ\"ÔøΩÔøΩÔøΩHvMÔøΩ\u0004ÔøΩ.ÔøΩ\u0017ÔøΩÔøΩm7i\niÔøΩ¬∂ÔøΩÔøΩ\u0005\nÔøΩÔøΩ]zÔøΩÔøΩÔøΩÔøΩ&ÔøΩÔøΩÔøΩBÔøΩ,ÔøΩ\u0016vÔøΩp\na)ÔøΩ_ÔøΩ8ÔøΩ?ÔøΩ\nÔøΩŒ£ÔøΩÔøΩÔøΩÔøΩÔøΩ:ÔøΩÔøΩÔøΩÔøΩ|ÔøΩÔøΩÔøΩÔøΩ9zÔøΩÔøΩ\nTÔøΩÔøΩ+ÔøΩÔøΩÔøΩÔøΩ5√¨d9 ÔøΩ\u0004ÔøΩLN<\u001fÔøΩÔøΩÔøΩÔøΩ\u0001ÔøΩTÔøΩÔøΩ/ÔøΩ\u001f!ÔøΩeÔøΩ\u0013sÔøΩ#ÔøΩÔøΩBx~ÔøΩ+/ £\u001b+ÔøΩ/ÔøΩ\nÔøΩosÔøΩ*w{À©ÔøΩÔøΩOÔøΩÔøΩÔøΩÔøΩÔøΩ%ÔøΩÔøΩÔøΩÔøΩ]ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ)ueÔøΩÔøΩÔøΩÔøΩÔøΩ»ßw\u0006?ÔøΩ}ÔøΩÔøΩ\u0003ÔøΩÔøΩƒë\u0007ÔøΩ~ÔøΩÔøΩ\u0017ÔøΩÔøΩÔøΩiVKÔøΩf>8;fÔøΩÔøΩ&ÔøΩ-ÔøΩ#ÔøΩ\u0014ÔøΩ;DÔøΩÔøΩÔøΩ[ÔøΩÔøΩrgÔøΩ]ÔøΩ9ÔøΩÔøΩKÔøΩÔøΩoÔøΩÔøΩÕ∂ vÔøΩ\u0010ÔøΩ\nÔøΩÔøΩ\u001b’åŸÇ\n<fÔøΩÔøΩ<ÔøΩÔøΩlMEÔøΩÔøΩÔøΩBÔøΩ\u0003x\b/ÔøΩÔøΩ\"ÔøΩj[ÔøΩ\u0002ÔøΩÔøΩÔøΩÔøΩ[ÔøΩcÔøΩtÔøΩÔøΩkÔøΩ\b]iÔøΩÔøΩÔøΩ|ÔøΩÔøΩÔøΩÔøΩÔøΩNKÔøΩÔøΩZÔøΩJ\nÔøΩyÔøΩÔøΩSÔøΩ3_ÔøΩ}ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩs9ÔøΩ∆éxKÔøΩÿ§\u0016IRÔøΩEÔøΩÔøΩÔøΩ…∞ÔøΩÔøΩbÔøΩÔøΩrsÔøΩÔøΩ\u000eq'ÔøΩÀ§,kÔøΩMÔøΩÔøΩÔøΩÔøΩ\u001fpÔøΩ ë\nÔøΩ<ÔøΩÔøΩaÕ≥ÔøΩ\u0017ÔøΩÔøΩ+ÔøΩ ÔøΩ%ÔøΩÔøΩ4ZÔøΩXdÔøΩÔøΩiÔøΩ-ev∆ΩÔøΩÔøΩÔøΩ\nÔøΩ{ÔøΩÔøΩ;ÔøΩ{[ÔøΩ5ÔøΩ\u0005ÔøΩ{EAÔøΩN\n\u0007V\nrÔøΩÔøΩn\u0017ÔøΩV$ÔøΩ–óaW\u0015ÔøΩj]UÔøΩ\"ÔøΩÔøΩM7ÔøΩ<ÔøΩÔøΩÔøΩ2ÔøΩÔøΩiÔøΩ\u0012ÔøΩ√óOŒüÔøΩÔøΩNÔøΩ+ÔøΩÔøΩ;!Q\u0000\u0012ÔøΩ`w<ÔøΩÔøΩÔøΩIÔøΩUÔøΩÔøΩcÔøΩ{ÔøΩÔøΩÔøΩH\n6ÔøΩ…í~ÔøΩÔøΩÔøΩrÔøΩ[8ÔøΩÔøΩÔøΩ-CÔøΩrYÔøΩÔøΩ€≠,√öÔøΩg\nÔøΩÔøΩ\u0014$ÔøΩÔøΩÔøΩ~{ÔøΩÔøΩÔøΩ]qÔøΩ%{iIÔøΩ\u000fe∆¨ÔøΩY\trÔøΩi:ÔøΩÔøΩÔøΩ?\u0003hÔøΩ2\bÔøΩÔøΩ\u001bÔøΩ\u0006ÔøΩG\u0017\u0016ÔøΩÔøΩœÜÔøΩGÔøΩ◊¨ÔøΩÔøΩÔøΩ|ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩkÔøΩ.+lV[ÔøΩ\u0015JÔøΩÔøΩ\u0018}ÔøΩÔøΩ[zÔøΩVÔøΩÔøΩÔøΩzH~ÔøΩÔøΩUjÔøΩ+ÔøΩ}“´ÔøΩ\u0017ÔøΩ2\u0002ÔøΩÔøΩ\u001awa9ÔøΩkÔøΩ<KQA^qQOÔøΩhÔøΩTMÔøΩjÔøΩuÔøΩnœ§ÔøΩ\u000eÔøΩÔøΩJÔøΩ⁄æRÔøΩÔøΩÔøΩÔøΩÔøΩ9ÔøΩcÔøΩynÔøΩÔøΩÔøΩÔøΩS\u0006ÔøΩÔøΩW\u0007ÔøΩÔøΩÔøΩ;pgÔøΩÔøΩgOIu5À≥ÔøΩÔøΩnÔøΩdÔøΩÔøΩWÔøΩÔøΩ∆ôÔøΩ\nÔøΩÔøΩÔøΩÀªÔøΩÔøΩ)\u0006ÔøΩTÔøΩTzlÔøΩv[ÔøΩÔøΩd:EÔøΩiÔøΩMÔøΩÔøΩÔøΩLÔøΩÔøΩs\nÔøΩV,kj~—´ÔøΩ+ÔøΩ\u0014E)['ÔøΩÃãÔøΩ&ÔøΩ7€∂ÔøΩÔøΩ1ÔøΩÔøΩ…ñÔøΩÔøΩÔøΩK[ÔøΩÔøΩyÔøΩyÔøΩÔøΩ6ﬂâ\u000eÔøΩ|$>\u0015ÔøΩzÔøΩÔøΩ\u0017z\n&ÔøΩ\u0016~ÔøΩÔøΩ\"u&ÔøΩOÔøΩ/ÔøΩ‹ôÔøΩÔøΩ—èÔøΩﬂõ;ÔøΩÔøΩÔøΩÔøΩÔøΩGÔøΩÔøΩ≈≠\nÔøΩ7,ÔøΩÔøΩ+ÔøΩ\u0015AÔøΩÔøΩÔøΩÔøΩ~ÔøΩOœΩxÔøΩÔøΩ_ÔøΩÿØÔøΩÔøΩ€ü\nxvuuÔøΩÔøΩÔøΩ\u001bÔøΩÔøΩN[ÔøΩÔøΩ4‘∏-(œá\u001fM~ÔøΩÔøΩ\u001fÔøΩ3ÔøΩÔøΩÔøΩO~u‰ÆìÔøΩ\u001fÔøΩwÔøΩÔøΩsÔøΩ’∑ÔøΩÔøΩÔøΩ_ÔøΩÔøΩkÔøΩ?YÔøΩÔøΩÔøΩœ∑ÔøΩpÔøΩmÔøΩÔøΩÔøΩ\u0016\nÔøΩ\u0003ÔøΩÔøΩÔøΩÔøΩÔøΩRÔøΩÔøΩ6ÔøΩgJÔøΩ9ÔøΩ9ÔøΩÔøΩVÔøΩ\u0014ÔøΩ,+_ÔøΩRiÔøΩ‹ñÔøΩxÔøΩÔøΩÔøΩ!ÔøΩÔøΩÁ¨¥T8PWXÔøΩ2ÔøΩÔøΩÔøΩÔøΩÔøΩD\\U\"GÔøΩ_-ÔøΩ\n~PÔøΩ,ÔøΩeN:ÔøΩXÔøΩÔøΩ<ÔøΩÔøΩWÔøΩJÔøΩR6'\nÔøΩÔøΩÿãÔøΩEr—ãy6yNÔøΩ…¨ÔøΩ3+;.gÔøΩJV!ÔøΩÔøΩ-ÔøΩZÔøΩÔøΩ«¨ÔøΩÔøΩÔøΩÔøΩÔøΩ{*ÔøΩ◊∏+KÔøΩD‘ΩÔøΩ\\ÔøΩ\nÔøΩpÔøΩ\n\u000eÔøΩP=\u000eÔøΩW–ñÔøΩ-VÔøΩÔøΩ\\\u0015ÔøΩÔøΩÔøΩÔøΩÔøΩMÔøΩÔ†îÔøΩÔøΩ≈á\n,0;0,ÔøΩVi1ÔøΩNjÔøΩÔøΩ[C|I\u0015s^ÔøΩ_lÔøΩ^V…âVrÔøΩÔøΩÔøΩh%'ZÔøΩÔøΩZÔøΩÔøΩÔøΩeÔøΩ{h\n}XÔøΩÔøΩ1ÔøΩwQqg*ÔøΩRVeYÔøΩÔøΩKÔøΩÔøΩk\u0018ÔøΩÔøΩ;`ÔøΩÔøΩUÔøΩYRÔøΩÔøΩIÔøΩÔøΩZÔøΩÔøΩ'ÔøΩ|~~ÔøΩË∑áÔøΩ;ÔøΩkÔøΩÔøΩÔøΩÔøΩkvÔøΩÔøΩÔøΩÔøΩÔøΩ.ﬂêÔøΩ}~ÔøΩNÔøΩÔøΩw<{ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ‹ùjÔøΩÀëÔøΩÔøΩÔøΩÔøΩ\u0017ÔøΩOÔøΩÔøΩ\"ÔøΩÔøΩ7ÔøΩÔøΩÔøΩÔøΩ\u000f{ÔøΩWÔøΩc-ÔøΩ\u0007ÔøΩÔøΩoÔøΩÔøΩ~+o\u001fÔøΩlÔøΩÔøΩÔøΩÔøΩ\u001f|ÔøΩÔøΩc]\u000f\\ÔøΩÔøΩDÔøΩÔøΩ\u0016ÔøΩÔøΩVÔøΩÔøΩÔøΩk\nÔøΩÔøΩÔøΩÔøΩw9z.zÔøΩÔøΩÔøΩÃçÔøΩ åÔøΩc\u000eÔøΩÔøΩÔøΩÔøΩÀ¨\u0001ÔøΩŒåÔøΩÔøΩjÔøΩÔøΩnÔøΩÔøΩÔøΩ‹ÄÔøΩÔøΩ\u000fÔøΩGÔøΩﬂÑÔøΩ5(;,ÔøΩ]ÔøΩÔøΩ\u001aÔøΩslÔøΩRÔøΩvÔøΩÔøΩjÔøΩ\"\\ÔøΩ\u0002ÔøΩFÔøΩÔøΩ\\ÔøΩ_\n‹≠ÔøΩÔøΩ√µ\u001bÔøΩ\u0016ÔøΩÔøΩ2\u001fbÔøΩAw\u0000ÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩV'ÔøΩnHU\bÔøΩ[0VÔøΩ\u001fÔøΩÔøΩ~\u000fÔøΩÔøΩ.ÔøΩsl-ÔøΩjÔøΩ\u000e`›çLFÔøΩ.ÔøΩ{ÔøΩÔøΩÔøΩK—∂ÔøΩ‹âr\u0015JÔøΩ)[\u0004J+\u0011Ey\nwÔøΩ}ÔøΩ\u0005ﬁá‹òÔøΩXN&cÔøΩÔøΩÔøΩX^ÔøΩ1ÔøΩ[ÔøΩÔøΩ}\u001bÀΩÔøΩXÔøΩ\u0018[ÔøΩ\u000fÔøΩÔøΩÔøΩ‹≤ÔøΩ\u0018[y5cÔøΩÔøΩÔøΩÔøΩÔøΩ9cÔøΩ\nÔøΩ\u0001\u0007\nÔøΩVÔøΩÔøΩXÔøΩ\u0019ÔøΩ÷ºÔøΩÔøΩÔøΩ\u0017\u0019ÔøΩÔøΩÔøΩXÔøΩ8cÔøΩ73ÔøΩ\u000e47\n2ÔøΩÔøΩ$cÔøΩ\u0000_ÔøΩU∆∂ÔøΩÔøΩÔøΩ\nÔøΩ:ÔøΩ\u0018ÔøΩF,uÔøΩnÔøΩQ∆∂~\n,ÔøΩÔøΩ^ÔøΩÔøΩ~ÔøΩÔøΩÔøΩ\nPÔøΩ\u0004c;ÔøΩÔøΩÔøΩÔøΩX?ÔøΩÔøΩ\tÔøΩÔøΩ\u0006ÔøΩÔøΩ\u001bÔøΩÔøΩzÔøΩ\u0016ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ_ÔøΩmd\nÔøΩÔøΩ\nE\u0016ÔøΩ&ÔøΩkÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩVÔøΩ\u000f.ÔøΩ+\u000eÔøΩÔøΩ\nÔøΩ&ÔøΩÔøΩÔøΩÔøΩs\u0013ÔøΩÔøΩZÔøΩ?L8\u0003ÔøΩGÔøΩ\tgÔøΩ:ÔøΩ”ÑÔøΩÔøΩ\ni:VÔøΩÔøΩ=\u0013ÔøΩfÔøΩ]iÔøΩ9\u0019ÔøΩÔøΩ\u001f4ÔøΩ\\\u0016ÔøΩOÔøΩÁ±ê}‹ÑmÔøΩ?ÔøΩÔøΩ&ÔøΩœÆÔøΩÔøΩ<ÔøΩ\u0007\nÔøΩÔøΩÔøΩÔøΩ\n3&,3KÔøΩ\u0006\u0013ÔøΩ]KAÔøΩ\t[XqAÔøΩ\tgÔøΩÔøΩÔøΩÔøΩM8ÔøΩ-/\u00180ÔøΩ,\u0016I”±ÔøΩÔøΩÔøΩGL8ÔøΩu\u0015?kÔøΩ9ÔøΩÔøΩÔøΩ2M8ÔøΩm\\ÔøΩÔøΩÔøΩc-+^3aÔøΩr]AÿÑÔøΩYCI\u0005*ÔøΩÔøΩk=ÔøΩdÔøΩ\t[ÔøΩÔøΩK0ÔøΩNNÔøΩ\u0011\u0013ÔøΩ0WÔøΩ\u0014ÔøΩÔøΩÔøΩÔøΩ,y÷Ñ-ÔøΩÔøΩÔøΩ\u0001ÔøΩÔøΩÔøΩ]J^5aÿ¢ÔøΩ9ÔøΩÔøΩÔøΩÔøΩ+-0a\nÔøΩ+ÔøΩOÔøΩÔøΩMÔøΩ\nXÔøΩWÔøΩ¬æ\u0002\u0016ÔøΩ\u0015ÔøΩÔøΩÔøΩÔøΩÔøΩ}\u0005,ÔøΩ+`a_\u0001\nÔøΩ\nXÔøΩWÔøΩ¬æ\u0002\u0016ÔøΩ\u0015ÔøΩÔøΩÔøΩÔøΩÔøΩ}\u0005,ÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩ&\n]ÔøΩ\nÊ¢øÔøΩtÔøΩ\t[ÔøΩÔøΩTÔøΩ0ÔøΩÔøΩRÔøΩ5\u0013\u0006ÔøΩÔøΩÔøΩ\u0010ÔøΩÔøΩ=ÔøΩÔøΩÔøΩ\t[XSÔøΩ#\u0004;ÔøΩÔøΩÔøΩLÔøΩÔøΩ\u0011ÔøΩE\\ÔøΩÔøΩL\u0018:/=Mp1ÔøΩlÔøΩ\tÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩlÔøΩÔøΩ-l]Y\nÔøΩÔøΩ\tÔøΩN\u0013ÔøΩÔøΩ#\u0004ÔøΩ\u0012ÔøΩÔøΩ&ÔøΩÔøΩÔøΩÔøΩ`ÔøΩÔøΩkNŸ´&\n(\u00136ÔøΩ \u001fx’ÑÔøΩ\u000fÔøΩ~ÔøΩ\u0006{NyÔøΩ\tsÔøΩÔøΩ\u0012\\ÔøΩ}ÔøΩ|ÔøΩ\tÔøΩ\u0007 ÖÔøΩ\\\\?ÔøΩa\u0013ÔøΩ~ ∑\u0011\\OtÔøΩ¬ÑAÔøΩÔøΩ\u0000ÔøΩÔøΩÔøΩÔøΩo]ÔøΩÔøΩ\"ÔøΩÔøΩÔøΩÔøΩÔøΩ[ÔøΩÔøΩÔøΩ\b?oÔøΩ]ÔøΩRv\u0019dSlÔøΩÔøΩÔøΩƒüÔøΩÔøΩ1ÔøΩ}\u0017eÔøΩÔøΩ\u0012ÔøΩc1zÔøΩSÔøΩÔøΩRY\u0017ÔøΩÔøΩyÔøΩGÔøΩ0TÔøΩD0ÔøΩ\u0001P7ÔøΩÔøΩÔøΩ\u001f)5ÔøΩ9SÔøΩ\u0000F\",ÔøΩÔøΩÔøΩÔøΩ«üÔøΩ!ÔøΩkf\u001bÔøΩ’õÔøΩÔøΩz;0#ÔøΩv\u0017ÔøΩÔøΩgW%hÔøΩ.ÔøΩÔøΩQÔøΩÔøΩ_4\u0016ÔøΩ\u001aa6F}*ÔøΩC;I81ÔøΩÔøΩAÔøΩÔøΩÔøΩ?ÔøΩ?\u0003ÔøΩÔøΩÔøΩÔøΩ~ÔøΩ≈†ÔøΩÔøΩÔøΩd*z5ÔøΩ\u0014ÔøΩÔøΩgit\u0015DoÔøΩp\u0012&ÔøΩk ÔøΩÔøΩÔøΩl>_e5ÔøΩÔøΩ\u000fÔøΩ\u000eÔøΩ\n\"\t9ÔøΩ|ÔøΩ\u0006ÔøΩ:ÔøΩ>jRkÔøΩÔøΩÔøΩMKHÔøΩÔøΩNÔøΩq\u0010ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\n5ÔøΩÔøΩ\u000fÔøΩ\u0001ÔøΩÔøΩ0ŒµÔøΩ`k\u0017Q\u0016t\u0017ÔøΩÔøΩCÔøΩÔøΩ4e\u001f0ÔøΩÔøΩÔøΩ lÔøΩ\u0015ÔøΩ\n6ÔøΩÔøΩ\u000eÔøΩrÔøΩÔøΩÔøΩÔøΩNÔøΩ=ÔøΩ\u0019@Õ≠x%ÔøΩÔøΩÔøΩÔøΩÔøΩz\u0007q/ÔøΩCeÔøΩÔøΩGÔøΩÔøΩÔøΩ.ÔøΩÔøΩTÔøΩÔøΩÔøΩ8ÔøΩ;NRq‹îÔøΩ.ÔøΩÔøΩÔøΩ\u0018lÔøΩÔøΩ6ÔøΩÔøΩSÔøΩÔøΩ\n6}(IÔøΩVÔøΩ0ÔøΩN\u0001?ÔøΩ^3ÔøΩÔøΩÔøΩE6MÔøΩ\\~ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0018ÔøΩ\nNxÔøΩDHÔøΩ\u001ayÔøΩF}#DE#ÔøΩF1ÔøΩÔøΩ|ÔøΩ\\m\u0014ÔøΩ\u0013ÔøΩ\u0017\u0003\u001f<⁄∏ÔøΩÔøΩ5\u0013\u001fÔøΩÔøΩÔøΩÔøΩ&…ó5ÔøΩÔøΩ\u0019ÔøΩ1ÔøΩ+ÔøΩ\u001f@_ÔøΩÔøΩ\nÔøΩÔøΩÔøΩ.ÔøΩÔøΩÔøΩ)\u0017◊òÔøΩÔøΩ §IÔøΩRÔøΩ\u0005MÔøΩÔøΩÔøΩ\nÔøΩÔøΩ\u0016\\\u000fÔøΩÔøΩÔøΩÔøΩÔøΩ/eÔøΩ5$ÔøΩÔøΩÔøΩÔøΩ~ÔøΩp\tÔøΩÔøΩxeÔøΩÔøΩu=ÔøΩÔøΩÔøΩ\u0013?VÔøΩÔøΩÔøΩ÷âZ‚èÆÔøΩ@1\u0019%|ÔøΩ(M]d\naÔøΩÔøΩ#ÔøΩkGÔøΩÔøΩ\u0013ÔøΩ\u0000zC$ÔøΩGÔøΩÔøΩjÔøΩbÔøΩÔøΩH\u0014W\nÔøΩÔøΩ\u0014$MÔøΩHÔøΩSÔøΩÔøΩ/ÔøΩwuiÔøΩÔøΩ\"ÔøΩ\u0015ÔøΩ%ÔøΩÔøΩTÔøΩÔøΩ:ÔøΩÔøΩÔøΩÔøΩdÔøΩÔøΩ$ÔøΩ?YsqÔøΩ\nÔøΩ~ÔøΩÔøΩ\u0019\u0003ÔøΩÔøΩIÔøΩ8ÔøΩ?ÔøΩÔøΩGÔøΩÿªyÔøΩÔøΩÔøΩ ÔøΩ/ÔøΩjÔøΩÙèîó^ÔøΩC\u001f&—Çl#ÔøΩ/ÔøΩ\nÔøΩ0ÔøΩ\u0000ÔøΩ\u001aÔøΩNI\u0013ÔøΩ6@VÔøΩ^`ÔøΩÔøΩ\u0005ÔøΩ^ÔøΩÔøΩÔøΩÔøΩPÔøΩ4ÔøΩyÔøΩ\u0000ÔøΩ»∫ÔøΩ<ÔøΩQÔøΩÔøΩÔøΩ'bÔøΩÔøΩÔøΩiÔøΩÔøΩ\u0018K—ªÿéB[BÔøΩ\u0004ÔøΩÔøΩ%ÔøΩ8e1ÔøΩ\u0005ÔøΩ\u000eÔøΩIÔøΩ.hÔøΩÔøΩ\u0015\u0002ÔøΩ.3l^-ÔøΩHÔøΩÔøΩ=hSÔøΩ\u0002ÔøΩÔøΩ:ÔøΩ[OÔøΩN\u001b[ÔøΩZÔøΩ”®ÔøΩÔøΩqUÔøΩÔøΩ~\nJ\u0013ÔøΩ'ÔøΩkXÔøΩÔøΩ…üoŸåÔøΩu&ÔøΩÔøΩZPÔøΩÔøΩ\nÔøΩÔøΩÔøΩHÔøΩSÔøΩÔøΩJÔøΩ3ÔøΩÔøΩÔøΩFÔøΩÔøΩkÔøΩ\n\u0017F|ÔøΩ2ÔøΩ\u001fÔøΩ'8ÔøΩMÔøΩ9ÔøΩÔøΩ\nÔøΩ;ÔøΩÔøΩlÔøΩy\u001a 9Eﬁò2ÔøΩ\u001f'?ÔøΩTyÔøΩÔøΩ&ÔøΩÔøΩiÔøΩÔøΩdÔøΩ`ÔøΩ\u0003ÔøΩÔøΩÔøΩ\u0005ÔøΩÔøΩBfÔøΩ/:ÔøΩ,dÔøΩÔøΩs}ÔøΩrÔøΩNTÔøΩnÔøΩGÔøΩ\u0017ÔøΩ6RÔøΩS◊©3ÔøΩÔøΩ”ÇÔøΩÔøΩÔøΩÔøΩo–åÔøΩ(ÔøΩ2ÔøΩh~sgÔøΩ[ÔøΩGÔøΩÔøΩkÔøΩÔøΩ3LÔøΩORÔøΩUI.ÔøΩ\"FÔøΩV<ÔøΩÔøΩY.aFÔøΩÔøΩ^ÔøΩ%ÔøΩÔøΩÔøΩhÔøΩÔøΩÔøΩÔøΩaÔøΩÔøΩ\u0018qvÔøΩ^ÔøΩÔøΩSÔøΩÔøΩ\"ÔøΩ,2ÔøΩl*4ÔøΩÔøΩ\u0006H3<{ÔøΩ(6ÔøΩÔøΩ<4NÔøΩY8c\nÔøΩR\u001aÔøΩÔøΩÔøΩ\nNgÔøΩÔøΩyÔøΩÔøΩimÕåÔøΩ1ÔøΩÔøΩxÔøΩ\tÔøΩÔøΩ}aÔøΩÔøΩÔøΩÔøΩ\n8ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ~\"Œâ\u0017ÔøΩoÔøΩÔøΩÔøΩÔøΩÔøΩ&ÔøΩÔøΩt_ÔøΩÔøΩÔøΩ\bÔøΩKƒ©ÔøΩ>AÔøΩ\u0019%kMÔøΩÔøΩÔøΩÔøΩ€ÑÔøΩ&\u0016ÔøΩÔøΩRZ\u0015^4AÁΩâtLÔøΩ(S.>\nÔøΩÔøΩsÔøΩÔøΩKÔøΩÔøΩ\u001f-∆ÑtÔøΩÔøΩ9¬ØciÔøΩÔøΩ_ÔøΩÔøΩÔøΩIÔøΩHÔøΩÔøΩÔøΩÔøΩ\u0014$\u001fIeÔøΩ$ÔøΩ.ÔøΩÔøΩ'ZÔøΩtBHRÔøΩLÔøΩ\nÔøΩ!ÔøΩÔøΩÔøΩ\\ÔøΩÔøΩÔøΩÔøΩ7ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u001bÔøΩ$iÔøΩÔøΩÔøΩQÔøΩc*\u0013ÔøΩÕùÔøΩKÔøΩÔøΩÔøΩÔøΩÔøΩ\u000fÔøΩGÔøΩÔøΩ{ÔøΩHÔøΩ\n\\nÔøΩÔøΩ\bÔøΩ\u0017\u0016ÔøΩÔøΩ\bÔøΩaÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\bFbÔøΩEÔøΩÔøΩÔøΩ\u0018ÔøΩ\b]_\nÔøΩ ÔøΩ0ÔøΩR{ÔøΩB>\u0019ÔøΩÔøΩÔøΩM^ÔøΩÔøΩÔøΩhÔøΩÔøΩÔøΩ3ÔøΩ~^ÔøΩÔøΩ.ÔøΩR'ÔøΩÔøΩy94HÔøΩÔøΩ<œä)ÔøΩ~ÔøΩÔøΩ-ÔøΩ&N\u0006ÔøΩdÔøΩÔøΩÔøΩ0ÔøΩÔøΩ<A\u0018¬Æ\nÔøΩNÔøΩHÔøΩcÔøΩﬁà:AÔøΩÔøΩÔøΩÔøΩHÔøΩÔøΩÔøΩD[DÔøΩ»èÔøΩÔøΩÔøΩPCzÔøΩÔäìdÔøΩTNÔøΩÔøΩX%5ÔøΩxÔøΩÔøΩÔøΩÔøΩ=85ÔøΩÔøΩÔøΩ\u0001MÔøΩÔøΩ:8ÔøΩÔøΩÔøΩ!ÔøΩ\ttÔøΩ]ÔøΩÔøΩx,ÔøΩOÔøΩcQu<\u0012hPÔøΩÔøΩ\tÔøΩ\u0012HÔøΩÔøΩÔøΩ:\u0010ÔøΩ$yÔøΩÔøΩnÔøΩb^ÔøΩ∆çMÔøΩÔøΩ<\njG$ÔøΩÔøΩ\nÔøΩÔøΩ&tuÔøΩÔøΩkÔøΩ\t-8\u0018\nÔøΩtÔøΩOÔøΩTwÔøΩÔøΩÔøΩÔøΩ\u0007ÔøΩÔøΩÔøΩ?ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩXÔøΩC\u0017SÔøΩÔøΩHXOhq-ÔøΩÔøΩÔøΩj\u0002ÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩZÔøΩ\u000eÔøΩÔøΩ\nÔøΩPÔøΩÔøΩ\u0006U-ÔøΩkÔøΩÔøΩ@kHQÔøΩtÔøΩÔøΩjg,\u0012‹¥[ÔøΩÎú¥ÔøΩÔøΩÕ£ÔøΩÔøΩ¬ÅxLÔøΩÔøΩ\u0012k\t\u0019ÔøΩÔøΩÔøΩ?Pœë}ÔøΩ};\u0006ÔøΩmÔøΩÔøΩÔøΩ1ÔøΩmGÔøΩÔøΩcÔøΩÔøΩ}[WOÔøΩ@ÔøΩÔøΩqÂÆû\n_Oﬂ†-«ñ38\u001aÔøΩÔøΩDJÔøΩ\n\u0006ÔøΩÔøΩÔøΩÿ∏\u0016OLqÔøΩ“≤AqÔøΩÔøΩÔøΩ|tÔøΩx\u000eCCI]SÔøΩÔøΩ‘©XÔøΩÔøΩ\nÔøΩ&HÔøΩd4ÔøΩ≈â\u000e$\u001fÔøΩ9\u0011ÔøΩ\u001a\t\u0007ÔøΩ(ÔøΩÔøΩ#qM\u001b”¢ÔøΩ\u0006u\bÔøΩFÔøΩ\u0013ÔøΩ\u001a\u001bNÔøΩÔøΩQÔøΩLÔøΩÔøΩ\n\u0017mÔøΩ\u001fÔøΩT-\nbq5\u0018ÔøΩkÔøΩDdJ\nÔøΩcc\n|≈∞VlD#ÔøΩI`.ÔøΩ\nBÔøΩÔøΩÔøΩp2\u0001ÔøΩ`3\u0016ÔøΩ\u0016\nÔøΩFO1ÔøΩÔøΩ\niUÔøΩ'\u0003ÔøΩÔøΩ\u0013ÔøΩHÔøΩ?\n\u0001€∫ÔøΩ%\u0016ÔøΩnPÔøΩÔøΩF4]'ÔøΩI\nÔøΩd\u001a:\u0011ÔøΩT}\\\nÔøΩCÔøΩÔøΩ≈íÔøΩÔøΩb4\u0011ÔøΩÔøΩÔøΩ\\0\u0018ÔøΩÔøΩ‚è®qÔøΩ:ÔøΩ\n'ÔøΩbÔøΩƒÖLEÔøΩca.\u0010\u0016!ÔøΩÔøΩX|ÔøΩÔøΩ\u0010.\u0017ÔøΩ.ÔøΩ36\tÔøΩK\u000eGÔøΩÔøΩ(_\u0007ÔøΩÔøΩÔøΩÔøΩÔøΩS*ÔøΩÔøΩÔøΩ∆ßÔøΩÔøΩ\u00164tÔøΩBÔøΩÔøΩmÔøΩ\u0005ÔøΩÔøΩÔøΩ)ÔøΩ@RÔøΩiÔøΩ@,\u001aÔøΩÔøΩQSÔøΩÔøΩÔøΩ7!Î£±d$\bÔøΩÔøΩ\bÔøΩuÔøΩ\u000f\\,>«É%ÔøΩ0bKXÔøΩÔøΩe\u0004[X ÔøΩ\u000f$\u0016lÔøΩ\u0005ÔøΩ\\ÔøΩ.MÔøΩXNO\b dÔøΩÔøΩ\u0014!ÔøΩÔøΩOlÔøΩ\bÔøΩ\ntÔøΩÔøΩjm€∫÷µjks[}”∫ÔøΩÔøΩÔøΩÔøΩkzÔøΩÔøΩÔøΩ‹ºn\nÔøΩ÷ñVÔøΩuÔøΩÔøΩÔøΩÔøΩ7ÔøΩrF\u0013ÔøΩÔøΩMÔøΩÔøΩÔøΩÔøΩÔøΩ\nc)ÔøΩ\u0007bcÔøΩcBSÔøΩÔøΩÔøΩIÔøΩ\nÔøΩ3ÔøΩ\u0002ÔøΩÔøΩÔøΩ\u001fÔøΩ1\u0005ÔøΩÔøΩa=\u0016ÔøΩSwÔøΩ\u0003\tHÔøΩÔøΩ\u001f\u000fr\u00054olÔøΩÔøΩ)ÔøΩÔøΩÔøΩÔøΩÔøΩ>\u0014ÔøΩÔøΩ\tÔøΩ?>ÔøΩÔøΩÕ¥ÔøΩÔøΩyÔøΩ3ÔøΩH\nÔøΩÔøΩÔøΩX4\bSDÔøΩI}‹è`ÔøΩ#\nMÔøΩÔøΩ\u0003ÔøΩ\bMu“ØÔøΩAM\u000fÔøΩ ÔøΩ\u001aTu[\u0002fÔøΩyÔøΩ√∫\u00063Fy\u0014\nkÔøΩDKE\u0005ÔøΩ)\\)\u0012ÔøΩ’±\u0018\u0018–ìÔøΩ\u0000ÔøΩ;ÔøΩÔøΩÔøΩBÔøΩqÔøΩ|L\u00075ÔøΩ\bD\u001b\tsÔøΩ\nÔøΩuu\u0012ÔøΩ\u000f\u0007\njf\u0002ÔøΩ@\n\b7\u0011ÔøΩ‹É\u0010ÔøΩ\u0017ÔøΩ\u000491-ÔøΩ`\u0018\nEA4 °x,92\n'TÔøΩO$`vx\u000eÔøΩÔøΩxÔøΩÔøΩÔøΩlÔøΩYÔøΩÔøΩ&bÔøΩ\tnÔøΩP2.ÔøΩ\u0001bÔøΩknQÔøΩ_ÔøΩbXÔøΩ”ØCÔøΩ1N\u001fÔøΩ\nsÔøΩSÔøΩCsAÔøΩHÔøΩ$9RRÔøΩ3ÔøΩÔøΩÔøΩÔøΩÔøΩHÔøΩ)\u0015ÔøΩG(ÔøΩÔøΩ‚πöÔøΩxÔøΩ \u0005ÔøΩzb\nÔøΩ\nÔøΩÔøΩÔøΩÔøΩ\tA-\u0011\u000eÔøΩ*¬çÔøΩÔøΩ\u000fÔøΩÔøΩÔøΩH\nÔøΩrhÔøΩ\bhÔøΩ\b\u00178ÔøΩmc8\n#\u0003\u0007bÔøΩÔøΩH*NFb1l\u0016ÔøΩ%66\u0005ÔøΩÔøΩ\n\u00075\u00182ÔøΩ\n?\u0019ÔøΩÔøΩÔøΩÔøΩÔøΩ–òÔøΩ\u00132ÔøΩ.ÔøΩ\"ÔøΩaSÔøΩ%&<4\u0018\n$ÔøΩÔøΩ\nÔøΩ\u001fÔøΩcÔøΩÔøΩd0\nÔøΩÔøΩÔøΩ\u000f\"ÔøΩ'ÔøΩ\\÷Ü?\u0012\u0004ÔøΩÔøΩÔøΩÔøΩHÔøΩX\"ÔøΩ\u001fÔøΩ\u001aÔøΩÔøΩÔøΩ\tn:ÔøΩcÔøΩoB\n|ÔøΩ#NÔøΩÔøΩ\"ÔøΩ\u0013ÔøΩÔøΩ¬Ø\u001aÔøΩÔøΩ'ÔøΩ%_bÓ£ÉhÔøΩ\u000eÔøΩÔøΩ%ÔøΩq–ñlÔøΩ^]\u00123ƒñ~)_e[iÔøΩÔøΩRxÔøΩ]ÔøΩÔøΩ èÔøΩgPÔøΩ|dÔøΩÔøΩ\u001fIÔøΩÔøΩ\u0018\u0017/9ÔøΩhFrÔøΩ\u0019W“≠AÔøΩI~ÔøΩ_ZÔøΩWqpÔøΩÔøΩÔøΩÔøΩ*ÔøΩbÔøΩRÔøΩÔøΩÔøΩÔøΩRXWÔøΩQ{ÔøΩ4ÔøΩ4v?\u0013/ÔøΩ'ÔøΩCÔøΩ4ÔøΩÔøΩhrI)-NKÔøΩÔøΩrKÔøΩeÔøΩÔøΩÔøΩÔøΩlÔøΩÔøΩZ6.ÔøΩÔøΩÔøΩGÔøΩÔøΩ^.ÔøΩÔøΩ\nxiLqÔøΩÔøΩiÔøΩÔøΩ\u0002ÔøΩ+ÔøΩ\nWK{IÔøΩncÔøΩÔøΩÔøΩsÔøΩ;ÔøΩÔøΩ=ÔøΩ.ÔøΩÔøΩ\nÔøΩÔøΩ)l-ÔøΩÔøΩWÔøΩ‹¨ÔøΩUÔøΩ66\u0000ÔøΩÔøΩ∆ΩÔøΩU0ÔøΩ.6ƒÆÔøΩy~;ÔøΩvÔøΩAvÔøΩ=ÔøΩj\u0018ÔøΩ≈ÆcÔøΩ!ÔøΩ6ÔøΩÔøΩÔøΩUÔøΩfv=sÔøΩ\u0002ÔøΩ\u0004ÔøΩUÔøΩÔøΩÔøΩeÔøΩÔøΩÔøΩ\u0015ÔøΩÔøΩÔøΩÔøΩÔøΩ'ŒùÔøΩ(ÔøΩ==\nÔøΩ\nZw\u0003ÔøΩFÔøΩZ\u000f\n\u0018e+=O[\u0014ÔøΩ\bnﬂùËêåÔøΩÔøΩ4¬åÔøΩN\u0013ÔøΩÔøΩ&ÔøΩYWÔøΩÔøΩtGÔøΩÔøΩÔøΩÔøΩ(ÔøΩÔøΩY$ÔøΩMÔøΩfk\u001b<o>ÔøΩkIÔøΩgvIÔøΩÔøΩ\u001ff\nÔøΩXMÔøΩ`ÔøΩ^ÔøΩÔøΩv8ÔøΩﬂ≥~\u0014ÔøΩM#DOÔøΩÔøΩ,ÔøΩÔøΩÔøΩ\u000eÔøΩÔøΩ@ƒ®oÔøΩ\n)ÔøΩÔøΩÔøΩÔøΩ{\nÔøΩ?ÔøΩÔøΩ>ÔøΩ\u000eÔøΩ(ÔøΩ\u0018jÔøΩÔøΩÔøΩ(\nÔøΩÔøΩlÔøΩrNÔøΩ◊ÜÔøΩÔøΩÔøΩ6ÔøΩÔøΩ\t`ÔøΩQÔøΩÔøΩÔøΩ(V^\u0004?ÔøΩ)ÔøΩ\bÔøΩ8ÔøΩ_ÔøΩÔøΩ\nÔøΩﬂ°ÔøΩ@ÔøΩcÔøΩ'ÔøΩF|~sÔøΩÔøΩÔøΩ\nÔøΩz\u000f\u0000ÔøΩ\u0001e\nVq*ÔøΩRnb\nÔøΩ\u000f*ÔøΩBÔøΩ\nÔøΩ\u0017FÔøΩXÔøΩ\u0017FÔøΩÔøΩ”ëÔøΩ|GÔøΩÔøΩPtÔøΩ\u0000[ÔøΩ6ÔøΩÔøΩ7<NÔøΩÔøΩÔøΩMpÔøΩU~3ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩXÔøΩyZyUÔøΩ\u000fÔøΩ8ÔøΩWÔøΩÔøΩÔøΩiZÔøΩÔøΩF\u0014.ÔøΩÔøΩlÔøΩÔøΩsÔøΩ#OÔøΩÔøΩÔøΩsPÔøΩ\u0013<JÔøΩ~ÔøΩÔøΩÔøΩ?\u001a ÔøΩÔøΩÔøΩÔøΩ\nbÔøΩ1vRÔøΩ\n&v*\u000f)ÔøΩ\u001bÀú'ÔøΩ+ÔøΩ#ÔøΩw9\u0015ÔøΩÔøΩ\nÔøΩÔøΩ¬õY[ÔøΩÔøΩDGÔøΩÔøΩ\nÔøΩN+oAÔøΩoÔøΩjoÔøΩ÷¥yXGÔøΩÔøΩY÷Ñ\"CÔøΩ/\u0003zÔøΩÔøΩ…ØÔøΩ\u0001ÔøΩ\nÔøΩÔøΩ\nÔøΩÔøΩ\nÔøΩÔøΩ\npÔøΩ\u0006ÔøΩdLy\n#ÔøΩ\u0003ÔøΩQyÔøΩÔøΩ+ÔøΩdÔøΩQÔøΩ\u0007l\u0001ÔøΩ)\u0003\u001a|ÔøΩÔøΩÔøΩZÔøΩ\u0013ÔøΩ-ÔøΩÔøΩ–ÑÔøΩ8t'ÔøΩÔøΩÔøΩÔøΩÔøΩ|ÔøΩÔøΩÔøΩFa\u0011ÔøΩÔøΩ<ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ<€Å\"ÔøΩÔøΩ\u0017fWÔøΩxb«ïœì(ÔøΩgKÔøΩÔøΩÔøΩ2ÔøΩÔøΩO\n[`ÔøΩMÔøΩ\u0006O+ÔøΩÔøΩÔøΩI\u0013ÔøΩÔøΩ\u0006ÔøΩÔøΩKÔøΩŸï;hÔøΩŸº\u0002ÔøΩAX\u0010ÔøΩ1ÔøΩÔøΩÔøΩÔøΩB9ÔøΩb\u0001ÔøΩ d\u0018d{Q\u0014ÔøΩÔøΩÔøΩÔøΩÔøΩ=ÔøΩÔøΩÔøΩu4yÔøΩÔøΩÔøΩÔøΩ|ZÔøΩ\nÔøΩ_E⁄∫ÔøΩXVI<_9\n`ÔøΩqÔøΩ\u0017NÔøΩCÔøΩ3ÔøΩN0ÔøΩÔøΩÔøΩd>ÔøΩ7€∂ÔøΩÔøΩt\\ÔøΩ#ÔøΩÔøΩ\ngÔøΩÔøΩ6ÔøΩJ\tÔøΩjd\nÔøΩÔøΩÔøΩ)ÔøΩÔøΩu\u0013ÔøΩ€∞ÔøΩSÔøΩ€å;ÔøΩ5[ÔøΩÔøΩÔøΩ3n\"ÔøΩZÔøΩ\u0007`ÔøΩVÿ®\u0015ÔøΩoE0ÔøΩÔøΩÔøΩ=ÔøΩÔøΩBÔøΩxPÔøΩ\u0010ÔøΩ\nÔøΩ\u000fÔøΩ\u0018ÔøΩ4ÔøΩ\u0005ÔøΩÔøΩ\u0000ÔøΩ\u0003CzÔøΩ\u0019ÔøΩ+\u001b ÔøΩ\u0006v\u000eEÔøΩ\u00017ÔøΩ7QdÔøΩ7ÔøΩ-(ÔøΩÔøΩ<ÔøΩr\u0006%ÔøΩzÔøΩÔøΩÔøΩÔøΩoÔøΩ\nÔøΩP\u001fFÔøΩAÔøΩ\u0011ÔøΩ\u000eÔøΩ^ÔøΩ}(ÔøΩPÔøΩÔøΩÔøΩ@y\u0013%ÔøΩÔøΩTÔøΩ\u000eÔøΩTD\u0013ÔøΩC(ÔøΩ(ÔøΩ\u0015ÔøΩŸàÔøΩR\u0007>ÔøΩÔøΩ'G\u0014ÔøΩ}`eÔøΩÔøΩ\u000eÔøΩGÔøΩÔøΩÔøΩÔøΩÔøΩtP>ÔøΩ\nÔøΩ\nÔøΩ8ÔøΩ8X`ÔøΩÔøΩ_]ÔøΩÔøΩÔøΩ»´\u0006^’¢j›ó=ÔøΩ}(ÔøΩ[LÔøΩl5[ÔøΩ2ÔøΩÔøΩÔøΩ¬ø$WÔøΩÔøΩÔøΩÔøΩ}ÔøΩÔøΩÔøΩÔøΩ)ÔøΩÔøΩÔøΩ3\u000fgÔøΩ';ÔøΩÔøΩOÔøΩÔøΩÔøΩÔøΩÔøΩzNJ\u000e\\9pÔøΩÔøΩﬁ©ÔøΩl?ÔøΩ~ÔøΩ]9ÔøΩ;ÔøΩ;ÔøΩSNÔøΩtÔøΩÔøΩÔøΩ/)'ÔøΩO◊üÔøΩWÔøΩÔøΩÔøΩMÔøΩÔøΩÔøΩÔøΩ_-NÔøΩQÔøΩ\"ÔøΩ,{ÔøΩÔøΩrPÔøΩGÔøΩ8ÔøΩFe\n|ÔøΩÔøΩ/w<ÔøΩPÔøΩÔøΩ\n‘üÔøΩ8rÔøΩ\\ÔøΩpÔøΩÔøΩÔøΩÔøΩ\u0013ÔøΩÔøΩr3ÔøΩ3OdÔøΩÔøΩ<ÔøΩÔøΩffFÔøΩÔøΩÔøΩÔøΩCÔøΩÔøΩ3ÔøΩefÔøΩ/dlÔøΩÔøΩfZÔøΩÔøΩÔøΩ\tÔøΩ\nC=ÔøΩ\"ÔøΩCÔøΩ\u000f\u0013‰†ë\u0013ÔøΩOÔøΩÔøΩaÔøΩﬁázÔøΩÔøΩÔøΩÔøΩÔøΩ\tÔøΩBÔøΩÔøΩ!ÔøΩ*ÔøΩÔøΩgÔøΩ\nB}\u0018ÔøΩÔøΩÔøΩÔøΩ*ÔøΩMÔøΩ\u001aÔøΩ\n)ÔøΩ\u0017ÔøΩ\u001bG}\u0018EÔøΩÔøΩ]YÔøΩTÔøΩÔøΩÔøΩ\nÔøΩjÔøΩÃ™ÔøΩ7ÔøΩÔøΩSÔøΩgÔøΩÔøΩÔøΩÔøΩ\u0013ÔøΩÔøΩMÔøΩ\nÔøΩÔøΩ\nÔøΩÔøΩ\u0005ÔøΩÔøΩ\u0005ÔøΩ|ÔøΩÔøΩ~\u0001t\u0001ÔøΩTÔøΩÔøΩÔøΩ\tÔøΩyÔøΩ=OxÔøΩ\u0003ÔøΩCÔøΩÔøΩ€ázÔøΩ /ÔøΩ~ÔøΩÔøΩP7qH~ﬁ®jÔøΩwÔøΩÔøΩ_\u0001≈ΩÔøΩÔøΩG9ÔøΩÔøΩÔøΩFÔøΩ[PbteGÔøΩDÔøΩÂØ†ÔøΩÔøΩGgÔøΩÔøΩy\u000eÔøΩÔøΩGÔøΩ\u001a$C4ÔøΩÔøΩÔøΩL4+ÔøΩÔøΩ--ÔøΩÔøΩÔøΩÔøΩÔøΩGAÔøΩ(ÔøΩ\n%BGAÔøΩ(HÔøΩÔøΩÔøΩ\tÔøΩÔøΩÔøΩÔøΩqÔøΩ\u0018ÔøΩEÔøΩÔøΩÔøΩtÔøΩFlÔøΩÔøΩÔøΩ#ÔøΩ\u0011\u0014ÔøΩÔøΩ@}?AÔøΩÔøΩÔøΩ\u0010ÔøΩ\bÔøΩÔøΩÔøΩÔøΩ”®ÔøΩ\u00104ÔøΩÔøΩXzÔøΩ^ÔøΩ8ÔøΩ\u0013%5ÔøΩ\"\u001f≈ø#ÔøΩÔøΩÔøΩMÔøΩ…õ+ÔøΩÔøΩÔøΩqj(,ÔøΩ\u0016ÔøΩÔøΩO\u001aÔøΩBÔøΩÔøΩ\u0003ÔøΩ÷ÅfV4\u0006o:ÔøΩd\u00056ÔøΩÔøΩÔøΩÔøΩ€§ÔøΩS}?ÔøΩ_ÔøΩÔøΩZÔøΩÔøΩÔøΩÔøΩ*ÔøΩÔøΩlÔøΩÔøΩÔøΩÔøΩÔøΩ*[GÔøΩ|5ÔøΩ#6ÔøΩMÔøΩ_ÔøΩÔøΩFo~ÔøΩÔøΩ?ÔøΩm?ÔøΩÔøΩ=PmÔøΩFÔøΩÔøΩÔøΩÔøΩ2ÔøΩ\u001b6iÔøΩÔøΩÔøΩÔøΩÔøΩoÔøΩÔøΩÔøΩÔøΩ=^i{ÔøΩÔøΩvoÔøΩÔøΩÔøΩJÔøΩÔøΩJÔøΩÔøΩJNÔøΩ\u0016ÔøΩ!ÔøΩ\\ÔøΩkÔøΩ\u0006ÔøΩWzWÔøΩÔøΩ?ÔøΩÔøΩUmÔøΩÔøΩ~ÔøΩ⁄æÔøΩ⁄ÜTÔøΩ&\u0015ÔøΩÔøΩ[ÔøΩ<mÔøΩWÔøΩÔøΩ2ÔøΩÔøΩ\u001f_gsÔøΩÔøΩUÔøΩÔøΩ=)C7ÔøΩÔøΩ\n;ÔøΩ>.ÔøΩÔøΩ«òMÔøΩ1\\ÔøΩÔøΩ9%ÔøΩ\u001ayÔøΩÔøΩ[ÔøΩfÔøΩÔøΩÔøΩ@SnÔøΩvÔøΩ)3|q4EÔøΩÔøΩ^gGÔøΩlÔøΩfp2qÔøΩÔøΩ“åÔøΩÔøΩyÔøΩÔøΩ6\nÔøΩÔøΩjÔøΩn@ÔøΩaÔøΩ6:ÔøΩyÔøΩUÔøΩÔøΩ}#TÔøΩÔøΩ=#t\u0019ÔøΩwÔøΩÔøΩ:4ÔøΩÔøΩÔøΩ)ÔøΩ,$ÔøΩÔøΩÔøΩÔøΩFÔøΩ>ÔøΩÔøΩ^cÔøΩÔøΩÔøΩÔøΩkV#?ÔøΩvÔøΩÔøΩm\u0001ÔøΩÔøΩbuÔøΩ\u0007ÔøΩ]ZÔøΩnÔøΩy9\u0017ÔøΩ_\u001b.0'=hÔøΩjÔøΩ|ÔøΩpUÔøΩÔøΩÔøΩh\n0\\N4_7B\nhÔøΩ3BÔøΩÔøΩÔøΩÔøΩ\u0011z\u0005ÔøΩQÔøΩ6ÔøΩÔøΩ\naÔøΩDÔøΩX\nÔøΩÔøΩÔøΩ+ÔøΩÔøΩ\u0001ÔøΩÔøΩ)ÔøΩ\u001bÔøΩF41√∑\nÔøΩ~ÔøΩÔøΩÔøΩhÔøΩFÔøΩ+|ÔøΩ4#ÔøΩÔøΩÔøΩ\u0010s\u0011ÔøΩ~#ÔøΩÔøΩOg4\u0005ÔøΩ8ÔøΩÔøΩÔøΩÔøΩÔøΩzÔøΩ|ÔøΩÔøΩÔøΩ*ÔøΩ âtÿ§\nSÔøΩnÔøΩÔøΩ\u001fÔøΩNiÔøΩÔøΩx\nW\u0013ÔøΩÔøΩ\nW\nÔøΩÔøΩBsW\u0018!7ÔøΩ6ÔøΩ\u0016:ÔøΩZÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩ\u0002kÔøΩ}ÔøΩÔøΩÔøΩÔøΩ\u0006'TeÔøΩ\n\u0006ÔøΩÔøΩ\bÔøΩuÔøΩ_ÔøΩ\tÔøΩÔøΩ)ÔøΩ3ÔøΩTÔøΩÔøΩj!k'ÔøΩ\n\u0017ÔøΩr\u0018.ÔøΩÔøΩC)ÔøΩÔøΩÔøΩb\u000eÔøΩÔøΩÔøΩ>ÔøΩÔøΩ\u0000tÔøΩoÔøΩÔøΩv\u001bÔøΩÔøΩÔøΩsVÔøΩpÔøΩÔøΩ\u0016ÔøΩcÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ7ÔøΩÔøΩÔøΩ5ÔøΩÔøΩ√è9O\u0003ÔøΩÔøΩvÔøΩÔøΩ\\ÁãÆWÔøΩÔøΩ\nU:ÔøΩÔøΩ\n\u0018ÔøΩrÔøΩsÔøΩ\u0006ÔøΩ5SŒπÔøΩÔøΩÔøΩYÔøΩeÔøΩ\u001906\n\u001av>\u0012\"\nﬂØÔøΩ4ÔøΩÔøΩ`ÔøΩ,aÔøΩÔøΩÔøΩvÔøΩ_ÔøΩÔøΩÔøΩ/ÔøΩÔøΩq\nÔøΩ\u0000ÔøΩ;ÔøΩ\u001a ÔøΩ)◊îÔøΩÔøΩÔøΩ€úIÔøΩBÔøΩwÔøΩSwU8ÔøΩkopÔøΩXÔøΩ\u0017ZÔøΩ\nÔøΩv9G!ÔøΩ\bÔøΩhÔøΩ\u0011ÔøΩÔøΩuÔøΩsÔøΩzÔøΩÔøΩ\u0006ÔøΩOÔøΩ\u0003ÔøΩIÔøΩÔøΩ\u0010IÔøΩÔøΩÔøΩ\u0006ÔøΩ\nÔøΩrn\u0005\u0007\u0018ÔøΩÔøΩ\u0007ÔøΩÔøΩÔøΩÔøΩK\u000fÔøΩ6ÔøΩ?ÔøΩuÔøΩÍ•ÆŸü:ÔøΩi}JÔøΩn,\nBÔøΩ{\u001bÔøΩÔøΩŒ∫5k8k0ÔøΩ\u0013ÔøΩŒöÔøΩÔøΩYÔøΩÔøΩ.ÔøΩ*ÔøΩ\u0016Z\nÔøΩ|kÔøΩ5ÔøΩjÔøΩfZ-ÔøΩÔøΩÔøΩV&\u0017ÔøΩppÔøΩÔøΩ\u0004\u0017g:xÔøΩi·µÖ`ÔøΩÔøΩuÔøΩnqp6ÔøΩÔøΩÔøΩÔøΩmÔøΩtÔøΩÔøΩ+ÔøΩ\u000etNÔøΩÔøΩ{ÔøΩÔøΩÔøΩnsÔøΩNgÔøΩlœå$}~HÔøΩ>\u0011`ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0003UsRÔøΩÔøΩÔøΩ3ÔøΩ:ÔøΩÔøΩÔøΩ^ÔøΩ;ÔøΩY\u0002ÔøΩiÔøΩÔøΩ9ÔøΩ\nÓôìÔøΩÔøΩ\u0019ÔøΩ*ÔøΩ.ÔøΩÔøΩ)ÔøΩÔøΩÔøΩO}ÔøΩÔøΩÔøΩ[?ÔøΩÔøΩÔøΩ!ÔøΩ|bK…ñÔøΩÔøΩÔøΩÔøΩ[ÔøΩ/QÔøΩ3ÔøΩÔøΩE_D-qÔøΩœªÔøΩÔøΩÔøΩRÔøΩÔøΩÔøΩÔøΩ*ÔøΩÔøΩ=\n8W1ÔøΩ;ÔøΩv@ÔøΩ~ÔøΩ\u0013rDÔøΩÔøΩÔøΩÔøΩ\ty?oÔøΩÔøΩ<!ÔøΩ ëÔøΩ]ÔøΩ_\u001aÔøΩ\n\u0002ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ@c>ÔøΩ\u0000MÔøΩÔøΩÔøΩs4ÔøΩ_ÔøΩ\bMÔøΩAwÔøΩL{ÔøΩ@ÔøΩ!ÔøΩp$\u0004ÔøΩ\u000eBÔøΩN u-FR>#u\u0011RÔøΩÔøΩ\u0019BÔøΩO,ÔøΩ\u0002\u001fXÔøΩÔøΩ\u001bÔøΩeDÔøΩÔøΩ\u0016teD\bÔøΩÔøΩÔøΩÔøΩÔøΩ‘ÄRÔøΩÔøΩÔøΩÔøΩxjÔøΩ0SÔøΩÔøΩ\n√µbÔøΩ{bÔøΩ{|xNÔøΩ\u0016ÔøΩÔøΩÔøΩ\bnkY\nÔøΩP#ÔøΩ\u0002ÔøΩÔøΩÔøΩÔøΩOÔøΩÔøΩ/LÔøΩf7ODÔøΩÔøΩhU=ÔøΩÔøΩz4ÔøΩ}”üÔøΩ\u0018-ÔøΩ>4ÔøΩÔøΩ3ÔøΩ\t>ÔøΩN+5ÔøΩÔøΩ\u0003ÔøΩÔøΩÔøΩkÔøΩ\u0013UZÔøΩtÔøΩÔøΩ[ÔøΩŸºÔøΩ\u0012ÔøΩ{ÔøΩÔøΩÔøΩÔøΩ\u0019ÔøΩÔøΩgpÔøΩÔøΩ\nÔøΩÔøΩmlÔøΩnÔøΩÔøΩw\u000fÔøΩÔøΩÔøΩÔøΩv‡ºµÔøΩNÔøΩÔøΩvÔøΩ%ÔøΩÔøΩ∆âÔøΩÔøΩÔøΩ\u000e\\bÔøΩ\u0000\u001fÔøΩÔøΩk\nÔøΩk\nÔøΩkÔøΩyÔøΩhÔøΩÔøΩ]ÔøΩRoÔøΩÔøΩ\u0019+ÔøΩÔøΩ\nCÔøΩÔøΩÔøΩÔøΩÔøΩhÔøΩWÔøΩjÔøΩsÔøΩcÔøΩÔøΩBÔøΩÔøΩU%ÔøΩÔøΩ?ia“É,ÔøΩ=4ÔøΩWÔøΩ9mCÔøΩCÔøΩ\nÔøΩ\n|\b!ÕáÔøΩÔøΩm7ÔøΩJnÔøΩ|UÔøΩÔøΩ“ÉÔøΩ\u0003ÔøΩ\u0005UÔøΩ,QÔøΩ\u0013ÔøΩÔøΩ\nÔøΩD\u0012–±ÔøΩ\n]ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ∆ÅÔøΩ\u0000ÔøΩÔøΩ?`\u0002ÔøΩEÔøΩ^s<ÔøΩÔøΩ\nnÔøΩÔøΩeÔøΩÔøΩkœåÔøΩÔøΩS\u0012ÔøΩ/ÔøΩÔøΩÔøΩ{HgnÔøΩXÔøΩÔøΩfX\u0013R”Å9\nÔøΩs3ÔøΩÔøΩÔøΩoﬂøÔøΩÔøΩÔøΩ)'ÔøΩ\nÔøΩ\nÔøΩÔøΩOÔøΩÔøΩ\nÔøΩ\nNÔøΩÔøΩ)'ÔøΩOÔøΩÔøΩiWNÔøΩNÔøΩÔøΩ\u0000ÔøΩÔøΩS/ÔøΩyI9QÔøΩÔøΩLÔøΩÔøΩjrÔøΩÔøΩ/\u001bÔøΩ√ÖIÔøΩÔøΩÔøΩÔøΩÔøΩ$-…çÀÑ[wsÔøΩS:ÔøΩÔøΩÔøΩÔøΩrÔøΩÔøΩOÔøΩÔøΩ<7ÔøΩÔøΩÔøΩsÔøΩ\nÔøΩ.\u0006ÔøΩ4EÔøΩÔøΩiÔøΩEnÔøΩÔøΩ\u0014\tÔøΩt\nendstream\nendobj\n37 0 obj\n25807\nendobj\n36 0 obj\n39356\nendobj\n11 0 obj\n<</Length1 38 0 R/Length 39 0 R/Filter/FlateDecode>>stream\nxÔøΩÔøΩ|y|UÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ;ÔøΩÔøΩ<œπsroÔøΩÔøΩÔøΩÔøΩ@\u0012 ÔøΩÔøΩ<\u0004\bÔøΩÔøΩ2\b\u0001ÔøΩEp\u0004ÔøΩÔøΩ\u0003\u000e‡¨†%\u0010ÔøΩÔøΩ(ÔøΩR[ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ~\u0016ﬂ¢ÔøΩjÔøΩm\u0011'rÔøΩÔøΩÔøΩ7\u0011lÔøΩÔøΩÔøΩÔøΩ‹úÔøΩÔøΩ3ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ>\u0007aÔøΩÔøΩ\u0002mA\n\u001a;fBÔøΩ|oÔøΩ\bÔøΩÔøΩ\tÔøΩÔøΩsÔøΩÔøΩY!ÔøΩÔøΩÔøΩ@\bg`yjÔøΩÔøΩ‹ÖÔøΩ'\n@ÔøΩÔøΩÔøΩÔøΩh›Ç\u0015\nÔøΩÔøΩÔøΩ;x\u001a!ÔøΩXÔøΩ\u0004C\u0016.]ÔøΩÔøΩÔøΩSÔøΩÔøΩ#ÔøΩÔøΩ\nÔøΩÔøΩ\u0017\u0017Õü3oi€∑S\u0010ÔøΩÔøΩ\nÔøΩWÔøΩ\b*‰õòÔøΩÔøΩ\nÔøΩ ÔøΩÔøΩe◊≠{ÔøΩmÔøΩ ÔøΩÔøΩAÔøΩÔøΩÔøΩÔøΩsÔøΩ ÔøΩ;fÔøΩ:ÔøΩÔøΩÔøΩœóÔøΩYÔøΩBÔøΩ\"oChÔøΩ\n(ÔøΩÔøΩÔøΩÔøΩlÔøΩÔøΩÔøΩÔøΩ\u001aÔøΩp~ÔøΩ`ÔøΩÔøΩU◊±#ÔøΩ' t√ÉÔøΩÔøΩÔøΩ\u0015+ÁØ∏iÔøΩ+ÔøΩ ÔøΩÔøΩ\u000eÔøΩt\u001aaÔøΩ\nﬂÖ\u0004poÔøΩ\u0005\u0015(ÔøΩ\u0003|:ÔøΩŸã\u0016\u0010-\u0016\u0010\"`\u0004ÔøΩ@¬∞ÔøΩÔøΩVtÔøΩ_ÔøΩÔøΩkÔøΩ#ÔøΩ‹ÖKÔøΩÔøΩÔøΩÔøΩBÔøΩÔøΩ/q\bÔøΩÔøΩÔøΩWÔøΩ\n0ÔøΩ\u0017m99b1ÔøΩÔøΩ’àÔøΩÔøΩxÔøΩ\u001bÔøΩQ\u0004ÔøΩP\u0002ÔøΩQ9ÔøΩ@\u0019ÔøΩEUÔøΩ\u001aÔøΩÔøΩP‘àÔøΩÔøΩp4\u0012ÔøΩÔøΩ64\u0006ÔøΩEÔøΩÔøΩ\u00044\u0011MBSÔøΩ\n4\u0017ÔøΩCÔøΩÔøΩ\u0002ÔøΩ\u0010-B◊†ÔøΩh\u0019ÔøΩ\u0016-G+ÔøΩJÔøΩ\n]ÔøΩVÔøΩ5h\nZÔøΩ6\u0014\nÔøΩ\nÔøΩ_ÔøΩÔøΩ?ÔøΩEÔøΩ¬∑ÔøΩ\u0015ÔøΩÔøΩÔøΩ\u0015ÔøΩ\tÔøΩ\u0015.\u0014N\u0017ÔøΩOÔøΩTÔøΩœÖÔøΩ\u0014ÔøΩXÔøΩ}ÔøΩ«Ö_\u0016^ÔøΩÔøΩ7\nÔøΩ\u0016ÔøΩ\u0014\u000e\u0016ÔøΩ(ÔøΩ\u0014ÔøΩ\u0014ÔøΩ\u0016~\u0004ÔøΩG!ÔøΩbÔøΩÔøΩÔøΩCÔøΩG _a7ÔøΩÔøΩ\u0015ÔøΩAÔøΩ)ÔøΩÔøΩÔøΩ?ÔøΩ\tvÔøΩ2\u001aÔøΩ`ÔøΩ3ÔøΩ\btÔøΩÔøΩ!,ÔøΩÔøΩ%?\nÔøΩ\u0004ÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩŒªJKÔøΩ/ÔøΩÔøΩC;ÔøΩ\nmB7ÔøΩfhÔøΩÔøΩ\u001bh\tÔøΩÔøΩ8t\u0000’°/ÔøΩ\u001fÔøΩ\bhÔøΩ\u0000Z\nRÔøΩP?2ÔøΩ9h8ÔøΩÔøΩÔøΩNd*ÔøΩ\u0001[ÔøΩ\u0017>%\u001f#ÔøΩ\nB7ÔøΩBÔøΩÔøΩ\nÔøΩÔøΩ5$DÔøΩq\u0005ÔøΩ~ÔøΩÔøΩ\u0014\u0016\"ÔøΩÔøΩ]TÔøΩnCÔøΩ\nD\"6ÔøΩÔøΩFÔøΩ\u0016ÔøΩT»ÉÔøΩÔøΩ@ÔøΩÔøΩ:<ÔøΩÔøΩ\"ÔøΩ\u0007ÔøΩm@◊£€±\tÔøΩÔøΩj|=\nÔøΩ=ÔøΩC/ÔøΩÔøΩD-9\u0002ÔøΩ◊ÜÔøΩ\u0002\tO\u0005ÔøΩ\nf1\\S\u0000R?ÔøΩÔøΩ4¬ïÔøΩÔøΩ\n8ÔøΩO\u0017^\u0000ÔøΩ\tÔøΩÔøΩq4\u0014WÔøΩhÔøΩ\u0004rÔøΩ2–õZ‘ÄnEÔøΩÔøΩ\u0007ÔøΩ\u001fp\u0002\u000faÔøΩÔøΩqdÔøΩgÔøΩÔøΩÔøΩc%6b/>Ux\u0004ÔøΩÔøΩ◊ÜfÔøΩÔøΩﬁéÔøΩGÔøΩ–õÔøΩMÔøΩÔøΩ\u0013IÔøΩÔøΩ-x>ÔøΩ\tRÔøΩ\u0016]\u000fÔøΩÔøΩ\u0003ÔøΩ\u000eÔøΩ\u0003KÔøΩ\u0014ÔøΩÔøΩÔøΩ2/ÔøΩ\u001b\nÔøΩA\u001fÔøΩÔøΩÔøΩUpÔøΩ&4\nÔøΩ{\u0013z\u0000ÔøΩÔøΩYt\u0014ÔøΩF?ÔøΩ6y\u0017;ÔøΩXÔøΩ\u0000ÔøΩ;{ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩg\nÔøΩÔøΩ?ÔøΩ\nÔøΩu\u0012ÔøΩÔøΩh3ÔøΩ\u000eÔøΩy\nÔøΩ\u0004ÔøΩÔøΩŒ°ÔøΩ1ÔøΩÔøΩXÔøΩBRÔøΩ}FÔøΩ>&0\u0015Pa+ÔøΩ+ÔøΩÔøΩÔøΩSÔøΩ\u001a[ÔøΩ6ÔøΩ\nÔøΩ#~ÔøΩÔøΩ8ÔøΩ+ÔøΩuÔøΩÔøΩDITd)ÔøΩÔøΩÔøΩ'ÔøΩ3€ôÔøΩÔøΩ_ÿø\u0016\u001a\nœÅÔøΩÔøΩÔøΩÔøΩ)\u0012!\u001fÔøΩÔøΩÔøΩ_ÔøΩÔøΩ=ﬁàÔøΩ\u0004ŸΩÔøΩ~ÔøΩÔøΩÔøΩ^ÔøΩsÔøΩ7ÔøΩ\u0005ÔøΩ\u0000-ÔøΩ\u0018ÔøΩÔøΩ\u0007ÔøΩ\u0011ÔøΩ\u0015—ì\u0017ÔøΩÔøΩÔøΩKÔøΩw\u0005_ÔøΩF_B2hÔøΩ\u0000ÔøΩÔøΩ\u0014ÔøΩ*ÔøΩ\u0005G@\u000fÔøΩ\u0002wÔøΩ\u001b$ÔøΩ:ÔøΩ%ÔøΩÔøΩÔøΩ[lÔøΩ9|\u0003ﬁäÔøΩÔøΩNÔøΩ\nﬂèÔøΩÔøΩ/ÔøΩÔøΩÔøΩ[ÔøΩ\u0003ÔøΩ~ÔøΩyÔøΩÔøΩÔøΩ9ÔøΩÔøΩrvÔøΩ`ÔøΩÔøΩpÔøΩhNÔøΩÔøΩÔøΩÔøΩB+<ÔøΩ\u000eŒù\u0001ÔøΩÔøΩÔøΩ6ÔøΩ\u000fÔøΩÔøΩ\ntÔøΩ\u0011hÔøΩCÔøΩ\u0018:\u0005ÔøΩÔøΩ\u0015ÔøΩ\u000eÔøΩE\u0007OÔøΩ«µx<^ÔøΩÔøΩÔøΩ7ÔøΩ;ÔøΩ^ÔøΩ\n\u0019I\u0016ÔøΩÔøΩd\u0005ÔøΩ\u0019\u0007ÔøΩcBÔøΩ6ÔøΩÔøΩÔøΩgÕæ/ÔøΩ ÿë\u000fÔøΩÔøΩ\nDÔøΩF\nÔøΩP\nÔøΩ=\u0015~ÔøΩ\u0000%ÔøΩÔøΩ56ÔøΩL\u001f\u0002ÔøΩ?\tÔøΩÔøΩ\u0019hÔøΩÔøΩÔøΩÔøΩ_ÔøΩÔøΩÔøΩj\u0004ÔøΩ,ÔøΩ\u0006ÔøΩÔøΩ!ÔøΩ\nÔøΩI ÔøΩÔøΩx&ÔøΩÔøΩ\u0017ÔøΩMÔøΩIÔøΩ\u001fÔøΩÔøΩÔøΩNÔøΩÔøΩLÔøΩÔøΩNr\u001fyÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩdÔøΩe\nfzÔøΩÔøΩ2yÔøΩ ÔøΩ\tÔøΩÔøΩÔøΩ*hÔøΩÔøΩÔøΩ/ÔøΩÔøΩpÔøΩpÔøΩhÔøΩÔøΩj—≥ÔøΩ_ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ<o»áÔøΩ\u0013ÔøΩÔøΩ_*L-ÔøΩ)ÔøΩ-<^x\u0016PÔøΩ@ÔøΩ4ÔøΩS\u0019ÔøΩ]\u0007ÔøΩ\u001b~!ÔøΩ√ëÔøΩ\u0015ÔøΩF3ÔøΩÔøΩÔøΩÔøΩNÔøΩÔøΩ.t\u0017ÔøΩ\u001b~ÔøΩÔøΩ3ÔøΩÔøΩÔøΩÔøΩ\nh‹ØÔøΩoÔøΩ\u0007ÔøΩÔøΩÔøΩÔøΩ\u0018}\u0002ÔøΩÔøΩÔøΩÔøΩ/ÔøΩ%x&3ÔøΩÔøΩ4ÔøΩK\u0015ÔøΩÔøΩÔøΩÔøΩ\nÔøΩ\nÔøΩÔøΩÔøΩ›Ñ\u001fÔøΩ\u000fÔøΩq7>ÔøΩOÔøΩ7ÔøΩoÔøΩ\u001fÔøΩÔøΩ,ÔøΩ.ÔøΩÔøΩÔøΩDKt$IÔøΩH\u0013\u0019A∆êÔøΩd.ÔøΩOVÔøΩÔøΩÔøΩAÔøΩ0yÔøΩ\n#'ÔøΩOA ø' \u001fÔøΩ<c\u0007I43#ÔøΩ\u000ef&ÔøΩÔøΩzÔøΩFÔøΩqÔøΩ\u0018ÔøΩ\u000eÔøΩ;ÔøΩ]ÔøΩCÔøΩkh\u001b\u0016dÔøΩa}lÔøΩÔøΩa\u0017ÔøΩ7ÔøΩg\u0005ahÔøΩyÔøΩ≈Ç=ÔøΩÔøΩÔøΩP&\\,< ÔøΩ\u0011ÔøΩ)ÔøΩD$\u0014ÔøΩE#EcEœà\u000eÔøΩ\nÔøΩS\u000eÔøΩ{ÔøΩÔøΩ^ÔøΩ\u0007\u001aÔøΩ\nÔøΩÔøΩÔøΩpÔøΩ\n~ÔøΩ\nÔøΩÔøΩÔøΩ_ÔøΩCl\u001fQÔøΩvÔøΩÔøΩA$ÔøΩÔøΩ@ÔøΩÔøΩÔøΩÔøΩd;\u0013ÔøΩÔøΩÔøΩ:lÔøΩ~|\u0007j!\nÔøΩÔøΩÔøΩ<JFÔøΩv”øÔøΩ–ã+@\u000f'\n~ÔøΩ\u001aÔøΩÔøΩ\u0003n√ã\u0000oÔøΩ\u0002ÔøΩiÔøΩ}ÔøΩÔøΩ\u0013(Px\u0017iÔøΩ›Ö%ÔøΩ\b6AÔøΩÔøΩ_x\bÔøΩÔøΩ\u0016‹äOC\u001fZH:ÔøΩÔøΩÔøΩKÔøΩ\u001a4ÔøΩCÔøΩ=–õÔøΩÔøΩÔøΩgÔøΩÔøΩÔøΩ7ÔøΩ\n\u0012\u0005m\u001bÔøΩÔøΩ #ÿ≤\u0010Hi=vÔøΩ\u0004ÔøΩÔøΩ\nbÔøΩÔøΩÔøΩ=»ÇÔøΩÿ•\u0002ÔøΩpÔøΩOÔøΩ\u0010ÔøΩGÔøΩ'ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0004ÔøΩÔøΩ\u0001ÔøΩÔøΩÔøΩ#0bÔøΩ\u0002ÔøΩ!\u0017ÔøΩ\nÔøΩ\bÔøΩÔøΩ\nÔøΩ[ÔøΩ\naÔøΩÔøΩq<\u0006ÔøΩÔøΩŒàA?~ÔøΩÔøΩÔøΩQ4ÔøΩYÔøΩYÔøΩÔøΩÔøΩÔøΩ}\u0017ÔøΩGrd\u001a\u0013ÔøΩÔøΩdÔøΩ\n0!ÔøΩÔøΩÕ®\nÔøΩÔøΩÔøΩ\u0005|?ÔøΩ\u001a{ÔøΩ.ÔøΩ\u0005ÔøΩÔøΩ#ÔøΩ\u0019ÔøΩ\bÔøΩÔøΩpÔøΩÔøΩ3wÔøΩEÔøΩÔøΩÔøΩgÔøΩHÔøΩx\u0018ÔøΩByÔøΩ!ÔøΩ\u001aÔøΩ∆èÔøΩ.0a1ÔøΩÔøΩ~$\u0004ÔøΩÔøΩÔøΩÔøΩc\u0016ÔøΩÔøΩÔøΩo\u0005?aÔøΩƒ∂1GÔøΩ5ÔøΩÔøΩÔøΩÔøΩ%ÔøΩMÔøΩp\u001bS]ÔøΩC\u0001ÔøΩ◊å\"ÔøΩÔøΩ\u0002XpR(ÔøΩÔøΩÔøΩÔøΩÔøΩCÔøΩtÔøΩÔøΩ\ngÔøΩ8;ÔøΩ\nÔøΩ›ëÔøΩÔøΩ\"&|\u000fÔøΩL0ÔøΩÔøΩÔøΩÔøΩ&ÔøΩÕ§\n-\u0010|*\u001aÔøΩ÷ìF@ÔøΩÔøΩÔøΩ\u0016\n@eÔøΩ<ÔøΩBÔøΩÔøΩÔøΩÔøΩ\u0006ZÔøΩÔøΩÔøΩÔøΩÔøΩwd\nrÔøΩ/ÔøΩh\nÔøΩ\u0013zÔøΩ\u001fÔøΩd\" ÔøΩ\u0001ÔøΩ\u0010?\u0007ÔøΩ\nÔøΩ65ÔøΩ\u0015ÔøΩÔøΩÔøΩ\u0007ÔøΩlcV\u0003ÔøΩ\nEg@€Ø\u0007l◊ëÔøΩ`g\u0016ÔøΩÒàÄï`y{ÔøΩ\u001bÔøΩÔøΩ\u001fÔøΩ5ÔøΩ\u0017ÔøΩÔøΩÔøΩ_\u0006kÔøΩ\u001frNÔøΩÔøΩ\nÔøΩ5ÔøΩ\u000fÔøΩÔøΩÔøΩÔøΩUWe3\u0015ÔøΩÔøΩT2\u0011ÔøΩEÔøΩ\"ÔøΩP0ÔøΩÔøΩy=nÔøΩÔøΩaÔøΩY-fÔøΩ—†ÔøΩi5jÔøΩR!ÔøΩI%bÔøΩPÔøΩ2\u0004ÔøΩXÔøΩoÔøΩlwwpv7\u001bÔøΩ\n\u0019ÔøΩeÔøΩ\nÔøΩÔøΩsYÔøΩÔøΩn7T\nÔøΩrÔøΩnÔøΩl~7ÔøΩÔøΩ{rÔøΩÔøΩ\u001fÔøΩÔøΩ\u0015ÔøΩÔøΩ\u0006ÔøΩÔøΩjw\nÔøΩÔøΩÔøΩÔøΩÔøΩ>wÔøΩ/ÔøΩ|ÔøΩ^<mÔøΩTÔøΩÔøΩÔøΩÔøΩkwwÔøΩÔøΩÔøΩ6>\u0017ÔøΩW@ÔøΩÔøΩ\u0003ÔøΩÔøΩÔøΩEMÔøΩn<ÔøΩÔøΩÔøΩ=|Õ¢ÔøΩÔøΩÔøΩMpÔøΩÔøΩ2iÔøΩÔøΩqÔøΩ4\nC\u0007ÔøΩ2ÔøΩÔøΩ ÔøΩmÔøΩ8ÔøΩMÔøΩÔøΩÔøΩ\u0010SsÔøΩAÔøΩÔøΩ\nÔøΩÔøΩnÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩkÔøΩwÔøΩÔøΩ\u0004ÔøΩÔøΩÔøΩÔøΩ\n;njsÔøΩÔøΩÔøΩiÔøΩ«∫qÔøΩ\\ÔøΩÔøΩÔøΩÔøΩ7ÔøΩ[\u0015ÔøΩwAÔøΩÔøΩeÔøΩÔøΩÔøΩÔøΩ\"ÔøΩ2ÔøΩkÔøΩ”†\nÓÉ±ÔøΩ]ÔøΩÔøΩÔøΩÔøΩ’≥ÔøΩÔøΩyÔøΩysfLÔøΩfÔøΩÔøΩkhÔøΩp›¶n”ÜsÔøΩÔøΩprmÔøΩ‘≠ÔøΩoÔøΩ1]ÔøΩÔøΩk‹¥ÔøΩ’µÔøΩ›ΩwÔøΩÔøΩÀ∑zË∫Ω\nÔøΩ\u0001«íÔøΩÔøΩÔøΩ]ÔøΩÔøΩ“∑ÔøΩF4'ÔøΩFÔøΩÔøΩÔøΩG)>ÔøΩ|_3ÔøΩÔøΩÔøΩÔøΩÔøΩ-ÔøΩ\nÔøΩ-ÔøΩZ<\u001bÔøΩaÔøΩÔøΩFÔøΩÔøΩ{\u000eYÔøΩÔøΩÔøΩÔøΩYdmvwMÔøΩÔøΩÔøΩt7ÔøΩ|ÔøΩsÔøΩÔøΩ\u0007ÔøΩÔøΩkÔøΩÔøΩ\nÔøΩ\\ÔøΩ%\n;ÔøΩÔøΩ\u0014[ÔøΩRUÔøΩÔøΩ\u0015ÔøΩgÔøΩ\u000fnÔøΩsÔøΩÔøΩ4ÔøΩ:~ÔøΩ91ÔøΩ#_\nhAÔøΩ{ÔøΩ\u001bÔøΩdÔøΩ\u000f\nÔøΩÔøΩÔøΩÔøΩWÔøΩÔøΩÔøΩ’∞\u001bÔøΩÔøΩc8ÔøΩ{\nÔøΩÔøΩnIÔøΩÔøΩ.u\nÔøΩÔøΩÔøΩw\n\u0002jÔøΩÔøΩÔøΩK\u0004bÔøΩÔøΩ}~eÕúRÔøΩ0ÔøΩÔøΩ\u0012ÔøΩ,UÔøΩAÔøΩÔøΩÔøΩ\u0003ÔøΩÔøΩhÔøΩÔøΩÔøΩÔøΩÍÖ®\u0011\u0004\tÔøΩXœóÔøΩÔøΩÿö^ÔøΩwÔøΩ\nÔøΩ\u001b\u0012h>4v*\nÔøΩ^ÔøΩÔøΩ6ÔøΩxÔøΩTwÔøΩrÔøΩj(to\u00197ÔøΩXvÔøΩÔøΩmÔøΩ\u0010ÔøΩÔøΩÔøΩwÔøΩÔøΩtÔøΩÔøΩ-ÔøΩItÀñÔøΩ-ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩ…∏ÔøΩ[\nÔøΩWÔøΩÔøΩÔøΩÔøΩE5ÔøΩÔøΩÔøΩÔøΩlÔøΩ_ÔøΩÔøΩ:ÔøΩÔøΩ:nÔøΩTwsÔøΩÔøΩR€∂NÔøΩÔøΩTÔøΩ^=ÔøΩÔøΩÔøΩÔøΩÔøΩ5NelÔøΩÔøΩ#6ÔøΩÔøΩ\nÔøΩ8cpgZÔøΩ*ÔøΩf\u0003ÔøΩ/ÔøΩ5y^ÔøΩH\nÔøΩÔøΩÔøΩ`ÔøΩÔøΩnÔøΩÔøΩÔøΩuÔøΩÔøΩÔøΩÔøΩ<ÔøΩÔøΩÔøΩ\u0005=ÔøΩOÔøΩ?ÔøΩtÔøΩÔøΩ5ÔøΩ+ÀµWÔøΩÔøΩÔøΩ=y\u0017\u00037ÔøΩ\u0006IÔøΩÔøΩi]]ÔøΩ+ÔøΩ\n\u0007ÔøΩÔøΩÔøΩ\u001aÔøΩs\u000fÔøΩÔøΩ5ÔøΩÔøΩÔøΩÔøΩjÔøΩ[ÔøΩÔøΩ:\u000e\u0014ÔøΩÔøΩkEÔøΩÔøΩ\u0001ÔøΩÔøΩ\u0016NÔøΩu\u000fÔøΩÔøΩ\nb\u0011ÔøΩÔøΩS\u001bKhÔøΩ\nÔøΩÔøΩÔøΩ\u0000+m8LÔøΩÔøΩBQ/YÔøΩÈêÄÔøΩÔøΩAR\u0011ÔøΩ5F\u0016ÔøΩPÔøΩ5azÔøΩ\nÔøΩ\u00132GÔøΩ\u0017ÔøΩÔøΩÔøΩR_ÔøΩkÔøΩC\nÔøΩW_ÔøΩU:ÔøΩÔøΩx4\u0001XÔøΩaDÔøΩÔøΩÔøΩÔøΩKÔøΩ\u0000ÔøΩÔøΩÔøΩ=ÔøΩ\b0{ÔøΩÔøΩ\u0006ÔøΩÔøΩÔøΩ\u0000ÔøΩÔøΩÔøΩÔøΩ1 #kf<bÔøΩÔøΩ'ÔøΩÔøΩ>ÔøΩÔøΩ5ÔøΩÔøΩÔøΩfÔøΩÔøΩÔøΩnÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ[&ÔøΩ\b|\u0002ÔøΩSÔøΩMÔøΩÔøΩ\nÔøΩÔøΩIÔøΩeÔøΩyÔøΩ\u0005ÔøΩ5ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0018~ÔøΩÔøΩÔøΩÔøΩÔøΩ~]–õ“ß\ni\u0007\nÔøΩÔøΩ-iO:ÔøΩ\u0010gÔøΩÔøΩ\u00049]MF\u0017ÔøΩiÔøΩ~ÔøΩÔøΩÔøΩÔøΩzÔøΩRÔøΩÔøΩ!ux<ÔøΩÔøΩ∆çÔøΩzÔøΩ\u000eÔøΩ\u0004ÔøΩ\n\u0012ÔøΩÔøΩÔøΩÔøΩ\n\u0006ÔøΩ«´ÔøΩxÔøΩ>ÔøΩœ†ubÔøΩÔøΩ\u0018ÔøΩ\u0003ÔøΩÔøΩOÔøΩÔøΩx\u0011ÔøΩ€•`<\u0018ÔøΩÔøΩKÔøΩÔøΩÔøΩ1ÔøΩÔøΩÔøΩ6ÔøΩ2`CoÔøΩtÔøΩÔøΩ&cÔøΩeÔøΩ\nÔøΩÔøΩÔøΩÔøΩ,ÔøΩÔøΩÔøΩTÔøΩÔøΩeÔøΩ ZÔøΩJ5ÔøΩ\u0017\u000fÔøΩ\u0014ÔøΩSÔøΩdT8\tTÔøΩÔøΩÔøΩÔøΩUGÔøΩ;|ÔøΩ”ü`ÔøΩ1”ë9\u001aU_ÔøΩ^ÔøΩF/FÔøΩ\u0017>ÔøΩvhÔøΩÔøΩÔøΩFÔøΩÔøΩÔøΩV√ØÔøΩNÔøΩKÔøΩAfÔøΩ2\u0011\u0015oRÔøΩÔøΩÔøΩ\ne!cFÔøΩ>ÔøΩ>=ÔøΩ\u0016>X\u0012)ÔøΩu\"ÔøΩÔøΩÔøΩNT\u0007ÔøΩÔøΩÔøΩhGG\u0007ÔøΩ@ÔøΩÔøΩ\u00133BÔøΩÔøΩd4VÔøΩ<U\u0015ÔøΩUÔøΩÔøΩÔøΩPi4AUee\u0015‹£ÔøΩu-\u0014ÔøΩtCÔøΩdÔøΩÔøΩÔøΩÔøΩF\njÔøΩTÔøΩÔøΩÔøΩ+iÔøΩÔøΩÔøΩÔøΩÔøΩeÔøΩ:W0eOÔøΩÔøΩÔøΩIÔøΩ%ÔøΩ=8ÔøΩ\u001268ÏÅÄ]\u0017sÔøΩz·•Ü*ÔøΩ;A\u0002\u0001ÔøΩÔøΩAÔøΩÔøΩ|OÔøΩÔøΩÔøΩ\u0013ÔøΩÔøΩÔøΩ=\t>ÔøΩ\u0015ÔøΩÔøΩ?\u001fGÔøΩÔøΩ'\\ÔøΩ&ÔøΩÔøΩNÔøΩÔøΩ\u000fmÔøΩÔøΩ\b\nq\n\u0015zÔøΩCÔøΩ\n»ÅmvÔøΩSoÔøΩÔøΩÔøΩ\u0006_B\u0011K`ÔøΩ\u0010ÔøΩ\u0013aÔøΩ^\n9ÔøΩlFBhŸ†ÔøΩAx\u0002h`ÔøΩpÔøΩÔøΩÔøΩr+ÔøΩ8i{ÔøΩ~ÔøΩÔøΩÔøΩÔøΩ4\u001f÷™ÔøΩXﬂãW\nMHÔøΩ&}Bz\u0002ÔøΩÔøΩÔøΩÔøΩvÔøΩÔøΩFAÔøΩ;ÔøΩ/\u001fGÔøΩÔøΩ\u0016z\n=tÔøΩÔøΩÔøΩÔøΩO9Y\n*8sNÔøΩirQÔøΩ◊éÔøΩÔøΩÔøΩÔøΩÔøΩJÔøΩ\u0001ÔøΩiÔøΩÔøΩÔøΩ\u0000ZÔøΩÔøΩÔøΩOÔøΩGÔøΩ!ÔøΩÔøΩ\u0010ÔøΩÔøΩ;ÔøΩÔøΩÔøΩÔøΩfÔøΩ\u0006ÔøΩÔøΩÔøΩ>\u000fÔøΩÔøΩÔøΩUÔøΩÔøΩl&ÔøΩ\u0013`ÔøΩÔøΩÔøΩ\u0015ÔøΩÔøΩÔøΩJ-ÔøΩÔøΩÔøΩ\n\\?\nÔøΩ<6}ÔøΩÔøΩÔøΩ{ÔøΩÔøΩ'ÔøΩ1ÔøΩÔøΩÔøΩÔøΩÔøΩ¬û\u0019ÔøΩÔøΩÔøΩÔøΩ{\nZkÕ£X1yÔøΩ=gÔøΩÔøΩ\u0015ÔøΩ\u00027ÔøΩÔøΩÔøΩÔøΩÔøΩ=ptJÔøΩÔøΩ5ÔøΩÔøΩÔøΩLfNUÔøΩ,xIÔøΩ“¶v\n$CÔøΩ\u000e<ÔøΩÔøΩÔøΩ>ÔøΩ\nÔøΩÔøΩÔøΩ'ÔøΩ\tlÔøΩV\u000fÔøΩ<WÔøΩRÔøΩ\u0002fÔøΩ9ÔøΩ\"ÔøΩZLÔøΩ\u0013ÔøΩÔøΩ$$,)\nTKjÔøΩ#E-ÔøΩ\u0016ÔøΩHÔøΩtÔøΩ\u0014ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0013ÔøΩ”∫CÔøΩ—Ä:\u0004ÔøΩÔøΩ\nf%^ÔøΩÔøΩAÔøΩÔøΩÔøΩ\u001aÔøΩ\u0012ÔøΩD`CbÔøΩÔøΩÔøΩÔøΩlÔøΩXZÔøΩÔøΩ9lI\u001bcÔøΩÔøΩ|~ÔøΩH\u0010ÔøΩÔøΩÔøΩ5*ÔøΩÔøΩ@\nÔøΩ\u0010ÔøΩÔøΩ^ÔøΩFŒ§ÔøΩ2!ÔøΩÔøΩ\u0012\n€†ÔøΩÔøΩD{ÔøΩXi\tF_5Syt@CÔøΩ}\u0003ÔøΩ|ÔøΩÔøΩÔøΩ\u000f5ÔøΩ5ÔøΩÔøΩwDÔøΩ0ÔøΩ6ÔøΩ4ÔøΩÔøΩÔøΩu^ÔøΩÔøΩÔøΩƒºÔøΩ$ÔøΩ\u000fzÔøΩV$ÔøΩÔøΩjÔøΩ\nÔøΩR*ÔøΩÔøΩD^OÀá -ÔøΩ\u0010uÔøΩ\u000e\nmÔøΩÔøΩÔøΩÔøΩ*ÔøΩÔøΩÔøΩ\nf3ZE9\u0010EpsÔøΩ!\u0010\u0010/\nÔøΩÔøΩÔøΩÔøΩi#oÔøΩÔøΩÔøΩÔøΩÔøΩ{ÔøΩÔøΩÔøΩÔøΩ{\u0017/\u00186ÔøΩÔøΩ\u0015gÔøΩÔøΩÿ∏1ÔøΩ^ÔøΩ\u0011^_ÔøΩiÔøΩT;ÔøΩÔøΩYÔøΩÔøΩ]ÔøΩÔøΩÔøΩ…µ\u0013FÔøΩ\n\u0012ÔøΩj\"U\u000f5ÔøΩeﬁ£ÔøΩ:hÔøΩÔøΩÔøΩaÔøΩÔøΩÔøΩ#xÔøΩ0]%P\nGCÔøΩp=ÔøΩÔøΩ5ÔøΩÔøΩP}ÔøΩvÔøΩ√≥=ÔøΩÔøΩ<ÔøΩ?ÔøΩÔøΩÔøΩ]ÔøΩJÔøΩÔøΩE`\nÔøΩÔøΩÔøΩ[ÔøΩxCÔøΩ+ÔøΩ1\np\u0019HÔøΩÔøΩ&ÔøΩ'F>·îæÔøΩÔøΩÔøΩ0\t#\u0014VX4ÔøΩÔøΩ=ÔøΩÔøΩKE\u0001ÔøΩÔøΩ6u\u0003ÔøΩÔøΩÔøΩ\\ ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0015ÔøΩÔøΩ\u0002Z?ÔøΩhPÔøΩQÔøΩRÔøΩ*ÔøΩKA\u0014ÔøΩ2/\u0015ÔøΩS\bÔøΩ\u001aÔøΩcÔøΩÔøΩÔøΩÀÖÔøΩ\u0016ÔøΩ^aÔøΩÔøΩÔøΩmÔøΩ@hÔøΩD'\u0017ÔøΩÔøΩ\tÔøΩÔøΩ:O”æÔøΩs ÔøΩhÔøΩÔøΩÔøΩ(\nÔøΩÔøΩ\nEÔøΩtvÔøΩÔøΩnœÑÔøΩÔøΩ\u0004\u0018ÔøΩÔøΩÔøΩÔøΩÔøΩ\tr\u0016>ÔøΩÔøΩnÔøΩÔøΩ\u0003\u0012\u000e\bÔøΩÔøΩ|ÔøΩÔøΩd3!ÔøΩÔøΩ\u0001ÔøΩ0ÔøΩEÔøΩŸπg?ÔøΩÔøΩXÔøΩ4hÔøΩÔøΩ\"ÔøΩÔøΩTÔøΩÔøΩ^}ÔøΩiÔøΩuÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩ{ÔøΩ>ÔøΩÔøΩ«ß\u000fÔøΩÔøΩyÔøΩNÔøΩ;ÔøΩÔøΩ\u0019ÔøΩÔøΩÔøΩÕªw/XÔøΩ6\u001fÔøΩ~jÔøΩ\u0013ÔøΩ\u0010\u0018ÔøΩ\nEÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩ\u001aÔøΩÔøΩdÔøΩQNÔøΩÔøΩRxnÔøΩcÔøΩ\u000f\u001bMÔøΩfœ¥p8\u0015iÔøΩÔøΩ\u0015ÔøΩ^ÔøΩ\u0011ÔøΩ*\u0017ÔøΩÔøΩ*yDÔøΩrzÔøΩNÔøΩÔøΩ&sÔøΩÔøΩ=NÔøΩÔøΩÔøΩÿã\u001b\u000fÔøΩ$+\u0018y/\nqÔøΩÔøΩÔøΩÔøΩ≈æcÔøΩÔøΩZ'gÔøΩ:)ÔøΩÔøΩÔøΩfÔøΩÔøΩ,√ßÔøΩÔøΩÔøΩÔøΩ89ÔøΩÔøΩXÔøΩÈåôÔøΩÔøΩN'qZ ú&gÔøΩtÔøΩmE@*ÔøΩ?ÔøΩÔøΩbs2'ÔøΩ\nNN\u0001+z2ÔøΩÔøΩO\u000fÔøΩx8jÔøΩ`ÔøΩÔøΩÔøΩO}ÔøΩo\u0000ÔøΩ\u0006ÔøΩBÔøΩ‹Ä.ÔøΩÔøΩÔøΩÔøΩ^ÔøΩ_ÔøΩ6:ÔøΩÚàâìÔøΩ\u001b\u0014ÔøΩ\u0019Gu9ÔøΩWÔøΩÔøΩÔøΩ#ÔøΩÔøΩÔøΩS\n@\\4ÔøΩÔøΩHÔøΩ\u0012ÔøΩÔøΩ1EQT—æR\u0012S\tÔøΩÔøΩÔøΩ,~ÔøΩÔøΩÔøΩ6\u0012ÔøΩÔøΩÔøΩzMÔøΩSÔøΩ\nÔøΩ\u001a_”åÔøΩÔøΩHÔøΩrÔøΩÔøΩe36\nÔøΩ5eÔøΩÔøΩÔøΩHYM~UÔøΩ\bÔøΩZÔøΩÔøΩO0-\u001aÔøΩM\u0007\u0002ÔøΩÔøΩÔøΩkÔøΩFÔøΩ,\u0004ÔøΩÔøΩÔøΩ\u00072ÔøΩ+ÔøΩÔøΩ\u0014:ÔøΩÔøΩÔøΩ16yAqÔøΩ2VeÔøΩ\u0019T5!AXÔøΩS=ÔøΩ0IÔøΩ\u0000Vs\u0016f1(9'MÔøΩÔøΩR\"ÔøΩ?\"ÔøΩÔøΩÔøΩ#\\ÔøΩÔøΩÔøΩ$sÔøΩ52“Ö^ÔøΩ\nÔøΩXÔøΩÔøΩÔøΩ öÔøΩÔøΩgÔøΩ_ÔøΩÔøΩÔøΩÔøΩ;ÔøΩ{‹ß›¨{ÔøΩÔøΩ&ÔøΩÔøΩÔøΩeyÔøΩÔøΩK#ÔøΩ5s)X<ÔøΩÔøΩ^ÔøΩi31_ÔøΩ>ÔøΩÔøΩÔøΩ<ÔøΩ\u0012ÔøΩ/ÔøΩ<.ÔøΩÔøΩÔøΩÔøΩÔøΩ\u001fÔøΩsÔøΩ«§ÔøΩs}ÔøΩ5ÔøΩÔøΩÔøΩÔøΩ(ÔøΩ\nÔøΩ\nRÔøΩÔøΩ!ÔøΩÔøΩ~TÔøΩE>]ÔøΩÔøΩÔøΩ2hv\u0002‘ªÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ*ÔøΩÔøΩ!\u0001ÔøΩÔøΩÔøΩÔøΩ\u0016GÔøΩÔøΩÔøΩ/ÔøΩ2rÔøΩG\u001b7\u0005ÔøΩ\u0004ÔøΩ+ÔøΩnWÔøΩ{*ÔøΩXÔøΩ*\u0017jÔøΩ\u000ef{ÔøΩÔøΩÔøΩmhJyÔøΩ*wÔøΩÔøΩ-zÔøΩ+OÔøΩ\u0007oÔøΩT\u001fÔøΩ÷ΩÔøΩ8;ÔøΩ6\u0001\u0013JÔøΩÔøΩSÔøΩÔøΩ6T\nzÔøΩ\u001b*&\u0012F(eÔøΩÔøΩÔøΩ»ºÔøΩvH{ÔøΩÔøΩÔøΩY8ÔøΩÔøΩnÔøΩUwÔøΩyÔøΩÔøΩQÔøΩÔøΩZQÔøΩÔøΩÔøΩ2BÔøΩbÔøΩ\"ÔøΩÔøΩÔøΩÔøΩ\u0013IÔøΩrÔøΩ_&b\u0004\u0002ÔøΩÔøΩÔøΩÔøΩ)\nhE[Z'dD[\u001aÔøΩwÔøΩÔøΩÔøΩÔøΩÔøΩbUÔøΩÔøΩ\u0010\u000feÔøΩÔøΩmÔøΩkÔøΩ\"ÔøΩ\n\u0016gÔøΩÔøΩGSÔøΩC{ÔøΩXÔøΩj+ÔøΩÔøΩ≈¢>S»†ÔøΩÔøΩÔøΩ)4\u001fÔøΩ1t≈∑\nÔøΩ8÷Éq\nÔøΩ\u0003\u0011ÔøΩ-ÔøΩkÔøΩÔøΩÔøΩŸì?wÔøΩÔøΩ\u0017ÔøΩ«∂[nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ~5ÔøΩÂéØÔøΩÔøΩÔøΩÔøΩÔøΩp`«Ω8ÔøΩ¬èÔøΩwÔøΩÔøΩO{ÔøΩBÔøΩ$EÔøΩ\u0010ÔøΩ¬ãÔøΩ\nÔøΩÔøΩNNÔøΩÔøΩÔøΩ\\\n\u0016)h9ÔøΩÔøΩN_ÔøΩ\u0007ÔøΩ\u0017'ÔøΩÔøΩFlIvÔøΩhÔøΩdÔøΩsÔøΩ{=ÔøΩ\u0018ÔøΩÔøΩ|&ÔøΩTÔøΩ\u00049\u0011ÔøΩÔøΩÔøΩ3¬ß03M2ÔøΩ9ÔøΩÔøΩ\\#ÔøΩFÔøΩÿπ^ÔøΩﬁπ1(ÔøΩ\\6.ÔøΩ4»åGXÔøΩduÔøΩ`L ¢QÔøΩÔøΩW6«ßÔøΩÔøΩÔøΩÔøΩ6ÔøΩ}1ÔøΩÔøΩ{|ÿófY$\u0012ÔøΩÔøΩ.ÔøΩ[oÔøΩÔøΩQ\u0014UF]ÔøΩÔøΩ>\u0006>mTjÔøΩŸ™ÔøΩn[<ÔøΩÔøΩwÔøΩ\u0000Oz‰¢ÄÔøΩ\u0017ÔøΩ\u000fÔøΩ\u00036ÔøΩÔøΩ3ÔøΩÔøΩ^ÔøΩÔøΩPÔøΩÔøΩÔøΩIÔøΩGQ¬†\u0018‘örÔøΩÔøΩF>ÔøΩÔøΩ\u0013ÔøΩ2|\nÔøΩWÔøΩÔøΩjNgÔøΩfÔøΩÔøΩpÔøΩRe3ÔøΩLÔøΩ*iÔøΩmÔøΩÔøΩ\u0012ÔøΩ\u0000ÔøΩÔøΩ;w!\n^ƒë\u0001RSÔøΩUp\u0019ÔøΩÔøΩ\u0014ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ8ÔøΩ`\u0019\u0004ÔøΩÔøΩjÔøΩ3ÔøΩA^ÔøΩÔøΩ]\u000eÔøΩ_\tÔøΩÔøΩÔøΩÔøΩnÔøΩTÔøΩ`*ÔøΩ\u000f3ÔøΩÔøΩ\"ÔøΩÔøΩoÔøΩ\"ÔøΩ\u001a\u0011ÔøΩÔøΩRQ\u0014ÔøΩgn}ÔøΩg3 á:6\u0018ÔøΩ\u0012ÔøΩ&ÔøΩÔøΩÔøΩPÔøΩ\bƒºÔøΩ\u0019ÔøΩ:{hÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩnÔøΩUÔøΩ\nÔøΩÔøΩ‰∞©{XÔøΩn^~zÔøΩZÔøΩSÔøΩMÔøΩ›ö\n≈ÉÔøΩÔøΩÔøΩÔøΩÔøΩ2ÔøΩÔøΩ\nÔøΩÔøΩIÔøΩ&+;MF\u000f+ÔøΩ\u0003ÔøΩÔøΩÔøΩÔøΩp\u0013ÔøΩ—°ÔøΩ\u001b»µfÔøΩJW\u001fÔøΩ+\u0015ÔøΩÔøΩÔøΩ6PÔøΩÔøΩ\t\u0019ÔøΩWÔøΩ;\u001aÔøΩk9ÔøΩMzDÔøΩÔøΩLhX#]\u0013a&ÔøΩV4\u0012ÔøΩqÔøΩ\tÔøΩ\u000e\nÔøΩHÔøΩT\u0003ﬁÜÔøΩmuXÔøΩÔøΩQGÔøΩÔøΩ\u0006ÔøΩ\nÔøΩuÔøΩH\bÔøΩ\nfÔøΩÔøΩaÔøΩÔøΩ¬éHÔøΩÔøΩÔøΩ\u0001ÔøΩ^ÔøΩt\u0013=\n<ÔøΩÔøΩsÔøΩÔøΩÔøΩÔøΩt9F^ÔøΩ/IÔøΩbsM.\u0007ÔøΩ\u0007)ÔøΩD,\u0016ÔøΩ8ÔøΩ\n\u0016&ÔøΩÔøΩZ;T\u000eÔøΩÔøΩ\u0010\nrÔøΩÔøΩÔøΩ\u0007ÔøΩrÔøΩ\u0019iQÔøΩpÔøΩXEe&ÔøΩ\u0005ÔøΩiÔøΩ\u0011ÔøΩDÔøΩOÔøΩ\u0011>ÔøΩ\nÔøΩlÔøΩ⁄º-ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ/ÔøΩÔøΩ4ÔøΩcN:LÔøΩ6ÔøΩMÔøΩÔøΩ0)\u0014\u000f.ÔøΩ5\u0005TÔøΩJÔøΩt$ÔøΩ\u0019\u0003\nÔøΩÔøΩ$`ÔøΩ“úSKK<yNÔøΩÔøΩsqÔøΩÔøΩŸêﬁ¢ÔøΩÔøΩ«ÄIÔøΩÔøΩÔøΩsÔøΩÔøΩÔøΩÔøΩ#N\u001fÔøΩÔøΩÔøΩÔøΩ\u0003Ã∫ÔøΩG\tvﬂÖsÔøΩzxzÔøΩÔøΩkÔøΩ\u0003ÔøΩKÔøΩÔøΩr@3ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ?TÔøΩÔøΩŸ∂¬°gÔøΩ\u0018‘Å\u0007\nÔøΩ8R\u0001[TÔøΩÔøΩ!ÔøΩArPÔøΩÔøΩÔøΩÔøΩÔøΩ+eÔøΩ\u0007\nRÔøΩR≈¢\nÔøΩ~RÔøΩÔøΩ{~XÔøΩÔøΩÔøΩ7dÔøΩ\\ÔøΩ%3ZJÔøΩÔøΩfoÔøΩÔøΩÔøΩwfÔøΩÔøΩÔøΩÔøΩP^e\nkÔøΩÔøΩ\nÔøΩ;6:2ÔøΩÔøΩÔøΩÔøΩ+ÔøΩXÔøΩ:kÔøΩ\nÔøΩ%ÔøΩ&'&YÔøΩ;itÔøΩÔøΩy<ÔøΩÔøΩJk@\u001b\b`ÔøΩÔøΩÔøΩ\\sÔøΩ\nbÔøΩÔøΩ(\u0000ÔøΩ_k\u000f›åÔøΩÔøΩÔøΩÔøΩK\nVDnd*ÔøΩeJÔøΩÔøΩ=ÔøΩ\nÔøΩQcÔøΩÔøΩÔøΩ\u0011ÔøΩ\u0004ÔøΩ*¬çÔøΩ ÔøΩE.ÔøΩÔøΩŸâhÔøΩÔøΩÔøΩÔøΩ\u001b\u0013+uÔøΩR2N6VÔøΩÔøΩ\nÕÜia{ÔøΩÔøΩ∆èÔøΩ8\u0018\u0017ÔøΩ1ZÔøΩA%ÔøΩ:ÔøΩ\u0006ÔøΩÔøΩÔøΩ\nÔøΩÔøΩWÔøΩ\u0012ÔøΩÔøΩÔøΩ…ôÔøΩ\u0016iÔøΩiÔøΩÔøΩZÔøΩÔøΩ≈µÔøΩÔøΩ)—öÔøΩNÔøΩÔøΩÔøΩ%ÔøΩ\u0001ÔøΩPtÔøΩÔøΩÔøΩ\n`EÔøΩÔøΩÔøΩÔøΩ[\u001aÔøΩ\\Xœóz\u0002ÔøΩbÔøΩÔøΩgÔøΩZ\u000fÔøΩRsÔøΩÔøΩ‘∂ÔøΩv–™ÔøΩQJ\u0013ÔøΩ*ÔøΩ{\u001f1Á¥úÔøΩ\nu\u0002ÔøΩÔøΩÔøΩÔøΩÔøΩyÔøΩ\u000fÔøΩ\u0011@Y\u0011ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ-\u0019ÔøΩÔøΩÔøΩÔøΩ0ÔøΩÔøΩÔøΩ(ÔøΩ\u000eÔøΩÔøΩ\u0003`ÔøΩ\u0001\u0012DÔøΩÔøΩÔøΩYtJÔøΩt9Á¨±ÔøΩÔøΩ`ÔøΩ\u001bÔøΩ<ÔøΩÔøΩ))ÔøΩÔøΩ›¥vÔøΩJÔøΩÔøΩLv›ñ\nfÔøΩÔøΩ\u0015ÃæU\u001aÔøΩK\u0014\u0000/ÿô\u001fÔøΩÔøΩ\n,ÔøΩ\u000fÔøΩÔøΩ\t\\ÔøΩ$|ÔøΩÔøΩQÔøΩÔøΩÔøΩ\nAÔøΩ–äÔøΩ+ÔøΩ\u001bFÔøΩM\u0012ÔøΩWÔøΩVÔøΩÔøΩÔøΩ\u0010>.>\u00199^ÔøΩÔøΩUÔøΩÔøΩ\nÔøΩ&+ÔøΩ\u0014ÔøΩx9ÔøΩ`ÔøΩ?]ÔøΩT\"ÔøΩ’î2\u0002ÔøΩM\u0019|Õ∂^ÔøΩ,ÔøΩ\n{SÔøΩ6ÔøΩÔøΩÔøΩ FÔøΩd/s\u0007ÔøΩÔøΩ(ÔøΩÔøΩ»ßtYmzÔøΩÔøΩ\u0016ÔøΩKepÔøΩ äÔøΩ\u0015aÔøΩU\n;ÔøΩ‹àD`@\u0012Y\u0011ÔøΩ#ÔøΩb\u0012ÔøΩ\u0013ÔøΩÔøΩŒä8WvÔøΩ\bsÔøΩÔøΩÔøΩ\u0015ÔøΩÔøΩÔøΩÔøΩ\"ÔøΩÔøΩ\u0004S\nÔøΩ\u0011=lÔøΩÔøΩzÔøΩÔøΩ€±Ú¨ï¢\u0002ÔøΩsZ;\u0014lÔøΩÔøΩy+ÔøΩY*ÔøΩ&kÔøΩÔøΩÔøΩÔøΩ98ÔøΩÔøΩ\n%`ÔøΩÔøΩÔøΩ9ÔøΩÔøΩÔøΩ/ZÔøΩh…ØÓÑÆÔøΩÔøΩw9ÔøΩ\u0005ÔøΩ3`|\u0006ÔøΩÔøΩÔøΩ]zÔøΩ\u0012ÔøΩt]›¶3ÔøΩÔøΩBTYÔøΩ7\b\u0004\u0016ÔøΩ&\u001fÁ®¨ÔøΩÔøΩÔøΩÔøΩ@ÔøΩÔøΩLÔøΩÔøΩ\u0013ÔøΩ6/ÔøΩÔøΩÔøΩ~C~ÔøΩÔøΩÔøΩÔøΩR-FSÔøΩMÔøΩÔøΩH;ÔøΩÔøΩÔøΩM.ÔøΩ#smÔøΩ5ÔøΩÔøΩ\nÔøΩ\u000fmI/ÔøΩmÔøΩÔøΩÔøΩÔøΩ\u0005wtÔøΩ)ÔøΩ3ÔøΩÔøΩÔøΩÔøΩ&ÔøΩK\u0013\bÔøΩ’õÔøΩÔøΩ\u0013ÔøΩ6ÔøΩ(‘Ø\u001bUÔøΩrÔøΩ\u001bsÔøΩNÔøΩÔøΩ\u0019ÔøΩ\nÔøΩFÔøΩ}\u0002lL\u0010ÔøΩq\u0017ÔøΩIlÔøΩÔøΩÔøΩÔøΩFÔøΩÔøΩeÔøΩÔøΩÔøΩÔøΩbÔøΩ\u0011ÔøΩuÔøΩÔøΩÔøΩ/ÔøΩ\nÔøΩWvÔøΩtÔøΩÔøΩTBÔøΩÔøΩ\bÔøΩ\"oÔøΩ3L(ÔøΩfÔøΩ:ÔøΩC.cÔøΩ\u000eÔøΩUoqXÔøΩﬁÄ#\b\u0014√©ÔøΩÔøΩu:ÔøΩÔøΩÔøΩu\u0002/\u0001\u000eÔøΩÔøΩÔøΩdÔøΩ\u001aI`\n^dP.ÔøΩ…êÔøΩKÔøΩs1]yXÔøΩÔøΩRÔøΩ\nÔøΩ\u0018\u0018ÔøΩxÔøΩIÔøΩ;ÔøΩÔøΩÔøΩ\n)6\n]QÔøΩ+ÔøΩÔøΩ4ÔøΩÔøΩ'ÔøΩ#:JÔøΩu%ÔøΩÔøΩ)'\u0001ÔøΩÔøΩq*KÔøΩÔøΩ\bÔøΩ4ÔøΩA^7‘úÔøΩÔøΩÔøΩÔøΩÔøΩ\u0001ÔøΩÔøΩÔøΩ*ÔøΩÔøΩUÔøΩMpÔøΩ-:ÔøΩÔøΩ!ÔøΩrÔøΩŒ§+ÔøΩÔøΩÕôg&ÔøΩÔøΩ@ÔøΩÔøΩ\"O\"ÔøΩÔøΩÔøΩÔøΩÔøΩ‹ÄeÔøΩÔøΩ0u\u001fÔøΩÔøΩ\u0015VÔøΩÔøΩb1WhÔøΩÔøΩÔøΩ\u0012JL:\u0000ÔøΩCÔøΩ\u0018ÔøΩ\u000f`[TÔøΩÔøΩÔøΩ ÔøΩÔøΩ\bÔøΩpÔøΩÔøΩSÔøΩa&ÔøΩ7zÔøΩjÔøΩÔøΩxÔøΩÔøΩÔøΩ_ÔøΩQ÷àÔøΩÔøΩÔøΩ3ÔøΩ\u000eÔøΩÔøΩ\u0013\u001fÔøΩyÔøΩÔøΩ-ÔøΩÔøΩKdÔøΩSÔøΩÔøΩ\u0017~ÔøΩiÔøΩ;FÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩUefÔøΩ8\u0010`ÔøΩ\u001aGr-S1>ÔøΩÔøΩpÔøΩÔøΩ7ÔøΩÔøΩ»ØÔøΩgÔøΩ0@ÔøΩ(9ÔøΩsÔøΩ\u001a1eÔøΩS\u0000ÔøΩ\u0017ÔøΩ7\"'\u0016\nG\u0002`\nÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\u0004ÔøΩÔøΩwJfÔøΩÔøΩ\n]ƒåÔøΩQÔøΩÔøΩ+\u0019ÔøΩÔøΩÔøΩ&ÔøΩÔøΩÔøΩÔøΩƒâÔøΩ*ÔøΩ\nD\"ÔøΩ;mÔøΩjw8ÔøΩ\u0002\nÔøΩ≈Ä\n\u0016\nÔøΩ2ÔøΩÔøΩÔøΩ\u0004\nu2ÔøΩÔøΩ.ÔøΩ\u0005:ÔøΩÔøΩÔøΩ\n[ÔøΩj“¢b\\\n)0ÔøΩYÔøΩF\u0007DÔøΩ.ÔøΩ\tP\n\u0011aÔøΩ\n\u0019ÔøΩÔøΩ\\ÔøΩÔøΩm#1ÔøΩhÓíÄ{ÔøΩ\u0002ÔøΩyWUeÔøΩÔøΩmÔøΩÔøΩÔøΩÔøΩÔøΩ~\n8ÔøΩÔøΩ\u0007=G#\u0006ÔøΩr\u001bÔøΩÔøΩ\u0001ÔøΩÔøΩÔøΩÔøΩÔøΩ\n\u0010ÔøΩ\u0000'ÔøΩ”ö\nÔøΩ\u00106\nÔøΩÔøΩÔøΩuÔøΩiÔøΩËÑéLÔøΩcÔøΩÔøΩÔøΩ\u001f9ÔøΩÔøΩvfbÔøΩÔøΩQÔøΩ#/ÔøΩÔøΩ2KÔøΩ<ÔøΩ3c|u.VÔøΩ~ÔøΩ\u0001ÔøΩÔøΩLcoÔøΩÔøΩÔøΩhÔøΩ]ÔøΩÔøΩÔøΩ\u0017ZnÔøΩ\nVE+ÔøΩÔøΩ∆êsÔøΩÔøΩÔøΩÔøΩÔøΩ\u0000]9ÔøΩÔøΩ\bÔøΩÔøΩ\u001bJÔøΩSÔøΩÔøΩ2\u000eObfJfJÔøΩGÔøΩ1/ÔøΩÔøΩÔøΩÔøΩÔøΩJnÔøΩÔøΩ\u0016ÔøΩÔøΩvÔøΩ!ÔøΩ8].ÔøΩÔøΩ=\nGRÔøΩ\n\u0003s#. I\u0005VÔøΩÔøΩ=ÔøΩW\u001f\u0010\nÔøΩ(ÔøΩa'ÔøΩÔøΩÔøΩ\u0001bÔøΩqÔøΩÔøΩaÔøΩ[ÔøΩÔøΩpÔøΩÔøΩ\u0012FrÔøΩpÔøΩEÔøΩCÔøΩÔøΩÔøΩ\u0014#\u0013ÔøΩÔøΩÔøΩÔøΩ\n…∏ÔøΩXÔøΩÔøΩÔøΩaÔøΩÔøΩÔøΩrCÔøΩ›ë\u0001~ÔøΩÔøΩk=ÔøΩ~\u001bÔøΩ\\p\nÔøΩCÔøΩv$ÔøΩ6ÔøΩÔøΩ.ÔøΩÔøΩ;KÔøΩ\u0004z\toÔøΩ'ÔøΩEÃÑjÃ∑ÔøΩeÔøΩÔøΩoÔøΩR\u0007ÔøΩFu\u0001F[UÔøΩ}\n&\u0014ÔøΩ\nWaÔøΩWÔøΩÔøΩ|ÔøΩBÔøΩ~◊Å;ÔøΩUxÔøΩ^sÔøΩÔøΩaÔøΩH,”®ÔøΩÔøΩ\ts úeBÔøΩCÔøΩÔøΩÔøΩ2\nÔøΩÃ∏ÔøΩ\u0015GÔøΩ6ÔøΩ\u0002ÔøΩÔøΩÔøΩNÔøΩN(\u0016+ÔøΩÔøΩ\u001f\u00196aÔøΩÔøΩ\u0006ÔøΩliB+WKhDrCÔøΩCÔøΩ+ÔøΩ%HÔøΩÔøΩÔøΩGGÔøΩ^ÔøΩ\u001b\u0013ÔøΩÔøΩ#!ÔøΩÔøΩÔøΩÔøΩkÔøΩMÔøΩÔøΩÔøΩVÔøΩD2E9A?ÔøΩ0ÔøΩ1ÔøΩ9ﬂ∞ÿæÿµFÔøΩÔøΩ~ÔøΩÔøΩVÔøΩNÔøΩ\u0003ÔøΩÔøΩtÔøΩÔøΩw;\u000e\u001bN9^vÔøΩDJÔøΩÔøΩh*\u0010cÔøΩ\u0010KLÔøΩÔøΩfN#ÔøΩÔøΩ997#+ÔøΩFÔøΩÔøΩÔøΩÔøΩÔøΩ‹¥ÔøΩÔøΩrÔøΩÔøΩ\n,ÔøΩaXÔøΩ»≤t\u001bÔøΩYÔøΩ,ÔøΩ~ÔøΩ?\u0003ÔøΩ\nuÔøΩ$ÔøΩiÔøΩ)\u001fÔøΩ,ÔøΩRp9uÔøΩyÔøΩÔøΩÔøΩÔøΩÔøΩ~ÔøΩWÔøΩ}ÿä\u000fÔøΩ2ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u001fÔøΩÔøΩÔøΩcyÔøΩÔøΩ2ÔøΩÔøΩÔøΩÔøΩ\u0019ÔøΩ<0ÔøΩÔøΩÔøΩÔøΩOÔøΩ#XÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ=ÔøΩvÔøΩ[ÔøΩ\n^ÔøΩÔøΩmÔøΩÔøΩ\u000fÔøΩÔøΩÔøΩÔøΩÔøΩ?N[ÔøΩ\n?ﬂªÔøΩ◊•ÔøΩÔøΩ\u0019ÔøΩÔøΩ:ÔøΩ\u0000wÔøΩÔøΩÔøΩRCÔøΩÔøΩÔøΩÔøΩƒ∫ÔøΩÔøΩÔøΩÔøΩ\nÔøΩ\u0017\n?EÔøΩ\u001a$ÔøΩ\u0012\u0013k\u0017KÔøΩVÔøΩD4ÔøΩÔøΩTÔøΩ:\u0003ÔøΩ#9ÔøΩ_ÔøΩsÔøΩÔøΩÔøΩÔøΩóÜãÔøΩ/s\"}m]ÔøΩN\"\nÔøΩÔøΩs5FÔøΩÔøΩÔøΩPÔøΩ\nÔøΩlC\u0012\u0015\u0015%\u000f5*ÔøΩCBÔøΩ0.]\nÔøΩÔøΩ:ÔøΩRfÔøΩ\n\u0001\u001fÔøΩNÔøΩÔøΩvIf3ÔøΩÔøΩ\tÔøΩÔøΩ\u0017uÔøΩÔøΩÔøΩ8ÔøΩ%ÔøΩÔøΩ\nYÔøΩnÔøΩnÔøΩÔøΩNÔøΩ\nÔøΩ@ÔøΩK2ÔøΩÔøΩ\"`ÔøΩ\u00128\u0011pÔøΩÔøΩ⁄ä>*Mz`w>ÔøΩ\u001bÔøΩi,√ßÔøΩÔøΩ\u0017Œ§lÔøΩmÔøΩÔøΩÔøΩYÔøΩHl&ÔøΩ\t.*]ÔøΩÔøΩ; ÔøΩÔøΩÔøΩq>m\u0003\u00020J4=h,ÔøΩÔøΩÔøΩ\n}\nÔøΩ>~OÔøΩ!ÔøΩp!.√ùÔøΩÔøΩ\u0001;ÔøΩ\u0007,a\u0001ÔøΩÔøΩpCÔøΩCÔøΩOPÔøΩÈ†ÅeÔøΩl0ÔøΩr\u0006@\u0004ÔøΩÔøΩjaIÔøΩÔøΩeÔøΩ]v`ÔøΩÔøΩHÔøΩ\u0000*\u0001\"]ÔøΩÔøΩ\nE4ÔøΩ|ÔøΩÔøΩ[OÔøΩÔøΩÔøΩ3?\n)dÔøΩÔøΩ\u0004ÔøΩ\tU\u0015ÔøΩÔøΩB›∫Ô£´\u0013\\lÔøΩ[#5ÔøΩÔøΩrÔøΩNÔøΩ*\u0003ÔøΩÔøΩÃêÔøΩÔøΩY|pÔøΩP#ÔøΩÔøΩÔøΩ¬≠>ÔøΩ∆µ‘¥ÔøΩ6\u0018ÔøΩyÔøΩG}ÔøΩÔøΩÔøΩ\n.ÔøΩÔøΩ0ÔøΩ.ÔøΩÔøΩ5ÔøΩÔøΩÔøΩÎ™¢ÔøΩ@ÔøΩÔøΩÀ∑ÔøΩtÔøΩaÔøΩÔøΩ{ÔøΩiOKÔøΩ&ÔøΩ\u00184)ÔøΩÔøΩÔøΩ=ÔøΩ,qÔøΩ=Y|\nÔøΩÔøΩl\u0017~ÔøΩvoŸÉÔøΩ\u0017*ÔøΩFe)ÔøΩÔøΩLr}ÔøΩS∆ß“§ÔøΩlÔøΩÔøΩ»ΩÔøΩÔøΩ\\ÔøΩ\ngÔøΩt[\u000e2\nÔøΩ1ÔøΩY&fH\nÔøΩ9( °ÔøΩ\n7ÔøΩ\u0017ÔøΩ$ÔøΩ`\u0002\u0004\u0013ÔøΩe \u00184ÔøΩ\u0015ÔøΩPÔøΩ<\u00180ÔøΩiCÔøΩÔøΩ\u0011ÔøΩevq:\u0014ÔøΩ{ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0010ÔøΩ\u001b\nÔøΩdoÔøΩO=ÔøΩ€ÅÔøΩ&8ÔøΩ’™ÔøΩ\u0019*ÔøΩAÔøΩZÔøΩ%\u00197\"ÔøΩ0ÔøΩ\u0000ÔøΩÔøΩ\nÔøΩhÔøΩfÔøΩPÔøΩw\u00072»†6ÔøΩ\nÔøΩ]ÔøΩ\u0010‹ñC\u0013ÔøΩÔøΩÔøΩÔøΩ.\u0014\u0003ÔøΩÔøΩE\u000ePÔøΩD&ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩcÀÑÔøΩ\nÔøΩy\u0007qÔøΩW\u001aLÔøΩJiÔøΩÔøΩWD[yoÔøΩ\n46ÔøΩ@\u000fÔøΩ\u0001gÔøΩOÕ•2ÔøΩÔøΩÔøΩÔøΩwÔøΩ)ÔøΩ\bÔøΩÔøΩÔøΩR<ÔøΩ\u0018ÔøΩÔøΩÔøΩBÔøΩbÔøΩ>uÔøΩÔøΩqYÔøΩÔøΩÔøΩ\u0001ÔøΩ+QYÔøΩ9ÔøΩÔøΩ\u0015ÔøΩ^\u0011ÔøΩ1ÔøΩAÔøΩ.\u0006u-ÔøΩÔøΩ(\u001aÔøΩXY\nÔøΩhÔøΩ.ÔøΩÔøΩÔøΩ]3nÔøΩÔøΩS([x\u0007e`\t\u0017>ÔøΩÔøΩÔøΩj\u001aW\u0007\u001f\u0006ÔøΩQÔøΩSÔøΩÔøΩÔøΩ,ia’Äf\u0016QÔøΩT%\u0002p'%kJqÔøΩ«Ø\u001a\u0019ÔøΩXÔøΩ0ÔøΩÔøΩzÔøΩÔ©èFÔøΩÔøΩÔøΩÔøΩ\n3ÔøΩeÔøΩ{W-\u00182ÔøΩÔøΩÔøΩ)7rÔøΩÔøΩÔøΩÿ≤-ÔøΩÔøΩ1ÔøΩÔøΩÔøΩMWHÔøΩrÔøΩÔøΩ1›ºhY4ÔøΩ\n€∫ÔøΩ)ÔøΩ~ÔøΩ\n<gÔøΩDnÔøΩJ{›§ÔøΩÔøΩÔøΩMc\nÔøΩÔøΩÔøΩÔøΩFS\u001bÔøΩ,|ÔøΩN\u0006ÔøΩÔøΩ≈ìÔøΩ[oÔøΩ›ò$3\u00153ÔøΩ3UÔøΩ\u0015ÀïÔøΩUÔøΩ’õ\u0014ÔøΩÔøΩÔøΩU\u001bÔøΩ[ÔøΩ[\u0012ÔøΩ(\nU>ÔøΩRÔøΩQÔøΩ\"\u0013ÔøΩ\u0018_ÔøΩ\u0017ÔøΩ^ÔøΩQÔøΩ2ÔøΩ]|[ÔøΩmÔøΩÂªïÔøΩ’ª*ÔøΩC/ÔøΩ\u000f(\u000fÔøΩ^P?ÔøΩx>y\nÔøΩ\"?ÔøΩ<ÔøΩÔøΩI\nM^H8ÔøΩÔøΩqÔøΩÔøΩÔøΩ\tÔøΩÔøΩÔøΩIÔøΩPo“èÔøΩÔøΩTÔøΩRﬂí\u0010ÔøΩÔøΩ\u0004+\n;z\u0019''\nÔøΩ3ÔøΩÔøΩ{<\u0006ÔøΩÔøΩÔøΩ\u0013ÔøΩÔøΩÔøΩR#ÔøΩÔøΩdÔøΩD\nÔøΩJ_pÔøΩR)ÔøΩÔøΩ]ÔøΩÔøΩÔøΩyÔøΩ€ºÔøΩxÔøΩ\u0011ÔøΩÁ¨áx(ÔøΩÔøΩr4ÔøΩÔøΩ24ÔøΩlÔøΩP&ÔøΩiÔøΩlÔøΩ0\nÔøΩÔøΩ\nZ.ÔøΩÔøΩÔøΩÔøΩl√µÔøΩ0=p\u000f:\nÔøΩÔøΩzGÔøΩ#ÔøΩLYt\u0002\u0003RÔøΩ ÉÔøΩÔøΩA\u0015\u0010qÔøΩ9ÔøΩ\u0017ÔøΩ}ÔøΩÔøΩiÔøΩÔøΩAÔøΩ-ÔøΩ@ÔøΩÔøΩÔøΩh\u0002\u0019 ò(RÔøΩÔøΩÔøΩ\u0018ÔøΩIWQ\u0002!jÔøΩÔøΩ\u0014f6ÒÉãïU\u0015ÔøΩÔøΩ\u0004DÔøΩ\u001bÔøΩÔøΩÔøΩhÔøΩÔøΩÔøΩ+WxE\u0007ÔøΩyÔøΩ+ÔøΩÔøΩÔøΩÔøΩÔøΩ;ÔøΩTÔøΩÔøΩÔøΩÔøΩ\bÔøΩ\u001aGÔøΩ=ÔøΩÔøΩÔøΩÔøΩyÔøΩM\u0013ptÔøΩ_ÔøΩÔøΩ[ÔøΩ2dÔøΩÔøΩÔøΩÔøΩW-ÔøΩÔøΩ/<~MSÔøΩÔøΩ_ÔøΩKÔøΩkÔøΩÔøΩÔøΩLÔøΩ5\u0011∆ôÔøΩÔøΩÔøΩ\u0005ÔøΩÔøΩÔøΩ0vÔøΩÔøΩÔøΩÔøΩÔøΩO_ÔøΩÔøΩÔøΩ∆®j,ÔøΩ?\u0006ÔøΩÔøΩÔøΩcÔøΩwÔøΩÔøΩc\u0018ÔøΩ9u6ÔøΩÔøΩaQfÔøΩÔøΩTÔøΩ(ÔøΩÔøΩÔøΩÔøΩÔøΩkÔøΩ-ÔøΩÔøΩÔøΩbÔøΩDÔøΩÔøΩ\nÔøΩÔøΩ,\u0012aÔøΩÔøΩ\nÔøΩfÔøΩ\"ÔøΩ[ÔøΩÔøΩ&ÔøΩ\nZF\nÔøΩ*dÔøΩ>:\nÔøΩ\u001f*ÔøΩÔøΩ1|[0{ÔøΩ\\0\u0013ÔøΩÔøΩ3o1ÔøΩe~ÔøΩ,0[√Æ.7j\tÔøΩÔøΩr}ÔøΩ!8&8+x*ÔøΩ\u0006O2>JoÔøΩÔøΩÔøΩÔøΩDÔøΩ]<\u000fOÔøΩ,ÔøΩ\bÔøΩ.8ÔøΩXÔøΩ\u0016ÔøΩ]ÔøΩnÔøΩÔøΩNÔøΩ97ÔøΩÔøΩ%ÔøΩÔøΩPÔøΩ\u0007t\u0016D\u0007oÔøΩÔøΩ@NÔøΩ‘ºÔøΩÔøΩ1K`ZÔøΩgÔøΩÔøΩÔøΩg\u0001ÔøΩ%1/%jM`#ÔøΩÔøΩ\u0005xyÔøΩÔøΩ.\u0006ÔøΩ\u0007/%ÔøΩtQÔøΩÔøΩ#ÔøΩ@bÔøΩÔøΩÔøΩÔøΩ4bÔøΩ&ÔøΩ-zÔøΩBÔøΩQÔøΩ+ÔøΩÔøΩ÷ùÔøΩÔøΩÔøΩ\bÔøΩG[j,\tÔøΩÔøΩ~T[ÔøΩÔøΩW]ÔøΩÔøΩÔøΩÔøΩÔøΩaÔøΩÔøΩÔøΩTÔøΩ.oÔøΩÔøΩHÔøΩÔøΩcÔøΩÔøΩÔøΩdÔøΩÔøΩ\u0015\u0012ÔøΩ\u0014^ÔøΩ\u000fgÔøΩÔøΩÔøΩ$\u001aÔøΩFc)ÔøΩÔøΩÔøΩÔøΩqÔøΩ>ÔøΩ~„ìç/ÔøΩÔøΩ÷ûrÔøΩÔøΩÔøΩiÔøΩj\u0017ÔøΩ\u0017ÔøΩÔøΩWÔøΩoÔøΩÔøΩÔøΩBÔøΩPÔøΩRÔøΩÔøΩGÔøΩÔøΩÔøΩGÔøΩ4ÔøΩÔøΩÔøΩzÔøΩ9ÔøΩÔøΩ\u0012ÔøΩ2\u0015ÔøΩbÔøΩÔøΩ2ÔøΩrÔøΩÔøΩk\u0015ÔøΩÔøΩÔøΩÔøΩoVi5ÔøΩÔøΩ$…äÔøΩÔøΩÔøΩzÔøΩ‹áwÔøΩ#ÔøΩÔøΩÔøΩÔøΩrdG)@aÔøΩIsÔøΩÔøΩÔøΩ∆∑42ÔøΩÔøΩN\u0003ÿïP\u001bÔøΩ\u001b:B\u001bZ\u0015\ngÔøΩ\u0011ÔøΩv\u0004GÔøΩÔøΩÔøΩ9ÔøΩoVÔøΩÔøΩO\u00018)v~ÔøΩÔøΩ\u0007ÔøΩ,ÔøΩh\n$!ÔøΩ\tÿÖ> ÔøΩ}9(ÔøΩ\u0006}>ÔøΩ\u001b(9hÔøΩ*ÔøΩ\n|ÔøΩv UÔøΩÔøΩÔøΩ4ÔøΩ\u0007ÔøΩÔøΩ\u0016ÔøΩJCÔøΩ<]3QÔøΩÔøΩÔøΩ\u0004\u0015\t\bÔøΩOÔøΩ%ÔøΩÔøΩD\u0003=ÔøΩZm,,\u0001,\u0015`(ÔøΩF\nÔøΩÔøΩVtÔøΩtqVÔøΩoÔøΩÔøΩ„ôäÔøΩÔøΩÔøΩqÔøΩ/ÔøΩÊ¢∂\u001b\u0012ÔøΩnÔøΩÔøΩÔøΩoÔøΩjÔøΩ\nok\u001fÔøΩyÔøΩÔøΩÔøΩÔøΩ‹¶pÔøΩ3nBÔøΩÔøΩgÔøΩ|ÔøΩui-”∫ÔøΩ2yÔøΩ\n2wLÔøΩÔøΩ{4ÔøΩ`&ÔøΩ<~[[ÔøΩÔøΩlÔøΩ√©\u001b·èÜÔøΩU\u001avNÔøΩyÔøΩÔøΩ~tÏåøllÔøΩÔøΩfÔøΩcÔøΩÔøΩ\u0003ÔøΩU\u000fÔøΩÔøΩÔøΩi\n7ÔøΩ\u0011.ÔøΩÔøΩ\u000fgn\u00039Wc\n7ÔøΩÔøΩ4NÔøΩkÔøΩ\u0012ÔøΩÔøΩ\u0012ÔøΩ\nÔøΩ<C\u000eÔøΩ\u0013ÔøΩÔøΩÔøΩÔøΩÔøΩ◊òÔøΩÔøΩ7,oX\u0015-ÔøΩvÔøΩ56ÔøΩMÔøΩÔøΩIÔøΩ#Í¥¶ÔøΩ\u001aÔøΩŒûÔøΩÔøΩÔøΩ6ÔøΩ'-\u0010ÔøΩÔøΩ\u0003ÔøΩÔøΩb\u0013ÔøΩÔøΩÔøΩ~MÔøΩDÔøΩ◊£!ÔøΩÔøΩ”ãOsUi\n'ÔøΩfT\u001aÔøΩÔøΩhÔøΩÔøΩÔøΩ&ÔøΩQÔøΩ2ÔøΩ5ÔøΩÔøΩÔøΩ-ÔøΩn\u0013ÔøΩ6ÔøΩÔøΩÔøΩ1ÔøΩÔøΩTÔøΩÔøΩÔøΩÔøΩ\nUcÔøΩfU1UÔøΩÔøΩÔøΩSÔøΩ\u001fÔøΩÔøΩÔøΩ\"ÔøΩ`ÔøΩÔøΩÔøΩ›ïﬁõ>ÔøΩf∆¶ÔøΩÔøΩÔøΩ;ÔøΩJsi&M;ZnÔøΩÔøΩu\u0014;Z\u0007u#ÔøΩÔøΩ;9ÔøΩPCÔøΩÁ¥üÔøΩÔøΩÔøΩ\u0018~ÔøΩ\nÔøΩÔøΩÔøΩ \u0000h\u0003{ÔøΩ8ÔøΩ\u0014ÔøΩ+1ÔøΩ\u0011ÔøΩ`ÔøΩ+\nÔøΩQq#QÔøΩ<\u0016\u0007ÔøΩJ6ÔøΩÔøΩÔøΩÔøΩÔøΩxÔøΩ\u0010nÔøΩZ\u0015ÔøΩÔøΩÔøΩUÔøΩÔøΩ2ÔøΩÔøΩ6ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0007ÔøΩ…∏ÔøΩCÔøΩzÔøΩX'\u0016ÔøΩÔøΩÔøΩÔøΩÔøΩŸâjeÔøΩ0CÔøΩÔøΩÔøΩ^3ÔøΩÔøΩÔøΩ~ÔøΩ#ÔøΩo\nÔøΩVkdJ]ÔøΩ'RÔøΩ¬çy3?ÔøΩÔøΩÔøΩÔøΩIWÔøΩÔøΩ\bÔøΩbÔøΩÔøΩ;ÔøΩÔøΩzÔøΩÔøΩÔøΩ&aÔøΩ230ÔøΩÔøΩDÔøΩNËßµÔøΩ'ÔøΩc\"'\nZR5»ÄVIaÔøΩqÔøΩu\u0019)]ÔøΩÃôlF∆ïW@ÔøΩ<ÔøΩÔøΩIÔøΩÔøΩkÔøΩÔøΩÔøΩÔøΩ,ÔøΩ@&l0ÔøΩ1ÔøΩ2LÔøΩ`ÔøΩ?ÔøΩ[ÔøΩÔøΩÃ∂8[j'%ÔøΩfÔøΩÔøΩ\u000fÔøΩw\u001bÔøΩAGqÔøΩÔøΩÔøΩpÔøΩ'ÔøΩÔøΩÔøΩp\u0010ÔøΩeÔøΩÔøΩ\nÔøΩJÔøΩÔøΩÔøΩACÔøΩ@v\bÔøΩÔøΩC∆ûÔøΩÔøΩnÔøΩÔøΩÔøΩnÔøΩ\u0014KeÔøΩ|ÔøΩ\u0017ÔøΩÔøΩ\nÔøΩ\u0013ÔøΩdÔøΩO_ÔøΩK\u0005m5ÔøΩ>=ÔøΩC(ÔøΩ\u0018ÔøΩ\u0004ÔøΩ!\bnCÔøΩÔøΩÔøΩ\u0016ÔøΩz\nÔøΩ\u001frÔøΩrÔøΩÔøΩÔøΩÔøΩÔøΩHÔøΩÔøΩÔøΩAÔøΩ\u001aÔøΩ2LOÔøΩÔøΩNF\u0015@Z\tÔøΩYÔøΩXFÔøΩ%ÔøΩÔøΩÔøΩ\u0012ÔøΩÒ±å¥\u00177rRÔøΩ\\r\u0017“©uDWÔøΩwÔøΩ\u0013@ÔøΩ\u0000B8ÔøΩ\nÿú\nnÔøΩÔøΩ9ÔøΩÔøΩ\u0007q(ÔøΩ;\u0018ÔøΩ\u00068ÔøΩy\u001bÔøΩYÔøΩt&]ÔøΩÔøΩÔøΩÔøΩRÔøΩÔøΩ4\u0002\tƒ¨ÔøΩ\\?8r}ÔøΩoÔøΩÔøΩÔøΩ]\u0011g,ÔøΩ3\u0019ÔøΩ4\u000e\nsuÔøΩÔøΩE1\u001f^,r=ÔøΩ'\u0000CÔøΩÔøΩÔøΩ+\u0007ÔøΩ\u001fÔøΩ2TÔøΩ_\bÔøΩÔøΩWF\nÔøΩ\nE\u0017ÔøΩ\u001fÔøΩÿâÔøΩÔøΩÔøΩ\u001aÔøΩBÔøΩ\nÔøΩ\ny/ÔøΩ9CwÔøΩ\nÔøΩ:ÔøΩÔøΩG6.ÔøΩÔøΩ*xÔøΩL$W\u0019<ÔøΩÔøΩmTÔøΩÔøΩÔøΩaÔøΩEÔøΩFÔøΩÔøΩyÔøΩÔøΩRÔøΩÔøΩ<ÔøΩ0ÔøΩÔøΩT,7ÔøΩÔøΩÔøΩÔøΩkÔøΩ^ÔøΩ\nÔøΩ_<)VÔøΩg\u0004ÔøΩaÔøΩAÔøΩ\u0011\u0005ÔøΩWqWÔøΩgÔøΩÔøΩ:\u0006ÔøΩÔøΩÔøΩÔøΩÔøΩ\no\u0016ÔøΩFÔøΩÔøΩ8r\u0015«π\u0004ÔøΩÔøΩ/ÔøΩÔøΩ.ÔøΩaÔøΩ\u0011ÔøΩKFvÔøΩ`ÔøΩkÔøΩÔøΩV\u001f+NÔøΩÔøΩ9mmÔøΩÔøΩÿ£!ÔøΩAÔøΩÔøΩ6ÔøΩÔøΩaCÔøΩr\u0007ÔøΩ√ëBÔøΩÔøΩY\u0016◊óÔøΩÔøΩÔøΩ^ÔøΩOÔøΩÔøΩkÔøΩzÔøΩ’ä1\"ÔøΩy\u0001ÔøΩJÔøΩÔøΩÔøΩ\u0004ÔøΩ2mE8^ÔøΩW{ÔøΩiÔøΩÔøΩÔøΩ\u0018ÔøΩÔøΩYÔøΩÔøΩpÔøΩBÔøΩKi9ÔøΩX-ÔøΩÔøΩbÔøΩIÔøΩ*$a0*\u0003\t[ÔøΩÔøΩ.=ÔøΩpÔøΩOÔøΩq>ÔøΩÈöåÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ&8ÔøΩÔøΩÔøΩiÔøΩlÔøΩ${L\u0007ÔøΩ)ÔøΩDK>!÷òxcÔøΩGÔøΩÔøΩﬂáÔøΩJcEpBÔøΩHIÔøΩÔøΩ\u0013\nzsÔøΩÔøΩ\u0017ÔøΩÔøΩh\u0018ÔøΩQÔøΩ\u0001WÔøΩXÔøΩÔøΩdÔøΩljk÷±# ÔøΩ\u001f~`ÔøΩ#ÔøΩgÔøΩÔøΩ≈átÔøΩÔøΩ\u0001ÔøΩÔøΩÔøΩr◊Ω#n8ÔøΩ\u0017ÔøΩÔøΩƒÆÔøΩÔøΩUÔøΩÔøΩ\u0001ÔøΩ\u00185/ÔøΩÔøΩ\u0012ÔøΩÔøΩ?€∫ÔøΩÔøΩkÔøΩc\u000f'=IA ÔøΩh]ÔøΩÔøΩ|ÔøΩÔøΩÔøΩUÔøΩ\u0018[ÔøΩ9/\u0019ÔøΩ\bÔøΩ<#ÔøΩÔøΩ^\u0000\nÔøΩÔøΩcÔøΩe⁄âÔøΩ…ë'#ÔøΩÔøΩÔøΩ\u0012ÔøΩÔøΩPqÔøΩzcH<\u0001-\tÔøΩ\tÔøΩ+ÔøΩÔøΩc1~ÔøΩ:Q\nÔøΩ∆æ8ÔøΩ√ó\n[ÔøΩPeÔøΩ\u0015OÔøΩÔøΩÔøΩ/Õä‚±ÅÔøΩÔøΩDÔøΩ?\n]”é\u001a#Ã°ÔøΩÔøΩÔøΩyÔøΩlÔøΩÔøΩÔøΩÔøΩ)\u0017/ÔøΩLÔøΩK\u0003ÔøΩÔøΩÔøΩÔøΩuÔøΩÔøΩdMÔøΩ|\nÔøΩÔøΩ@uÔøΩÔøΩ\nT”ëÔøΩ\u001fÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ‘∏ÔøΩwÔøΩ\u0018ÔøΩÔøΩﬂÜÔøΩÔøΩÔøΩÔøΩÔøΩ\u0015HÔøΩÔøΩ\n-M\u0015(R\u0006ÔøΩ\u001f∆≠\u0007$<8l]ÔøΩÔøΩÔøΩÔøΩuÔøΩÔøΩÔøΩÔøΩzÔøΩ\u0017\n)ŒæÔøΩÔøΩÔøΩÔøΩjWÔøΩÔøΩÔøΩx\u0003IÔøΩ*ÔøΩEÔøΩ\b4ÔøΩÔøΩÔøΩÔøΩŸ∏ÔøΩqÔøΩ/ÔøΩÔøΩ\u0006\nÔøΩÔøΩÔøΩÔøΩX{xX’∞ÔøΩÔøΩÔøΩÔøΩbÔøΩV\u0011ÔøΩJÔøΩ›úN\u0006bÔøΩÔøΩ]ÔøΩQÔøΩÔøΩXÔøΩÔøΩÔøΩÔøΩÔøΩ\u0013~…ÆÔøΩÔøΩ\u001a\u00162tÔøΩ\nÔøΩ\u0015>\u0011\\\nÔøΩ\u0002iÔøΩnÔøΩÔøΩÔøΩVÔøΩqÔøΩdÔøΩ|ÔøΩbÔøΩ~FuGMGÔøΩÃ∫IÔøΩ\u0017ÔøΩ1.ÔøΩÔøΩÔøΩÔøΩ7nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ(ÔøΩÔøΩÔøΩÔøΩG\u001bÔøΩÔøΩÔøΩ\u0015ÔøΩW=_} wÔøΩÔøΩ@ÔøΩÔøΩÔøΩ5=ÔøΩ|ÔøΩÔøΩhÔøΩ—ëÔøΩ%ÔøΩÔøΩÔøΩ\u001671ÔøΩQ{ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩmM\u000f63ÔøΩs\u001b*WÔøΩllZ3ÔøΩ…ú0ÔøΩ\u0003ÔøΩ–àÔøΩÔøΩ\u0013\u0005\nÔøΩ|k/ÔøΩs\u0013ÔøΩÔøΩS\u0013⁄êÔøΩFÔøΩ[ÔøΩ*ÔøΩ5\u0018ÔøΩfÔøΩ\u001aÔøΩ–¥H4ÔøΩ5$ÔøΩ[,ÔøΩHJ\u001fÔøΩÔøΩÔøΩ55ÔøΩÔøΩz}mm=\u001aÔøΩ&ÔøΩtÔøΩÔøΩÔøΩ[ZZÔøΩÔøΩÔøΩÔøΩ#kkkÔøΩÔøΩÔøΩÔøΩTÔøΩÔøΩÔøΩÔøΩEÔøΩÔøΩÔøΩP-ÔøΩÔøΩ\u0003ÔøΩ^:ÔøΩ\b,\nÔøΩÔøΩŸë?GHÔøΩÔøΩdÔøΩ.ÔøΩÔøΩ\u0007jp\nO\u0019j9_ÔøΩÔøΩÔøΩÔøΩ3ÔøΩkqÔøΩX0\\ÔøΩÔøΩ\u0017kOÔøΩÔøΩh$a\u000eÔøΩÔøΩ8ÔøΩÔøΩÔøΩÔøΩ:ÔøΩ/ÔøΩ'.>ÈÅÉÔøΩÔøΩÔøΩX,ÔøΩÔøΩEŒîÔøΩdÔøΩÔøΩÔøΩo!-ÔøΩÔøΩ\u0011SÔøΩÔøΩÔøΩ\u0014ÔøΩÔøΩÔøΩÔøΩ^1ÔøΩÔøΩ\n()ÔøΩC\u0001ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ4vÔøΩÔøΩÔøΩaÔøΩAÔøΩ\u0015ÔøΩ@ÔøΩ2ÔøΩ=\u00076ÔøΩ4ÔøΩUÔøΩBÔøΩœ∑(ÔøΩPGGqÔøΩRÔøΩ\u0000ÔøΩÔøΩ+≈®FAZIÔøΩ@ÔøΩSÔøΩ7ÔøΩ\u0016\u0013TÔøΩÔøΩPLL%\u0015ÔøΩÔøΩT| xAg1ÔøΩTÔøΩÔøΩÔøΩ\"ÔøΩÔøΩQ+”øiÔøΩÔøΩ\u0015ÔøΩÔøΩÔøΩÔøΩÔøΩ@DB∆ßÔøΩ*\u0015CÔøΩÔøΩMSÔøΩÃöÔøΩÔøΩ)\u001ff\u001aÔøΩÔøΩ=ÔøΩÔøΩ$jÔøΩÔøΩÔøΩ%BÔøΩ‘í+ÔøΩX6O\nÔøΩÔøΩÔøΩÔøΩ.ÔøΩÔøΩ‰™≤d◊∫1ÔøΩ\u000fÔøΩ\u001b’û@ÔøΩÔøΩ\u0019ÔøΩÔøΩÔøΩÔøΩÔøΩEaÔøΩÔøΩ\u0016ÔøΩU~»ûÔøΩÔøΩWÔøΩÔøΩÔøΩÔøΩÔøΩ<Nolj\u0018V34ÔøΩqÔøΩR)\u0015ÔøΩb#\nÔøΩÔøΩÔøΩSÔøΩÔøΩÔøΩxÔøΩrÔøΩNkQ*ÔøΩÔøΩ?zÔøΩsÔøΩqÔøΩÔøΩf5ÔøΩ\nhuUÔøΩÔøΩdÔøΩDÔøΩÔøΩÔøΩ\u0007ÔøΩÔøΩ|SÔøΩPt\u0007ÿø4>ÔøΩiÔøΩ1ÔøΩ`e*ÔøΩFÔøΩSÔøΩ\tÔøΩÔøΩ\u0014_ÔøΩGÔøΩdÔøΩÔøΩÔøΩÔøΩ\u0017ÔøΩÔøΩ\nÔøΩÔøΩÔøΩU\u0019%\u0007+\tmMYVL7ÔøΩ…≤:\u000evÔøΩ”ïÔøΩ)qI‹™ÔøΩ4.H\bÔøΩ\u0007ÔøΩ\nCzÔøΩgÔøΩwLhLtL|ÔøΩkf|ÔøΩrÔøΩÔøΩK◊•LÔøΩÔøΩÔøΩÔøΩË°®ÔøΩ!>ÔøΩ5ÔøΩÔøΩ4ÔøΩ\u001aÔøΩ\n1ÔøΩÔøΩÔøΩ\u0010h\b2\nÔøΩ\u0006wÔøΩÔøΩIÔøΩ\u0013)bÔøΩ'ÔøΩ\t\u0017ÔøΩ“ªÔøΩDÔøΩÔøΩ9ÔøΩj1\u0016ÔøΩ%\u000eÔøΩ⁄Ä\n6ÔøΩÔøΩ\u0010ÔøΩÔøΩ\n!\u0011\u0016:DÔøΩ`2HÔøΩÔøΩÔøΩ#ÔøΩr:}ÔøΩÔøΩ>ÔøΩÔøΩ€úNg2ÔøΩO:]…îRÔøΩÔøΩSÔøΩt:%W(ÔøΩACÔøΩ\u0012aÔøΩSJÔøΩÔøΩÔøΩtÔøΩe(\u0014\n\u001a\nzÔøΩX,\"ÔøΩd\u0002IRJÔøΩÔøΩ\u0015\u0017&a'\u0006UÔøΩ2ÔøΩ\u000f%ÔøΩ%{…∂CÔøΩmTÔøΩ8ÔøΩ%ÔøΩQ!\u0017\u0010ZKÔøΩ\tÔøΩÔøΩ\u0019ÔøΩJÔøΩy€†\u0007ÔøΩ”ò*ÔøΩ\u001ftÔøΩÔøΩ4ÔøΩÔøΩÔøΩÔøΩ\u0001ÔøΩÔøΩ8\u0001:/8ÔøΩÔøΩÔøΩ\u001f2ÔøΩÔøΩ@ÔøΩÔøΩÔøΩÔøΩÔøΩ2\u0019ÔøΩ\u0017\nÔøΩﬁàÔøΩÔøΩ\bÔøΩD\nÔøΩAÔøΩ\n\u0004€å\u0007\nBÔøΩÔøΩ&P1XÔøΩOÔøΩÔøΩÔøΩ06ÔøΩ\u0016\nÔøΩ,√ód\nZÔøΩÔøΩÔøΩÀ∞eb\u0012ÔøΩÔøΩÔøΩ=R\u0006NÔøΩÔøΩpÔøΩFÔøΩÔøΩfÔøΩWÔøΩÔøΩÔøΩO\t\\ÔøΩ?ÔøΩÔøΩ.\nÍ´©#ÔøΩ\u0000nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ:\u0011ÔøΩy!KÔøΩjÔøΩÔøΩÔøΩ;ÔøΩÔøΩK&;ÔøΩQa ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ~ÔøΩ1ÔøΩx~\u000fÔøΩpÔøΩÔøΩÔøΩnƒ°ÔøΩÔøΩÔøΩ3-ÔøΩÔøΩÔøΩ\u0013 &E\u001bÔøΩÔøΩiÔøΩiX\u000eÔøΩÔøΩf.\nÔøΩÔøΩhÔøΩÔøΩÔøΩÔøΩ \u000eVÔøΩÔøΩ\n4sFXÃ∞(’ôfÔøΩ9ÔøΩÔøΩzÔøΩÔøΩÔøΩÔøΩ!ÔøΩÔøΩXÔøΩÔøΩuÔøΩÔøΩÔøΩ0ÔøΩuÔøΩBzX‘ªl\u0015ÔøΩÔøΩÔøΩÔøΩ\bÔøΩ88ÔøΩÔøΩ\u0017/ÔøΩt~?\u0012ÔøΩÔøΩ\u0012ÔøΩÔøΩ)\nrÔøΩ+krÔøΩT/ÔøΩ\u001f2ÔøΩ\u0015\u0014TÔøΩ)\n\u0007TÔøΩÔøΩ&ÔøΩÔøΩÔøΩ$-ÔøΩÔøΩÔøΩÔøΩ}_ÔøΩ ÔøΩÔøΩ|ÔøΩ?ZÔøΩÔøΩN}ÔøΩÔøΩ({ÔøΩuÔøΩzÔøΩ\u0012ÔøΩN}FY7HÔøΩKÔøΩW\u0016\n\u0018ÔøΩPÔøΩiÔøΩ\nÔøΩs6\u0010ÔøΩNÔøΩÔøΩÔøΩ∆ß5xË™ô\u001bD*ÔøΩÔøΩ \u0001ÔøΩÔøΩ!\"ÔøΩWÔøΩ.\nÔøΩÔøΩ|s~\u0010ÔøΩWUÃî&\u0001ÔøΩÔøΩÔøΩÔøΩ\u0010ÔøΩÔøΩÔøΩÔøΩsmÀ®kÔøΩœòQWÔøΩ\bÔøΩ\u0002\u0006ÔøΩHÔøΩÔøΩÔøΩ\u001aÔøΩQ\u000eÔøΩ—èÔøΩ\u0013ÔøΩUÔøΩj+G=9rÔøΩÔøΩ\u0015ÔøΩJ\u0014ÔøΩÔøΩÔøΩ∆¨m$ÔøΩ9,ﬂöÔøΩÔøΩcÔøΩ4ÔøΩo\tÔøΩÔøΩ^ÔøΩQÔøΩ\u0014I\u0004ÔøΩÔøΩ%ÔøΩÔøΩ>#O\u000f3\nm_7ÔøΩÔøΩÔøΩ-ÔøΩKÔøΩ-ÔøΩX)ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0000ÔøΩBÔøΩÔøΩÔøΩ~/\u0000ÔøΩqÔøΩÔøΩ\bÔøΩÔøΩÔøΩÔøΩl\nÔøΩÔøΩy8ÔøΩÔøΩ\u0019ÔøΩQÔøΩ\u0011ÔøΩT(ÔøΩysÔøΩ,5\u0019ÔøΩ9ÔøΩÔøΩ\n\u0017L\to\n<IÔøΩ{ÔøΩ*Nx^\t»å^uNÔøΩUÔøΩÔøΩ\naÔøΩÔøΩ\u0011\u000eÀ¥FÔøΩCÔøΩLv\u0007ÔøΩÔøΩÔøΩ\u000eMX&s\u001aMzÔøΩlÔøΩbÔøΩ+ÔøΩ@ÔøΩ\nÔøΩKÔøΩÔøΩV#ÔøΩ:|ÔøΩHÔøΩdTÔøΩÔøΩ<\u0000\u0016ÔøΩzÔøΩÔøΩÔøΩÔøΩÔøΩ`ÔøΩÔøΩÔøΩ\u000ekÔøΩ\u0018ÔøΩÔøΩÔøΩÔøΩGÔøΩHoÔøΩ\u001fÔøΩÔøΩÔøΩ22∆ãÔøΩXÔøΩ\n`ÔøΩÔøΩÔøΩ4ÔøΩGÔøΩÔøΩÔøΩ9GÔøΩhsÔøΩÔøΩÔøΩ]\u0003/M\\ÔøΩ~ÔøΩÔøΩFr\u0006GÔøΩs≈°ÔøΩÔøΩ2bO_]ÔøΩ\u0013ÔøΩÔøΩÔøΩ\u0003ÔøΩt\u0007.ÔøΩÀ≥ ÔøΩÔøΩÔøΩVÔøΩ\u0003D[«ãÔøΩhy0GÔøΩÔøΩyÔøΩÔøΩÔøΩÔøΩ\u0007fﬁΩ4XÔøΩ@ÔøΩÔøΩÔøΩlÔøΩÔøΩÔøΩpnÔøΩkÔøΩÔøΩÔøΩzÔøΩ_CÔøΩ\nÔøΩÔøΩ#7-ÔøΩÔøΩzo~ÔøΩ-eÔøΩ2ÔøΩ-O3Ïé§/!ÔøΩÔøΩ1ÔøΩÔøΩÔøΩk4t~ÔøΩ\b1>ÔøΩ\u0013ÔøΩqÔøΩA-XÔøΩÔøΩÔøΩÔøΩ+ÔøΩÔøΩÔøΩ\bÔøΩ\t9ÔøΩ–ìÔøΩ\nÔøΩ9ÔøΩyÔøΩÔøΩÔøΩMÔøΩ\u0016yÔøΩyÔøΩÔøΩÔøΩ\u0011∆òZÔøΩE1U>Uq\nYÔøΩÔøΩ#kÔøΩGÔøΩ*ÔøΩ&ÔøΩ3\nIÔøΩe?\u0017ÔøΩ$ÔøΩfÔøΩÔøΩs]JÔøΩÔøΩ*\u0001?{ÔøΩ\u0005ÔøΩ(FBVÀ∏ÔøΩPÔøΩ §ÔøΩ^ÔøΩJI/ÔøΩ-FÔøΩ.ÔøΩÔøΩ,ÔøΩÔøΩÔøΩeÔøΩÕùiÔøΩaŸΩÔøΩ^ÔøΩ?xÔøΩ\nUÔøΩwDÔøΩÔøΩÔøΩÔøΩÔøΩ\u0017ÔøΩÔøΩ\\Â∫èÔøΩyÔøΩ1ÔøΩukyXÔøΩÔøΩÀã)\u001aÔøΩÔøΩÔøΩ\u0006ÔøΩÔøΩÔøΩK/HPÔøΩÔøΩXMÔøΩÔøΩ\nJ:⁄¶ÔøΩÔøΩ\u0000%ÔøΩÔøΩÔøΩ\u000eÔøΩ+ÔøΩ7\u0010\u0006ÔøΩ∆âÔøΩœëÔøΩÔøΩRR\u001a\n-ÔøΩmNÔøΩpÔøΩ\nÔøΩ9|ÔøΩ@ÔøΩÔøΩJ_ÔøΩÔøΩ:ÔøΩ\u0001ÔøΩXIC”û,ÔøΩÔøΩ>}œëÔøΩÔøΩÔøΩq`QuÔøΩ\u0011ÔøΩ“øÔøΩ\nŒüÔøΩÔøΩÔøΩW9ÔøΩLÔøΩÔøΩ?ÔøΩÔøΩ4ÔøΩpÔøΩÔøΩ\u0005¬äZÔøΩ|ÔøΩDHP\u000eÔøΩ»ÉÔøΩÔøΩÔøΩo\u0011\u0005tÔøΩ#\nÔøΩÔøΩÔøΩ\n:ÔøΩLÔøΩÔøΩ\nÔøΩmƒΩÔøΩCÔøΩXÔøΩÔøΩKÔøΩ\u0018IÔøΩÔøΩÔøΩ*MŒ£F)ÔøΩÔøΩFÔøΩ\u00191ÔøΩÔøΩÔøΩÔøΩÔøΩ[ÔøΩ3ÔøΩ$vÔøΩÔøΩ\u0017?ÔøΩÔøΩ\u0011ÔøΩ\nÔøΩÔøΩTÔøΩ\u0015[ÔøΩaÔøΩZÔøΩ\u001b-ÔøΩÔøΩÔøΩSÔøΩÔøΩXK\u0013+ÔøΩÔøΩ\u0017)ÔøΩN7ÔøΩr2ÔøΩ5ÔøΩŸ∫ÔøΩzÔøΩÔøΩX{ÔøΩÔøΩWb5YÔøΩ“ÖÔøΩyÔøΩ88gb`8ÔøΩJÔøΩÔøΩZdÔøΩÔøΩÔøΩ\u0015gN\nÔøΩÔøΩÔøΩ\\ÔøΩÔøΩÔøΩ‘áÔøΩu-ÔøΩÔøΩhCÔøΩ%ÔøΩYÔøΩÔøΩ\u0003ÔøΩ&ÔøΩAÔøΩE\u00076'ÔøΩ4w\u00162ÔøΩÔøΩ3X›ßÔøΩÔøΩÔøΩÔøΩÔøΩHÔøΩÔøΩc\u0001ÔøΩÔøΩÔøΩbÔøΩbÔøΩ≈ØÔøΩ*Ï£ÆÔøΩ\u000fÔøΩXÔøΩÔøΩ.ÔøΩcIÔøΩB\u00057.!ÔøΩ?$ÔøΩ^UÔøΩHÔøΩaÀ∏ÔøΩwNÔøΩ\u0007\u0004ÔøΩÔøΩ\u000fÔøΩÔøΩo\tÔøΩ/MÔøΩ\u0004”ÅÔøΩEÔøΩ~\u0017ÔøΩjÔøΩÔøΩk@.]ÔøΩ\u000f\u0005ÔøΩ\nÔøΩÔøΩ\u0010ÔøΩÔøΩÔøΩÔøΩ’çÔøΩFÔøΩ\nÔøΩBÔøΩuÔøΩ5ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩg9\u000eÔøΩ\u0015ÔøΩh_ÔøΩ\u0012ÔøΩ\nÔøΩ¬∑q\u0012ÔøΩÔøΩ\nqEÔøΩÔøΩ\u0018zÔøΩÔøΩfÔøΩYÔøΩÔøΩÔøΩ\nÔøΩ<DÔøΩÔøΩC!ÔøΩe\nÔøΩÔøΩÔøΩ\u0005%\nÔøΩÔøΩ⁄π\u0011ÔøΩ\u0019;ÔøΩÔøΩ\n;ÔøΩÔøΩ\u000fÔøΩÔøΩ4ÔøΩÔøΩP€ßÔøΩÔøΩtÔøΩX|ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩhtÔøΩÔøΩÔøΩÔøΩÔøΩB\tÔøΩ\bÔøΩÔøΩAÔøΩ\n#ÔøΩÔøΩÔøΩÔøΩÔøΩ`ÔøΩÔøΩÔøΩÔøΩ~VÔøΩ\u001fymcÔøΩmÔøΩÔøΩÔøΩÔøΩwÔøΩ{\nÔøΩkÔøΩ\u0017^zÔøΩÔøΩnÔøΩ;ÔøΩÔøΩ€π\u001b+ÔøΩfÔøΩ`IÔøΩÔøΩ⁄∫ÔøΩÕ°ÔøΩ7ÔøΩ;ÔøΩÔøΩ~vÔøΩÔøΩ\u0007ÔøΩÔøΩqÔøΩÔøΩ\u000fÔøΩS_$ÔøΩz:wÔøΩÔøΩÔøΩ)VÔøΩ\n$CÔøΩÔøΩ\u0000\u001fw\u001f\u0012 †ÔøΩ^wDbÔøΩ/ÔøΩÔøΩÔøΩ]ÔøΩ~S:UuÔøΩ}ÔøΩ\u0017ÔøΩÔøΩ++ÔøΩÔøΩÔøΩÔøΩÔøΩkX(ÔøΩÔøΩ\u0004ÔøΩÔøΩ\u0016ÔøΩÔøΩc17WAu]ÔøΩDeh\bÔøΩÔøΩ##ÔøΩ#\neÔøΩÔøΩÔøΩ[ÔøΩÔøΩÔøΩ[ÔøΩÔøΩ=7ÔøΩ@ÔøΩz$ÔøΩŒ™Ms=ÔøΩ<ÔøΩ[ÔøΩÔøΩ\b$>yÔøΩx\u001fÔøΩZÔøΩVRV\u0016ÔøΩÔøΩÔøΩÎ∏∞ÔøΩKÔøΩVWÔøΩ”áÔøΩ\\}ÔøΩÔøΩe5ÔøΩa]ÔøΩ_C\u001a:XÔøΩ2ÔøΩHÔøΩHÔøΩ}-\u0017J$ÔøΩ2ÔøΩÔøΩRÔøΩÔøΩÔøΩÔøΩ}ÔøΩKÔøΩÔøΩ+»πÔøΩ\u0013x\u0004ÔøΩÔøΩÔøΩÔøΩ\u0014ÔøΩÔøΩ^\nÔøΩsÔøΩ\\mkÔøΩÔøΩsdÔøΩÔøΩGÔøΩ\u0015ÔøΩÔøΩNÔøΩ\u0001\u0005ÔøΩÔøΩÔøΩ,ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0001}ÔøΩsÔøΩ;\u0000ÔøΩÔøΩ<-ÔøΩÔøΩqÔøΩÔøΩ8ÔøΩ≈èpQqÔøΩJ}ÔøΩÔøΩÔøΩt\u0007\tÔøΩÔøΩuÔøΩX]78*ÔøΩÔøΩ\\›é(ÔøΩc)TmÔøΩÔøΩÔøΩR2ÔøΩ:ÔøΩÔøΩ”ÑBAÔøΩ\u0013\u0006YB—âÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\u0017X\u0016ÔøΩ-ÔøΩÔøΩTﬂºÔøΩÔøΩ\u0011ÔøΩPÔøΩÔøΩ\u0006sRmÔøΩÔøΩO\nÔøΩÔøΩ*\nÔøΩÔøΩÔøΩÔøΩaNUÔøΩÔøΩc\u0018ÔøΩÔøΩÔøΩ!ÔøΩÔøΩÔøΩ\nsÔøΩÔøΩVÔøΩ\\ÔøΩ|ÔøΩR/0ÔøΩ\u001air2ÔøΩ3\u0019ÔøΩCÔøΩÔøΩÔøΩÔøΩÔøΩ\tOPiÔøΩÔøΩÔøΩ]ÔøΩÔøΩ∆∏LÔøΩQÔøΩ5WÔøΩ\u001aB8ÔøΩÔøΩÔøΩUÔøΩ\u000ecÔøΩ\u0013sÔøΩÔøΩÔøΩKÔøΩÔøΩR\u000eÔøΩ$~*Ÿó\u0011\u0012ÔøΩ„Ωê_ÔøΩX\bÔøΩ\u0014AÔøΩ\u0006DÔøΩÔøΩÔøΩ9ÔøΩÔøΩ+ÔøΩ:FÿÜ;ÔøΩ⁄ß8\u0016ÔøΩ√û?8ÔøΩrHCÔøΩÔøΩ\ndÔøΩÔøΩ6ÔøΩQ\u0007\u0013tÔøΩWÔøΩÔøΩG)X9i.\nO9“åÔøΩu0<\u000e\u0006c!GP+\u00050\nÔøΩÔøΩ\u000eÔøΩ{ÔøΩÔøΩÔøΩÔøΩ;0q8%Ri\tÔøΩÔøΩ>ÔøΩMoÔøΩÔøΩ‚±òÔøΩÔøΩ\u0004ÔøΩÔøΩÔøΩÔøΩtÔøΩ` ÔøΩpÔøΩÔøΩiÔøΩ\u0010ÔøΩÔøΩ√é\u0019ÔøΩÔøΩZ\nÔøΩYÔøΩ.5ÔøΩÔøΩÔøΩ\u000e[ÔøΩÔøΩ[ÔøΩÔøΩÔøΩgÔøΩ hvdÔøΩÔøΩÔøΩÔøΩ/\u001fÔøΩgÔøΩÔøΩÔøΩÔøΩdÔøΩÔøΩÔøΩ÷ΩÔøΩÔøΩ/ÔøΩ\nÔøΩ\u0002\nÔøΩ\nÔøΩÔøΩ*NÔøΩÔøΩ‘öÔøΩÔøΩÔøΩ+2ÔøΩÔøΩ\t\\ÔøΩ\u0013“îÔøΩÔøΩÔøΩ\\7ÔøΩ[M.ÔøΩÔøΩU.ÔøΩ\u001b\nÔøΩ\u0001ÔøΩÔøΩÔøΩÔøΩÔøΩ\n:ÔøΩÔøΩÔøΩÔøΩ(FÔøΩ.ÔøΩ8~EÔøΩÔøΩ-M\u000f\nÔøΩÔøΩ>ÔøΩÔøΩ:aÔøΩ\u00078\u001aÔøΩH|ÔøΩHe7ÔøΩÔøΩ\n^X!ÔøΩ*zÔøΩt(^ÔøΩ=\u0006ÔøΩÔøΩ(ÔøΩÔøΩÔøΩ\u0003ÔøΩ\u000eÔøΩÔøΩAÔøΩ¬´*\u000eÔøΩÔøΩAÔøΩobtÔøΩuÔøΩÔøΩSÔøΩÔøΩÔøΩh\nÔøΩ7KÔøΩ*ÔøΩÔøΩ\u000eÔøΩ“±ÔøΩÔøΩ>\noÔøΩÔøΩÔøΩrÔøΩ,?TfÔøΩ\u001bsÔøΩpÔøΩS7N*\u0018ÔøΩÔøΩIÔøΩ) \u0012)ÔøΩZ~ÔøΩÔøΩhHÔøΩ\b\u0004ÔøΩÔøΩÔøΩÔøΩ\u0018ÔøΩw\u0007ÔøΩ1ÔøΩÔøΩ/\t\u0004ÔøΩdÔøΩrÔøΩ:ÔøΩ_^ÔøΩ(@uÔøΩ4ÔøΩÔøΩHÔøΩ9'ÔøΩÔøΩ0ÔøΩÔøΩ(cÔøΩÔøΩÔøΩTÔøΩzÔøΩRÔøΩÔøΩ$ÔøΩCÔøΩ\n\u001bÔøΩ≈çÔøΩÔøΩq\u0010ÔøΩ\u0016kÔøΩÔøΩ,ÔøΩ\u00059ÔøΩF\nÔøΩÔøΩlÔøΩÔøΩÔøΩÔøΩ%\u0012ÔøΩVC\b\u0016K\\aÔøΩDm>ItÔøΩ\u0002\nRJtÔøΩFÔøΩ\u0002\u00078)ÔøΩ,ÔøΩS\"ÔøΩl1ÔøΩxO9ÔøΩÔøΩ…±ÔøΩÔøΩÔøΩ\nÔøΩxÔøΩ3ÔøΩÔøΩ[$ÔøΩ%oKÔøΩJÔøΩÔøΩ=AÔøΩ$ÔøΩJS/ÔøΩÔøΩÔøΩ%ÔøΩÔøΩ$.ÔøΩu]ÔøΩeÔøΩÔøΩR\u0016ÔøΩGÔøΩy~\"ÔøΩ`(ÔøΩ6TÔøΩ\u0019\u0015ÔøΩ2)\u001b$\u0003/'HÔøΩP\u0013-ÔøΩÔøΩu4ÔøΩ9ÔøΩÔøΩÔøΩÔøΩ\u0002_ÔøΩrÔøΩÔøΩPÔøΩxÔøΩÔøΩÔøΩ ÔøΩÔøΩ,HJÔøΩ)ÔøΩÔøΩzIÔøΩ2KÔøΩ…ØÔøΩt%ÔøΩ\bÔøΩ⁄ÉÔøΩ+gÔøΩ_ÔøΩy\u0007ÔøΩ\u0018\u001a\u0006ÔøΩÔøΩ?ÔøΩXÔøΩeÔøΩeÔøΩX\u0003ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0006FÔøΩÔøΩÔøΩÔøΩ7 âÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩiÔøΩ/\u0007\u0000\u0007t≈ìÔøΩÔøΩ‹ñÔøΩ\u0000ÔøΩÔøΩÔøΩq&ÔøΩ3ÔøΩ\n2VÔøΩÔøΩÔøΩÔøΩ?jÔøΩ_uÔøΩ,>\nÔøΩÔøΩWÔøΩÔøΩÔøΩAÔøΩI<ÔøΩÔøΩ\nMÕèÔøΩÔøΩ\n~ÔøΩ@iP>\u0010|6ÔøΩ2{Ds,(ÔøΩJ%ÔøΩJÔøΩ\nÔøΩb\nÔøΩ,ÔøΩCdtÔøΩ\nÔøΩ–¢drNÔøΩ\u0011\u0012ÔøΩ$dÔøΩÔøΩ!4ÔøΩÔøΩ\u000e\u0003\u001bÔøΩ\u0001'\u0005\u0017ÔøΩÔøΩ\u0007\u0017ÔøΩ3ÔøΩ\bÔøΩÔøΩBÔøΩ4\u0004ÔøΩÔøΩ vÔøΩÔøΩNÔøΩZÔøΩ<\u0012q\"ÔøΩÔøΩÔøΩ\"\"\n›ÉÔøΩÔøΩ \u0017\n\u001b\\\u0011\u0014\u0004ÔøΩiÔøΩÔøΩ\u001fNÔøΩÔøΩÔøΩkU\u000eÔøΩ#ÔøΩ`ÔøΩ8ÔøΩÔøΩÔøΩAJ\u0002WNÔøΩ~»•ÔøΩÔøΩÔøΩÔøΩ“¨\n^ÔøΩÔøΩÔøΩP\n\u0003ÔøΩm;ÔøΩ\u0013qÔøΩÔøΩÌãùmÁíÉÔøΩQK.H›øÔøΩÔøΩ]ÔøΩÔøΩ\u0015ÔøΩÔøΩÔøΩ\u001aÔøΩbÔøΩ6ÔøΩ\u0011\u0015__ÔøΩgÔøΩ^6ÔøΩÔøΩÔøΩÔøΩ^\u0001)n\u0007\u001bEvÔøΩ=ÔøΩnmCÔøΩÔøΩÔøΩjÔøΩÔøΩÔøΩkÔøΩUKÔøΩÔøΩÔøΩ(ÔøΩÔøΩÔøΩuÔøΩÔøΩyÔøΩÔøΩbÔøΩÔøΩÔøΩÔøΩ\nÔøΩZrÔøΩ\n\u0002ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩuÔøΩ!ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ0ÔøΩbÔøΩÔøΩ\u0011ÔøΩ¬§ÔøΩVÔøΩ\u0017;ÔøΩÔøΩ◊áÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩ\u0005ÔøΩ\u001bÔøΩÔøΩ\n%99ÔøΩÔøΩÔøΩ7(ÔøΩf%VjNÔøΩÔøΩ\u0010KÔøΩ\nCÔøΩ2ÔøΩaÔøΩz|\u000eÔøΩ\u00014F\n(ÔøΩJÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩPdhMÔøΩÔøΩlÔøΩ\u0011’Ü#\nÔøΩÔøΩÔøΩÔøΩ7\u0013\u000ehUÔøΩhÔøΩÔøΩÔøΩaÔøΩÔøΩGÀÅÔøΩ\u00003ÔøΩ\nÔøΩ\u0017ÔøΩÔøΩ[.ÔøΩjs,,\nQ‘ù\u0012’πÔøΩ\nÔøΩ\n«äÔøΩ\bÔøΩ\u0016^-ÔøΩd;ÔøΩÔøΩD7ÔøΩ7\to\u0011ÔøΩÔøΩÔøΩ-ÔøΩÔøΩ>)<ÔøΩ\nw\u001bo`ÔøΩÔøΩnÔøΩ7ÔøΩÔøΩ\u0010~)\u0014xÔøΩF∆©ÔøΩÔøΩ/6ÔøΩ<^\u0003√≤ÔøΩzÔøΩÔøΩÔøΩE^ÔøΩAÔøΩSÔøΩ2^ÔøΩÔøΩ0\u001fcÔøΩ\u0002M_zLÔøΩÔøΩZ|ÔøΩÔøΩÔøΩDÔøΩÔøΩÔøΩÔøΩÔøΩH_ŸºÔøΩw\u0015\n#nÎ£üÔøΩ\u0000\u0011ksxÔøΩQÔøΩ\u0000È§ø\\iÔøΩ\u001f?GÔøΩÔøΩ\u0013\u0015\u0007u<ÔøΩÔøΩdÔøΩ+(ÔøΩÔøΩx0\u0004ÔøΩÔøΩO\u0013ÔøΩkZÔøΩÔøΩÔøΩcÔøΩI◊≠ÔøΩ_\u001bÔøΩ\u0004ÔøΩCÔøΩVÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ$\u0018ÔøΩÔøΩÔøΩÔøΩÔøΩ’øÔøΩÔøΩ·É±ÔøΩÔøΩ3ÔøΩÔøΩzdÔøΩ\u000f\u000foÔøΩ\nÔøΩ\tm\u0002ÔøΩ\"XÔøΩlÔøΩ\nÔøΩ*ÔøΩÔøΩÔøΩd!YCÔøΩc^V\bÔøΩHÔøΩ6aÔøΩÔøΩ\u0015ÔøΩ\u0000ÔøΩÔøΩ{ÔøΩ\",\n*ÔøΩÔøΩHÔøΩÔøΩÔøΩIÔøΩ&J\u0014F\u0016([q?ÔøΩCj\u0015fÔøΩ8ÔøΩVÔøΩ>.~ÔøΩÔøΩr\u0002_@VFÔøΩIl6$\u0010ÔøΩÔøΩ\b<ÔøΩ0R#+ÔøΩÔøΩÔøΩTXÔøΩÀÑ8ÔøΩ\u001a€°\u000eÔøΩÔøΩ]zÔøΩÔøΩÔøΩ_\u0006pÔøΩwÀ£\u0017;ÔøΩÔøΩ}YÔøΩÔøΩ\u0015ÔøΩ0ÔøΩ\\‘ªÔøΩm\u0004\nÔøΩÔøΩ_ÔøΩÔøΩ\u0011\nu\u0016G</ÔøΩÔøΩÔøΩÔøΩoÔøΩÔøΩÔøΩÔøΩ^1SÔøΩﬂØ\no-3xÔøΩÔøΩ\u0018\b\u0013ÔøΩXÔøΩ\u0014xÔøΩ`ÔøΩw{ÔøΩÔøΩxÔøΩDdZgb-\u0013hÔøΩÔøΩuÔøΩÔøΩÔøΩ?ÔøΩ~\u0015\u0013ÔøΩÔøΩ4ÔøΩÔøΩ\u0003=ÔøΩyÔøΩ\u0014ÔøΩ&{D|ÔøΩÔøΩ\u0005ÔøΩ~ÔøΩ)ÔøΩ\tÔøΩÔøΩƒø2KÔøΩ3c?3ÔøΩ7ÔøΩ2ÔøΩÔøΩeh#\nÔøΩÔøΩZdÔøΩ’úÔøΩÔøΩÔøΩÃ¨Q@\nz\nÔøΩ\u00152&ÔøΩÔøΩbY/!ÔøΩ\\ÔøΩ_%ÔøΩÔøΩiÔøΩM.?ELÔøΩ‡Øê\u0013\tÔøΩ\u00079\b9ÔøΩÔøΩÔøΩNÔøΩÔøΩÔøΩ<\u001fÔøΩÔøΩHg`]<ÔøΩSS\u001a‘ÇvÔøΩÔøΩ\bÔøΩy ÔøΩ«¥ÔøΩÔøΩIÔøΩ\nO\u0016\nÔøΩÔøΩCÔøΩÔøΩÔøΩÔøΩ\u0012ÔøΩ>nU\u0005yÔøΩÔøΩÔøΩÔøΩœ¥mYpÔøΩt_≈ôÔøΩ7vÔøΩR7ÔøΩÔøΩ\u001fÔøΩ\u0013ÔøΩ/6ÔøΩ\u0003ÔøΩ\nÔøΩko^tÔøΩVÔøΩÔøΩw\nÔøΩajÔøΩÔøΩ?ÔøΩÔøΩÔøΩ)G}ÔøΩ&ÔøΩ]ÔøΩC\u001b$ÔøΩ/ÔøΩ\u0017*NsÔøΩ5ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩuÔøΩMÔøΩÔøΩÔøΩÔøΩœõ\u000fÔøΩ_\"/ÔøΩÔøΩHÔøΩÔøΩO≈ï3ÔøΩ,LÔøΩÔøΩWÎâÄHTÔøΩ\b#dÔøΩÔøΩ_ÔøΩh|_ÔøΩÔøΩÔøΩÔøΩ\u0006\u0011ÔøΩÔøΩÔøΩÔøΩ\u0004ÔøΩ\u0013NÔøΩ\u0007cÔøΩÔøΩ\u0004\u0003ÔøΩÔøΩÔøΩ\nÔøΩ)d'ÔøΩ\u0003`ÔøΩ\t\u0012CQ)ÔøΩÔøΩÔøΩF11\n$ÔøΩÔøΩÔøΩÔøΩ\u0006'5Xs\n?\u0006ÔøΩÔøΩ\u0014TM∆®\u0010&ÔøΩÔøΩÔøΩ\nÔøΩN2\n(ÔøΩ~ÔøΩÔøΩÔøΩ;_ÔøΩ\u0005:ÔøΩ\u0001ÔøΩrÔøΩz7\u0017ÔøΩ5ÔøΩÔøΩ\u000f8ÔøΩC∆ªaÔøΩÔøΩÔøΩ8ÔøΩÔøΩÔøΩ\nÔøΩÔøΩDÔøΩÔøΩ,ÔøΩÔøΩnÔøΩ$ÔøΩ^ÔøΩsÔøΩ\\;yﬁà\u0015S\u0002ÔøΩ'Á¨πgÔøΩÔøΩÔøΩ\u0016ÔøΩÔøΩ]ÔøΩ\nCÔøΩ\u0015\u0001ÔøΩUÔøΩix`ÔøΩÔøΩ1ÔøΩÔøΩGÔøΩÔøΩÔøΩÔøΩ◊åYÔøΩCvÔøΩÔøΩ;ÔøΩÔøΩÔøΩÔøΩ\u0018}ÔøΩsÔøΩwÔøΩcŒ∞ÔøΩ\\,\u0018nÔøΩÔøΩÔøΩÔøΩÔøΩw \u0006]\u0005mÔøΩ)ÔøΩÔøΩ\tÔøΩ\"ÔøΩ~z\n\u0005ÔøΩÔøΩ\\ÔøΩŒâ4K‰∑önÔøΩÔøΩ\u0016AÔøΩ\"\u0017jQÔøΩ\nMÔøΩÔøΩÔøΩ\nÔøΩjÔøΩÔøΩÔøΩÔøΩÔøΩVÔøΩÔøΩÔøΩ[À∫b{d\u000f+vÔøΩ\u001f\u000eÔøΩ*{,ÔøΩOÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩT7>)?ÔøΩ8ÔøΩ>eÔøΩ,ÔøΩ\u000eÔøΩÔøΩeT|\u0012u~ÔøΩ\u0015DÔøΩ\u0011*<ÔøΩÔøΩJÔøΩ\"ÔøΩ\u0013\u0004As?ÔøΩÔøΩ(C\"\u0012K:ÔøΩÔøΩ9ÔøΩÔøΩÔøΩlPÔøΩh0ÔøΩ6l1ÔøΩ5ÔøΩ\u0006kÔøΩœèÔøΩ`ÔøΩ\u0006ÔøΩÔøΩ\"ÔøΩ\u0005ÔøΩv\u0014ÔøΩÔøΩÔøΩ)ÔøΩ\"?IÔøΩ:CÔøΩ;ÔøΩÔøΩ\u0013ÔøΩhÔøΩV\u000eÔøΩÔøΩ3\u0019ÔøΩ_u)ÔøΩ)ÔøΩÔøΩÔøΩÔøΩÔøΩ'ÔøΩÔøΩ'ÔøΩ\n?{‡Æ£ÔøΩÔøΩÔøΩtÔøΩÔøΩÔøΩÔøΩk\neÔøΩ]\u000fmÔøΩvÔøΩ#ÔøΩÔøΩWÔøΩÔøΩNÔøΩ_ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩe;ÔøΩÔøΩzÔøΩÔøΩÔøΩÔøΩ\nV\u0007ÔøΩ[ÔøΩ`ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩsÔøΩÔøΩm]vÔøΩÔøΩÔøΩ\u0013ÔøΩÔøΩÔøΩﬁû\u001fÔøΩt\tv\u0002ÔøΩÔøΩ\n\u0013ÔøΩ1ÔøΩI4ÔøΩ^|cÔøΩ\n=\u0003ÔøΩ{sÔøΩ@Z\nÔøΩ6ÔøΩÔøΩÔøΩQ`ÔøΩ\n;N‘®ÔøΩÔøΩÔøΩÔøΩÔøΩﬂßÔøΩ\u001854ÔøΩ'ÔøΩ\"jÔøΩÔøΩUZL}7ÔøΩÔøΩFÔøΩÔøΩLxÔøΩ1ÔøΩÔøΩ||ÔøΩ='ÔøΩÔøΩÔøΩÔøΩ—∂.<ÔøΩÔøΩÔøΩqÿ∑}ÔøΩ;ÔøΩÔøΩ]<ÔøΩ\bÔøΩ\u0005ÔøΩÔøΩÔøΩ2\nÔøΩ\u001f\u001fG\nÔøΩhÔøΩ)gÔøΩÔøΩ\n5fkÔøΩWdoX\u0018ÔøΩÔøΩl6:ÔøΩZAÔøΩ`ÔøΩÔøΩzÔøΩ\u001aWÔøΩÔøΩ&ÔøΩÔøΩmÔøΩy_\u0010ÔøΩ €ßÔøΩoÔøΩoÔøΩÔøΩÔøΩÔøΩÔøΩK;ÔøΩBÔøΩÔøΩl5;ÔøΩ>\u0001ÔøΩÔøΩÔøΩ\u001f>ÔøΩÔøΩ!ÔøΩ7*\u0001<ÔøΩBNÔøΩ\u0014ÔøΩÔøΩÀµgÔøΩDk\nÔøΩÔøΩ\n\u0013w/ÔøΩÔøΩÔøΩÔøΩc9ÔøΩÃëÔøΩ\u0018cKÔøΩ\u0004~ÔøΩ$B\u0010\nÔøΩÔøΩ\nEY\u000e|“§ÔøΩÔøΩÔøΩK\nQE\u0005*}ÔøΩÔøΩ\u001fÔøΩ0ÔøΩÔøΩQYUÔøΩ=ÔøΩxQ\u0012\u0004hRÔøΩFLqÔøΩneÔøΩÔøΩÔøΩ[ÔøΩ0qHSÔøΩ\u0003{q’Å=ÔøΩNÔøΩ_9x\nÔøΩc…à5ÔøΩ.ÔøΩ\u0018ÔøΩÔøΩncŸ®ÔøΩÔøΩrjÔøΩÔøΩ<ÔøΩ|~ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\n!ÔøΩÔøΩÔøΩÔøΩuÔøΩÔøΩYÔøΩÔøΩD1ÔøΩÔøΩ'l\n,ÔøΩ\u0014=ECÔøΩ{ÔøΩÔøΩf\u0004ÔøΩ≈îÔøΩiÔøΩ\u0010ÔøΩÔøΩ) ÔøΩ#5ÔøΩ+R\u0001ÔøΩÔøΩÔøΩG99CÔøΩ\nCXFJÔøΩÔøΩÔøΩÔøΩD ÔøΩÔøΩ\u00190ÔøΩ\u0004ÔøΩ\n^tÔøΩ%ÔøΩRÔøΩÔøΩ=ÔøΩÔøΩC\n\u0013D\u0002fÔøΩaÔøΩ\"ÔøΩÔøΩ\u0004ÔøΩÔøΩ\u0013-ÔøΩÔøΩÔøΩqÔøΩÔøΩ\u000eÔøΩÔøΩo&ÔøΩ=\u0011jÔøΩœïfs\u0015-6.ÔøΩzÔøΩS9<\u0006\nÔøΩ\n/ÔøΩTÔøΩAÔøΩÔøΩ\u0017ÔøΩ%\u000fÔøΩ_ÔøΩu\u000e\u0017\u0011ÔøΩÔøΩ~ÔøΩJÔøΩ›õÔøΩ^ÔøΩÔøΩÔøΩGÔøΩS)ÔøΩU\\T)ÔøΩ\u001beXEÔøΩÔøΩ\nTÔøΩVÔøΩVIÔøΩ\nÔøΩ\nÔøΩdÔøΩÔøΩH%2ÔøΩ|ÔøΩtÔøΩÔøΩÔøΩ(ÔøΩ\nÔøΩÔøΩNÔøΩÔøΩ1Y2BÔøΩ\u0014ÔøΩA\u0006ÔøΩ\t\u0012ÔøΩb\u0006\tDbF&T»âLÔøΩ\u0004ÔøΩÔøΩÔøΩ2@+ÔøΩT\"ÔøΩV.ÔøΩÔøΩ%bÔøΩÔøΩÔøΩ7ÔøΩ\u0010ÔøΩÔøΩ\nÔøΩTJÔøΩKÔøΩAÔøΩ\u00119yÔøΩ\b#\u0016\n\u0010ÔøΩÔøΩ=ÔøΩT\u001aÔøΩ\"V\u0018\u0016(ÔøΩÔøΩ»≥ÔøΩÔøΩ\u0010“ÉÔøΩ\u0013ÔøΩÔøΩÔøΩ\nÔøΩI\u0010ÔøΩ\b\u0004ÔøΩLÔøΩÔøΩ\u0004N-ÔøΩÔøΩÔøΩF\u0012\u0013|DÔøΩPÔøΩeÔøΩÔøΩÔøΩ\bÔøΩÔøΩÔøΩdÔøΩLÔøΩ\nÔøΩÔøΩBÔøΩP)\u0005ÔøΩÔøΩÔøΩ\n#~!ÔøΩÔøΩÔøΩ\u0000AÔøΩ‘º\u0003ÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩA>«á\u0007iÔøΩ\u0010\u0012hÔøΩ(5ÔøΩQu\u001feDIhÔøΩÔøΩÔøΩ<I\\lÔøΩÔøΩÔøΩ\u0001i\u0015ÔøΩÔøΩNÔøΩÔøΩÔøΩÔøΩ\u000e\u001aHÔøΩÔøΩm,\u001fÔøΩÔøΩÔøΩÔøΩPÔøΩGÔøΩÔøΩ'ÔøΩO]sÔøΩ‚º±ÔøΩIÔøΩÔøΩ\nÔøΩ-\u0018}ÔøΩÔøΩÔøΩÔøΩ·•∑3KÔøΩÔøΩÔøΩ€ÄÔøΩzÚ£àãGÔøΩÔøΩc\u0012@\bÔøΩDÔøΩÔøΩ\u000f\bÔøΩIUÔøΩ*ÔøΩK@\u0004V\u0000ÔøΩO\u000eÔøΩ-\u0001ÔøΩ\u000fÔøΩÔøΩÔøΩÔøΩ4ÔøΩÔøΩÔøΩPÔøΩ]\u001b\\ÔøΩ][\u0011ÔøΩ’õ_ÔøΩ\u0003ÔøΩ`ÔøΩÔøΩ\u0000(ÔøΩ/ÔøΩÔøΩ?ÀøÔøΩÔøΩzÔøΩ\nÔøΩ\u000e@A?SJÔøΩ}ÔøΩÔøΩœüÔøΩiÔøΩYÔøΩÔøΩ/ÔøΩ\u00161ÔøΩÔøΩÔøΩ']ÔøΩ/ÔøΩÔøΩ»êC_]\u0012ÔøΩÔøΩ.ÔøΩ(NÔøΩt$ÔøΩÔøΩ+“∞\u0016yÔøΩÔøΩhÔøΩxÔøΩ%ÔøΩÔøΩÔøΩÔøΩ\"2\u00033ÔøΩÔøΩ/-ÔøΩa;ÔøΩÔøΩÔøΩ#ÔøΩ~YÔøΩgÔøΩWÔøΩ;\u0010\u0007K5…°ÔøΩÔøΩÔøΩÔøΩÔøΩ\"T\u0007K\bÔøΩÔøΩ\u0001ucXTpÔøΩÔøΩÔøΩÔøΩÔøΩD∆ÅÔøΩ\u0004?C\u001bJÔøΩÔøΩ`Q¬±1ÔøΩÔøΩuÔøΩ¬åÔøΩqÔøΩ}H#ÔøΩÔøΩÔøΩÔøΩÔøΩ7$ÔøΩ\u0018ÔøΩ\nÔøΩÔøΩ3v\u0015ÔøΩS0\u0019uÔøΩÔøΩÔøΩaÔøΩ^ÔøΩÔøΩÔøΩd_ÔøΩ4lÔøΩ\u0015AÔøΩÔøΩÔøΩ\u0016X6ÔøΩ‚Ü•\tÔøΩÔøΩÔøΩ>t;ÔøΩÔøΩÔøΩ|\u0002\n\u0007ÔøΩ{Jœ≤\b\u001aÔøΩÔøΩÔøΩÔøΩÔøΩ\u0013ÔøΩf\u0001B¬ø!$\u0006ÔøΩ ÔøΩ\nBÔøΩw\u0011RÔøΩ\u0011R}ÔøΩÔøΩÔøΩ&ÔøΩt+\u00102ÔøΩFÔøΩ$DÔøΩ|\nBÔøΩÔøΩ\u0010ÔøΩÔøΩ\n!ÔøΩbÔøΩ\n,ÔøΩÔøΩ\bÔøΩ~ÔøΩP`\u001bÔøΩ(\bÔøΩÔøΩ6BÔøΩÔøΩ\u0010p\u0006hÔøΩÔøΩ\u0011ÔøΩXÔøΩPÔøΩÔøΩÔøΩÔøΩlÔøΩm\\ÔøΩÔøΩ≈Ç$H\u0011\u0012ÔøΩ')Z‹ï(ÔøΩÔøΩ(ÔøΩ\u0004@ÔøΩ\u0014SÔøΩ\u0000_ÔøΩLQÔøΩDJ\u0016cÔøΩÔøΩ\u0012XÔøΩ[\u0001XxwAÔøΩÔøΩ\nÔøΩnÔøΩi<ÔøΩÔøΩ#ÔøΩ>ÔøΩÔøΩÔøΩN3cÔøΩÔøΩ\u0005ÔøΩ~PÔøΩ_ÔøΩN;ÔøΩÔøΩzÔøΩigÔøΩÔøΩÔøΩÔøΩÔøΩq3ÔøΩqÔøΩ\n7MÔøΩÔøΩ{\u0001ÔøΩÔøΩ,ÔøΩi\bÔøΩÔøΩgÔøΩ=ÔøΩÔøΩÔøΩ\u0017\u0014ÔøΩ\n0vÔøΩ\n∆∫ÔøΩ3vd\u0012ÔøΩÔøΩ ÔøΩ7\u0019;ÔøΩ\nÔøΩz?c,\nÔøΩÔøΩ\n1ÔøΩÔøΩÔøΩœÅÔøΩÔøΩÔøΩ\u0010ÔøΩÔøΩ\tÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩwÔøΩÔøΩÔøΩ\u0013iÔøΩNxÔøΩZÔøΩÔøΩÔøΩÔøΩ\u0006ÔøΩR\u0001ÔøΩÔøΩÔøΩÔøΩmfÔøΩÔøΩkﬁÅ3\u0000ÔøΩÔøΩÔøΩ!ÔøΩT;ÔøΩÔøΩR|ÔøΩ/ÔøΩÔøΩÔøΩÔøΩ\u0010ÔøΩÔøΩÔøΩ\bXf\u0019ÔøΩ\u001f\u0004ÔøΩf\u0007ÔøΩ\nÔøΩÔøΩ5ÔøΩÔøΩ\nÔøΩÔøΩuÔøΩÔøΩ\u0004\\ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩKÔøΩÔøΩÔøΩÔøΩ\u0007\u0004\\SqÔøΩ:/ÔøΩ\nÔøΩÔøΩT)ÔøΩZÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ/ÔøΩf\u0004ÔøΩÔøΩ]ÔøΩÔøΩ“∑ÔøΩÔøΩÔøΩÔøΩÔøΩbsSÔøΩ5\u0001ÔøΩXeÔøΩ\n\u0002ÔøΩYwÔøΩOÔøΩnÔøΩÔøΩ> ÔøΩ\nV[?(ÔøΩJÔøΩPÔøΩÔøΩÔøΩXÔøΩÔøΩÔøΩ√∂ÔøΩ_\u0017p5ÔøΩÔøΩÔøΩÔøΩÔøΩk\\ÔøΩÔøΩ6\nx\u0003;ÔøΩÔøΩM\u0001◊≤ÔøΩÔøΩÔøΩ\u0004ÏïøZÔøΩ\u0015ÔøΩF\u0016hÿèÔøΩbÔøΩFÔøΩolÔøΩ\tÔøΩ\nÔøΩÔøΩ\tÔøΩ ÔøΩÔøΩ\u0004ÔøΩÔøΩ'\bÔøΩDÔøΩ7ÔøΩÔøΩÔøΩÔøΩÔøΩ\n\u0004W\u0011~IÔøΩÔøΩÔøΩ`\u000fÔøΩ\u001bÔøΩ\u0002\u0006|ÔøΩ\u0016ÔøΩÔøΩÔøΩ9ÔøΩÔøΩÔøΩaÔøΩ_\u000esÔøΩrÔøΩÔøΩÔøΩÔøΩ‹ø\nÔøΩÔøΩÔøΩ0ÔøΩ/ÔøΩÔøΩ9ÔøΩÔøΩÔøΩaÔøΩ_\u000esÔøΩrÔøΩÔøΩÔøΩÔøΩ‹ø\nÔøΩÔøΩEÔøΩ\u0006mÔøΩxQÔøΩ`ÔøΩÔøΩ^ÔøΩ7\u0000~ÔøΩÔøΩ}\u0003ÔøΩ;ÔøΩÔøΩ\u0018ÔøΩO\tÔøΩÔøΩ:\u001aS\u0004o\u0004ÔøΩÔøΩÔøΩM\u0001ÔøΩŸæ∆ß\tÔøΩ#ÔøΩ?\u00120ÔøΩÔøΩ+ÔøΩÔøΩ ~ÔøΩ\u0016\u0001#ÔøΩ?\bÔøΩZfÔøΩe6ÔøΩFÔøΩQ\u0001\u0003ÔøΩÔøΩÔøΩ\u0004o'ÔøΩ)`ƒü\"ÔøΩ\u0011ÔøΩÔøΩ|ZÔøΩÔøΩgÔøΩ#\u00047\u0011ÔøΩÔøΩ\u0005ÔøΩÔøΩ/\u0010ÔøΩÔøΩlÔøΩ]eÔøΩ*ÔøΩÔøΩ\u0004ÔøΩ|ÔøΩÔøΩÔøΩ»ßÔøΩEÔøΩÔøΩÔøΩI&ÔøΩ\u0000ÔøΩO\n\u0018ÔøΩAÔøΩ€êO”úÔøΩÔøΩO\u0013ÔøΩÔøΩSfOÔøΩÔøΩ=ezyÔøΩÔøΩÔøΩ-ÔøΩÔøΩ-ÔøΩÔøΩ-ÔøΩKmÔøΩ/ÔøΩlÔøΩeÔøΩ\t\tÔøΩÔøΩ\u0004\nÔøΩÔøΩ\u0000ÔøΩ8ÔøΩ)ÔøΩGÔøΩIÔøΩKp\u0004ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0002\u0018[\nÔøΩ\u0006QÔøΩÔøΩIÔøΩÔøΩ\u0000@\u0003ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩKÔøΩÔøΩl\nFÔøΩ,[ÔøΩÔøΩ\u0001w\nzÔøΩ^ÔøΩ\nÔøΩW\u0007k\u0013PÔøΩÔøΩQÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩg?84ÔøΩ\nÔøΩÔøΩÔøΩ\nÔøΩqXÔøΩ`)¬©ÔøΩ$ÔøΩ9ÔøΩ1\u0001ÔøΩ\u0001ÔøΩÔøΩH~ÔøΩ\u000eÔøΩ!ÔøΩ8ÔøΩ]\u0002ÔøΩ\tÔøΩÔøΩÔøΩ5S\u0001ÔøΩÔøΩL\u0006=ÔøΩBÔøΩÔøΩ8`\nÔøΩq\u0004◊≥ÔøΩÔøΩÔøΩFiÔøΩÔøΩÔøΩÔøΩÔøΩ\bÔøΩÔøΩ`ÔøΩ\u0004iÔøΩ\u0012ÔøΩ<\nÔøΩÔøΩ$ÔøΩÔøΩÔøΩ\u0016X#SÔøΩ:ÔøΩÔøΩ=ÔøΩgIÔøΩÔøΩ\u0001ÔøΩ9ÔøΩdÔøΩ$GÔøΩvCÔøΩOÔøΩ\u0018!\u001a\nFlÔøΩÔøΩÔøΩÔøΩÔøΩqÁºó9ÔøΩÔøΩ\nm+ÔøΩÔøΩ\u0000ÔøΩIÔøΩc\nÔøΩCpÔøΩÔøΩÔøΩ\u0010>\u0005XÔøΩo\bÔøΩ\u0013ÔøΩ\u001f\u0004ÔøΩ\u0018ÔøΩÔøΩÔøΩcÔøΩs\u0010^#ÔøΩ\ng^<KÔøΩ5NÔøΩÔøΩÔøΩÔøΩrÔøΩ#ÔøΩÔøΩÔøΩ\u0016ÕêÔøΩ\u0019ÔøΩ\niÔøΩ~\\ÔøΩ?\ny&ÔøΩ\bÔøΩÔøΩÔøΩÔøΩÔøΩeÔøΩ6D,eÔøΩÔøΩ*ÔøΩÔøΩÔøΩYÔøΩœñ÷åÔøΩÔøΩLÔøΩoÔøΩ4\u0017ÔøΩÔøΩÔøΩO\u0011=ÔøΩ\u0004sÔøΩÔøΩHÔøΩ(ÔøΩ\t7E\\tÔøΩo\u001afÔøΩÔøΩ'ÔøΩjÔøΩ0>Ct&»Å6ÔøΩ(ÔøΩk:7ÔøΩLÔøΩs9ÔøΩiÔøΩÔøΩÔøΩ\u0010ÔøΩÔøΩÔøΩqÔøΩC|\npIÔøΩ/AÔøΩKÔøΩÔøΩ^ÔøΩÔøΩ\n-ÔøΩÔøΩqÔøΩ\tÔøΩ7Z/.ÔøΩﬂÇ~ÔøΩrÔøΩK=)<ÔøΩ\u0016ÔøΩoÔøΩÔøΩIÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩ *÷ÆÔøΩÔøΩhÔøΩ\u0019ÔøΩÔøΩ,ÔøΩÔøΩÔøΩskÔøΩÔøΩÔøΩÔøΩ¬µ\u0003ÔøΩÔøΩiÔøΩÔøΩÔøΩÔøΩÔøΩ\u001a_p?ÔøΩÔøΩlÔøΩ\u000e_ÔøΩ&>1ÔøΩ&HÔøΩ[ÔøΩ*b1MÔøΩ$\nwÔøΩÔøΩbuÔøΩÔøΩÔøΩyÔøΩjTÔøΩÔøΩÔøΩÔøΩwÔøΩDmÔøΩÔøΩ-ÔøΩÔøΩYÔøΩRIÔøΩmÔøΩÔøΩ!ÔøΩÔøΩ2ÔøΩ\nÔøΩÔøΩ\u0012yÔøΩÔøΩÔøΩ%D\\,SÔøΩ@ÔøΩÔøΩ`ÔøΩ,ÔøΩÔøΩÔøΩKÔøΩpÔøΩ £\u001b+\bF\u0003ÔøΩ?œ™ÔøΩÔøΩÔøΩbÔøΩÔøΩÔøΩÔøΩÔøΩiÔøΩ\nÔøΩIÔøΩÔøΩÔøΩC\n#ÔøΩ\u0007\u0000ÔøΩ\u0013ÔøΩ61ÔøΩcÔøΩÔøΩÔøΩ*\u001fXÔøΩÔøΩÔøΩ\u0019ÔøΩC(IÔøΩC\u0019fÔøΩÔøΩWÔøΩb\nÔøΩ\u0015ÔøΩ\u0017ÔøΩÔøΩÔøΩ\\ÔøΩ\u0011ÔøΩXŒ±\"ÔøΩÔøΩ~ÔøΩÔøΩÔøΩ\u001a8T\u0003ÔøΩ\u001bÔøΩqÔøΩcÔøΩ*['ÔøΩÔøΩÔøΩÔøΩV^ÔøΩBLÔøΩ6ÔøΩÔøΩ\\\"ÔøΩ\u000fFPOÔøΩ\u0003ÔøΩQÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ:\u0019>qHÔøΩ6\bwmÔøΩÔøΩwÔøΩÔøΩgÔøΩ{ÔøΩ\nÔøΩ\u000e\u0018\nÔøΩHÔøΩÔøΩÔøΩX\u0018.ÔøΩuÔøΩ\nÔøΩ\u0013\u0002^ÔøΩ\nÔøΩÔøΩd=pÔøΩh\u0007{ÔøΩ+\u0000zÔøΩÔøΩÔøΩ\u0018UÔøΩ/ÔøΩ'\u0010\u001aÔøΩÔøΩÃïÔøΩÔøΩ“ÜÔøΩÔøΩ(ÔøΩ\u0018ÔøΩÔøΩÔøΩ∆¨ÔøΩÔøΩEqÔøΩ\\1CÔøΩ\u0011ÔøΩ#|pÔøΩÔøΩ\u0017/E\u0000ÔøΩm¬∞\u0003/W6ÔøΩÔøΩLÔøΩ\\ÔøΩÔøΩÔøΩÔøΩ\u0004ÔøΩrÔøΩÔøΩÔøΩnÔøΩÔøΩ⁄™SGÔøΩ{ÔøΩxÔøΩ)?5ÔøΩ88AÔøΩÔøΩEVÔøΩ)ÔøΩÔøΩÔøΩibg=X\u0016G9ÔøΩ5FÔøΩiÔøΩÔøΩ9ÔøΩÔøΩ*ÔøΩeSÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩAx6ÔøΩÔøΩÔøΩ*ÔøΩQ∆≥1]⁄ã&)\nLÔøΩlÔøΩ^QÔøΩS^ÔøΩ0ÔøΩlÔøΩ$ÔøΩÔøΩ\u0000rÔøΩÔøΩeÔøΩz'(7ÔøΩ\u0015\u0011jÔøΩ}ÔøΩœö\\ÔøΩÔøΩELÔøΩÔøΩFÔøΩÔøΩÔøΩWÔøΩﬁ¶ÔøΩuÔøΩy)qv\\y\u0002ÔøΩy,ÔøΩ.,ÔøΩÔøΩ\u0003\u0017k\u0010ÔøΩ\u001bo\n'ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0016ÔøΩ6J\nIÔøΩ%ÔøΩEUfÔøΩÔøΩÔøΩÔøΩTg\u000fRfÔøΩÔøΩ[3bWÔøΩ{\u001bÔøΩÔøΩLÔøΩŸ™hU\nE3tjÔøΩ)ÔøΩDÔøΩ*eÔøΩÔøΩÔøΩ\u0014ÔøΩi\ns7ÔøΩÔøΩo-«∏v}\u00149<ÔøΩÕíÔøΩ<.ÔøΩR}ÔøΩÔøΩXÔøΩÔøΩ\\ÔøΩTÔøΩÔøΩ\u0014#≈äÔøΩ%ÔøΩÔøΩÔøΩÔøΩÔøΩ+C'ÔøΩ,ÔøΩÔøΩÔøΩp\u0014t]ﬁü\u000fÔøΩ\"ÔøΩ\u0018Õ©ÔøΩÔøΩbÔøΩÔøΩÔøΩ&MÔøΩ\"ÔøΩ…èÔøΩJhÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩ\u0015ÔøΩÔøΩÔøΩb∆≠‹ìÔøΩJÔøΩ@ÔøΩÔøΩ.IÔøΩÔøΩ=ÔøΩÔøΩ\u0014ÔøΩ\u0006Uq~\u0006ÔøΩÔøΩÔøΩ34ÔøΩr?ÔøΩÔøΩ\u0011SÔøΩ√à\tﬂ§`\u000eÔøΩÔøΩÔøΩ@\u0017ÔøΩ\u0015fYqÔøΩ^ÔøΩ'ÔøΩ4ÔøΩÔøΩÔøΩÔøΩ[(E{ÔøΩÔøΩ\n\u0012gT{EÔøΩÔøΩXÔøΩÔòä'\u0016sE\nÔøΩS|eWxÔøΩÔøΩYÔøΩw@f\u00197~2»êOfWPÔøΩ≈πÔøΩ!\nÔøΩÔøΩÔøΩÔøΩÔøΩ\t⁄â>\u0005ÔøΩ€°uÔøΩ\u0012ÔøΩ\\ÔøΩtÔøΩÔøΩHÔøΩyÔøΩÔøΩÔøΩhÔøΩÔøΩ\n\u0005J3ÔøΩ+ÔøΩÔøΩ\u0013≈öÔøΩÔøΩXÔøΩ8÷æjÔøΩ/ÔøΩ\nÔøΩÔøΩÔøΩ\t-ÔøΩÔøΩ/ÔøΩÔøΩ”∫ÔøΩ\u000fÔøΩt\u0000ÔøΩÔøΩÔøΩV∆¥4ÔøΩ0ÔøΩj&\u0019\nÔøΩ\u0003ÔøΩÔøΩÔøΩC‘éÔøΩÔøΩ13ÔøΩEÔøΩÔøΩ\nO√ºÔøΩÔøΩ#\nm–Ñ\u0002j4ÔøΩTÔøΩ\u0018S”éÔøΩÔøΩÔøΩm›öÔøΩÔøΩÔøΩFJÔøΩ’ìzN=cÔøΩÔøΩÔøΩÔøΩÍ∏•ÔøΩÔøΩÔøΩf]RÔøΩÔøΩM\u0017S-} ∞\nÔøΩÔøΩ„™ëV\n=;ÔøΩÔøΩjÔøΩÔøΩWÔøΩGÔøΩSÔøΩD@ÔøΩÔøΩqUOÔøΩzn\u001aÔøΩ\u0002EN=ÔøΩ\u0016UÔøΩ;Z“àÔøΩÔøΩÔøΩ-\u001bÔøΩÔøΩ\u0002ÔøΩ!uﬂà\u0011ÔøΩLÔøΩL8ÔøΩÔøΩ\nÔøΩÔøΩxtÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ<5~|ÔøΩxtÔøΩÔøΩÔøΩÔøΩÔøΩ!ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩcÔøΩjÔøΩÿôÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u001aoÔøΩÔøΩÔøΩaÔøΩNÔøΩÔøΩ\bÔøΩÔøΩ\u0019ÔøΩÔøΩÔøΩ3ÔøΩ:ÔøΩÔøΩ\u0003ÔøΩSÔøΩÔøΩÔøΩÔøΩ%ÔøΩ\nÔøΩRÔøΩÔøΩÔøΩÔøΩYuÔøΩÔøΩÔøΩÃò9CÔøΩfÔøΩqÔøΩ\">ÔøΩ}ÔøΩF&ÔøΩ\n\u0012ÔøΩi ◊¶,]OÔøΩi'ÔøΩNÔøΩÔøΩimFWÔøΩIG3ÔøΩ0ÔøΩY!\n*ÔøΩÔøΩ,]ÔøΩ\n`fÔøΩqÔøΩÔøΩcNrVMXfjY.\u0013ÔøΩ2ÔøΩt\"ÔøΩ\u0001ÔøΩÔøΩ8ÔøΩÔøΩ2&ÔøΩ\u000eÔøΩ\u00061Õ¥^ÔøΩÔøΩÔøΩvQ(ÔøΩ\u000eÔøΩLQÔøΩ\nÔøΩÔøΩÔøΩh…¨6ÔøΩ\u0004ÔøΩm[wÔøΩg\u0007‘≥ÔøΩn€§<i\u0001:\tg;&LÔøΩ3zÔøΩH\u0018ÔøΩÔøΩÔøΩÔøΩ`≈¥cÔøΩÔøΩhÔøΩ\u0016ÔøΩ\u001b\u00183ZRÔøΩ(ÔøΩ\u000e\"ÔøΩ\"ÔøΩÔøΩzÔøΩjÔøΩÔøΩFÔøΩ@ÔøΩ`\u0011ÔøΩÀôÔøΩ%ÔøΩÔøΩaÔøΩ\u0000[\u0010ÔøΩÔøΩA\nf'ÔøΩÔøΩ=ÔøΩÔøΩ\u0000/nÔøΩ6ÔøΩÔøΩÔøΩÔøΩÔøΩ,\u001anÔøΩB+\u0017\"{\nO,+ÔøΩÔøΩgÔøΩ\u0007ÔøΩÔøΩMÔøΩÔøΩÔøΩtLÔøΩÔøΩB\u0003KÔøΩMÔøΩÔøΩÔøΩÔøΩMÔøΩ!ÔøΩg\n\b_ÔøΩÔøΩÔøΩÔøΩ#\nxR7 ÔøΩÔøΩ«êÔøΩÔøΩ#ÔøΩ\u0005\n8ZÔøΩYÔøΩ1*ÔøΩ\tÔøΩ\u00137fK\"ÔøΩ&ÔøΩ m&ÔøΩ\"#XGszÔøΩÔøΩÔøΩXTmSÔøΩuwvÔøΩWÔøΩÔøΩÔøΩm\nÔøΩ\nÔøΩÔøΩgÔøΩ\u0001ÔøΩ\u0011\nvvBÔøΩ\u0015ÔøΩRÔøΩ\u000e\u001f:rËà∑fÔøΩq2=ÔøΩÔøΩÔøΩ\\.ÔøΩ*:>fÔøΩÔøΩsBW\u0007,-ÔøΩÔøΩÔøΩÔøΩ\u0006ÔøΩÔøΩ”òÔøΩAlÃÇÔøΩÔøΩaÔøΩÔøΩÔøΩ9#ÔøΩ\u0006'4+ÔøΩ\u0006\b\nÔøΩ(ÔøΩ,ÔøΩ$\u0014`ÔøΩÔøΩO\u0018ÔøΩÔøΩÔøΩZ&ÔøΩkÔøΩt 9ÔøΩXmxi\u0000\u001bÔøΩ0ÔøΩqpEZÔøΩÔøΩ\u0019\nÔøΩÔøΩ ÔøΩ(7mƒ¶!5’úfÔøΩqÔøΩ6ÔøΩ ÔøΩ\u0002*T\u0010p#ÔøΩ7;iÔøΩÔøΩÔøΩ4f—§\u000eÔøΩÔøΩ≈¨@ÔøΩB(%„∂ö2A\u0000;\u001bÔøΩAx'ÔøΩIÔøΩ\u001bÔøΩÔøΩ)ÔøΩlÔøΩÔøΩÔøΩÔøΩjS\u0006\u0006mÔøΩkoÔøΩ9ÔøΩ~\bÔøΩÔøΩ.\nÔøΩ*+@ÔøΩÔøΩ\u0004ÔøΩ\bÔøΩl\\e\u0013ÔøΩÔøΩ%ÔøΩÔøΩÔøΩ Q\u001aÔøΩÔøΩ\u0011ÔøΩÔøΩÔøΩÔøΩ4\u0004ÔøΩÔøΩ?ÔøΩÔøΩ!r@I\nÔøΩÔøΩ\nU+\u0014\u0015L4c&gÔøΩ\u0013ÔøΩÔøΩÔøΩÔøΩ\u0001ÔøΩ\u0006ZÔøΩ,ÔøΩoÔøΩ1XÔøΩOÔøΩÔøΩÔøΩ&ÔøΩ\u0007[\u001a\u0018OvQpÔøΩ\\\n\u0003)ÔøΩÔøΩ@ÔøΩÔøΩ8sTÔøΩ2ÔøΩÔøΩ’®\u0014ÔøΩ&)ÔøΩ\u000fÔøΩÔøΩÔøΩÔøΩ)ÔøΩ\u0016\u0013ÔøΩÔøΩvfÔøΩÔøΩÔøΩiÔøΩÔøΩ \u0004nÔøΩ\u0011ÔøΩUH7ÔøΩ\u0016ÔøΩ2\"ÔøΩÔøΩP\u000fÔøΩÔøΩÔøΩÔøΩLÔøΩÔøΩIÔøΩ:&ÔøΩÔøΩ\u0001\u00158ff3ÔøΩbÔøΩLÔøΩ&l\u0018 ÔøΩÔøΩÔøΩ\u0005ÔøΩÔøΩ6ÔøΩ:82kÔøΩ8ÔøΩ4ÔøΩK6\tÔøΩ“¶ÔøΩÔøΩÔøΩÔøΩÔøΩ<*,\n6&,,&ÔøΩ–∏\u0019ÔøΩr\u0015ÔøΩXKÔøΩ&ÔøΩA1ÔøΩ$5^›µ8ÔøΩrÔøΩ@]\u0003_ÔøΩ\u0004ÔøΩÔøΩN*ŸûrÔøΩÔøΩÔøΩÔøΩ)ÔøΩÔøΩÔøΩÔøΩÔøΩxÔøΩp\u001b\nÔøΩÔøΩ-NÔøΩÔøΩIÔøΩÔøΩÔøΩÔøΩÔøΩ]ÔøΩH|ÔøΩ^ÔøΩÔøΩÔøΩ1z3ÔøΩÔøΩThÔøΩR_\"ÔøΩÔøΩÔøΩ_ÔøΩÔøΩÔøΩ^ÔøΩvÔøΩÔøΩ÷§ÔøΩovÔøΩ9ÔøΩsÔøΩuÔøΩÔøΩÔøΩ _ÔøΩÔøΩÔøΩÀøÔøΩKÔøΩÔøΩkrÔøΩÔøΩw=ÔøΩ\u0001ÔøΩ4P;ÔøΩ ÔøΩYÔøΩÔøΩÀåÔøΩFÔøΩUkÔøΩ“õÔøΩ[ÔøΩÔøΩ_ƒër]IÔøΩ;ÔøΩÔøΩÔøΩ;ÔøΩÔøΩvÔøΩn—ìÔøΩ\u0018#RGÔøΩ“óÔøΩÔøΩ<ÔøΩLIcÔøΩÔøΩÔøΩÔøΩ-X/-\n5ÔøΩ\u0013ÔøΩ÷£\n\u00049\n\"XÔøΩÔøΩ_Œñ>yÔøΩ/ÔøΩvÔøΩÔøΩÔøΩ\u001bÔøΩ\u0004\u0019>gUÔøΩÔøΩÔøΩwÔøΩ!xÔøΩ\u000eÔøΩÔøΩfÔøΩÔøΩ^xÔøΩ\nÔøΩÔøΩdgYÔøΩÔøΩÔøΩw áÔøΩ\u0004ÔøΩÔøΩ]`ÔøΩX7\u0018f\u0014ÔøΩÔøΩÔøΩYÔøΩÔøΩÀ∫ÔøΩÔøΩ’≥{X\u0007ÔøΩÔøΩÔøΩÔøΩ6ÔøΩ\u0006ÔøΩÔøΩmcÔøΩY\u0013ÔøΩÔøΩnc;ÔøΩ\u000eV{mÔøΩ\nÔøΩÔøΩÔøΩÔøΩ`(ÔøΩ\b}kÔøΩÔøΩ¬æÔøΩ!>ÔøΩÔøΩ\u001fz4ÔøΩEÔøΩŒûÔøΩÔøΩ\n\\ÔøΩÔøΩÔøΩf\nZ\u0005.ÔøΩ|ÔøΩu\u0005ÔøΩSÔøΩÔøΩZa{\u0013ÔøΩZ,ÔøΩÔøΩ\tÔøΩp7\u0007\u0016\u000eÔøΩÔøΩﬁèÔøΩ»ãÔøΩ\u0013ÔøΩ\\ÔøΩ|ÔøΩÔøΩÔøΩ\u0016ÔøΩ\u0005BÔøΩF7ÔøΩÔøΩÔøΩÔøΩkLÔøΩKÔøΩ0`ÔøΩÔøΩÔøΩXÔøΩGÔøΩ\u0014ÔøΩ!/ÔøΩ:ÔøΩ\u0000ÔøΩX`ÔøΩpeÔøΩÔøΩ\u0000ÔøΩJ\u0010fÔøΩÔøΩ\u000fÔøΩ'p-ÔøΩÔøΩfÔøΩÔøΩÔøΩ\n?ÔøΩÔøΩÔøΩÔøΩÔøΩtIÔøΩ.0ÔøΩ\u0003ÔøΩ\nÔøΩ\u0014ÔøΩUÔøΩÔøΩs\u0015ÔøΩ9◊úÔøΩx›ïÔøΩœ°H;\nÔøΩÔøΩ\u0010ÔøΩÔøΩÔøΩÔøΩ3\u0001b>*ŸàxSÔøΩÔøΩÔøΩ\u0003ÔøΩ_zÔøΩuÔøΩ\u0010\nG\u0016ÔøΩ\nPÔøΩ\u0000fÔøΩÔøΩ;ÔøΩwÔøΩxÔøΩJ}ÔøΩÔøΩS7ÔøΩÔøΩ{\u0003ÃíÔøΩÔøΩ\u0003◊ïW‰àº{?,ÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\u001bÔøΩÔøΩÔøΩb=\nÔøΩÔøΩ\u0012ÔøΩ]\b7/ÔøΩÔøΩ\u0017ÔøΩ@ÔøΩÔøΩ\u0005_ ÔøΩ\u000fÔøΩA?\u0018o…èÔøΩ\u0007\u001aÔøΩ›àÔøΩÔøΩP\nÔøΩÔøΩÔøΩ'TÔøΩÔøΩÔøΩÔøΩ!|hÔøΩ¬±ÔøΩ\u0010>\u0015ÔøΩ0ÔøΩ_?ÔøΩXÔøΩÔøΩ,ÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩ0ÔøΩ)\n\boF“ÆÔøΩÔøΩÔøΩ◊ÅÔøΩQ\u0016^ÔøΩÔøΩÔøΩ6ÔøΩÔøΩpÔøΩÔøΩÔøΩ=ÔøΩ:ÔøΩÔøΩ3TÔøΩ;\u0010ÔøΩÔøΩÔøΩ\u0017ÔøΩ~\u0019ÔøΩÔøΩk\nÔøΩÔøΩÔøΩÔøΩCÔøΩ{ÔøΩÔøΩÔøΩÔøΩÔøΩ>wÔøΩÔøΩ0ÔøΩÔøΩÔøΩCÔøΩ=ÔøΩ{ÔøΩ{ÔøΩ\u0006ÔøΩ\u000e`ÔøΩÔøΩ>_ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0018ÔøΩÔøΩÔøΩÔøΩ\"uÔøΩ&\u0005ÔøΩW\nT\\\u001bÔøΩÔøΩ0hÔøΩÔøΩ\nÔøΩÔøΩ(ÔøΩÔøΩ\u0002D€îÔøΩÔøΩUWÔøΩ\\\u0017+ÔøΩTÔøΩÔøΩ\\\u0006ÔøΩÔøΩÔøΩÔøΩa|NXÔøΩeeOhOKÔøΩ\u0019TiÔøΩÔøΩ?LÔøΩ\n4ÔøΩ\\ÔøΩY\u0005nÔøΩÔøΩZsk8ÔøΩ\nÔøΩÔøΩÔøΩÔøΩ{ÔøΩƒ©PÔøΩ0\nÔøΩYÔøΩÔøΩ<ÔøΩÔøΩÔøΩ\\ÔøΩ9ÔøΩÔøΩÔøΩÔøΩ\u0004ÔøΩÔøΩÔøΩÔøΩ\u0017zÔøΩBÔøΩÿ∑ÔøΩÔøΩ6#ÔøΩ@ÔøΩn!(ÔøΩÔøΩ?ÔøΩÔøΩÔøΩÔøΩwub\u000fÔøΩÔøΩnwÔøΩ\u0001?ÔøΩYÔøΩ+ÔøΩÔøΩÔøΩ\u0011\nÔøΩÔøΩaÔøΩÔøΩGjÔøΩÔøΩxÔøΩÔøΩiwÔøΩ…∑`)IÔøΩÔøΩ\u0017ÔøΩb\u0001ÔøΩKÔøΩlÔøΩÏ≠îÔøΩu]qÔøΩÔøΩzÔøΩÔøΩ~VÔøΩ\"ÔøΩ-ÔøΩ'ÔøΩMÔøΩÔøΩÔøΩ,+rÔøΩÔøΩ+ÔøΩÔøΩ/ÔøΩ\u0015ÔøΩÔøΩÔøΩ\\\u001fÔøΩs/BÔøΩ,\\ÔøΩÔøΩ%ÔøΩvh{ÔøΩ2ÔøΩÔøΩ\nƒêÔøΩNA\nÔøΩ^ÔøΩCÔøΩKP/F1ÔøΩ\\\\5ÔøΩÔøΩ!ÔøΩ\u0005ÔøΩÔøΩÔøΩ\u0018^yx\u0001ÔøΩÔøΩÔøΩ.ÔøΩuH\u0011ÔøΩ%IÔøΩZr1\u000f€±\u0003ÔøΩÔøΩÔøΩzO$ÔøΩÔøΩÔøΩ5W\nÔøΩd^ÔøΩ(ÔøΩ]ÔøΩ6EvvzÔøΩÔøΩÔøΩÔøΩFÔøΩwÔøΩÔøΩ;ÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩ=ÔøΩÔøΩÔøΩÔøΩÔøΩ÷π\u000eAAÛ∫ö∞ÔøΩÔøΩÔøΩ⁄øÔøΩvÔøΩ⁄ÉÔøΩÔøΩÔøΩÔøΩSÔøΩÔøΩÔøΩÔøΩÔøΩ]ÔøΩ;ÔøΩzSÔøΩjÔøΩÔøΩÔøΩÔøΩ\nPÔøΩQÔøΩtÔøΩÔøΩÕºÔøΩ(ÔøΩ\nÔøΩÔøΩÔøΩVÔøΩÔøΩÔøΩM#ÔøΩXÔøΩÔøΩÔøΩÔøΩlÔøΩyÔøΩ`ÔøΩmLqm+ÔøΩ;ÔøΩEÔøΩ÷ÇÔøΩ\u0017:OÔøΩÔøΩÔøΩ\u0012mtUÔøΩfÔøΩ\u0004ÔøΩ\u0015pÔøΩÔøΩÔøΩ\u0016ÔøΩÔøΩ\u00147ÔøΩ%\u0016ÔøΩ* ÔøΩ\u000fkÔøΩÔøΩAÔøΩRÔøΩ@YÔøΩÔøΩÔøΩwÔøΩÔøΩ3ÔøΩ\n\u000fÔøΩÔøΩÔøΩÔøΩfÔøΩ\u0007ÔøΩÔøΩÔøΩyÔøΩÔøΩS\u0016ÔøΩÔøΩODÔøΩtÔøΩ%\tÔøΩÔøΩ\u0018ÔøΩhÔøΩÔøΩGÔøΩ\u0005\u0017ÔøΩ\u0005¬¥ÔøΩLÔøΩ\u001fÔøΩÔøΩd!h)ÔøΩzÔøΩ\u0012ÔøΩÔøΩÔøΩiÔøΩÔøΩÔøΩ\u0006!DÔøΩÔøΩ\u0015ÔøΩZ\nÔøΩÔøΩ(ÔøΩWÔøΩÔøΩ.ÔøΩe/ÔøΩgÔøΩÔøΩaÔøΩÔøΩÔøΩÔøΩ\"ÔøΩÔøΩ\nÔøΩÔøΩ]tÔøΩ\u0017ÔøΩ\u001fÔøΩ\u0017%ÔøΩÔøΩ\u0011ÔøΩ}ÔøΩÔøΩ|\u0018ÔøΩÔøΩÔøΩ\u0006ÔøΩ_ÔøΩ\u001f*?\n~KÔøΩ;ÔøΩÔøΩKzUÔøΩ[ÔøΩÔøΩ ªÔøΩE7\u0010ÔøΩ\u0016$¬óÔøΩÔøΩÔøΩr3 ÔøΩÔøΩÔøΩÔøΩ}ÔøΩÔøΩÔøΩÔøΩQÔøΩÔøΩy?ÔøΩÔøΩÔøΩ~\nÔøΩ9\u001fŸ¢<\u000e*e[>T2ÔøΩ&ÔøΩb)ÔøΩqVÔøΩÔøΩÔøΩ\u0004g?ÔøΩÔøΩ1ÔøΩ\u0007ÔøΩSaBÔøΩÔøΩ#ÔøΩ- ±ÔøΩÔøΩ2ÔøΩWÔøΩÔøΩÔøΩ*wÔøΩ‹ßÔøΩ(ÔøΩU9ÔøΩÔøΩCÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\u0003Õ†\nHÔøΩÔøΩe^ÔøΩ=8ÔøΩÔøΩÔøΩzCÔøΩkV%=\tWk$P5WÔøΩ@ÔøΩQÔøΩÔøΩ\u001aÔøΩÔøΩTuW\nÔøΩjÔøΩÔøΩSÔøΩÔøΩjÔøΩgÔøΩÔøΩŒ≥ÔøΩSÔøΩÔøΩx<ÔøΩ\nÔøΩÔøΩÔøΩaÔøΩÔøΩÔøΩK\u001fDZÔøΩœ∂VÔøΩaWÔøΩÔøΩÔøΩMpÔøΩ\n[\u0017ÔøΩq\u001asI\n\u0017lÔøΩÔøΩ-ÔøΩkxÔøΩ/ÔøΩÔøΩ:ÔøΩXÔøΩt&ÔøΩÔøΩ:ÔøΩÔøΩ\nÔøΩÔøΩÔøΩUIÔøΩÔøΩ\ti8ÔøΩNÔøΩ\nOÔøΩÔøΩÔøΩÔøΩZ\u0016ÔøΩÔøΩÔøΩ_ÔøΩWÔøΩÔøΩIÔøΩÔøΩÔøΩlxÔøΩÔøΩ\u0001ÔøΩÔøΩÔøΩ^ÔøΩÔøΩÔøΩÔøΩEi\tg<—ÑOÔøΩÔøΩ=KÔøΩ<ÔøΩT\u0013ÔøΩ\u0013O<51ÔøΩÔøΩÔøΩÔøΩ6ÔøΩnÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩE[ÔøΩYÔøΩÔøΩUÔøΩÔøΩkÿïÔøΩÔøΩÔøΩÔøΩÔøΩ\u0017wMÔøΩC\b,ÔøΩ\u0018ÔøΩ\u001f\u001bS/ÔøΩÔøΩÔøΩzÔøΩÔøΩÔøΩÔøΩÔøΩkÔøΩÔøΩÔøΩÔøΩ8M\u001ar=2x\u0006ÔøΩÔøΩÔøΩÔøΩDÔøΩ\n\u0002ÔøΩa ÔøΩÔøΩ~ÔøΩÔøΩÕ±f$ÔøΩÔøΩ#ÔøΩÔøΩ8ÔøΩ\u0002ÔøΩÔøΩÃá\nÔøΩ=ÔøΩ\u0014\"SÔøΩÔøΩ\nÔøΩ\nÔøΩÔøΩ+ÔøΩ\u0003W\u0015ÔøΩhÔøΩ\u00196O4ÔøΩÔøΩ\nÔøΩÔøΩ\u0011ÔøΩGe4\u0015uÔøΩ#ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩA${ÔøΩ\u0002Ip/ÔøΩ\\›≥\u0017\bÔøΩÔøΩÔøΩC√ßÔøΩÔøΩ[ÔøΩÔøΩ#|ÔøΩ\u0011\u001aN/\u000fÔøΩÔøΩÔøΩÔøΩ|ÔøΩE\u0018ÔøΩÔøΩÔøΩ\u001fÔøΩ\u001fÔøΩo=ÔøΩAcÔøΩO\u001a\n=ÔøΩÔøΩÔøΩ&ÔøΩ/ÔøΩ~{]ÔøΩ\u000eÔøΩÔøΩÔøΩWÔøΩ>ÔøΩÔøΩÔøΩtÔøΩÔøΩÔøΩlCÔøΩDÔøΩÔøΩÔøΩ/ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ6ÔøΩ\u001fAÔøΩ[YÔøΩÔøΩ\u0004\\\u0015\\HÔøΩÔøΩÔøΩ\n_ozÔøΩÔøΩÔøΩÔøΩH^\nhÔøΩ\u0018jÔøΩÔøΩEq\bÔøΩ\u0019ÔøΩ6ÔøΩÔøΩÔøΩÔøΩPÔøΩ◊øÔøΩ\u001b\u0016y^\nÔøΩ\u0001ÔøΩ\n\u0016ÔøΩ8\u000eÔøΩA\\^\u001aÔøΩ\u001fÔøΩ\u001fÔøΩÔøΩÔøΩ\tÔøΩ0h\nÔøΩ?ÔøΩÔøΩÔøΩÔøΩfmÔøΩÔøΩÔøΩ\u000fLÔøΩÔøΩ\nÔøΩÔøΩ7ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0007&Z\u0007\u001bÔøΩ\u0001ÔøΩ&ÔøΩÔøΩÔøΩ\u0000ÔøΩÔøΩIUUÔøΩÔøΩ\bLÔøΩÔøΩVÔøΩÔøΩ⁄öÔøΩ\u0000ÔøΩFpÔøΩÔøΩÔøΩqDÔøΩZÔøΩ\u0012\n?ÔøΩÔøΩlÔøΩÔøΩ*ÔøΩIÔøΩoﬂõoÔøΩ]ÔøΩÔøΩÔøΩpÔøΩÔøΩ5|mÔøΩSÔøΩÔøΩÔøΩÔøΩÔøΩH<!ÔøΩ\u000eÔøΩ\u0006^ÔøΩÔøΩM\\ÔøΩ\u000foZ ÔøΩ\nendstream\nendobj\n39 0 obj\n22996\nendobj\n38 0 obj\n35292\nendobj\n15 0 obj\n<</Length1 40 0 R/Length 41 0 R/Filter/FlateDecode>>stream\nxÔøΩÔøΩ|y|TÔøΩÔøΩÔøΩ<ÔøΩÃú3ÔøΩÔøΩ}ÔøΩ9ÔøΩÔøΩdf2ÔøΩÔøΩLÔøΩ\tÔøΩ\na\n1AÔøΩ\u0004ÔøΩÔøΩE@EIÔøΩ\"ÔøΩÔøΩÔøΩ\"ÔøΩJjÔøΩp\u0001⁄äÔøΩÔøΩ\u0012@0ÔøΩTÔøΩÔøΩjm+mÔøΩÔøΩnÔøΩ{ÔøΩj5WÔøΩ-ÔøΩÔøΩ+ÔøΩyœô\u0004ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩyÔøΩÔøΩ?ÔøΩÔøΩÔøΩ{ÔøΩ'\b#ÔøΩth\u00101ÔøΩÔøΩcAÔøΩVÔøΩ:ÔøΩ\u0019ÔøΩÔøΩ\u0001ÔøΩ≈´ÔøΩ\\ÔøΩ◊°ÔøΩÔøΩ#ÔøΩa'ÔøΩU\u001bÔøΩ\u0011ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0010bÔøΩ\u0010‚éØÔøΩ[{eÔøΩÔøΩOd\u0010ÔøΩx\u0010R\\ÔøΩvÔøΩÔøΩ5{ÔøΩwz\u0015BÔøΩ0BÔøΩÔøΩ]wÔøΩÔøΩÔøΩ]bFhÔøΩ:ÔøΩ_ÔøΩ:h0'ÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩ]yÔøΩ&ÔøΩzÔøΩÔøΩÔøΩ~\nÔøΩÔøΩÔøΩﬂ∞j\u0005ÔøΩ_\u0017DÔøΩ\u001b?ÔøΩÔøΩÔøΩÔøΩ\u0015ÔøΩÔøΩÔøΩ{ÔøΩg\u0010ÔøΩÔøΩ.ÔøΩ/^ÔøΩÔøΩÔøΩKÔøΩÔøΩÔøΩ\nPÔøΩ\u0007BÔøΩ7ÔøΩ6\\}\n7w\u0015BÔøΩÔøΩÔøΩÔøΩÔøΩKÔøΩ~ÔøΩ&y\u0006ÔøΩGÔøΩ#dp ÔøΩ\u0014ÔøΩ0RÔøΩÔøΩ=ÔøΩÔøΩÔøΩ\n#ÔøΩÔøΩŸÉÔøΩ\u0010\u0013V\u0010ÔøΩd\u0014ÔøΩÔøΩ0,BÔøΩo\u001bDÔøΩIÔøΩÔøΩÔøΩ6 \tÔøΩÂ≥äÔøΩJ3qÔøΩ\nÔøΩ\u0017$ÔøΩÔøΩÔøΩrÔøΩ$ÔøΩÔøΩÔøΩhÔøΩi\u0011K\nÔøΩÔøΩ\n=ÔøΩD\u0004ÔøΩ(ÔøΩÔøΩ(ÔøΩR(ÔøΩÔøΩQ\u0016’†ZT@uÔøΩ\n5ÔøΩ\"ÔøΩÔøΩ.@-h\u0006jE3ÔøΩ,4\u001bÔøΩAÔøΩP;ÔøΩ\u0010uÔøΩNt\u0011ZÔøΩ\u0016ÔøΩEh\tÔøΩF+ÔøΩJÔøΩ\nÔøΩFÔøΩÔøΩ5h-ZÔøΩ.CWÔøΩÔøΩÔøΩJt\u0015⁄ÄÔøΩP?\u001a@WÔøΩk–µh#ÔøΩ\u000emBÔøΩÔøΩÔøΩÔøΩEtÔøΩ\nÔøΩ+ÔøΩÔøΩÔøΩÔøΩw(ÔøΩ\nÔøΩÔøΩ>*ÔøΩ\u000f=ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0014ÔøΩ/\u0011[>VÔøΩ{ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩIÔøΩÔøΩÔøΩKÔøΩ}TÔøΩÔøΩÔøΩÔøΩÔøΩ\nÂüîQ>P>\\~ÔøΩÔøΩLÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩPÔøΩ\u0007gÔøΩP~ÔøΩÔøΩ\u001fÔøΩWÔøΩﬂÖc\u0007ÔøΩÔøΩ/ÔøΩQÔøΩ]ÔøΩ^ÔøΩ\tÿæ_~V>ÔøΩÔøΩI|ÔøΩC7ÔøΩ2ÔøΩÔøΩÔøΩÔøΩaÔøΩÔøΩÔøΩÔøΩ{ÔøΩ{ÔøΩ\u001bÔøΩÔøΩœ∞ÔøΩÔøΩÔøΩÔøΩÔøΩ\\ÔøΩ+PÔøΩtyÔøΩ$\u0003“ÅÔøΩ?ÔøΩ#\u0014AÔøΩÔøΩÔøΩ(ÔøΩNÔøΩ\u001aÔøΩ*:ÔøΩÔøΩ«ÅbÔøΩÔøΩ^ÔøΩÔøΩ[h?“£ÔøΩÔøΩMÔøΩ\u0002ÔøΩgÔøΩ}(ÔøΩÔøΩÔøΩÔøΩ3ÔøΩ\n+ÔøΩNÔøΩ\nÔøΩ\u0018ÔøΩÔøΩ\u0001:\tÔøΩoCÔøΩ&ÔøΩO+ÔøΩ ÜÔøΩÔøΩ!mCwÔøΩÔøΩÔøΩYjÔøΩÔøΩ\u000fÔøΩÔøΩx=^ÔøΩ2PÔøΩMR8\tOÔøΩQ>ÔøΩÔøΩ(^ÔøΩyÔøΩ]ÔøΩ=ÔøΩ>ÔøΩÔøΩÔøΩ\u0001ÔøΩÔøΩG—á»àbh+ÔøΩ\u00162ÔøΩÔøΩÔøΩOÔøΩgÔøΩMÔøΩÔøΩ\u001fOÔøΩ\u001bÔøΩÔøΩ(ÔøΩz—ùlÔøΩ\n*_ÔøΩÔøΩÔøΩÔøΩÔøΩ7ÔøΩ\nJÔøΩhÔøΩÔøΩ]ÔøΩaÔøΩÔøΩoÔøΩÔøΩa;>\u0006\nÔøΩ\u0017ÔøΩC\u0016\u0003Gm\u0005yÔøΩ\u0003ÔøΩÔøΩ :FÔøΩÔøΩ\u0016ÔøΩ\nÔøΩÔøΩ(ÔøΩ\n\\ÔøΩ\u0002ÔøΩﬁÄÔøΩÔøΩf\\ÔøΩHÔøΩXÔøΩÔøΩÔøΩNh}\u0002ÔøΩ\u0007IÔøΩ\u001f3\nÔøΩG\u00128v9ÔøΩ&ÔøΩ\u000eÔøΩÔøΩ;ÔøΩ\u0014ÔøΩ;ÔøΩÔøΩ\u0002~\u0014?\nÔøΩ/ÔøΩ)ÔøΩÔøΩh\u0003ÔøΩÔøΩ\n$ÔøΩQÔøΩ'ÔøΩ3ÔøΩ(ÔøΩÔøΩ5ÔøΩNÔøΩÔøΩ[vÔøΩÔøΩEpl\u0007ÔøΩ\nÔøΩ?ÔøΩNÔøΩ6‹çÔøΩÔøΩWÔøΩÔøΩÔøΩlÔøΩÔøΩl)[ÔøΩ\u00019ÔøΩB]ÔøΩÔøΩÔøΩÔøΩ+ÔøΩÔøΩ38\nÔøΩÔøΩ\u0013ÔøΩ s\nÔøΩcÔøΩQ‘éÔøΩ\u0004_ÔøΩ\u001a=ÔøΩNÔøΩ_ÔøΩ{ÔøΩ\tÔøΩÔøΩÔøΩÔøΩ\u001fÔøΩ\nÔøΩ?ÔøΩoÔøΩÔøΩÔøΩ%ÔøΩÔøΩ\u001f ÔøΩOÔøΩ wÔøΩÔøΩRÔøΩ\u001b*+ÔøΩ\u0005ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ_\u0012\u0015ÔøΩÔøΩ\u0016ÔøΩÔøΩÔøΩzÔøΩÔøΩÔøΩ=–∑QÔøΩÔøΩ6ÔøΩÔøΩÔøΩ ÔøΩÔøΩ—ù@ÔøΩÔøΩh\u0014ÔøΩwÔøΩ+ÔøΩXÔøΩÔøΩhÔøΩ\u0017ÔøΩÔøΩZÔøΩ\u0003ﬂèGÔøΩ{ÔøΩ=ÔøΩ$\u0001ÔøΩOÔøΩ å0o2`ÔøΩ\u0014ÔøΩr#ÔøΩ…Ü|ÔøΩÔøΩ\u0010HÔøΩ:ÔøΩÔøΩ7ÔøΩÔøΩÔøΩÔøΩ›á^Co`+ÔøΩÔøΩ4|ÔøΩ;pÔøΩÔøΩd\nÔøΩ\u0001ÔøΩÔøΩÔøΩ[ÔøΩOÔøΩmÔøΩ\u000eÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ'ÔøΩ/ÔøΩCÔøΩ\u0003.ÔøΩ\u0005ÔøΩp-z\nzÔøΩ3lÔøΩwHÔøΩÔøΩÔøΩÔøΩÔøΩ_ÔøΩÕá…≥ÔøΩÔøΩ\u0011ÔøΩ\u0010S`ÔøΩ3\nÔøΩnÔøΩ\u000eÔøΩ^ÔøΩuÔøΩ\u0017ÔøΩ\u0000ÔøΩ4ÔøΩ;ÔøΩ\nÔøΩ\nÔøΩÔøΩ‹äÔøΩUÔøΩ_ÔøΩÔøΩ ∑\"ÔøΩ5ÔøΩÔøΩ^1ÔøΩLyÔøΩF+AÔøΩ\\.kÔøΩ>ÔøΩ\u0006–çÔøΩ&4ÔøΩÔøΩ\u0006~ÔøΩ\u0007ÔøΩAOÔøΩwÔøΩÔøΩÔøΩ@ÔøΩADÔøΩ\u0002\u0005\u0010\u000eÔøΩ;_\u0006OÔøΩ\u0012ÔøΩÔøΩ6|7l;ÔøΩ3ÔøΩ\u0015ÔøΩ\u001a~\u0003ÔøΩ\u0019N7\u0012ÔøΩ-NÔøΩH3i!3ÔøΩZr\u001blÔøΩÔøΩ\u0013ÔøΩ\nÔøΩ\u0011ÔøΩaV1[ÔøΩAÔøΩv1GÔøΩÔøΩXƒ≤lYQ\nÔøΩl≈ùÔøΩ'ÔøΩorqn6ÔøΩÔøΩÔøΩÔøΩŸ±ÔøΩÔøΩÔøΩÔøΩ?ÔøΩPÔøΩUZVÔøΩÔøΩÔøΩJÔøΩ/ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0011YÔøΩﬁàÔøΩÔøΩ[ÔøΩ\u0004\nÔøΩ\nÔøΩSÔøΩÔøΩG–èÔøΩÔøΩÔøΩoÔøΩwÔøΩ\u000fLÔøΩ\u00028ﬁÅCÔøΩ\n)ÔøΩZ3ÔøΩÔøΩÔøΩÔøΩ÷éÔøΩ√∂\bÔøΩ%x)l+ÔøΩJÔøΩ\u000eÔøΩÔøΩx\u0010ﬂåoÔøΩÔøΩÔøΩoÔøΩÔøΩÔøΩÔøΩAÔøΩÔøΩÔøΩÔøΩI|\u0004ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0006ÔøΩÔøΩ?ÔøΩÔøΩÔøΩAÔøΩÔøΩ\t\u0003ÔøΩ\n!1ÔøΩ!EÔøΩÔøΩ\u00162ÔøΩtÔøΩÔøΩ`[K6ÔøΩÔøΩG\u0006ÔøΩFÔøΩÔøΩ\u0013ÔøΩ\u00109JÔøΩaÔøΩLÔøΩI3+ÔøΩ~f'ÔøΩ\u0003ÔøΩUÔøΩmÔøΩ\nÔøΩÔøΩ)6ÔøΩ6ÔøΩÔøΩŸµÔøΩ-ÔøΩ[ÔøΩ/ÔøΩwÔøΩ/\u0015~EÔøΩbÔøΩbÔøΩÔøΩUÔøΩ[ÔøΩW.R^ÔøΩ|PÔøΩ_ÔøΩÔøΩÔøΩ,ÔøΩÔøΩ:ÔøΩÔøΩ‹çÔøΩÔøΩ\\ÔøΩÔøΩÔøΩÔøΩÔøΩ\t|ÔøΩ·Ø©ÔøΩÔøΩÔøΩ-|ÔøΩÔøΩÔøΩn\"ÔøΩ\\8ÔøΩ>ÔøΩ6ÔøΩ\bzLI\u00162ÎôªÔøΩ_)ÔøΩÔøΩ”åÔøΩÔøΩÔøΩÔøΩÀò+ÔøΩÔøΩcfÔøΩ0\u001bÔøΩbÔøΩ2\u000e2~E#ÔøΩ\u0006›ÖÔøΩÔøΩiÔøΩgrÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩc\ngÔøΩÔøΩÔøΩ#\u001bÔøΩ\u0016ÔøΩÔøΩÔøΩÔøΩY+{ÔøΩÔøΩ#ÔøΩG~ÔøΩ\u001aÔøΩ\u0016|ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩR~\t5*vÔøΩÔøΩ\u0015ÔøΩÔøΩ/ÔøΩ»û\u0004ÔøΩÔøΩ}ÔøΩÔøΩmÔøΩ\u0001ÔøΩÔøΩ\u0017ÔøΩ2r'ÔøΩbÔøΩ/ÔøΩeÔøΩÔøΩO*6AO#wÔøΩ*ÔøΩmv\u0017ÔøΩÔøΩ\tÔøΩÔøΩÔøΩÔøΩÔøΩ~ÔøΩ\u001a?ÔøΩsÔøΩ0ÔøΩÔøΩ\u0014ÔøΩ”†q«±\u000fÔøΩÔøΩ~‘áÔøΩC\u0012~\u0001ÔøΩ\u0011ÔøΩ\"ÔøΩÔøΩ1OÔøΩyD\nÔøΩ\u001a!:\\\u000fÔøΩÔøΩÔøΩL\u0000ÔøΩÕ®ÔøΩnR6ÔøΩ\u0012+ÔøΩ$ÔøΩÔøΩ\"ÔøΩEÔøΩ\tÔøΩA0hÔøΩ_ÔøΩÔøΩ1ÔøΩÔøΩÔøΩ;ÔøΩÔøΩ\u0012ÔøΩÔøΩ5ÔøΩ^\u0012\u0003ÔøΩÔøΩ\nÔøΩÔøΩ◊∏\u00169ÔøΩ\u0003ÔøΩÔøΩœî^ÔøΩ\u001a[ÔøΩÔøΩNÔøΩÔøΩ0)ÔøΩÔøΩYÔøΩCÔøΩDÔøΩ \u001b\u001fÔøΩ÷Ön\u0007ÔøΩÔøΩ<ÔøΩÔøΩ\n(K\nD7ÔøΩ\u0007ÔøΩjÔøΩÔøΩÔøΩÔøΩ?\t\u001a≈óÔøΩ\n÷ÄÔøΩÔøΩ√ªm\u0005{a#A–ÖÔøΩÔøΩÔøΩ\u0000ÔøΩÔøΩSÔøΩÔøΩmÔøΩÔøΩÔøΩuX\u0004ÔøΩ:ÔøΩÔøΩ,=r\u0017ÔøΩ\nÔøΩÔøΩ\u0017ÔøΩÔù∞ÔøΩF=P{\u0004›£<ÔøΩÔøΩ5ÔøΩÔøΩvÔøΩXÔøΩÔøΩ\nÔøΩÔøΩ\u000fÔøΩ\u0012ÔøΩ9ÔøΩ\nÔøΩwÔøΩ&xÔøΩÔøΩÔøΩ;l\nÔøΩZ\u0004ÔøΩÔøΩ\u000fW<RÔøΩ\nÔøΩÔøΩ\u0004oÔøΩ&&h\nÔøΩÔøΩ4ÔøΩÔøΩNv6hÔøΩÔøΩÀóÔøΩ\u0017^\u00066j\nÔøΩÔøΩ7ÔøΩeÔøΩ\u0007P\nÔøΩÔøΩÔøΩ-ÔøΩ;ÔøΩÔøΩÔøΩwÔøΩ\u0017ÔøΩÔøΩÔøΩÔøΩÔøΩ\u000fÔøΩÔøΩÔøΩÔøΩAÔøΩZÔøΩ)ÔøΩÔøΩbEÔøΩÕÉÔøΩ}\u0003\u001f\u0007{ÔøΩ{|'ÔøΩÔøΩÔøΩÔøΩwÔøΩÔøΩ\"ÿÅÔøΩ\nÔøΩ\u000fÔøΩÔøΩÔøΩ)^@CÔøΩoAw6ÔøΩÔøΩ*ÔøΩ\u0006YÔøΩ?ÔøΩÔøΩC+ÔøΩÔøΩÔøΩ\u0002ÔøΩÔøΩﬂ†ÔøΩf3ÔøΩPÔøΩt!9PÔøΩÔøΩÔøΩz\u001fÔøΩ\u0007\u000f¬èÔøΩh]y=hÔøΩ\u0017ÔøΩ^N\u0001ÔøΩg\u0010ÔøΩ\u0014{%IjÔøΩ6ÔøΩiJcÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩd3ÔøΩÔøΩTÔøΩ*\u0011ÔøΩE#ÔøΩP0 ÔøΩ}^ÔøΩÔøΩÔøΩtÔøΩm\u0016ÔøΩÔøΩ(\u0018ÔøΩ:ÔøΩFÔøΩÔøΩ9ÔøΩÔøΩe\bFÔøΩÔøΩÔøΩÔøΩ^q$ÔøΩ;ÔøΩFCÔøΩgÔøΩi=ÔøΩ\u0002\u001aVÔøΩÔøΩÔøΩ;\"BÔøΩÃØÔøΩ3\"ÔøΩ ßÔøΩ_?SÔøΩ3ÔøΩÔøΩ”ôRÔøΩLÔøΩ‹ôX\u0010ÔøΩPS:%ÔøΩÔøΩƒëÔøΩÔøΩ\bÔøΩÔøΩxÔøΩÔøΩ.(sFÔøΩ[\n\u0019ÔøΩÔøΩÔøΩryX.ÔøΩ\n\bÔøΩ\u0005bÔøΩcÔøΩ\nq\u0004ÔøΩÔøΩÔøΩ#37ÔøΩ\u001bjÔøΩ\u0001ÔøΩ;ÔøΩQÔøΩÔøΩZ.UÔøΩSÔøΩZ\u0003E\nÔøΩFÏ°æ\u0003ÔøΩ>\nÔøΩ\u0005bom<@\u0010ÔøΩÔøΩÔøΩ\u001aqÔøΩfÔøΩÔøΩ8C3ÔøΩ\u001bÔøΩ0ÔøΩÔøΩ\u0015ÔøΩG:ÔøΩwÔøΩÔøΩp\u0007\u0002ÔøΩÔøΩÔøΩ\bnY\u0015Z9ÔøΩB\u0017ÔøΩ\u0018ÔøΩÔøΩ)ÔøΩE~ÃàÔøΩeÔøΩÔøΩ\u001f#^FÔøΩ\u0006ÔøΩ)\nH\n\u001bÔøΩkT@+{ÔøΩÔøΩ’°ÔøΩ+.ÔøΩ\u001aaVtÔøΩg\u0018ÔøΩÔøΩÔøΩ\u0019#ÔøΩÔøΩO9ÔøΩÔøΩÔøΩÔøΩM-]ÔøΩÔøΩ?ÔøΩfÔøΩZ\nÔøΩÔøΩÔøΩ:4ÔøΩM\nÔøΩ3ÔøΩÔøΩÔøΩÔøΩ\u0001ÔøΩvwÔøΩ=FHdfÔøΩÔøΩLxÔøΩ]–Öm\nDx\u0016ÔøΩÔøΩÔøΩk\u0004ÔøΩ\u0006\u000f\u0014ÔøΩwÔøΩoÔøΩ|›•ÔøΩVÔøΩÔøΩ{ÔøΩ8ÔøΩ\n]\u0010Z7ty/\u0010ÔøΩ54ÔøΩ.ÔøΩ\n8ÔøΩrIGÔøΩ'ÔøΩÔøΩU\nZÔøΩ\u0015\nÔøΩ4ÔøΩCÔøΩ+fx\u000eXÔøΩÔøΩEÔøΩ\u000f9%ÔøΩÔøΩÔøΩ#ÔøΩÔøΩ\u0001ÔøΩXÔøΩÔøΩ\u0003zÔøΩDAÔøΩ;ÔøΩpÔøΩcrI>ÔøΩÔøΩÔøΩ.:◊ØÔøΩÔøΩQh\u000eÔøΩ√àÔøΩJÔøΩ7ÔøΩ\nÔøΩ75ÔøΩÔøΩÔøΩ\u00064ÔøΩÔøΩ\u0001NÔøΩ_7ÔøΩÔøΩFV\u0003=.\u001bQÔøΩÔøΩ\u000e\tÔøΩÔøΩ.ÔøΩÔøΩG\u0014\u0011!$\u000eÔøΩ\n\u0001ÔøΩCcÔøΩ~ÔøΩeÔøΩDÔøΩ2\"ÔøΩ\nÔøΩ\"ÔøΩsÔøΩ\u0006ÔøΩ'ÔøΩ#ÔøΩÔøΩHU\u0015e\u0010ÔøΩ\u0005(\nÔøΩ8MÔøΩ\u0017“©ÔøΩÔøΩd$ÔøΩ'ÔøΩÔøΩAÔøΩÔøΩNÔøΩÔøΩ\u0015›ç\u0019ÔøΩÔøΩ@ÔøΩÔøΩÔøΩÔøΩQ\tÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩJ]D+ÔøΩ\u0007ÔøΩÔøΩIvÔøΩÔøΩ^zÔøΩÔøΩÔøΩ\u0011ÔøΩ\"zdpÔøΩ»πÔøΩ{CÔøΩÔøΩÔøΩÔøΩ\bÔøΩ:ÔøΩGÔøΩÔøΩ\u0019\u0004ÔøΩÔøΩu]ÔøΩ\bÔøΩÔøΩ_\u000e_Z9ﬁ∂ ÔøΩ6iÔøΩÔøΩ:ÔøΩ;—∑m\nÔøΩVÔøΩ\no8wlÔøΩÔøΩ+\u0007ÔøΩÔøΩGÔøΩ\bÔøΩ‘ú\u0010ÔøΩÔøΩEKÔøΩh\u0003ÔøΩ)\"3CÔøΩÔøΩÔøΩÔøΩ\u0006QÔøΩw\n1ÔøΩt1nÔøΩ])\u00117#ÔøΩ\nÔøΩÔøΩÔøΩswÔøΩÔøΩ.-ÔøΩ\u0017\u001bQÔøΩÔøΩÔøΩzÔøΩ„ÅÅÔøΩ\u0016,ÔøΩ\n\u0011zgWÔøΩnu ÔøΩÔøΩxÔøΩhÔøΩ4ÔøΩJŒæÔøΩlÔøΩF\u001aÔøΩ_ÔøΩOÔøΩZÔøΩkÔøΩÔøΩ\nbÔøΩÔøΩÔøΩ(i[ÔøΩthHÔøΩÔøΩc3AY\nÔøΩ\nÔøΩ3ÔøΩzÔøΩVÔøΩÔøΩ\u0007WÔøΩD!4tÔøΩÔøΩbÔøΩÔøΩÔøΩZ{'ÔøΩ?Z~ÔøΩNÔøΩÔøΩÃªÔøΩÔøΩ#ÔøΩÔøΩÔøΩ4ÔøΩuJ\u001b\u0005\u0005ÔøΩ\nÔøΩÔøΩÔøΩ\u0007\b~ÔøΩÔøΩ\u0010ÔøΩFÔøΩÔøΩ|\u0010)ÔøΩQÔøΩÔøΩg\u0019ÔøΩÔøΩhÔøΩ0FN^ÔøΩx\u0019ÔøΩ\u0013ÔøΩÔøΩ\u0004RÔøΩ+ÔøΩ%»ë\u0014>o\u001aoÔøΩP8ÔøΩÔøΩ>ﬁÑÔøΩÔøΩ,ÔøΩÔøΩÔøΩ&\u001b0\u0006ÔøΩ\u0011H0Xƒ≥\"sÏ¨§@_ÔøΩÔøΩp\nÔøΩ\u0017ÔøΩ#ÔøΩÔøΩ<»èÔøΩÔøΩ‹ºÔøΩ`ÔøΩ\tjÔøΩÔøΩÔøΩ\u0019PZ\nfÔøΩetÔøΩÔøΩ\nÔøΩW\u0019\u0018-\u001f;\u0018)ÔøΩÔøΩPÔøΩ+/ÔøΩÔøΩj9?ÔøΩÔøΩ4ÔøΩÔøΩJÔøΩÔøΩWiÔøΩÔøΩÔøΩ\u0007ÔøΩr&= ÔøΩÔøΩ:ÔøΩ\u0006n^4ÔøΩ5ÔøΩ\u0014ÔøΩÔøΩ:\u0002›Ü%ÔøΩ\"KÔøΩÔøΩrÔøΩZaÔøΩoÔøΩ0ÔøΩnÔøΩ\u000f\u0019ÔøΩ\tÔøΩLÔøΩ}wÔøΩ\u001f6<,ÔøΩ4>ÔøΩ;j8*ÔøΩÔøΩ:ÔøΩ{ÔøΩÔøΩSÔøΩuÔøΩO}ÔøΩ7ÔøΩ+|bÔøΩHÔøΩÔøΩÔøΩÔøΩÔøΩ\u001fÔøΩ\u0017ÔøΩ/|)ÔøΩÔøΩÔøΩMÔøΩ>ÔøΩÔøΩ>ÔøΩÔøΩ>ÔøΩGÔøΩWÔøΩU6ÔøΩÔøΩmÔøΩ\tÔøΩÔøΩFÔøΩ€∫ÔøΩg\u0010DÔøΩÔøΩÔøΩ\u0004ÔøΩÔøΩÔøΩÔøΩgÔøΩÔøΩrÔøΩGÔøΩ\u001bÔøΩÔøΩÔøΩ,ÔøΩÔøΩÔøΩﬁΩ\b|ÔøΩAÔøΩdGÔøΩaIÔøΩ\n\u0006ÔøΩjÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩT\u0006ÔøΩÔøΩÔøΩÔøΩKÔøΩQÔøΩ=ÔøΩÔøΩ√æQÔøΩ\u0017%}ÔøΩÔøΩÔøΩÔøΩÔøΩ?!^1DÔøΩÔøΩÔøΩtÔøΩÔøΩÔøΩ9\\¬òÔøΩ3\u0006ÔøΩ\u0010`ÔøΩÔøΩLOÔøΩ0ﬁ¥M_ÔøΩTl\u0011ÔøΩoÔøΩÔøΩW;ÔøΩÔøΩ\u0014[ÔøΩ'\nH\u0018ÔøΩ¬±ÔøΩÔøΩn\u0013ÔøΩ\noÔøΩÔøΩ&ÔøΩÔøΩ{ÔøΩÔøΩ?<–É\u0003ÔøΩÔøΩjÔøΩŸ≠ÔøΩB]}]=ÔøΩa[ÔøΩ\u0002&;ÔøΩ!ÃìÔøΩÔøΩ88eeiÔøΩ\"gn\u001aÔøΩc\bÔøΩ[ÔøΩY0ÔøΩÔøΩÔøΩbÔøΩÔøΩ\u000f?ÔøΩ?~ÔøΩ#ÔøΩÔøΩpÔøΩÔøΩÔøΩÔøΩÔøΩ6{ÔøΩ\u000fÔøΩ1_\u0011ÔøΩÔøΩ’ÅÔøΩrÔøΩ#ÔøΩÔøΩ?PÔøΩÔøΩ.ÔøΩÔøΩÔøΩ\u0002HÔøΩHÔøΩINÔøΩRÔøΩiyÔøΩÔøΩSÔøΩÔøΩÔøΩ\"gÔøΩ;ÔøΩE-ÔøΩnJq^ÔøΩÔøΩ|ÔøΩÔøΩ\n»•2\u0014ÔøΩTÔøΩÔøΩ\\U7€•zBÔøΩÔøΩ*ÔøΩ|J\u0013ÔøΩÔøΩÔøΩqWÔøΩ]\u0015ÔøΩÔøΩÔøΩ)ÔøΩÔøΩ|vÔøΩr\u0006◊¶ÔøΩÔøΩ^ÔøΩÔøΩÔøΩÔøΩnuÔøΩÔøΩÔøΩ’ï]XsÔøΩr5ÔøΩ^ÔøΩŒµÔøΩ}En#ÔøΩQÔøΩÔøΩ€®ﬁ§ÔøΩA{ÔøΩkÔøΩ{ÔøΩgÔøΩxmÔøΩ6ÔøΩ.~ÔøΩsGÔøΩÔøΩÔøΩÔøΩ{ÔøΩÔøΩÔøΩoÔøΩÔøΩÔøΩÔøΩÔøΩzÔøΩ}oÔøΩÔøΩÃΩÔøΩ}ÔøΩSÔøΩÔøΩ4OÔøΩÔøΩÔøΩÔøΩÔøΩ<ÔøΩ}\"sÔøΩ;ÔøΩ?ÔøΩ\nu=ÔøΩÔøΩIÔøΩ\nÔøΩ\nÔøΩYÔøΩ\u0017ÔøΩÔøΩuÔøΩKÔøΩÔøΩjÔøΩÔøΩÔøΩ\u0006ÔøΩzÔøΩ\u0006ÔøΩUiÔøΩRÔøΩR~ÔøΩÔøΩiSÔøΩÔøΩœéÔøΩeÔøΩnÔøΩÔøΩÔøΩÔøΩ,ÔøΩÔøΩuÔøΩK5\nÔøΩ!5ÔøΩÔøΩxlÔøΩ*OÔøΩ_ÔøΩ\u00155*ÔøΩÔøΩ√´TjÔøΩÔøΩ\nLÔøΩ#%v#ÔøΩÔøΩ‚ñ¨ÔøΩx&·éõÔøΩFÔøΩ)Êã∫c≈ö\u0006wqÔøΩÔøΩw»≠QÔøΩÔøΩÔøΩ\nÔøΩ%ÔøΩsÔøΩVÔøΩ\tÔøΩÔøΩ|ÔøΩÔøΩÔøΩÔøΩÔøΩj ãVÔøΩ\u0007\u001a<\u0019/ÔøΩ\u0007ÔøΩ\u0019K6ÔøΩÔøΩQr\n=ÔøΩÔøΩÔøΩ@ÔøΩÔøΩlÔøΩÔøΩÔøΩ≈¢\t\u0011ÔøΩZÔøΩÔøΩjÔøΩ.ÔøΩÔøΩ\u001aÔøΩÔøΩÔøΩRÔøΩÔøΩ\n\\ÔøΩÔøΩIÔøΩt6ÔøΩÔøΩ\u0019ÔøΩ\u0019ÔøΩa:jÔøΩÔøΩÔøΩÔøΩÔøΩ…ïÔøΩ5ÔøΩkÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ.“∏\u000fÔøΩ4ÔøΩ\u0013\u0011ÔøΩÔøΩÔøΩ5ÔøΩÔøΩS{BÔøΩhÔøΩhÔøΩ2J.?\u0014ÔøΩÔøΩÔøΩÔøΩÔøΩgÔøΩS8ÔøΩ\u0010ÔøΩœúÔøΩÔøΩ\u0007ÔøΩ\u001bÔøΩÔøΩ\nCÀô–¥MQÔøΩ‹¶ÔøΩr\nrÔøΩy\u0005~ÔøΩÔøΩ\u0007^OÔøΩÔøΩ,~~ÔøΩ\tÔøΩ&\n6NÔøΩÔøΩÔøΩ3ÔøΩ\u0003ÔøΩ\n8\nÔøΩ$ÔøΩxÔøΩ\u0012ÔøΩÔøΩtÔøΩ>ÔøΩÔøΩYHÔøΩ\u000eÔøΩÔøΩY>!Ÿç{ÔøΩ5ÔøΩ\u0004ÔøΩ\u0006\u0010\u0014ÔøΩÔøΩÔøΩh4T\bXCAÔøΩ\u0014;6ÔøΩbÔøΩÔøΩv^#\u0006qÔøΩ[ÔøΩ-ÔøΩ\n|ÔøΩdÔøΩÔøΩxÔøΩÔøΩÔøΩÔøΩ√•+ÔøΩZKÔøΩ\u0014ÔøΩÔøΩÔøΩ–ê¬ö?ÔøΩEÔøΩKgv:ÔøΩ\t\"ÔøΩ\u001bÔøΩiÔøΩbÔøΩÔøΩ⁄¢S\u0015ÔøΩ\"ÔøΩ|ÔøΩÔøΩ/_`VÔøΩ}ÔøΩ]ÔøΩ\n{4\u0012ÔøΩdÔøΩÔøΩoÔøΩsdÔøΩÔøΩÔøΩ⁄®YgÔøΩ–îÔøΩm\nÔøΩÔøΩOnÔøΩÔøΩÔøΩ>BÔøΩkÔøΩÔøΩgV\u0001ÔøΩÔøΩÔøΩ,ÔøΩÔøΩÔøΩÔøΩRÿÄ\nDÔøΩ \u0003\u001bG\tEÔøΩ\u0003w\u0010ÔøΩÔøΩq\u0014œîNÔøΩ5‘π\u00187ÔøΩ‹±‹π‹µ‹≠TÔøΩ\u0014zTuÔøΩÔøΩÔøΩFsÔøΩÔøΩ\u001aÔøΩFCÔøΩÔøΩÔøΩﬂóÔøΩÔøΩnÔøΩoÔøΩlÔøΩmÔøΩÔøΩjÿñÔøΩÔøΩÔøΩÔøΩ\t&]NÔøΩÔøΩ\u0015ÔøΩ9oÔøΩ[ÔøΩÔøΩ\nIÔøΩÔøΩOÔøΩ'\u0012iP\nÔøΩH3ÔøΩuf}Y605?ÔøΩ0[7ÔøΩjÔøΩfÔøΩnÔøΩÔøΩ8ÔøΩ8ÔøΩÔøΩc?qÔøΩÔøΩ\u0005wÔøΩBÔøΩBÔøΩBWwÔøΩ≈πÔøΩÔøΩ\u0017\u0017.ÔøΩ[ZÔøΩ\u0007IHÔøΩ5ÔøΩDH#6NId\u001b\u0007L\u0003ÔøΩÔøΩÔøΩ\u0007ÔøΩ\u00073;ÔøΩÔøΩ2ÔøΩÔøΩTÔøΩ8yÔøΩÔøΩtÔøΩÔøΩBÔøΩÔøΩÔøΩ6\u0010ÔøΩ~ÔøΩ\u0016(÷≠\u0018ÔøΩÔøΩÔøΩ(ÔøΩ&ÔøΩ\n\u000f’ÄÔøΩlÔøΩ}ÔøΩÁΩ¥%ÔøΩ|ÔøΩRÔøΩL6iÔøΩ\u0016ÔøΩVÔøΩÔøΩVÔøΩŸ®JŒî!<ÔøΩÔøΩ2^√ÑÔøΩ\u0016\u0015y\u0006KÔøΩ`\nÔøΩ}\u0014GGqH\u00122∆óÔøΩÔøΩ}#\u0016ÔøΩÔøΩÔøΩÔøΩ\u001b\u0019ÔøΩÔøΩ€ûÔøΩ?ÔøΩK\n*ÔøΩÔøΩ'ÔøΩwW„ó´?ÔøΩ.W3ÔøΩ“¨ÔøΩTÔøΩ\u0016T\u0018T-VgÔøΩÔøΩUÔøΩÔøΩ/‚ô®ÔøΩg\u0002ÔøΩr$ÔøΩÔøΩÔøΩÔøΩ$ÔøΩ\u0007ÔøΩÔøΩ\u0006ŒåÔøΩÔøΩÔøΩ\u000fÔøΩ\u000f\u00143IÔøΩÔøΩcÔøΩcM¬òÔøΩd/bÔøΩPENÔøΩ\u001bXÔøΩÔøΩ3ÔøΩÔøΩcgÔøΩÔøΩR\u000f\u0016ÔøΩÔøΩ\\ÔøΩmÔøΩ,’ÖÔøΩÔøΩ9\n’§T9ÔøΩ0Ds8lÔøΩÔøΩÔøΩBUÔøΩÔøΩÔøΩFÔøΩJ∆ÑÔøΩ\n6ÔøΩ\u0013U\u0011S(ÔøΩÔøΩÔøΩ2ÔøΩÔøΩÔøΩÔøΩdU\u0012,ÔøΩÔøΩÔøΩnÔøΩ=hÔøΩÔøΩÔøΩKRÔøΩ“¨—≠\u0015V%ŸûÔøΩ\nÔøΩ?ÔøΩ\u0004ÔøΩÔøΩCECÔøΩq\u0018ÔøΩlÔøΩPÔøΩÔøΩ\u000eÔøΩ\u0000ÔøΩ0ÔøΩÔøΩIÔøΩÔøΩmv\u001fÔøΩZÔøΩÔøΩ \u0015\n\u0010ÔøΩÔøΩ1ÔøΩ#ÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩBÔøΩ.WkÔøΩAK=ÔøΩtÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩHNÔøΩÔøΩÔøΩwÔøΩ}ÔøΩ‚îºÔøΩG.ÔøΩ\u0017ÃÖÔøΩÔøΩÔøΩÔøΩ-ﬂ™oÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩ_ÔøΩÔøΩÔøΩÔøΩ\nÔøΩ\u0015WÔøΩÔøΩÔøΩÔøΩ\\ÔøΩuÔøΩÔøΩ\\€ñ5WÔøΩ;ÔøΩÔøΩÔøΩUÔøΩf_\u0006ÔøΩÔøΩoÔøΩﬁ∫ÔøΩvMÔøΩÔøΩÔøΩkÔøΩZxœØ\nNÔøΩr~\bÔøΩÔøΩGÔøΩÔøΩÔøΩÔøΩÔøΩHÔøΩ}ÔøΩÔøΩÔøΩ:1ÔøΩÔøΩ,ÔøΩ<kÔøΩGÔøΩÔøΩ\"1ÔøΩ›ö@0ÔøΩw{\u0002ÔøΩÔøΩÔøΩ]\n\bÔøΩÔøΩÔøΩÔøΩ\u0002AÔøΩwÔøΩ\u0003!ÔøΩÔøΩÔøΩ\u000eÔøΩ\"~w:\u0010*ÔøΩÔøΩS\u0003!ÔøΩe(\nvOÔøΩ:UÔøΩQÔøΩÔøΩt\u001a42o2\u0007ÔøΩ\u0014ÔøΩÔøΩ\u0007ÔøΩ\u0018ÔøΩ\u0006ÔøΩÔøΩ{ÔøΩ'ÔøΩ'ÔøΩÔøΩÔøΩ(\u0011%ÔøΩ0ÔøΩw÷±YÔøΩ8\nÔøΩjÔøΩ\u0004\nÔøΩÔøΩÔøΩ<ÔøΩÔöπÔøΩ\u000fÔøΩdÔøΩpfÔøΩÔøΩPBÔøΩ\u0000ËºÅ&ŸõÔøΩÔøΩ·∞ôÔøΩ\u0019hÔøΩ4ÔøΩTQ%AÔøΩ\u0004ÔøΩyYÔøΩTt\nÔøΩTÔøΩÔøΩ\u0001ÔøΩnÔøΩÔøΩÔøΩ(ÔøΩj\u0001IÔøΩ\u0017ÔøΩÔøΩ-ÔøΩ|\tÔøΩK6ÔøΩÔøΩb2ÔøΩ%3ÔøΩŸ§h◊©ÔøΩÔøΩlvÔøΩÔøΩÏÇ®s|H>T3ÔøΩBvaÔøΩQ9BZkÔøΩIÔøΩÔøΩÔøΩ\u0016ﬂ∫.ÔøΩ49\"\u0011ÔøΩ0}ÔøΩÔøΩÔøΩÔøΩV*5ÔøΩÔøΩÔøΩÔøΩ“™ÔøΩjÔøΩ\u0015ÔøΩF)ÔøΩ\u0003ÔøΩs\u0013ÔøΩÿ£\u001bÔøΩÔøΩÔøΩ\u0014/8\u0003+ÔøΩÔøΩÔøΩxÔøΩÔøΩ\u0004ÔøΩ\u000eÔøΩÔøΩ\u0014\b:ÔøΩn\n\bÔøΩÔøΩnc d2\u0012ÔøΩyÔøΩ3ÔøΩÔøΩÔøΩ\u0007ÔøΩÔøΩ<SÔøΩqÔøΩÔøΩÔøΩ{yf9ÔøΩ?ÔøΩ3ÔøΩ\nÔøΩ/0\u00188\u0019`ÔøΩÔøΩÔøΩ@oÔøΩ9\u00168\u0011 ÔøΩ[/ÔøΩÔøΩ\u0004ÔøΩÔøΩÔøΩ\u001fÔøΩpPÔøΩÔøΩ*\nÔøΩLFÔøΩÔøΩ\nÔøΩÔøΩCÔøΩÔøΩ>\u001aÔøΩCÔøΩHÔøΩÔøΩÔøΩÔøΩgÔøΩÔøΩt\nÔøΩÔøΩ_\u0015OÔøΩÔøΩÔøΩÔøΩÔøΩ6d¬¶\n\u0018\u0013ÔøΩÔøΩÔøΩÔøΩ`ÔøΩÔøΩÔøΩ\u0018IÔøΩ\u001bÃõÔøΩ\u000fÔøΩÔøΩIÔøΩpÔøΩ`ÔøΩ$PÔøΩ\n\u0006({\u0005\u0019ÔøΩ[\u0015\bYÔøΩn\u0017|ÔøΩ…à\t\tÔøΩÔøΩ\u0016ÔøΩ)\u0018\nÔøΩﬂï\nÔøΩgÔøΩZÔøΩÔøΩÔøΩÔøΩÔøΩM*ÔøΩPÔøΩÔøΩ7-0\u001aE!+H\u0002#ÔøΩJP\u001b,4\u0017ÔøΩ\u0004\u0016\u0013xOÔøΩdÔøΩ$ÔøΩ\u0016ÔøΩÔøΩ@ \u001bÔøΩ«Ç8ËåØÔøΩnEÔøΩÔøΩ7ÔøΩÔøΩoÔøΩ.@\u001fÔøΩÕÖ¬áÔøΩÔøΩÔøΩ+,76ÔøΩÔøΩÔøΩ3\"S\u0011\u0017\nÔøΩÔøΩd\u0013ÔøΩz\u0006@\u001fÔøΩU&ÔøΩ)ÔøΩÔøΩQÔøΩ‘ÅÊöñÔøΩÔøΩÔøΩ\nÔøΩrÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0005|ÔøΩÔøΩ&ÔøΩ/lÔøΩ\n\u0010F\u000fÔøΩÔøΩFÔøΩIÔøΩ\u000fÔøΩÔøΩ(\"ÔøΩ}ÔøΩ|ÔøΩfB\n80ÔøΩÔøΩÔøΩÔøΩGG@ÔøΩIÔøΩ\"-\nÔøΩÔøΩÔøΩrvÔøΩY\u0004ÔøΩÔøΩ‚ªíÔøΩT4ÔøΩLE\"Xaw\u0016ÔøΩÔøΩvPSÔøΩ€úÔøΩdÔøΩ8l)\u0012ÔøΩ(k ÔøΩYÔøΩ»°ﬁçzÔøΩ\u0000ÔøΩD>ÔøΩÔøΩ!LÔøΩ|ÔøΩTD)\nÔøΩq\u001f3ÔøΩ\u0013ÔøΩÔøΩÔøΩwÔøΩÔøΩ√∂ÔøΩŸõÔøΩ—él)ÔøΩY\u0012ÔøΩMÔøΩ:ÔøΩ;E1ÔøΩ,ÔøΩÔøΩÔøΩÔøΩ\"ÔøΩÔøΩ8ÔøΩÔøΩvvÔøΩŸó&kÔøΩÔøΩ÷îYEÔøΩ{vÔøΩÔøΩÔøΩ\u0003WÔøΩ<ÔøΩH\nÔøΩÔøΩÔøΩj‚åπÔøΩDp\bN\"ÔøΩIuÔøΩuÔøΩÔøΩ>GÔøΩsS’∞cÔøΩ9ÔøΩ\u0018qj“ôÔøΩÔøΩm\u001aÔøΩQWÔøΩÔøΩÎ´ªÔøΩÔøΩ>{ÔøΩÔøΩÔøΩ2ÔøΩkÔøΩÔøΩ1ÔøΩyÔøΩÔøΩÔøΩÔøΩ[\u0010\u0014ÔøΩ=\u0010 ªÔøΩ(V\n\"nÔøΩiÔøΩ\u0010`ÔøΩ6)YÔøΩPÔøΩÔøΩp\u0004ÔøΩÔøΩ\u0014ÔøΩÔøΩ\u0007U8ÔøΩÔøΩ\u0005ÔøΩÔøΩN”∞ÔøΩ\u0018L\n&ÔøΩLÔøΩiÔøΩÔøΩlbMÔøΩQÔøΩ\u000fIP7uFÔøΩ!ÍèíÔøΩÔøΩÔøΩ\n;\nÔøΩÔøΩÔøΩÔøΩÔøΩ\u0007ÔøΩÔøΩlyÔøΩÔøΩ9ggÔøΩ‘Äu…ãÔøΩ\u0007|$\nKƒ™bÔøΩR\u001b\nG\n\u0001ÔøΩ\u0014,ÔøΩ\u0005#ÔøΩTÔøΩÔøΩ.\u0004ÔøΩ ÔøΩ ULÔøΩ∆öÔøΩ>ÔøΩ*\u0014ÔøΩ-K2YEÔøΩ\n\u0000\nÔøΩ\u001ax\u0000\nÔøΩcsÔøΩÔøΩÔøΩHÔøΩÔøΩIÔøΩ\u0006ÔøΩÔøΩÔøΩ\u0017EÔøΩÔøΩTÔøΩÔøΩ\u0018\u0018\nÔøΩ\n`/l\u0015ÔøΩÔøΩ~\nÔøΩ\\ÔøΩÔøΩÔøΩÔøΩÔøΩmÔøΩÔøΩÔøΩmÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0017\u0011ÔøΩÔøΩBÔøΩÔøΩÔøΩÔøΩKÔøΩÔøΩlÔøΩÔøΩ5\u0007ÔøΩ{sÔøΩÔøΩ\nfÔøΩÔøΩ\u0001sÔøΩpÔøΩÔøΩkÔøΩÔøΩÔøΩ^-\nÔøΩ/\u001aÔøΩwÔøΩi\u000eDÔøΩÔøΩ»ïÔøΩ\u0015ÔøΩ\u001aœæÔøΩÔøΩÔøΩ\nÔøΩÔøΩeK\n+ÔøΩCÔøΩÔøΩ!ƒéÔøΩ\u001aÔøΩ\nÔøΩq)^JÔøΩzÔøΩÔøΩÔøΩÔøΩWÔøΩ+ÔøΩWÔøΩÔøΩLÔøΩ9ÔøΩ\u0011xPÔøΩ{ÔøΩÔøΩq7GÔøΩÔøΩgÔøΩ@\u0010ÔøΩÔøΩ!\u0010ÔøΩ\n!ÔøΩ'ÔøΩÔøΩ\u000fÔøΩÔøΩcÔøΩ\u0019ÔøΩÔøΩ$ÔøΩÔøΩÔøΩdÔøΩÔøΩuÔøΩÔøΩ\u0000ÔøΩGIÔøΩ9^\u0015ÔøΩÔøΩÔøΩI\u001fÔøΩX==ÔøΩ|ÔøΩoÔøΩoÔøΩÔøΩÔøΩ=OÔøΩÔøΩF\u0000€ÆÔøΩÔøΩÔøΩ\n\u0018ÔøΩÔøΩ\u0007NÔøΩÔøΩ@O?ÔøΩ1‰Å≠ÔøΩÔøΩ\u00040\tgdÔøΩ$M\u0001ÔøΩÔøΩÔøΩ\u000feEGÔøΩ\u0005\u0016ÔøΩ\u0010ﬁ†ÔøΩ\u0002:[ÔøΩe\u0010ÔøΩÔøΩ|ÔøΩ\np!3ÔøΩ\nCTcÔøΩÔøΩ]ÔøΩ20}fÔøΩ\u0015*\u0001ÔøΩ[\nÔøΩÔøΩÂ¢Çb^ÔøΩ’ÖÔøΩÔøΩÔøΩ/ÔøΩLr;ÔøΩ’õÔøΩ_ÔøΩÔøΩÔøΩÔøΩÔøΩÔõÄﬂµ»ç\u000eH5\u000fÔøΩÔøΩqOÔøΩÔøΩ\u0014ÔøΩÔøΩÔøΩfn\u001bÔøΩÔøΩc[x]\n1÷∏RÔøΩhÔøΩ3\u0019ÔøΩ F`D&ÔøΩHÔøΩÔøΩÔøΩÔøΩ]ÔøΩj.ÔøΩ^ÔøΩKÔøΩÔøΩ&A%ÔøΩÔøΩAÔøΩW\u0011ÔøΩ\nÔøΩÔøΩUÔøΩÔøΩ9ÔøΩ.ÙÉöÇÔøΩÃõ23ÔøΩbÔøΩ!ÔøΩÔøΩÔøΩÔøΩÔøΩ^kL#7vÔøΩÔøΩÔøΩÔøΩÔøΩM\u0001%AÔøΩKc'ÔøΩÔøΩÔøΩ[ÔøΩÔøΩÔøΩZeV<ÔøΩ\nÔøΩÔøΩÔøΩÔøΩ`\u0011\u0019\u0005\u0014ÔøΩi}ÔøΩ\n:ÔøΩ(P\u000e3\u0019ÔøΩXÔøΩÔøΩa\nÔøΩRÔøΩÔøΩÔøΩIÔøΩÔøΩ-xÔøΩ?ÔøΩ\\ÔøΩÔøΩÔøΩ+\u000fÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ6ÔøΩÔøΩ\u0006ÔøΩ}\nÔøΩ\u001b7·ñü\nÔøΩÔøΩm_ÔøΩÔøΩ“≥ÔøΩÔøΩÔøΩUx:ÔøΩÔøΩÔøΩ;ÔøΩ.ÔøΩÔøΩÔøΩ4~\u0010ÔøΩÔøΩÔøΩhÔøΩ\u0014ÔøΩ&ÔøΩÔøΩÔøΩ6FeÔøΩuÔøΩp2*ÔøΩ8ÔøΩ5ÔøΩ&tÔøΩ\u0004~\u0014ÔøΩhÔøΩÔøΩÔøΩÔøΩVÔøΩ:ÔøΩÔøΩ$ÔøΩﬂ∞ÔøΩ@\nNÔøΩ#ÔøΩU:ÔøΩ}ÔøΩÔøΩÔøΩÔøΩSÔøΩÔøΩ#HiÔøΩÔøΩÔøΩ\u0015ÔøΩ\nÔøΩ4\n\u0005+ÔøΩÔøΩ\n<ÔøΩÔøΩÔøΩÔøΩÔøΩ8N\u00131Yj\u001aÔøΩÔøΩ.XÔøΩÔøΩÔøΩt*ÔøΩÔøΩ”¨SYTÔøΩÔøΩÔøΩÔøΩW/_{ÔøΩRÔøΩÔøΩDÔøΩÔøΩÔøΩm(ÔøΩﬁíÔøΩiÔøΩ\u001aÔøΩÔøΩÔøΩ}ÔøΩo\u0004ÔøΩFaÔøΩeÔøΩÔøΩyÔøΩÔøΩ\nÔøΩ\u001bﬁ∑\u0005ÔøΩa4YÔøΩ>ÔøΩÔøΩÔøΩmÔøΩ;|$ÔøΩ+ÔøΩn\u0014\br~ÔøΩ.\u0010ÔøΩ\u0007ÔøΩÔøΩÔøΩ^ÔøΩ#Œ∏ÕÜxOSÔøΩ\tSÔøΩ#ÔøΩÔøΩ&…§0ÔøΩ\tMÔøΩX\na1ÔøΩÔøΩB{B'CL(@ÔøΩ{WpEÔøΩÔøΩÔøΩBO\u001a#ÔøΩÔøΩ`ÔøΩÔøΩn,\u0016'tÔøΩÔøΩÔøΩ3XÔøΩÔøΩ%ÔøΩ3x\u0016cÔøΩ\u0015\u0012ÔøΩ—ø\u0018ÔøΩÔøΩÔøΩÔøΩhÔøΩÔøΩ7›ÑzÔøΩÔøΩ@OÔøΩ+ÔøΩN\u0015ÔøΩ»öÔøΩ\u0002ÔøΩ\nƒÄÔøΩÔøΩÔøΩÔøΩeÔøΩ\nÔøΩÔøΩ'÷û#qÔøΩÔøΩS_yÔøΩ“µÔøΩﬂ∫ÔøΩ#\\[ÔøΩÔøΩÔøΩWGÔøΩ\u0003W3Î∑äÔøΩÔøΩPÈáø.}ÔøΩ√∑WzÔøΩ]ÔøΩc'ÔøΩ·•Ω7ÔøΩ|ÔøΩYÔøΩÔøΩ\nÔøΩ\u0016@SÔøΩ\n=ÔøΩ^ÔøΩkyÔøΩQ8EOÔøΩ\u000eÔøΩ\n,ÔøΩÔøΩ~ÔøΩÔøΩenÔøΩÔøΩÔøΩÔøΩzÔøΩhÔøΩP<ÔøΩÔøΩÔøΩÔøΩUUÔøΩÔøΩJÔøΩÔøΩÔøΩ|ÔøΩmÔøΩ}ÔøΩcÔøΩÔøΩ√åJ…©xÔøΩÔøΩ:GÔøΩÔøΩ‹•‹¶\n\u0012nÔøΩ~ÔøΩ<ÔøΩ8l~ÔøΩÔøΩgÔøΩÔøΩpÔøΩÔøΩÔøΩ1ÔøΩ\u0010ÔøΩx\u0001Tq/ÔøΩÔøΩÔøΩqÔøΩÔøΩvÔøΩ+ÔøΩÔøΩ”úÔøΩ≈úÔøΩV¬®ÔøΩ/ÔøΩDÔøΩÔøΩIÔøΩÔøΩN5ÔøΩÔøΩRuÔøΩ\u0017ÔøΩÔøΩÔøΩÔøΩdÔøΩl7owÔøΩ4?ÔøΩzL=ÔøΩ\u001fVÔøΩÔøΩBÔøΩBNjœ®-ÔøΩ\t\u000exÔøΩ\u0004GDnÔøΩÔøΩ√çp,ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩJÔøΩÔøΩ\n6xÔøΩuÔøΩuÔøΩÔøΩ}+kÔøΩÔøΩÔøΩbÔøΩ\u001a`KÔøΩÔøΩFÔøΩLÔøΩwÔøΩŸ¶\"[ÔøΩÔøΩ\\ÔøΩÔøΩÓàëÔøΩ~ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ7ÿ∂ÔøΩvÔøΩ\u0018ÔøΩ\u0019ÔøΩeÔøΩÔøΩoÔøΩ<ÔøΩÔøΩ;ÔøΩÔøΩyFÔøΩ%ÔøΩÔøΩÔøΩÔøΩ\u0011ÔøΩÔøΩÔøΩSz+ÔøΩÔøΩ”ë5&%ÔøΩÔøΩz\u001aÔøΩbÔøΩ^–ãzÔøΩ\nÔøΩÈõ®ÔøΩ3ÔøΩ-ÔøΩÔøΩ6ÔøΩo\u00006ÔøΩÔøΩÔøΩ\u0007ÔøΩÔøΩÔøΩ\u000f>ÔøΩ0ÔøΩ”ü\u0014ÔøΩÔøΩ\n4ÔøΩÔøΩJ\u000e\u0018ÔøΩ\u0019PÔøΩ\u00077XÔøΩ\u0013IÔøΩ\b%uÔøΩe\u0001B\nÔøΩÔøΩ\u0007ÔøΩt=ÔøΩDÔøΩkÔøΩw\u0003ÔøΩMÔøΩ\n\u0003ÔøΩÔøΩÔøΩÔøΩÔøΩ4ÔøΩÔøΩVJ\u0017uÔøΩÔøΩTÔøΩ∆ã\\%SÔøΩÔøΩ]ÔøΩÔøΩ+ÔøΩ&jÔøΩJM]ÔøΩÔøΩ‰ö§W\u0015ÔøΩÔøΩ8EcQ' û\nNNÔøΩ+ÔøΩ€¨ÔøΩÔøΩ6;\u0018ÔøΩ )ÔøΩMTÔøΩ#ÔøΩh\u0005=ÔøΩ\u000eÔøΩ^ÔøΩmÔøΩmiÔøΩÔøΩÔøΩ\u000fÔøΩÔøΩÔøΩﬂè<ÔøΩÔøΩÔøΩmxÔøΩBpÔøΩÔøΩ[p\nÔøΩÔøΩkÔøΩYÔøΩ…≤ÔøΩÔøΩ\u0018ÔøΩÔøΩ\tÔøΩÔøΩ|ÔøΩÔøΩ+ÔøΩ ÔøΩ\u0004\u001ah&pÔøΩ\\f?\nÔøΩÔøΩÔøΩÔøΩYlÔøΩEÔøΩÔøΩt\nÔøΩ(;ÔøΩuK\u0014ÔøΩc^ÔøΩ.P,P.ÔøΩÔøΩ]\nnÔøΩbÔøΩb\u0010\n\u0006ÔøΩuÔøΩ&ÔøΩ\u0010OÔøΩ\u000f\u0014ÔøΩz<\n/v,ÔøΩ,\u000fÔøΩ:z=\u001b\n\u0003ÔøΩ!ÔøΩÔøΩÔøΩaÔøΩÔøΩqÔøΩ\u0018ÔøΩ\u001f:ÔøΩ_ÔøΩ?ÔøΩ~ÔøΩÔøΩÔøΩ?ÔøΩÔøΩÔøΩx\u0006;ÔøΩdÔøΩiÔøΩÔøΩNÔøΩÔøΩÔøΩ`ÔøΩtÔøΩ3ÔøΩÔøΩEÔøΩG\u0011vÔøΩÔøΩAÔøΩ\u0005ÔøΩŒîÔøΩB\u0000ÔøΩÔøΩÔøΩNP@\bÔøΩÔøΩÔøΩÔøΩ\u0017\u0018\u000eÔøΩ\tÔøΩ\u0004ÔøΩ\n2p:ÔøΩ\nÔøΩÔøΩoÔøΩÔøΩÔøΩÔøΩ\"*ÔøΩKÔøΩ>KÔøΩfRÔøΩÔøΩÔøΩa4ÔøΩÔøΩÔøΩÔøΩÔøΩCÔøΩCKÔøΩ\u0019\u0001eÔøΩÔøΩzQ\u001f\u001aF#ÔøΩ\u0018:ÔøΩTÔøΩÔøΩÔøΩÔøΩÔøΩvÔøΩÔøΩ\"ÔøΩ.ÔøΩ€Ö]ÔøΩX+ÔøΩN+1R\nJQÔøΩUJJÔøΩÔøΩ%ÔøΩrÔøΩ|ÔøΩ\u0002ÔøΩ\u0007ÔøΩÔøΩÔøΩz\u0006ÔøΩÔøΩÔøΩ{NÔøΩ\u000fP>J&ÔøΩÔøΩÔøΩÔøΩeÔøΩ\u0001ÔøΩ[ÔøΩmhhÔøΩ\nÔøΩÔøΩ ÔøΩ}ÔøΩCDÔøΩ\"ÔøΩA√íÔøΩÔøΩÔøΩÔøΩ\nA(bJxÔøΩ(Bv@(N\u0018ÔøΩn‹è'HÔøΩ*zZFÔøΩ\u0015ÔøΩmÔøΩÿ®ÔøΩÔøΩgÔøΩFﬁΩÂëè0~vÔøΩ\u000fjRS|FM(4mÔøΩÔøΩÔøΩÔøΩŸæÔøΩÔøΩÔøΩ<ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩwÔøΩ~G{4\u0013ÔøΩnÔøΩÔøΩÔøΩÔøΩÔøΩc_ÔøΩToÔøΩÔøΩ∆àÔøΩ2\nÔøΩ^ÔøΩgHÔøΩ<wÔøΩÔøΩ3\u001b3\bÔøΩÔøΩÔøΩ\u0017(ÔøΩÔøΩÔøΩÔøΩn,ÔøΩbÔøΩÔøΩ\u0018%gÔøΩ\nÔøΩ'|5PÔøΩ4ÔøΩy\tﬂ¨ÔøΩAcÔøΩg\u001feÔøΩœÜÔøΩ\t_vÔøΩÔøΩ=\u001bÔøΩÔøΩÔøΩÕÑÔøΩ4-ÔøΩ(ÔøΩ>}ÔøΩoÔøΩ\n>QÔøΩ.\u0015\u0013q\nqÔøΩYÔøΩÔøΩpM)E$ÔøΩUk8%ÔøΩÔøΩfÕ¨ÔøΩ:ÔøΩÔøΩnÔøΩÔøΩ%\u0018√ÅÔøΩÔøΩÔøΩÔøΩ\u0011ÔøΩÔøΩÔøΩÔøΩ \u0019ÔøΩ\u0013ÔøΩÔøΩpCÔøΩ\nÔøΩ’è‘ìzÔøΩfk_2=<oÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩ\u000fÔøΩ\u0013ÔøΩ.ÔøΩÔøΩvÔøΩ#\u0016[ÔøΩÔøΩÔøΩÔøΩ{ÔøΩ,=\u0014x|ÔøΩc\u0014ÔøΩÔøΩ-ÔøΩÔøΩÔøΩLÔøΩÔøΩÔøΩ\u0014ÔøΩ\bg(H>U…ö.lÔøΩt\u0006uzÔøΩYÔøΩk\u001fÔøΩÔøΩc\"nRD\u001f;ÔøΩÔøΩ>\tÔøΩÔøΩÔøΩXÔøΩaÔøΩA\u0017\tEÔøΩ⁄Ä\u0007ÔøΩ\nA}ƒÉÔøΩ[jÔøΩD=\u0006\u0010ÔøΩkÔøΩ\nÔøΩpÔøΩ\ng%ÔøΩÔøΩÔøΩÔøΩ6ÔøΩ:M\u001aÔøΩÔøΩÔøΩÔøΩÔøΩFKÔøΩ@ÔøΩWÕúÔøΩÔøΩÔøΩÔøΩÔøΩH[cÔøΩ\nÔøΩ\\mJÔøΩÔøΩ-ÔøΩ—∫ÔøΩÔøΩ9ÔøΩ\u0001ÔøΩN]7ÔøΩÔøΩdÔøΩ\u0012ÔøΩÔøΩYwlqÔøΩyÔøΩX\u001bgÔøΩjÔøΩ\u00155ÔøΩ@ÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩi+MiÔøΩuÔøΩ8ÔøΩÔøΩ\u00133`KÔøΩ|ÔøΩÔøΩ\u0010ÔøΩZÔøΩ|S[€¢ÔøΩ\u001bK\u001b\u0017ÔøΩ68l\u0017BÔøΩN<ÔøΩW-\u0015fkÔøΩÔøΩÔøΩKÔøΩÔøΩ1\n6^\u0004m5ÔøΩ7U_ÔøΩ.ÔøΩsÔøΩÔøΩÔøΩ)ÔøΩÔøΩ%\u000fÔøΩ\u0002NCÔøΩ\u000fÔøΩYÔøΩKÔøΩ\u0013G\u0015wÔøΩ/\u0010FÔøΩÔøΩÔøΩB\u0014?ÔøΩ<ÔøΩ<ÔøΩÔøΩÔøΩÔøΩ*ÔøΩTSÔøΩÔøΩkÔøΩÔøΩÔøΩÔøΩÔøΩ6ÔøΩqÔøΩiÔøΩÔøΩÔøΩÔøΩFÔøΩ\u0012ÔøΩM7ÔøΩ,3\nv-bÔøΩ6\u0004ÔøΩ\nÔøΩ&ÔøΩ;ÔøΩÔøΩÔøΩjÔøΩÔøΩa\u0005ÔøΩU\n*ÔøΩ+\u0018ÔøΩ'Z\u001bBÔøΩÔøΩV+ÔøΩ:u}ÔøΩa\n;\b…àÔøΩA:A'ÔøΩP<ÔøΩ;ÔøΩÔøΩt@ÔøΩÔøΩ\nÔøΩÔøΩ»´\u0013ÔøΩÔøΩ\n…©ÔøΩ\u001fÔøΩx\u0003ÔøΩF{Q&ÔøΩLŒ∏Sd4\\Td|\"vÔøΩ\nÔøΩthÔøΩ\nj~6 bÔøΩÔøΩÔøΩA^ÔøΩ[D\u0015$2\u0001DÔøΩUÔøΩÔøΩ2\u000e\u0019ÔøΩ\u0015ÔøΩIÔøΩÔøΩb2ÔøΩÔøΩ\te4ÔøΩlÔøΩx\u0015%'ÔøΩrÔøΩCÔøΩÔøΩÔøΩwÔøΩ|ÔøΩsÔøΩbÔøΩÔøΩÔøΩTÔøΩ9ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩGW\u0017\nqÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0006\u001b\u001bÔøΩ√èÔøΩv\tÔøΩÔøΩÔøΩÔøΩÔøΩ\u001fjsÔøΩÔøΩ<ÔøΩÔøΩ\u0002“©ÔøΩn~\u000eÔøΩ3ÔøΩ\nÔøΩ6ÔøΩX\u0018jIÔøΩA\"ÔøΩÔøΩfÔøΩÔøΩÔøΩÔøΩÔøΩ*ÔøΩ\u0007—åÔøΩrx{(\u0016\u0013ÔøΩG}ÔøΩ\u0019HÔøΩÔøΩ2ZD\u0001ÔøΩÔøΩA\u0015V\u0015\u0005-ÔøΩv3\nÔøΩ@“ñ+ÔøΩÔøΩÔøΩÔøΩj\u0015ÔøΩB∆∞ÔøΩÔøΩ\u0017ÔøΩ8,\u0012$\nyÔøΩ@{+ÔøΩÔøΩÔøΩÔøΩWM\u0000ÔøΩÔøΩÔøΩ\n\u0000ÔøΩ&C\u0004al`ÔøΩÔøΩXÔøΩÔøΩ\"\u0012&ÔøΩ\u0004LÔøΩ@?ÔøΩnRKMrÔøΩ\u0004ÔøΩO2ÔøΩD7ÔøΩyWoÔøΩÔøΩÔøΩ\u000fÔøΩÔøΩXMÔøΩt÷¨ÔøΩ`Z)93ÔøΩT+t!ÔøΩ?ÔøΩÔøΩVfÔøΩ/~—íÔøΩ’µZ\u0012ÔøΩÔøΩÔøΩÃã\u0001ÔøΩÔøΩm2ÔøΩÔøΩÔøΩ3ÔøΩC\u0019\u0010ÔøΩ\u0014@Ÿß\u0015ÔøΩ\u000fœînÔøΩ\nÔøΩÔøΩÔøΩ·ôöwHÔøΩ8ibÔøΩÔøΩl\tÔøΩÔøΩÔøΩÔøΩ=ÔøΩ)%q\u0019ÔøΩÔøΩ_\u0006\nÔøΩ\u0015ÔøΩ5ÔøΩkLÔøΩh\nÔøΩÔøΩ4>ÔøΩyFÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩ9NÔøΩ_ÔøΩ_ÿ≠VÔøΩeÔøΩ\nÔøΩÔøΩis⁄Ω\u000eNeÔøΩ84ﬁºsÔøΩsÔøΩ}ÔøΩÔøΩ9ÔøΩÔøΩÔøΩ]NÔøΩSÔøΩcÔøΩDÔøΩtÔøΩ)\u001bÔøΩYÔøΩ(^'ÔøΩTÔøΩEÔøΩL{}ÔøΩÔøΩIZAÔøΩÔøΩÔøΩƒªÔøΩÔøΩÔøΩÔøΩÔøΩ<ÔøΩC\nÔøΩÔøΩ!LÔøΩÔøΩQÔøΩMIÔøΩÔøΩÔøΩÔøΩa^nÔøΩ`ÔøΩjfÕ£ÔøΩÔøΩÔøΩ\u0012|ÔøΩ\nÔøΩÔøΩ8(2ÔøΩÔøΩ\nÔøΩÔøΩ\u0005ÔøΩ\u0005\u0002ÔøΩ«ídYN6ÔøΩÔøΩd\u0007yÔøΩÔøΩEÔøΩ'ÔøΩ\u0011ÔøΩ8ÔøΩÔøΩ\u0003ÔøΩÔøΩ\u0013ÔøΩÔøΩÔøΩSMc\u0017ÔøΩ7ÔøΩyOÔøΩÔøΩÔøΩ1ÔøΩÔøΩ4ÔøΩÔøΩpSÔøΩxÔøΩ\u0014ÔøΩkFSÔøΩlDÔøΩ7m\u0013\u0014[ÔøΩÔøΩÔøΩ\u0001ÔøΩ\u0001jÔøΩÔøΩÔøΩ\u0007f\u0002\u0005ÔøΩdÔøΩ\u0005|W7ÔøΩÔøΩ\nÔøΩ\u0002ÔøΩÔøΩÍôßÔøΩÔøΩ=ÔøΩW`qÔøΩUÔøΩwG#Œ∑\nÔøΩÔøΩÔøΩÔøΩÔøΩ«øÔøΩÔøΩWÔøΩ_2”Ö\u0015ÔøΩ/#ÔøΩ\u0002ÔøΩÔøΩÔøΩ7=~mÔøΩÔøΩ\u001fÔøΩ=ÔøΩvÔøΩw\u000fÔøΩN7\b5iÔøΩÔøΩsÔøΩ\u001f1ÔøΩ\u0001ÔøΩ\u0004ÔøΩ\u0016ÔøΩ\u001f\\ÔøΩÔøΩÔøΩW\nT(ÔøΩ4ÔøΩÔøΩÔøΩÔøΩ\u001a$ÔøΩ ÖÔøΩRÔøΩHÔøΩÔøΩÔøΩÔøΩÔøΩ(\u001b5ÔøΩfÔøΩrÔøΩ\u0001mE;ÔøΩ\nÔøΩ@ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ7WÔøΩÔøΩ|ÔøΩ7\u000fÔøΩCÔøΩ`ÔøΩ(\tf0Qr\u0011ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0019ÔøΩ9JÔøΩÔøΩÔøΩiw\u0012eÔøΩ5ÔøΩD~ÔøΩk%ÔøΩÔøΩdÔøΩB)ÔøΩ≈ïÔøΩÔøΩCb\u0012ÔøΩ+ÔøΩS\nÔøΩ9ÔøΩW%ÔøΩUU7ÔøΩÔøΩz9\u0006mÔøΩ\u0010ÔøΩ\u001aÔøΩ\n\u0015\u001fÔøΩn\"*\b|OÔøΩÔøΩuÔøΩÔøΩÔøΩGn|ÔøΩÔøΩ_ÔøΩ|ÔøΩÔøΩ+ÔøΩÔøΩ\u0016ÔøΩÔøΩÔøΩUgÔøΩÔøΩDÔøΩÔøΩÔøΩ<ÔøΩÔøΩ\u0011ÔøΩhÔøΩÔøΩÔøΩJÔøΩ?-\nÔøΩÔøΩWÔøΩÔøΩÔøΩ—ÅÔøΩV\n<ÔøΩÔøΩ\u001f=|u60uAÔøΩ\u0011`ÔøΩÔøΩÔøΩc\u0014k\u0005ÔøΩRjy\nÔøΩnÔøΩeÔøΩÔøΩÔøΩÔøΩ ≠ﬁªÔøΩozÔøΩ\u0002)\u0004\u00161ÔøΩÔøΩ%ÔøΩ+<\u001b\u0015ÔøΩ=ÔøΩ»êkÔøΩÔøΩ=fÔøΩÔøΩÔøΩ%\u0003\naÔøΩ`4ÔøΩÔøΩ6;oÔøΩ\u0011ÔøΩ\u0019ÔøΩ\nÔøΩ(\u0006,\"√ä\u0001ÔøΩÔøΩÔøΩp\u000eV\u0001ÔøΩÔøΩ\u000fÔøΩbÔøΩÔøΩ<pÔøΩÔøΩ1KZÔøΩ\u000eÔøΩ\n\"ÔøΩ\u0012\b \u0016=\u000fx›çg\n\u0004ÔøΩpÔøΩÔøΩÔøΩ\u001a\u0000ÔøΩ\u0014ÔøΩ\nÔøΩÔøΩm\u0014qD {@ÔøΩ”õH*Q\u0012ÔøΩ\bDp\u0006ÔøΩÔøΩÔøΩÔøΩe\nÔøΩÔøΩiÔøΩÔøΩÔøΩM4Èß£t2F\u0003C;FCEcÔøΩXÔøΩÔøΩ(ÔøΩÔøΩÔøΩÔøΩ\u0004NÔøΩÔøΩ\u0012\n*ÔøΩ\\ÔøΩn0ÔøΩÔøΩÔøΩ)ÔøΩÔøΩÔøΩ\n:nÔøΩ\u00058ÔøΩÔøΩÔøΩRy\nzÔøΩÔøΩ&\u0007\bbÔøΩÔøΩ|ai]7V=|€í[ÔøΩ_ÔøΩÔøΩÔøΩ\nÔøΩ!W,ÔøΩÔøΩ~ÔøΩ]w^ÔøΩ\"f\u0015ÔøΩ:\u0012ÔøΩuÔøΩÔøΩ\u0015G\u0006cÔøΩ\nj=I!ÔøΩ?ÔøΩÔøΩÔøΩÔøΩ4ÔøΩ9bÔøΩÔøΩÔøΩ1ÔøΩ\u001aoÔøΩ\"ÔøΩÔøΩG\u0011abÔøΩ\u001a!ÔøΩ\u0002ÔøΩegÔøΩv$ÔøΩ\u000f.\u0014>GÔøΩv`4\n(\u0004xÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0002ÔøΩVÔøΩOÔøΩﬂÄÔøΩÔøΩaTRBÔøΩrÔøΩÔøΩÔøΩT-iÔøΩÔøΩÔøΩÔøΩÔøΩtkÔøΩF\u001f\u0016ÔøΩ\n+‘ú:ÔøΩNÔøΩÔøΩÔøΩrÔøΩ9ÔøΩ!7ÔøΩEi\u001b\u0018ÔøΩ7uoÍèáÔøΩG~\u001bz'ÔøΩ^ÔøΩCÔøΩÔøΩ–áÔøΩS\u001aSsÔøΩ'uUzKj\u0007ÔøΩAv0ÔøΩÔøΩA◊†{–≥=ÔøΩÔøΩZGGÔøΩ‘åJÔøΩÔøΩÔøΩSÔøΩ\u0007ÔøΩ\bÔøΩ\nÔøΩf1yl^g¬ù⁄©⁄©~XÔøΩvÔøΩÔøΩaÔøΩ)ÔøΩÔøΩÔøΩÔøΩ:rÔøΩsÔøΩ\u0012ÔøΩRÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ>b>ÔøΩh\u0013|ÔøΩ\u000fÔøΩD|ÿè3ÔøΩ\\ÔøΩÔøΩAÔøΩRÔøΩ(vIÔøΩ*ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0012]ÔøΩE\u000f:_ÔøΩ—ÉAÔøΩ)\nÔøΩiXCLÔøΩ\u0014>ÔøΩ\u0013TÔøΩÔøΩÔøΩAH\u0011ÔøΩÔøΩ\\ÔøΩp:\nÔøΩQÔøΩÔøΩ2ÔøΩÔøΩIC~fÔøΩÔøΩÔøΩVÔøΩÔøΩÔøΩg\u0001&0ÔøΩX$MÔøΩ\u0001ÔøΩ\u001aÔøΩ\nÔøΩ\u0006ÔøΩ0ÔøΩÔøΩ$gÔøΩÔøΩÔøΩÔøΩOÔøΩÔøΩÔøΩX_l0∆àÔøΩlÔøΩƒûÔøΩ\"ÔøΩÔøΩ‚Å∂I[D«∫dÔøΩ6NÔøΩY9\u0000ÔøΩ\u0001F9XÔøΩPÔøΩÔøΩ\ngh\b`ÔøΩZÔøΩSÔøΩ\nÔøΩÔøΩtIÔøΩp(\u0014ÔøΩi,:ÔøΩfrHÔøΩÔøΩ2&ÔøΩ3ÔøΩQÔøΩÔøΩA1O<ÔøΩ\u0017\u0005ÔøΩÔøΩÔøΩ\u001bÔøΩÔøΩS&x\u000fÔøΩÔøΩ2\u0010\u0017Wxp%vpÔøΩMtÔøΩÔøΩÔøΩ”æÔøΩ>\u0017>7~\u0019gÔøΩM\u0007‰ëÆ.…π\u001bÔøΩ&ÔøΩÔøΩ›öÔøΩtÔøΩÔøΩa◊∞{ÿ≥3ÔøΩ@hwZÔøΩÔøΩMÔøΩE\u0019\b\u0000ÔøΩgBÔøΩù©áÔøΩ\u000fÔøΩÔøΩÔøΩ)ÔøΩ\u001b„¢≥ÔøΩÔøΩ;ÔøΩXR\u0017\tÔøΩÔøΩ\n6tQ\nÔøΩT\u0017ÔøΩÔøΩ)%Ô™¢VÔøΩÔøΩ\"M\u0000K\u0000ÔøΩÔøΩ3g1\\\u0001ÿ°JÔøΩÔøΩÔøΩqs151\u0013‚†©r/\u0003\u0018\u0004ÔøΩ\u0004ÔøΩ0\u0015SÔøΩÔøΩ^sZ2\u0018ÔøΩ4CÔøΩ\u0011tÔøΩ\nÔøΩÔøΩi…§ÔøΩÔøΩÔøΩÔøΩ\nÔøΩ\nFyÔøΩ\u001a\u000eÔøΩo?,\u0007—±1$ÔøΩPy\\ÔøΩ>ÔøΩ^h\u0018≈òÔøΩÔøΩÔøΩ¬±ÔøΩÔøΩÔøΩÔøΩp zÔøΩÔøΩ3\u0017ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ“µ\nÔøΩ\u0007ÔøΩv] ÔøΩŸµÔøΩu…ä“üÔøΩÔøΩoÔøΩkÔøΩ\u0019\u0005ÔøΩÔøΩÔøΩ_zÔøΩ€óÔøΩM7ÔøΩ\u0013’≥V}wÔøΩNÔøΩ⁄ÖgÔøΩuÔøΩÔøΩbÔøΩ%√çÔøΩ%ÔøΩ\u000fÔøΩ\nz\u0007ÔøΩÔøΩ\u0004\u0011bÔøΩrd6ÔøΩ/<ÔøΩÔøΩ·ªøÔøΩXÔøΩT_ÔøΩÔøΩ}ÔøΩÔøΩÔøΩx_ÔøΩ}\nnÔøΩÔøΩÔøΩÔøΩÔøΩﬂªÔøΩÔøΩ]XÔøΩ\tÔøΩÔøΩh1RÔøΩMÔøΩgÔøΩÔøΩÔøΩ.ÔøΩÔøΩU\u000fVk~ÔøΩÔøΩÀìÔøΩy~ÔøΩÔøΩÔøΩ[ÔøΩ/ÔøΩ\u001a~ÔøΩÔøΩÔøΩÔøΩ{ÔøΩw„üÑÔøΩ\n)ÔøΩÔøΩzÔøΩÔøΩ\u000f\u0004ÔøΩÔøΩÔøΩ\u0005B\u0006ÔøΩÔøΩ\u001a\b!ÔøΩÔøΩÔøΩÔøΩ∆õC\nÔøΩ2CÔøΩÔøΩ*nÔøΩY\tÔøΩÔøΩ&ÔøΩ\u0012\\YÔøΩÔøΩÔøΩs)\\sÔøΩ'ÔøΩ]ÔøΩ\u001aKÔøΩ#ÔøΩdwÔøΩÔøΩÔøΩ\u0013ÔøΩLu\n\u0007\nÔøΩ]i\u0010ÔøΩÔøΩ\u000e\u0005hÔøΩ\nÔøΩÔøΩaÔøΩv\u001aÔøΩ2ÔøΩÔøΩr6ÔøΩ=·•ÅÔøΩÔøΩÔøΩ-ÔøΩÔøΩÔøΩ\bÔøΩ\u0013NÔøΩ=ÔøΩH<ÔøΩÔøΩÔøΩ!^\u000f$1gU\u000eG‹°ÔøΩyÔøΩÔøΩ9\n7KÔøΩ/\u0018ÔøΩÔøΩÔøΩÔøΩAÔøΩ8\u0005\u0005D?¬≤\u0003ÔøΩÔøΩ4\nÔøΩÔøΩ\u0007(‹°\u0003ÔøΩÔøΩ\u0014\u0018ÔøΩ\bÔøΩÔøΩÔøΩ&|ÔøΩÔøΩ$ÔøΩ)ÔøΩÔøΩcÔøΩh{~ÔøΩÔøΩÔøΩÔøΩÔøΩ\nkÔøΩÔøΩ?ÔøΩÔøΩﬂø^30ÔøΩpÔøΩwÔøΩ\u0003ÔøΩo]ÔøΩÔøΩ$7ÔøΩÔøΩ\nÔøΩÔøΩ\"ÔøΩ\u0006ÔøΩ5ÔøΩzZj;xÔøΩÔøΩ'ÔøΩÔøΩÔøΩÔøΩÔøΩ\nv=ÔøΩf\u0006z6\u0001Q9@\n\u001aÔøΩÔøΩÔøΩ%ÔøΩp\u0018ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ=ÔøΩÔøΩaÔøΩ\u0018ÔøΩ\n\u0013ÔøΩ&a\u001a4ÔøΩÔøΩÔøΩÔøΩyCc%Og+y(\"ÔøΩRÔøΩ”ïw$|ÔøΩA]ÔøΩgÔøΩ\u001b\nƒúÔøΩE_`ÔøΩ÷©5\u000fÔøΩÔøΩ\\D(ÔøΩÔøΩÔøΩ&ÔøΩ0ÔøΩÔøΩ\u0019*:-\u0005F\u0016ÔøΩÔøΩ\u0002sÔøΩVÔøΩsÔøΩÔøΩ\u000e)YtÔøΩÔøΩkÔøΩ\u000f;pÔøΩ\u0003ÔøΩ:ÔøΩ\n√é=ÔøΩÔøΩ\u000eÔøΩÔøΩ`ÔøΩÔøΩÔøΩÔøΩÔøΩV4T$kÔøΩ3ÔøΩWÔøΩiÔøΩ%@ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩSÔøΩ1ÔøΩIÔøΩ|ÔøΩÔøΩgXÔøΩ\u0013USÔøΩTU5MÔøΩÔøΩÔøΩfzÔøΩÔøΩÔøΩÔøΩ-CÔøΩÔøΩ\n[\u0014wÔøΩ\u0003MUUSJÔøΩqqqÔøΩ\u0013\u000eÔøΩÔøΩ\u0016ÔøΩ\u0015ÔøΩÔøΩD\u0019ÔøΩ\u00114ÔøΩ|ÔøΩU@?ÔøΩQ\u001aW\u001fE\u0019ÔøΩYÔøΩÔøΩ\u0019ÔøΩ!\u0017$ÔøΩÔøΩ\u001bÔøΩ\u001b\u0015CÔøΩ`fÔøΩXÔøΩÔøΩ2ÔøΩ\u0019ÔøΩ2ÔøΩ*krÔøΩb\u0011ÔøΩ0y?ÔøΩÔøΩÊ∞òÔøΩWÔøΩR/V?ÔøΩ>QÔøΩ'ÔøΩ\nÀúN\u0012QDbÔøΩÔøΩ\u00125ÔøΩRkÔøΩÔøΩ!^\"ÔøΩQÔøΩ\u0017ÔøΩ\u0017wÔøΩÔøΩÔøΩSÔøΩQÔøΩÔøΩUÔøΩ(oÔøΩiÔøΩÔøΩ|ÔøΩ\u0019VoÔøΩ6ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩe\u001a6eEÔøΩÔøΩÔøΩÔøΩp*ÔøΩg4~ÔøΩ\thÔøΩU\\+ÔøΩÔøΩÔøΩÔøΩAÔøΩ~\u001bÔøΩ\n€àÔøΩDÔøΩRÔøΩÔøΩVÔøΩiÔøΩ‹¨ÔøΩÔøΩÔøΩÔøΩek\u0005\u0000&ÔøΩÔøΩÔøΩ\u0007z\u0000ÔøΩÔøΩ\u001fÔøΩÔøΩ\u0007ÔøΩ=\u0004ÔøΩRÔøΩÔøΩ\tÔøΩ\"\nÔøΩ+ÔøΩdÔøΩX$ÔøΩ'DÔøΩd!ÔøΩs\u0011\u0011W)R2ÔøΩÔøΩ\u0013ÔøΩÔøΩ\u0006\u001aÔøΩÔøΩÔøΩ\u000f0ÔøΩ'ÔøΩÔøΩ(+t1ÔøΩÔøΩ*|\u0005ÔøΩ(ÔøΩjÔøΩÔøΩPÔøΩHÔøΩ3LDhÔøΩOZ\u0006ÔøΩÔøΩÔøΩ\u001f?ÔøΩÔøΩ\u0001\u0010–ïÔøΩacÔøΩ\u0010ÔøΩÔøΩ”öÔøΩÔøΩje”™LWÎ≤ëÔøΩÔøΩÔøΩŒúÔøΩÔøΩkÔøΩÔøΩYÔøΩO>*#ÔøΩ/ÔøΩÔøΩÔøΩY\ncÔøΩÔøΩ\nÔøΩÓåæb«∫ÔøΩÔøΩ[\u001aÔøΩÔøΩ\u0002ÔøΩÔøΩOÔøΩ3rÔøΩÔøΩ\u001a\nRUÔøΩ/ÔøΩHMÔøΩÔøΩYÔøΩ-ÔøΩ\u0015ÔøΩ\nÔøΩ4ÔøΩ%ÔøΩ<\bÔøΩÔøΩÔøΩrÔøΩpÔøΩ@F\nÔøΩ\u0017\u001aÔøΩ\u0015\u0018/oKÔøΩ\nÔøΩÔøΩA2\u0006\u0013\t_xnÔøΩOÔøΩÔøΩsCﬁÑ/4ÔøΩÔøΩÔøΩPMÔøΩjzÔøΩW3√ÉBÔøΩ9Œï\"\nÔøΩNÔøΩAÔøΩÔøΩÔøΩÔøΩÔøΩ0ÔøΩGxlÔøΩÔøΩÔøΩÔøΩÔøΩ[<ÀèÔøΩÔøΩ$w\"ÔøΩ\nWÔøΩ\u0013ÔøΩÔøΩÔøΩD_ÔøΩ\nL\n'F\u0012\nJ\b\t\"\u000f/ÔøΩ,ÔøΩ|ÔøΩ7ÔøΩÔøΩV*\u0006ÔøΩÔøΩ\\\u0005jÔøΩœüÔøΩ@C/ÔøΩ‚π†ÔøΩL\u001bÔøΩÔøΩÔøΩ(ŸàÔøΩÔøΩ{0ÔøΩ,ÔøΩk2ÔøΩBÔøΩ.ÔøΩpÔøΩ8AÔøΩÔøΩ\u000fÔøΩLÔøΩMÔøΩÔøΩÔøΩÔøΩ_ÔøΩÔøΩ\nnÔøΩÔøΩ=mÔøΩEÔøΩ^SsAiÔøΩY ©ÔøΩÔøΩÔøΩÔøΩmÔøΩÔøΩiÔøΩÔøΩ2ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ⁄∂ÔøΩÔøΩÔøΩÔøΩÔøΩ%~'ÔøΩP,jÔøΩÔøΩÔøΩmÈøπÔøΩÔøΩÔøΩy\u0001ÔøΩÔøΩZÔøΩ\u0017ÔøΩÔøΩ#\"\u0018YÔøΩÔøΩNÔøΩÔøΩWÔøΩ\u001bÔøΩs\u0014ÔøΩÔøΩ\u001fI”µÔøΩÔøΩx9!ÔøΩﬁù∆ùŒóÔøΩ/ÔøΩFÔøΩ\u001f9ÔøΩÔøΩ^ÔøΩ›Ö;ÔøΩ\nÔøΩÔøΩÔøΩÂ∫ø;ÔøΩÎ¨éÔøΩÔøΩÔøΩY\nN\u0017ÔøΩibqÔøΩÔøΩÔøΩ5ÀébÔøΩÔøΩÔøΩLÔøΩ\u0010ÔøΩÔøΩ\u0016ÔøΩÔøΩAc{ÔøΩÔøΩÔøΩÔøΩ3+cÔøΩÔøΩÔøΩÔøΩ\u0019“åÔøΩOÔøΩÔøΩ\b\u0010ÔøΩ:ÔøΩ\nÔøΩ\u0012/¬òe\u0015aKÔøΩ\u0019\u000fÔøΩ12\nÔøΩ\u0011ÔøΩ1ÔøΩ\tÔøΩIÔøΩÔøΩÔøΩÔøΩyzÔøΩÔøΩ.j\u001aÔøΩLÔøΩÎë±\"ÔøΩÔøΩÔøΩÔøΩÔøΩOUÔøΩs8t\nÔøΩÔøΩ ÔøΩiÔøΩÔøΩÔøΩÔøΩ)L\n\u0000ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0016ÔøΩXÔøΩd)ÔøΩ\u0016ÔøΩÔøΩBÔøΩ<ÔøΩsÔøΩ;ÔøΩÔøΩÔøΩiÔøΩXhpFuW’∑ÔøΩNÔøΩ\u0013ÔøΩ+ÔøΩ_ÔøΩ\nÔøΩAÔøΩÔøΩD|ÔøΩÔøΩÔøΩUd]ÔøΩvÔøΩÔøΩÔøΩÔøΩ[ÔøΩ|ÔøΩ\u0019gÔøΩE\u0011ÔøΩ\u0007ÔøΩÔøΩcÔøΩÔøΩÔøΩ\u0018ÔøΩ\u0000Gbl^SÔøΩo\u0014gÔøΩgÔøΩ\n\u0017oÔøΩÔøΩ9bÔøΩ@ÔøΩ/\u0012\nÔøΩ1<ÔøΩÔøΩÔøΩ3DMÔøΩÀèÔøΩV…¨\u0006ÔøΩ\u0001NÔøΩ“õÔøΩÔøΩÔøΩ\u001aÔøΩF\u0013ÔøΩÔøΩÔøΩ\nÔøΩ`lÔøΩ}x7~\nÔøΩÔøΩ2jÔøΩÔøΩtÔøΩMÔøΩNÔøΩ\nB2bfhÔøΩÔøΩ\u0013=\u0004ÔøΩ\u0013}uÔøΩÔøΩ\u0003\u001f\u0014@ÔøΩÔøΩ9ÔøΩÔøΩÔøΩKÔøΩL\u0007ÔøΩÔøΩÔøΩ)mÔøΩTÔøΩÔøΩ1\u0018=\u0006ÔøΩ\u0007\tFÔøΩÔøΩÔøΩ ÔøΩOÔøΩÔøΩXÔøΩsÔøΩaÔøΩÀÄ\u0011ÔøΩB`ÔøΩ#ÔøΩ\u0016+0ÔøΩ@)ÔøΩcÔøΩ“øÔøΩ7ÔøΩÔøΩÔøΩﬁüÔøΩÔøΩÔøΩÔøΩ”ªÔøΩÔøΩWÔøΩ\u0015ÔøΩ2ÔøΩÔøΩÔøΩfÔøΩÔøΩ\u000e^\nÔøΩÔøΩÔøΩÔøΩAÔøΩszÔøΩ\u001bGÔøΩ\nÏ¨õGÔøΩ\nÔøΩI\u0004zÔøΩÔøΩ\\ÔøΩyÔøΩ\n1Tƒ¨ÔøΩ\nÔøΩ\u0014\u001aÔøΩ+\nsÔøΩeÔøΩv#{{\nOI5OiK-K]nÔøΩ<u5ÔøΩŸ∏9u+ÔøΩÔøΩÔøΩÔøΩÔøΩBÔøΩÔøΩNÔøΩÔøΩuÔøΩÔøΩÔøΩYi\nÔøΩÔøΩL<a2ÔøΩ\tÔøΩÔøΩÔøΩÔøΩÔøΩ\u0006ÔøΩB(\u0016ÔøΩÔøΩÔøΩ\nbJÔøΩ\u0019ÔøΩZÔøΩÔøΩuÔøΩ4.P\nÔøΩkÔøΩ\u000e}mÔøΩ_=ÔøΩ&ÔøΩÔøΩAÔøΩ~5ÔøΩÔøΩD$fJ\u0002ÔøΩ(v\u0006ÔøΩ\u0002d0ÔøΩiÔøΩÔøΩ\u0012ÔøΩW\u0004z\u001biÔøΩÔøΩ\nÔøΩ>ÔøΩ1:ÔøΩ\u0003ÔøΩ=ÔøΩÔøΩÔøΩÔøΩÔøΩ^ÔøΩÔøΩ[ÔøΩa1SÔøΩt|$\u001fÔøΩFÔøΩÔøΩ\u0002W+ÔøΩ\u000eÔøΩÔøΩÔøΩNÔøΩ5ÔøΩÔøΩs⁄∫\u0012ÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩuÔøΩsY\tÔøΩ\u0001BÔøΩ4ÔøΩ9[}ÔøΩ9ÔøΩÔøΩÔøΩh\tÔøΩÔøΩLÔøΩa\tvEgÔøΩÔøΩ\u0018ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩkÔøΩ≈∂ÔøΩË¨èÔøΩÔøΩBÔøΩÔøΩÔøΩJÔøΩÔøΩÔøΩÔøΩ_,ue3aÔøΩ8ÔøΩÔøΩÔøΩ\u0015ÔøΩo}{ÔøΩÔøΩVCÔøΩÔøΩÔøΩ%9_$ÔøΩmÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩCÔøΩÔøΩÔøΩ‘ÜPc◊ÖkÔøΩÔøΩÔøΩÔøΩBÔøΩÔøΩÔøΩÔøΩsÔøΩcÔøΩvÔøΩV-ÔøΩJ\u0016I\u001b4ÔøΩÔøΩ\n1-ÔøΩÔøΩ&ÔøΩ_\nlﬁß:ÔøΩfLIÔøΩ\u0016ÔøΩ%w;ÔøΩSsgAÔøΩ5ÔøΩ\u001aÔøΩÔøΩÔøΩfVÂôßÔøΩÔøΩl\u0015[ÔøΩÔøΩ\u001aÔøΩÔøΩÔøΩ^^ÔøΩÔøΩD\u0014ÔøΩÔøΩÔøΩÔøΩs4s\nmÔøΩ-ÔøΩsÔøΩ.—¨ÔøΩ‹¶ÔøΩU}ÔøΩ∆∞ÔøΩvÔøΩÔøΩÔøΩÔøΩÔøΩ7ÔøΩ^>ÔøΩÔøΩM’âtÔøΩ\u0005ÔøΩ\u0006[ÔøΩ-\u001f;\u0002\u0018\u00020ÔøΩVv`\u001a\nÔøΩÔøΩSKÔøΩ\nÔøΩ^-#ÔøΩÔøΩF-ÔøΩmrÔøΩÒìÑ¶ÔøΩÔøΩXÔøΩÔøΩÔøΩ`2ÔøΩÔøΩ\u000eÔøΩÔøΩÔøΩ_ÔøΩBÔøΩÔøΩeÔøΩÔøΩ&“îbÔøΩ“ÉiÔøΩ.$RY\n\u0000ÔøΩÔøΩÔøΩÔøΩX\u001aÔøΩ{#(ÔøΩÔøΩjÔøΩÔøΩÔøΩ\nx-\nÔøΩ\b<»¢/ÔøΩÔøΩ?2\u0018\u0019ÔøΩÔøΩRÔøΩtÔøΩ\nFpÔøΩ\u0005“Ç8d\u0005\u001fÔøΩ_ÔøΩRÔøΩÔøΩ3ÔøΩ\u001aNÔøΩ\u0017EÔøΩÔøΩ\u001bÔøΩ\u0018ÔøΩ√ß9ÔøΩÔøΩaÔøΩeZÀπ\u0000ÔøΩ\u0000\u0018ÔøΩ3cIŸ†$'ÔøΩZ\u0013p\u0017ÔøΩÔøΩÔøΩS=ÔøΩX?ÔøΩ\u0015\nM\u001aÔøΩÔøΩ\u000fÔøΩÔøΩPÔøΩÔøΩ\",s÷¨ÔøΩ\u0014OHaÔøΩoÔøΩk J\u0015ÔøΩÔøΩ2\u0010\u0014ÔøΩDYÔøΩ\u0014EdÔøΩÔøΩ=ÔøΩd6ÔøΩu\nMQ\u0014=ÔøΩÔøΩœãÔøΩÔøΩ◊ò<ÔøΩ\u0007ÎÉê4*ÔøΩ<\u0014\u0010UÔøΩAÔøΩ3\"ÔøΩ…™ÔøΩ*\n\u000f\u0007\u0000ÔøΩÔøΩ\u0003CRHxÔøΩŸÑi\nÔøΩ2XÔøΩl\n|\u001aÔøΩÔøΩ'\u000f\nrvD_ÔøΩ\u0017ÔøΩt\u0004Ë£ÉZÔøΩÔøΩÔøΩ4ÔøΩÔøΩCÔøΩ\u0014ÔøΩÔøΩ{(\u0010siÔøΩj V}ÔøΩÔøΩjÔøΩ’êÔøΩ WÔøΩ7@\u0006ÔøΩ+2\u0019%\u0001VÔøΩÔøΩÔøΩH[ÔøΩsÔøΩ\u0013{ÔøΩjV8ÔøΩÔøΩZÔøΩd+ÔøΩ\n?ÔøΩoÔøΩÔøΩÔøΩpÔøΩÔøΩÔøΩ7ÔøΩ\u0012o~ÔøΩdAs$J2ÔøΩHfdÔøΩÔøΩ\u0017NÔøΩÔøΩvÔøΩÔøΩÔøΩ6ÔøΩÔøΩÔøΩiÔøΩ\u000fÔøΩ:f,nÔøΩwÔøΩFÔøΩÕóÔøΩÔøΩÔøΩÿ¥8ÔøΩ}M0ÔøΩjÔøΩÔøΩÕß\u0017\u000f'ÔøΩ\u0017$o+ÔøΩqÔøΩ\u0014\nÔøΩkjÔøΩ∆∑qOÔøΩ3ÔøΩ[ÔøΩÔøΩ\nU\"ÔøΩÔøΩÔøΩÔøΩ\nÔøΩCÔøΩ%e\u001aÔøΩÔøΩQ7P;`ÔøΩ\u0018ÔøΩÔøΩ\nÔøΩ\nÔøΩH√±ÔøΩ\u0013\nLRÔøΩ;\u001bz\u001bÔøΩhÔøΩ‘ÄE\nÔøΩvc≈âI'|ÔøΩÔøΩAuÔøΩ'ÔøΩÔøΩÔøΩÔøΩE+NL!V==ÔøΩ+ÔøΩÔøΩbÔøΩ\u000e!oÔøΩÔøΩÔøΩi6\u001a\u0005ÔøΩÔøΩ\u0011V\nÔøΩÔøΩ\u001a\u001bÔøΩ}ÔøΩÔøΩÔøΩ‘¨ZÔøΩ\ni\u0014\bWÔøΩ”ùÔøΩÔøΩt_ÔøΩ\nL\u000fÔøΩÔøΩH\u001aÔøΩÔøΩÔøΩ>ÔøΩ>ÔøΩf”ΩÔøΩ\u0015\u0007ÔøΩÔøΩÔøΩ›ÅÔøΩÔøΩ<\n^\u0019qG=\nÔøΩ\u0007sÔøΩÔøΩÔøΩ?fÔøΩ/ÔøΩÔøΩ~P\\ÔøΩÔøΩaÔøΩË´ÄÔøΩyÔøΩBÔøΩ)-pÔøΩÔøΩ7ÔøΩ<ÔøΩÔøΩ>ÔøΩYÔøΩÔøΩJÔøΩiVÔøΩVÔøΩÔøΩgdk.ÔøΩk-ÔøΩ,5N\nY\nÔøΩÔøΩX3zlRÔøΩ=ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0017KOÔøΩ^\\\":dOFÔøΩ\u0010œ∏ÔøΩÔøΩLÔøΩÔøΩÔøΩG|ÔøΩÔøΩÔøΩÔøΩ‘äÔøΩ91ÔøΩc\nÔøΩÔøΩ8ÔøΩ„ó•ÔøΩmAlÔøΩ-ÔøΩZÔøΩ43'ÔøΩDÔøΩ8ÔøΩÔøΩÔøΩ5a\u0006pQ$\u001aÔøΩÔøΩÔøΩÔøΩ\u0006ÔøΩ!|\u0003ÔøΩÔøΩ\\ÔøΩÔøΩZÔøΩ\u0018ÔøΩ\u0014\u0019ÔøΩÔøΩÔøΩ\u0007ÔøΩOÔøΩ#ÔøΩE_LÔøΩÔøΩVÔøΩx+ÔøΩ+|kÔøΩÔøΩ^ÔøΩ\u0018y<ÔøΩ?ÔøΩrÔøΩÔøΩÔøΩgÔøΩrZgB6ÔøΩ\"ÔøΩ8ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩe\u0019u\u0015O<\nlÔøΩ3ÔøΩÔøΩ(\u0012w#ÔøΩÔøΩÔøΩ\u0007B6:ÔøΩ6$ÔøΩT$\n\u000e\u0012l\u0001ÔøΩ(ÔøΩ\n\u0011\tWÔøΩÔøΩÔøΩ\tÔøΩ)z9fX\u000eW\"ÔøΩ3ÔøΩÔøΩ(ÔøΩÔøΩdÔøΩÔøΩ{ÔøΩ\nbÔøΩÔøΩ1FÔøΩI^ÔøΩÔøΩUYÔøΩ\u0013ÔøΩ(ÔøΩÔøΩÔøΩ\u0000ÔøΩ\bÔøΩ\tÔøΩÔøΩa0lR]_›â:ÔøΩ.ÔøΩ\u0007mÔøΩ]ÔøΩ\u0015GÔøΩ\u0013ÔøΩÔøΩm\"ÔøΩ\nÔøΩ\n(ÔøΩÔøΩTPwf\u0002uOLÔøΩ\n\u0003ÔøΩÔøΩTÔøΩ\u0019ÔøΩPMÔøΩt\tcÔøΩÔøΩ\u0002ÔøΩ-ÔøΩql*ÔøΩ\u0000ÔøΩÔøΩK\bÔøΩrÔøΩÔøΩ&Î†∞<]ÔøΩ\nÔøΩ#ÔøΩP&ÔøΩk|ÔøΩT\u0007S9\u0014\ngÔøΩZ:ÔøΩzbj5ÔøΩ$\u0000@`ÔøΩÔøΩ\u0019D@khÔøΩÔøΩÈÉñbÔøΩ*\u000fÔøΩ<ÔøΩ\u0011ÔøΩÔøΩ\u000f\n≈¨@ÔøΩVÔøΩÔøΩ|FP8ÔøΩ\u0000ÔøΩHÔøΩÔøΩ\u0005ÔøΩsti\u000eÔøΩÔøΩsÔøΩWÔøΩuÔøΩÔøΩKÔøΩÔøΩÔøΩÔøΩ\tÔøΩËºÇÔøΩ‹≠tÔøΩ\nÔøΩÔøΩwÔøΩÔøΩ{OcGÔøΩ–ÜÔøΩSÔøΩ\n’´ÔøΩÔøΩÔøΩÔøΩq\u0015ÔøΩÔøΩ`\\ÔøΩÔøΩuÔøΩÔøΩÔøΩÔøΩ[FÔøΩÔøΩ\u001bnÔøΩ“í{Òæõ∑ÔøΩ\u0018ÔøΩÔøΩÔøΩ\nÔøΩÔøΩ3 /ÔøΩZMÔøΩVÔøΩÔøΩÔøΩÔøΩ5ÔøΩÔøΩÔøΩÔøΩ=ÔøΩÔøΩÔøΩ\u00175ÔøΩÔøΩjÔøΩ-ÔøΩﬁÜÔøΩRm7ÔøΩÔøΩÔøΩÔøΩRmÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u001f[ÔøΩp\u0011…èÔøΩÔøΩB\u0004qÔøΩ\bÔøΩ1ÔøΩÔøΩÔøΩIÔøΩÔøΩÔøΩ\u0003ÔøΩ\u000fÔøΩO–¥ÔøΩÔøΩeÔøΩÔøΩavÔøΩUÔøΩÔøΩj%8(iwkÔøΩÔøΩÔøΩD\u0010j;ÔøΩ<'ÔøΩFÔøΩ\nÔøΩF:ÔøΩ/=ÔøΩÔøΩÔøΩ9ÔøΩgÔøΩ\\ÔøΩÔøΩÔøΩ%j\nÔøΩDAÔøΩÔøΩ1\u0000ÔøΩÔøΩ-]/\"\u0017SÔøΩXdaj?\u0016>vÔøΩW\u0005ÔøΩw5\u0006QÔøΩÔøΩÔøΩÔøΩ>J\"ÔøΩÔøΩ:ÔøΩÔøΩ\u001a\n\u0016\u0011ÔøΩÔøΩ%bÔøΩ\nJ\u000e\u000eJfÔøΩ b7\u0003ÔøΩUc\u0017ÔøΩS\u0001ÔøΩÔøΩfÔøΩ—πf\u0003tÔøΩ|\u0012ÔøΩ\u000enÈíå◊íkÔøΩ◊´ÔøΩÔøΩ_oÔøΩdÔøΩÔøΩqÔøΩÔøΩÔøΩÔøΩ\bÔøΩÔøΩ<ÔøΩÔøΩÔøΩÔøΩ\nÔøΩ?ÔøΩÔøΩÔøΩV_[\tÔøΩSÔøΩ\u0007ÔøΩ}ÔøΩ=H5ÔøΩIÔøΩo,J–âo\\ÔøΩÔøΩo]ÔøΩvÔøΩÔøΩ\u0016\u0014ÔøΩÔøΩ`ÔøΩÔøΩ+ÔøΩqÔøΩ,fÔøΩÔøΩmÔøΩo8;ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ7ÔøΩÔøΩÔøΩ“üÔøΩÔøΩÔøΩÔøΩ]ÔøΩtÔøΩÔøΩÔøΩÔøΩLÔøΩÔøΩ@ÔøΩ)h\u000eÔøΩ&\u001fH7ÔøΩlÔøΩ\u000fDwÔøΩ1ÔøΩ&ÔøΩÔøΩÔøΩU\u001b\u0017\u0010TÔøΩÔøΩV^tÔøΩÔøΩ6ÔøΩw,ÔøΩPmÔøΩoÔøΩ\u000evÔøΩÔøΩ\u0016ÔøΩÔøΩÔøΩ\nÔøΩÔøΩiÔøΩÔøΩÔøΩhÔøΩÔøΩÔøΩ>ÔøΩ}ÔøΩÔøΩ\nÔøΩÔøΩQ≈≥ÔøΩg\noÔøΩÔøΩh;ÔøΩÔøΩƒ≤ÔøΩÔøΩN/sÔøΩDkN(XÔøΩÔøΩÔøΩ\u0014OÔøΩsÔøΩÔøΩÔøΩÔøΩÔøΩ\u0005Ê∫ëÔøΩÔøΩ5ÔøΩ*ÔøΩŸ¢ÔøΩÔøΩi0Eh\nÔøΩÔøΩmÔøΩÔøΩÔøΩ\u0001\nÔøΩÏèº\na\"ÔøΩxÔøΩÔøΩÔøΩJÔøΩÔøΩiÔøΩ\n\u0002ÔøΩ2ÔøΩ>WNÔøΩs83@O1\nÔøΩÔøΩsÔøΩÔøΩ\\ÔøΩ\ns;-ÔøΩ2ÔøΩÔøΩ#|AÔøΩÔøΩ\u0005ÔøΩÔøΩ25ÔøΩÔøΩ9WÔøΩqÔøΩNÁ†ì8_\"ÔøΩBJÔøΩbÔøΩQ\u0013\nR+9ÔøΩ|<?ÔøΩ2ÔøΩÔøΩÔøΩ…¢,ÔøΩAZDÔøΩLVr\bYÔøΩ!ÔøΩ#ÔøΩ;ÔøΩd\nÔøΩjfp!^\u0018ÔøΩÔøΩqÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩx_ÔøΩXÔøΩDÔøΩÔøΩ_+.ÔøΩ.ÔøΩÔøΩÔøΩNP,ÔøΩgÔøΩ4ÔøΩÔøΩ2ÔøΩÔøΩÔøΩgÔøΩ5ÔøΩ\n\u001bl}ÔøΩÔøΩlÔøΩÔøΩ\u001a*ÔøΩÔøΩfÔøΩ\\ÔøΩe:\u0019ÔøΩÔøΩ`:/ÔøΩ–àÔøΩ!ÔøΩ7OÔøΩ#p%sÔøΩÔøΩeÔøΩÔøΩM(ÔøΩÔøΩ\u0007ÔøΩÔøΩÔøΩQIÔøΩÔøΩe'hlÔøΩÔøΩÔøΩÔøΩ\u0007ÔøΩ;90ÔøΩ/ÔøΩ:\u0003ÔøΩÔøΩ061ÔøΩiÔøΩC:\u0013ÔøΩY\u0018\u001b\u0010ÔøΩÔøΩ\u000e\u0017ÔøΩÔøΩÔøΩÔøΩÔøΩ,\nﬂìÔøΩÔøΩ3c=TÔøΩ=ÔøΩVÔøΩÔøΩ\bÔøΩSÔøΩ71ÔøΩxnpq“≠ÔøΩÔøΩmIckÔøΩÔøΩÔøΩÔøΩ\nX\u0011ÔøΩÔøΩÔøΩÔøΩjÔøΩ5ÔøΩrzÔøΩ#Z\nÔøΩÔøΩ.ÔøΩ,ÔøΩ`ÔøΩ\u0014ÔøΩ\u0007ÔøΩ\u0015ÔøΩEt\u0001n\u0016ÔøΩTEÔøΩ\u0007uÔøΩÔøΩ=ÔøΩÔøΩB\u0011ÔøΩpÔøΩÔøΩÔøΩEÔøΩ%\nÔøΩxÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩy5sEÔøΩ6ÔøΩP'ÔøΩ\u0016ÔøΩŒªeÔøΩ<ÔøΩÔøΩÔøΩ|\u000fZÔøΩÔøΩ/ÔøΩV{ÔøΩG\nÔøΩONÔøΩKÔøΩHŒóÔøΩ Ñb9x/ÔøΩ\u0004\u001aÔøΩÔøΩRÔøΩÔøΩ\u0016ÔøΩ\u0007\nÔøΩÔøΩÔøΩcÔøΩ\u000fÔøΩ&ÔøΩhrÔøΩIÔøΩB\u0003ÔøΩPÔøΩB\u0013ÔøΩlÔøΩUÔøΩÔøΩÔøΩ9ÔøΩQÔøΩu\\/_ÔøΩ+ÔøΩÔøΩ\tÔøΩÔøΩÔøΩÔøΩkP/,\\ÔøΩÔøΩ=ÔøΩÔøΩÔøΩÔøΩÔøΩ3J\u0005cH^ÔøΩp|ÔøΩY) ÔøΩÔøΩÔøΩÔøΩ‘û\nÔøΩ?ÔøΩÔøΩ+ÔøΩÔøΩiÔøΩ\u0005ny>YÔøΩ÷πÔøΩgÔøΩ;ÔøΩlÕïÔøΩÔøΩÔøΩ6ÔøΩ~ÔøΩŸßsÔøΩÔøΩÔøΩÔøΩ/L|ÔøΩÔøΩÔøΩÔøΩBÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩ#ÔøΩhÔøΩb\u00149ÔøΩQÔøΩt}\u0017ÔøΩÔøΩÔøΩgImÔøΩÔøΩ\u0011ÔøΩ@ÔøΩÔøΩ_ÔøΩ\u000f]BÔøΩmÔøΩ~◊í\n\u0006ÔøΩﬂΩÔøΩ7ÔøΩ\"ÔøΩ¥≠≥ÔøΩ6\u001bÔøΩÔøΩsÔøΩ@;ÔøΩ\u0001.\u0017IÔøΩÔøΩ2-Pw9ÔøΩ9\u0019ÔøΩ<ÔøΩÔøΩ\u0010ÔøΩ‹πÔøΩZÔøΩ8-ÔøΩÔøΩ*ÔøΩ\u000e\u0018ESÔøΩ!:EWÔøΩÔøΩÔøΩn4\u0015\n\u0005gÔøΩk.?G5CÔøΩÔøΩhuÔøΩq]ÔøΩ?ÔøΩÔøΩT=ÔøΩz»Ω;ÔøΩ$ÔøΩÔøΩÔøΩU}ÔøΩÔøΩ]ÔøΩ>ÔøΩ\u000fÔøΩ√ÄqÔøΩ8ÔøΩs>ÔøΩzÔøΩ},ÔøΩ\u001bÔøΩÔøΩÔøΩÔøΩ\n_ÔøΩ“ªU8(\u0007o{ÔøΩrÔøΩÔøΩÔøΩÔøΩD%ÔøΩ5ÔøΩÔøΩÔøΩbÔøΩ<\u0014ÔøΩÔøΩFÔøΩÔøΩKÔøΩ”ì7\u0004oD\u0003xÔøΩÔøΩ)n\u0014oRÔøΩfÔøΩ\u0011T5ÔøΩyuÔøΩQtÔøΩXy,ÆãªCÔøΩ›±ÔøΩÔøΩ‘õf;ÔøΩÔøΩaÔøΩ[ÔøΩ!ÔøΩÔøΩÔøΩ3ÔøΩÔøΩoÔøΩR*ÔøΩSt8ÔøΩYÔøΩ⁄¢RÔøΩÔøΩ.WXÔøΩCIÔøΩÔøΩ\u0011,O|ÔøΩl2ÔøΩ7ÔøΩt95ÔøΩQÔøΩLÔøΩÔøΩXPÔøΩÔøΩÔøΩ<ÔøΩ~[ÔøΩPoQÔøΩ)ÔøΩ\u0011$ef\u000fÔøΩÔøΩ\u0005ÔøΩÔøΩ[TÔøΩk]\u0014xÔøΩH\u0005ÔøΩk0ÔøΩU\u0013ÔøΩKÔøΩ\u0005ÔøΩ=ÔøΩ- ÔøΩ1\u0015QÔøΩ‚óè\bA<\u0018ÔøΩÔøΩ\u0006ÔøΩEÔøΩ#\u0006s>@ÔøΩ8NÔøΩOÔøΩÚá≤≥kÔøΩÔøΩ\u0013ÔøΩÔøΩÔøΩkÔøΩÔøΩ\u0003ÔøΩ14ÔøΩ(\u0003\n\u0012\u001aÔøΩÔøΩV\u0019ÔøΩÔøΩjÔøΩÔøΩÔøΩÔøΩ“±ÔøΩÔøΩÔøΩRFÔøΩÔøΩE\u0012SÔøΩ8ÔøΩ\u0016mÔøΩf\nÔøΩÔøΩsÔøΩÔøΩÔøΩ\u001a\u001aÔøΩ>yÔøΩ\\TSÔøΩÔøΩ6\u0017yÔøΩ\\tÔøΩ>ÔøΩqÔøΩÔøΩ\u001fÔøΩÔøΩÔøΩEÔøΩYfwyvÔøΩƒ™G:ÔøΩ-fÔøΩÔøΩ=ÔøΩÔøΩÔøΩ7ÔøΩÔøΩyM0ÔøΩÔøΩyKÔøΩSz!Q:jÔøΩÔøΩÔøΩÔøΩÃΩÔøΩÔøΩ\u0018 ñÔøΩDÔøΩÔøΩ’´\nÔøΩHÔøΩ5ÔøΩfÔøΩÔøΩ7FQÔøΩ\u0011TÔøΩ\n\u0005kt\u0006ÔøΩÔøΩ\u0016ÔøΩÔøΩÔøΩVQÔøΩÔøΩ'ÔøΩK\u0010sÔøΩ⁄∂ÔøΩÔøΩ\u0016ÔøΩmJÔøΩR)LÔøΩÔøΩwÔøΩÔøΩ\u0016WT\u00156ÔøΩ]ÔøΩd\u0003ÔøΩ3\u0015‹≥LÔøΩTÔøΩ‘ó9◊∏VÔøΩ◊•6ÔøΩ’õÔøΩ◊πÔøΩqoJmWow>ÔøΩ\nT=ÔøΩ?ÔøΩ\u0002:ÔøΩÔøΩ@\u0019\u0002ÔøΩ'ÔøΩÔøΩÔøΩ*5ÔøΩ\tÔøΩ‹íÔøΩÔøΩÔøΩÔøΩ(/:]ÔøΩlÔøΩÔøΩ\u0002'ÔøΩÔøΩIÔøΩOÔøΩUpIÔøΩK≈™ÔøΩ\u0014ÔøΩNÔøΩÔøΩÔøΩC\u0013\n\u0013ÔøΩÔøΩvÔøΩÔøΩÔøΩ2ÔøΩÔøΩ◊êÔøΩÔøΩ]NÔøΩ9ÔøΩ{ÔøΩ\u001aÔøΩÔøΩ>MÔøΩ(}ÔøΩÔøΩ‘åzKÔøΩÔøΩCÔøΩ\\≈®ÔøΩÔøΩX/yÔøΩÔøΩ\u0018Dl\u0010wÔøΩD‹±<ÔøΩ3ÔøΩÔøΩ\u0014I9sÔøΩ'ÔøΩ|7:0ÔøΩ3ÔøΩ~ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ3@ÔøΩÔøΩ9nÔøΩ„ßí\u0013kÔøΩ&ÔøΩ\u0004ÔøΩ\u0013ÔøΩ0ÈöÇÔøΩÔøΩ_ÔøΩÔøΩr\u0002]ÔøΩZÔøΩ=NÔøΩ\"ÔøΩ?SV&ÔøΩÔøΩ<lI\u0011L=ÔøΩÔøΩeÔøΩ‚ß≠ÔøΩtÔøΩÔøΩÔøΩ\u001b9>ÔøΩÔøΩUÔøΩÔøΩCÔøΩ,ÔøΩYÔøΩÔøΩÔøΩyÔøΩÔøΩ@1ÔøΩÔøΩÔøΩ\nO/=g\b8\u0005{\u000eh\u001fÔøΩÔøΩZKÔøΩÔøΩÔøΩ\u0012qÔøΩJÔøΩ\u0003ÔøΩ;\u0002ÔøΩÔøΩWÔøΩv«åTUÔøΩfÔøΩ÷ΩÔøΩ\nÔøΩWÔøΩÔøΩÔøΩ\u0016ÔøΩÔøΩ\n!ÔøΩY@*9ÔøΩÔøΩÔøΩH*ÔøΩÔøΩ\\ÔøΩy8ÔøΩÔøΩÔøΩÔøΩ\nŸì\u0005~ÔøΩÔøΩOÔøΩÔøΩmÂ∑™\u0006ÔøΩÔøΩÔøΩ\u000e~ÔøΩJ\u0015ÔøΩÔøΩÔøΩÔøΩ`ÔøΩÔøΩN\u0006B|@ÔøΩÔøΩÔøΩÔøΩ<\u0017 ƒØts\nÔøΩMpHo0xshoÔøΩÔøΩÔøΩ?ÔøΩ\n%ÔøΩÔøΩfÔøΩRIbÔøΩÔøΩÔøΩÔøΩ?ÔøΩxÔøΩÔøΩÔøΩ\u0019ÔøΩW>ÔøΩÔøΩmÔøΩ\b\u0002ÔøΩÔøΩÔøΩ1ÔøΩÔøΩÔøΩgRI:\u0003\u0017ÔøΩw=#ÔøΩ%ÔøΩÔøΩn∆ΩÔøΩÔøΩÔøΩWÔøΩS`\nH\bF¬ªÔøΩ'ÔøΩÔøΩmÔøΩÔøΩ\u0006AÔøΩÔøΩÔøΩgÔøΩLœ©q@ÔøΩtÔøΩ&ÔøΩ\u0011ÔøΩ\nÔøΩTtKÔøΩ2ÔøΩÔøΩ*>J\u001faÔøΩS$ÔøΩ\nW2ÔøΩ/ÔøΩÔøΩ0ÔøΩ\u0000ÔøΩÔøΩ9ceÔøΩF@∆â2tÔøΩÔøΩ1\u0015/ÔøΩ+7ÔøΩ\nÔøΩgÔøΩÔøΩgpÔøΩ5ÔøΩÔøΩ2\u0012ÔøΩÔøΩM\u0017-*ÔøΩ#ÔøΩ\u001b>ÔøΩz]vÔøΩÔøΩÔøΩÔøΩ_~\"ÔøΩÔøΩsÔøΩ\u0017fYÔøΩ!fÔøΩÔøΩÔøΩ/UÔøΩÔøΩBÔøΩ◊îÔøΩ<ÔøΩxiÔøΩ“ò]ÔøΩLÔøΩRz&b\u0017ÔøΩULÔøΩMÔøΩxÔøΩÔøΩÔøΩ+:ÔøΩ\u0006ÔøΩ\u0016\u001fÔøΩƒøÔøΩD60ÔøΩJeSÔøΩ\u001aÔøΩÔøΩÔøΩÔøΩÔøΩ0nÔøΩnÀøÔøΩyÔøΩÔøΩÔøΩÔøΩÔøΩr*C5ÔøΩƒöÔøΩv@}ÔøΩÔøΩÔøΩZÔøΩÔøΩXmXZÔøΩV7+ÔøΩÔøΩfcCÔøΩ9^ÔøΩg\u001bÔøΩh:ÔøΩ\u000eÔøΩLﬂúÔøΩx[^j\\ÔøΩ\\\nÔøΩlÔøΩÔøΩ€™ÔøΩ*l5nÔøΩmÔøΩÔøΩÔøΩÔøΩ\u0016v\u001bÔøΩpÔøΩ\u0010ÔøΩÔøΩ\u0015\u0006ÔøΩ`4ÔøΩÔøΩÔøΩÔøΩÔøΩO%ÔøΩ\t{ÔøΩQ-4.R-ÔøΩÔøΩldÔøΩ\u000eÔøΩ\u0013ÔøΩs,ÔøΩ/ÔøΩ5ZmŒ°VqÔøΩ3ÔøΩ/ÔøΩsÔøΩÔøΩiÔøΩ-cÔøΩ∆ÇVÔøΩÔøΩiÔøΩ[ÔøΩÔøΩ>ÔøΩÔøΩD6ÔøΩÔøΩÔøΩHhG\bÔøΩÔøΩÔøΩÔøΩÔøΩb.ÔøΩ\u001fÔøΩD,ÔøΩ\t\u001fÔøΩÔøΩÔøΩ\n\u0005\u0017qr\\ÔøΩ\u0010ÔøΩ\u0014\n\u0011ÔøΩ-\u0016ÔøΩÊ¥ñ\\N\nÔøΩ…°ÔøΩÔøΩsÔøΩÔøΩS”êÔøΩÔøΩÔøΩ\u000e5ÔøΩÔøΩs\u0005\u0003ÔøΩ.~*ÔøΩÔøΩ6\nÔøΩ\u0001‘ÜÔøΩHÔøΩ@5ÔøΩÔøΩÔøΩÔøΩÔøΩUkGqÔøΩÔøΩ\n6lÔøΩ\u0006ÔøΩUHtb'ÔøΩ\u0011ZÔøΩ 9GÔøΩ'ÔøΩÔøΩÔøΩ,mÔøΩ\u0016ÔøΩÔøΩ\u0002ÔøΩ\u0003FÔøΩÔøΩ⁄ÉÔøΩÔøΩ\u0018ÔøΩÔøΩC(ÔøΩs/ÔøΩWÔøΩÔøΩl$ÔøΩ\u0002?ÔøΩ^Y\u000ffL\u0018OÔøΩ$ÔøΩÔøΩÔøΩ&ÔøΩÔøΩCœ§EÔøΩÔøΩbÔøΩ\nÔøΩÓ°ú%\u0007ÔøΩ'V\nÔøΩ\nÔøΩÔøΩ\u0013+ÔøΩÔøΩT‹íq|*ÔøΩÔøΩÔøΩ…ßh\u0002{OÔøΩ\u0007Z\u0004ÔøΩ*ÔøΩÔøΩ)ÔøΩ8^hÔøΩ7mÔøΩ\nM[ÔøΩ\u001fÔøΩÔøΩqÔøΩ8\u0007\u0019\u000fÔøΩtj`ÔøΩ<eÔøΩ\u001fUVÔøΩi\u0000ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ?ÔøΩS\u0015ÔøΩtÔøΩ\u0002ÔøΩ?ÔøΩÔøΩÔøΩ\u0000ÔøΩ|\u0004XÔøΩÿ¨ÔøΩÔøΩBÔøΩÔøΩÔøΩBÔøΩÔøΩŸÆoVP\u0017ÔøΩÔøΩ+ÔøΩÔøΩhÔøΩÔøΩZKÔøΩ\u0013q\u0003ÔøΩÔøΩÔøΩ#ÔøΩbD4PÔøΩÔøΩÔøΩA\u0003ÔøΩH\u0012ÔøΩZÔøΩÔøΩup@'ÔøΩÔøΩ)\u000fQÔøΩÔøΩFh3ÔøΩÔøΩÔøΩ@jÔøΩ\u000eÔøΩ*ÔøΩÔøΩbÔøΩ‹∫ÔøΩ\u0000ÔøΩoÔøΩÔøΩ.ÔøΩÔøΩ\u0001ÔøΩ\u0006ÔøΩQLIVÔøΩÔøΩÔøΩÔøΩAkÔøΩVÔøΩLÔøΩ\u000fÔøΩ\u0013*NKfkÔøΩÔøΩÔøΩ\u0016ÔøΩYK1\u0001ÔøΩÔøΩÔøΩ\u0015UÔøΩÔøΩl≈ÑdÔøΩÔøΩZÔøΩÔøΩ;<ÔøΩNÔøΩ\u000e;ÔøΩÔøΩÔøΩÔøΩÔøΩ5ÔøΩÔøΩÔøΩÔøΩ?GÔøΩÔøΩÔøΩ\u000eP#;9ÔøΩÔøΩ^FÔøΩ\u00136ÔøΩ3WÔøΩÔøΩF%ÔøΩ\nÔøΩÔøΩÔøΩ‡®ßÔøΩ¬çÔøΩ'\u0002!ÔøΩmzÔøΩÔøΩ`\u0014ÔøΩ’Ñk\u0016m9ÔøΩpvÔøΩ‘ôvÔøΩÔøΩ€ø=#ÔøΩ.ÔøΩ&ÔøΩ.=ÔøΩÔøΩÔøΩÔøΩBÔøΩÔøΩ\nÔøΩBp›∫U.ÔøΩ7\u0012a\nÔøΩÔøΩ'JÔøΩÔøΩkÔøΩpÿ¢ÔøΩÔøΩ{ÔøΩ\u001f_ftÔøΩH8ÔøΩÔøΩxÔøΩ+ÔøΩ]_OÔøΩs\u0007hÔøΩÔøΩ\u0015wÔøΩ?ÔøΩÔøΩÔøΩÔøΩ)ÔøΩÔøΩ23ÔøΩ\u0019ÔøΩÔøΩÔøΩ€ò€¥ÔøΩÔøΩXÔøΩIo\n+ÔøΩÔøΩÔøΩÔøΩnÔøΩ.aÔøΩq$ÔøΩ\u0012ÔøΩ\u0002GÔøΩW-O\u0012\u000fÔøΩÔøΩÔøΩÔøΩ\u0013ÔøΩÔøΩÔøΩÔøΩQÔøΩÔøΩÔøΩ!ÔøΩnÔøΩÔøΩ>ÔøΩ3ÔøΩ#vÔøΩÔøΩ\u0014ÔøΩÔøΩÔøΩJÔøΩÔøΩJÔøΩS\nn\nÔøΩÔøΩEÔøΩvÔøΩHÔøΩÔøΩÔøΩ\u000fÔøΩÔøΩ(\u0016$]<ÔøΩM\u0006ÔøΩpÔøΩÔøΩÔøΩÔøΩt!»°ÔøΩﬁºÔøΩ76VÔøΩÔøΩÔøΩJ\nÔøΩÔøΩsÔøΩÔøΩ\tÔøΩÔøΩÔøΩ.\u001fYÔøΩÔøΩÔøΩ\u001f”üÔøΩ+ÔøΩÔøΩÔøΩÔøΩÔøΩ&ÔøΩ\nVV|ÔøΩÔøΩÔøΩ>#ÔøΩS\u0013d\u001fÔøΩÔøΩÔøΩÔøΩÔøΩ75ÔøΩ\u000f45ÔøΩ\u001bAJ&G1#1ÔøΩ-\u001aÔøΩF#ÔøΩÔøΩ\u0007ÔøΩ,a\u000fÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ(ÔøΩÔøΩ\u000e\u0015r\u0014ÔøΩWÔøΩ-ÔøΩ\u0001#yŒ∏ÔøΩjÔøΩYÔøΩÔøΩ»¥\u0005ÔøΩLÔøΩ/p\n<ÔøΩuÔøΩÔøΩÔøΩÔøΩ∆ºœûÔøΩÔøΩÔøΩGÔøΩ%œßÃºÔøΩÔøΩ\u0007ÔøΩÔøΩp8>c%Y:ÔøΩiÔøΩ\u000fÔøΩÔøΩÔøΩnÔøΩ\u0015\u0002WÔøΩÔøΩ5kﬂπ`6ÔøΩeai.ÔøΩQq72ÔøΩF)tÔøΩÔøΩ\t#ÔøΩ]ÔøΩÔøΩHÔøΩ\u000fÔøΩÔøΩÔøΩAlÔøΩ\bÔøΩUÔøΩÔøΩÔøΩN%V\u000eZ\u0016^\"\u0007=ÔøΩ&\u0006ÔøΩÔøΩ\bÔøΩlzÔøΩÔøΩVÔøΩ ÔøΩzÔøΩÔøΩ`\u0017ÔøΩ>Bn|ÔøΩÔøΩÔøΩGpÔøΩÔøΩ7ÔøΩ0ÔøΩÔøΩÔøΩÔøΩÔøΩ!2oÕ∑ÔøΩÔøΩ€∏\u000eÔøΩÔøΩÔøΩÔøΩÔøΩiÔøΩÔøΩÔøΩÔøΩÔøΩ?ÔøΩÔøΩCÔøΩ\u000eÔøΩÔøΩIÔøΩ\u0001ÔøΩÔøΩÔøΩÔøΩq\u0014eÔøΩ\u0000UÔøΩ|VÔøΩÔøΩ<a9ÔøΩ\u0016ÔøΩ<ÔøΩÔøΩÔøΩQ9OÔøΩÔøΩÔøΩFBÔøΩXmÔøΩ6ÔøΩ\u001ajÔøΩÔøΩqÔøΩX1F:ÔøΩÔøΩhn0<\u0014{9ÔøΩÔøΩÔøΩÔøΩI_Y\nÔøΩÔøΩÔøΩÔøΩÔøΩ`ÔøΩÔøΩ\u0016ÔøΩÔøΩw;\u0002!ÔøΩÔøΩA\b&ÔøΩÔøΩNUÔøΩÔøΩÔøΩMÔøΩÔøΩIÔøΩÔøΩÔøΩiÔøΩK\u0000ÔøΩ(ÔøΩDÔøΩÔøΩ{ÔøΩ[ÔøΩ\\ÔøΩLÔøΩÔøΩÔøΩ13ÔøΩ,€íeÔøΩeKÔøΩd[ÔøΩÔøΩ%ÔøΩÔøΩFÔøΩÔøΩ%ÔøΩ<3ÔøΩ\u0010GÔøΩ\u0010\n\u0018BÔøΩ\u0000ÔøΩ\n,ÔøΩ\u0011ÔøΩM,K`dCÔøΩC '\u001bÔøΩfÔøΩ\u0000ÔøΩ\nNÔøΩ8ÔøΩ]ÔøΩuH\u0016#ÔøΩÔøΩÔøΩÔøΩÔøΩ#ÔøΩÔøΩ]ÔøΩUÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ’´WÔøΩUoÔøΩBU\u0019ÔøΩÔøΩ(\u0019ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ/ÔøΩÔøΩ#ÔøΩÔøΩÔøΩ»©\b\u0017qÔøΩÔøΩ\u0019\u0019ÔøΩ3q<\u0013\u0014ÔøΩÔøΩyÔøΩ\u0002ÔøΩvÔøΩUÔøΩÔøΩ9\u0005\u0019[XHÔøΩ\bQ*ÔøΩ\u0003ÔøΩ0ÔøΩuÔøΩ\nYÔøΩÔøΩÔøΩÔøΩÔøΩ\u0002jSÔøΩÔøΩÔøΩbÔøΩ\u0006ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩhÔøΩiÔøΩÔøΩXÔøΩv\u0019ÔøΩÔøΩ(ÔøΩÔøΩÔøΩXÔøΩbPX@?l#rŒÄÔøΩÔøΩÔøΩÔøΩÔøΩTÔøΩÔøΩÔøΩÔøΩsoÔøΩSÔøΩ\bÔøΩ\u0002œæÔøΩOÔøΩ6\u0014f_sÔøΩÔøΩ_~\u0010ÔøΩk◊ï2kÔøΩoÔøΩXÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ/ÔøΩrÔøΩÔøΩd~ÔøΩwÔøΩsÔøΩÔøΩ{oÔøΩÔøΩGnmÔøΩ\u0013ÔøΩqÔøΩzÔøΩÔøΩÃìÔøΩ:RÔøΩfbMÔøΩÔøΩw\u0018ÔøΩnÔøΩÔøΩÔøΩÔøΩ\u0010pÔøΩfÔøΩ!ÔøΩ'ÔøΩ\u0015(WÔøΩX`\n\u0013ÔøΩH&ÔøΩXÔøΩ\u0006ÔøΩ\u0010/¬∞ÔøΩÔøΩO]ÔøΩ\"x{f\u0019ÔøΩ3ÔøΩÔøΩHQÔøΩ\u00004ÔøΩÔøΩÔøΩ\u0010\u0013z(ÔøΩÔøΩÔøΩÔøΩ|”Åu.kÔøΩÕ≤ÔøΩÔøΩÔøΩÔøΩ{ÔøΩÔøΩ~:ÔøΩÔøΩiÔøΩÔøΩÔøΩ\u0010ÔøΩ|ÔøΩcw=CÔøΩCwÔøΩÔøΩ\u0005ÔøΩ$ÔøΩÔøΩÔøΩ#\u0019ÔøΩpTnUYÔøΩÔøΩ7ÔøΩ&ÔøΩÔøΩ\u000f5ÔøΩÔøΩUÔøΩÔøΩ3ÔøΩ3ÔøΩÔøΩxÔøΩ\u0012ÔøΩ\u0012ÔøΩÔøΩÔøΩ2ÔøΩÔøΩ}ÔøΩsÔøΩ+\u0014\nÔøΩ/»≥ZÔøΩvÔøΩ1ƒ©}yÔøΩ\"_ÔøΩoÔøΩÔøΩkÔøΩYÔøΩÔøΩSjÔøΩ\n=\nÔøΩÔøΩh-^ÔøΩXÔøΩ\\ÔøΩYÔøΩ_\u0017X\u0017ÔøΩS1b\n1ÔøΩÔøΩ3pgp$ÔøΩÔøΩ~ÔøΩÔøΩ}ÔøΩxÔøΩ{,ÔøΩjÔøΩÔøΩÔøΩÔøΩ\u0007ÔøΩÔøΩÔøΩ\u001b8\u0019r\"\u0019\u0007ÔøΩÔøΩÔøΩ*ÔøΩ¬Ø\n»ÉeÔøΩÔøΩÔøΩÔøΩ\u0006ÔøΩ\u0006ÔøΩfÀÜÔøΩ^ÔøΩ>„ùñÔøΩ÷ΩÔøΩ;ÔøΩwÔøΩFBÔøΩ;TÔøΩÔøΩÔøΩÔøΩ:U\u0013\n2\u000eÔøΩ8`@ÔøΩÔøΩ^ÔøΩ\u001a+`;a4ÔøΩ\nFÔøΩÔøΩrÔøΩ(XÔøΩ@\u0006ÔøΩÔøΩapZ\n\u000e'lC'ÔøΩ\u0001??yÔøΩ\u0016AÔøΩx=ÔøΩRÔøΩT)<ÔøΩ@V0\u0018\u0000nÔøΩÔøΩ#JUÔøΩRÔøΩÔøΩd[ÔøΩÔøΩ\nÔøΩ7KÔøΩÔøΩÔøΩ=ÔøΩÔøΩ≈ö\u0005:OÔøΩ\u0007ÔøΩo&ÔøΩ\u000eÔøΩ(ÔøΩz\u0019\u0004ÔøΩN\u0007ÔøΩh¬â\n&ÔøΩdDzÔøΩÔøΩÔøΩ6\u0018ÔøΩFÔøΩÔøΩÔøΩ\u0011C\"1*\u0002\u0014PN,/ÔøΩ\n»ãÔøΩÔøΩ\u0019ÔøΩ\u0010\u0010ÔøΩÔøΩ Q4ÔøΩgÔøΩÔøΩ\u001aOÔøΩ#/ÔøΩ@ÔøΩÔøΩÔøΩlZÔøΩ\u0005{ÔøΩÔøΩÔøΩGÔøΩÔøΩÔøΩ÷∑ÔøΩÔøΩ[YÎΩû0lÔøΩÔøΩ/ÔøΩ\u0006/6z1}AÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ»áÔøΩaOÔøΩ\u0015ÔøΩÔøΩf\u001f\u0016|#>ÔøΩ7ÔøΩ?zAuÔøΩ?ÔøΩ<\u000e\u001bc%ÔøΩ\nj\n\u0005ÔøΩHÔøΩ\\ÔøΩ!ÔøΩÔøΩ^ÔøΩÔøΩÔøΩQ\u0005ÔøΩÔøΩÔøΩ\u0010ÔøΩ#ALŒïÔøΩA!8\u0016<\u0011<\u0019T\u0004[ÔøΩgÔøΩ4ÔøΩ%W?ÔøΩÔøΩÔøΩSÔøΩœü›æKÔøΩ\nCÔøΩ\n\" ÔøΩrÔøΩvÔøΩxÔøΩÔøΩÔøΩÔøΩ+\u001b=ÔøΩ\\I^ÔøΩÔøΩÔøΩ\u001bÔøΩÔøΩYÔøΩÔøΩ(1\u001a4ÔøΩQRÔøΩ7ÔøΩ π1ÔøΩÔøΩ—†ÔøΩ.ÔøΩÔøΩ\\\u0011\u000fÔøΩ\u0014ÔøΩ\u0006ÔøΩÔøΩ}ÔøΩ,m\u00159|IÔøΩNf\u0012\n$ÔøΩÔøΩ<'ÔøΩ\"ÔøΩÔøΩqsÔøΩWÔøΩ8ÔøΩ”ëÔøΩYÔøΩÔøΩÔøΩ\u0001rqCFw[ÔøΩ\u0012ÔøΩﬁ†IÔøΩÔøΩÕä;o\n)sÔøΩ^cÔøΩ≈üSÔøΩ_XÔøΩÔøΩRÔøΩÔøΩvÔøΩÔøΩ\nÔøΩÔøΩ\u0004ÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ-ÔøΩÔøΩEÔøΩsÔøΩ:ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ“¨gaÔøΩ7\u001bÔøΩYÔøΩS\u001fÔøΩ\nÔøΩL\u0007ÔøΩÔøΩÔøΩ?ÔøΩÔøΩY=ÔøΩ\u0012À¨.’âÔøΩÔøΩ8-»çÔøΩÔøΩb·¢°ÔøΩYÔøΩd»ç\u0018ÔøΩﬂßÔøΩ'ÔøΩ~ÔøΩ~ÔøΩÔøΩÔøΩGLÔøΩf<ÔøΩ<P>ÔøΩVÔøΩ[ÔøΩmÔøΩÔøΩfSÔøΩÔøΩÔøΩ\u00187≈ùÔøΩ2ÔøΩ\u000f\ngÔøΩÃàÔøΩÔøΩÔøΩÔøΩÔøΩ5ÔøΩaÔøΩ\u0018ŒöÔøΩ3CY\u0005J`ÔøΩs\u0011_UÔøΩÔøΩ\u0000ÔøΩ\u0006ÔøΩ2ÔøΩ\u0014\u0018y/ÔøΩ\nÔøΩ/¬ãÔøΩÔøΩlÔøΩ&ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ-xÔøΩÔøΩ\u0003„üåÔøΩ’¶UÔøΩÔøΩTÔøΩÔøΩÔøΩÔøΩÔøΩU9FgÔøΩÔøΩYÔøΩ,3ÔøΩ5&CÔøΩŒ¶ÔøΩ38ÔøΩNy#ÔøΩÔøΩkÔøΩ5\u00197ÔøΩ6f ≠ÔøΩÔøΩ<ÔøΩÔøΩÔøΩÔøΩ\u0006Ãò223ÔøΩVÔøΩÔøΩa\n\u0005\n~ÔøΩÔøΩQ9ÔøΩÔøΩÔøΩﬂΩÔøΩ\u001fÔøΩ.s,ÔøΩAaÔøΩÔøΩ4\u001a=ÔøΩ3ÔøΩÔøΩ\nÔÑâ\u0014ÔøΩL\u0016ÔøΩ\nÔøΩ\u0011ÔøΩ\u0014\nÔøΩÔøΩÔøΩ3\u0006ÔøΩÔøΩhQÔøΩ>d&\u0007uÔøΩZÔøΩoj4j9ÔøΩ`VÔøΩEÔøΩÔøΩhGÔøΩÔøΩ9->ÔøΩ=ÔøΩeÔøΩÔøΩ'»ùDÔøΩÔøΩÔøΩ\u0005[lÔøΩr\\ÔøΩÔøΩ'\nF!chÔøΩ\n\bÔøΩ5ÔøΩÔøΩHh?ÔøΩ“ñEÔøΩÔøΩÔøΩ\t\u0017ÔøΩÔøΩ\u0003[ÔøΩ]\tÔøΩ4\u0001€Æ7&ÔøΩeÔøΩÔøΩÔøΩ]ÔøΩ\nÔøΩ0ÔøΩ—≠Dg'/iÔøΩÔøΩÔøΩTR&ÔøΩÔøΩÔøΩÔøΩ\u0000DÔøΩ\u0004HÔøΩ/ÔøΩ/UÔøΩÔøΩ7ÔøΩÔøΩÔøΩBAÔøΩÔøΩÔøΩ.r<?!\u0019ÔøΩBiÔøΩÔøΩ\bJz\u0016hÔøΩÔøΩ@F9\u0006ÔøΩ'ÔøΩÔøΩÔøΩfÔøΩPÔøΩ\u0003DÔøΩ>\u0001ÔøΩÔøΩA\nTbÔøΩÔøΩÔøΩÔøΩ\"IÔøΩ%ÔøΩÔøΩMÔøΩÔøΩÔøΩƒπTÔøΩÔøΩÔøΩUÔøΩvÔøΩÔøΩ\u001aÔøΩUn\u0015’üYÔøΩUÔøΩ|ÔøΩK\u001bzÔøΩ?ÔøΩÔøΩ5?ÔøΩ.ÔøΩ^ÔøΩ\u0007ÔøΩg\nÔøΩÔøΩ\u000eÔøΩpgk\nzﬁñ]`ÔøΩFŸó>ÔøΩÔøΩiMÔøΩVÔøΩÔøΩÔøΩÔøΩ<\u0013ZÔøΩÔøΩÔøΩÔøΩorÔøΩÔøΩjÔøΩ\u0007ggÔøΩKqÔøΩÔøΩ…¶E\u0016ÔøΩÔøΩ4fÔøΩ5ÔøΩ\u0003+ÔøΩ&7ÔøΩwÔøΩ\u0004!ÔøΩ\u0000\\ÔøΩÔøΩ\n}iÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ5S]rÔøΩ|\u0012\n\u0015ÔøΩÔøΩcÔøΩ}AÔøΩ\u0019\nÔøΩ\u0006sÔøΩÔøΩﬂºÔøΩ<\nÔøΩ\nm–°XÔøΩÔøΩÔøΩ\u000eÔøΩﬂùÔøΩ\nÔøΩÔøΩÔøΩBÔøΩFÿ£ÔøΩJ≈àÔøΩ1ÔøΩ\u0016ÔøΩﬂØÔøΩ\nZ‹¢ÔøΩÔøΩÔøΩ◊éjÔøΩieÔøΩÔøΩ9ÔøΩI+%\n5ÔøΩ\u0007HaÔøΩEÔøΩKÔøΩÔøΩÔøΩÔøΩ\u0019MÔøΩÔøΩ&ÔøΩÔøΩÔøΩÔøΩUU!ÔøΩÔøΩiÔøΩ\u0005`ÔøΩ+ÔøΩ“ÖÔøΩÕãÔøΩÔøΩÔøΩPVxt%ÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩ8ÔøΩ^C\u001aT(ÔøΩ\"AÔøΩa\u0005ÔøΩPQÔøΩ\u0002ÔøΩÔøΩYÔøΩ\u000fÔøΩ\u000eÔøΩ\u0018ÔøΩ\nÔøΩ\u001bIcÔøΩÔøΩVDÔøΩ}ÔøΩSÔøΩ\u0018ÔøΩÔøΩÔøΩPÔøΩÔøΩÔøΩ5ÔøΩÔøΩ\u0002)ÔøΩ\u0011ÔøΩ0[\u00193bQÔøΩÔøΩ3ÔøΩÔøΩÔøΩÔøΩ\u0005ÔøΩeÔøΩ\u001ffÔøΩ\u0018ÔøΩ\"∆à\u001bp\nﬁèGÔøΩI,ÔøΩÔøΩfÔøΩÔøΩhÔøΩ€∏ÔøΩÔøΩ2ÔøΩÔøΩÔøΩ\u0017arÔøΩÔøΩÔøΩ3\u0002ÔøΩÔøΩjddS\u0017\u0018ÔøΩÔøΩDÔøΩ÷ãÔøΩgÔøΩ»∫ÔøΩ\u0005\u0005ÔøΩYÔøΩ`@5ÔøΩ\nÔøΩﬂ™zÔøΩ{ÔøΩ+ÔøΩÔøΩÔøΩ\u0016ÔøΩFÔøΩ∆ú=fÔøΩR\u0019ÔøΩÔøΩXÔøΩ5`ÔøΩÔøΩ|ÔøΩ\u0012+'ÔøΩÔøΩ\u000fÔøΩÔøΩ\u0001*ÔøΩg¬ÇdÔøΩr“ÑAÔøΩÔøΩq,ÔøΩ{4ÔøΩﬂ¥ÔøΩ\u0002ÔøΩÔøΩ\u0011\nZ\nF\nN\u0015p\u0005VÔøΩn\n$ÔøΩgÔøΩ\"ÔøΩBÔøΩÔøΩÔøΩÔøΩLEÔøΩ58{EÔøΩÔøΩÔøΩNKwÔøΩŒãÔøΩÔøΩYz4;ÔøΩ~\u0019G7\tEÔøΩ^UF^ÔøΩ#ÔøΩÔøΩÔøΩÔøΩ:ÔøΩWÔøΩnÔøΩNÔøΩÔøΩ\u0015ÔøΩÔøΩ\u0000y‘æVÔøΩÔøΩÔøΩÔøΩÔøΩ|-xhf#X@ÔøΩÔøΩÔøΩ\u000e\"[ÔøΩ*f/$ÔøΩ}ÔøΩÔøΩ\u0019ÔøΩ\u0005ÔøΩXÔøΩ<ÔøΩÔøΩÔøΩ+g\u001fÔøΩÔøΩkOÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩovÔøΩÔøΩÔøΩÔøΩÔøΩo›ã\nÔøΩ9ÔøΩfÔøΩÔøΩ\u0015ÔøΩÔøΩÔøΩÔøΩÔøΩ\u0016ÔøΩ\u0016YÔøΩWWÔøΩÔøΩÔøΩÔøΩm;5ÔøΩÔøΩ/~ÔøΩÔøΩ\tÔøΩw\u0014ÔøΩÔøΩÔøΩ:UsGcÀØÔøΩÔøΩÔøΩzÔøΩÔøΩ<\u0019E\u0017HzÔøΩÔøΩ\tÔøΩ\u0018\nÔøΩ'ÔøΩCXÔøΩm_ÔøΩÔøΩTÔøΩ0ÔøΩPÔøΩÔøΩÔøΩÔøΩ\u0016ÔøΩÔøΩÔøΩÔøΩÔøΩ\nwF}ÔøΩÔøΩÔøΩVÔøΩÔøΩÔøΩÔøΩ\u0015ÔøΩÔøΩ#ÔøΩ\u0011ÔøΩ>ÔøΩ>%h%ÔøΩ\u0002FÔøΩÔøΩj'ÔøΩOÔøΩ)ÔøΩ<bÔøΩ&_ÔøΩb\u0018\u0012\u0013ÔøΩÔøΩÔøΩzÔøΩ=ÔøΩÔøΩv8ÔøΩ>ÔøΩÔøΩÔøΩ(ÔøΩVj9\u0019ÔøΩÔøΩnÔøΩNg.Fn\n\u0018ÔøΩÔøΩÔøΩÔøΩDÔøΩÔøΩa/ÔøΩcVÔøΩ\n\u0003ÔøΩ\u0010Q\u0011\"ÔøΩ ÔøΩ\u0007ÔøΩÔøΩ~ÔøΩ‹©ÔøΩÔøΩ\nÔøΩÔøΩ\u0002+ÔøΩÔøΩL5ÔøΩÔøΩ;ÔøΩ\n\u000fÕ± A\u000fÔøΩÔøΩ\t5uZ|kÔøΩ\u001fÔøΩgﬁºÔøΩ\u0007ÔøΩÔøΩ[\u0012ÔøΩÔøΩÔøΩwoSÔøΩP<\u0016\t\u001b\u0001lÔøΩÔøΩ!ÔøΩ\u00113ÔøΩ_7ÔøΩÔøΩIÔøΩÔøΩÔøΩTÔøΩÔøΩzÔøΩÔøΩÔøΩÔøΩÔøΩ:5_\u0014ÔøΩ7u\\ÔøΩ\u0019\u0005”óÔøΩÔøΩÔøΩmÔøΩÔøΩÔøΩ1S\u001fÔøΩ≈ßÔøΩÔøΩÔøΩxÔøΩ7œ±ÔøΩ%YÔøΩÔøΩÔøΩYsSÔøΩÔøΩ¬öÔøΩÔøΩÔøΩ\u0019d;ÔøΩ\u0007]\u0010ÔøΩÔøΩ\nÔøΩp^ÔøΩ\n;\u001f0<ÔøΩxÔøΩÔøΩdÔøΩQÔøΩK\u0019\u001aÔøΩ\u0003ÁòÅÔøΩo æ!ÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩ\u000fÔøΩ\u000eÔøΩ/ÔøΩ*-ÔøΩÁòºUl\u0013+\n+ÔøΩ&\u000fÔøΩ\u0012ÔøΩ\"ÔøΩ\u0012D,ÔøΩ\nÔøΩ\u001fÔøΩ\u0005rY<…ºÔøΩÔøΩÔøΩp\nv€ìlÔøΩÔøΩÔøΩt\u0007tÔøΩnÔøΩ\nAbÔøΩ\u000e6\u0003ÔøΩÔøΩxÔøΩ\t;MU&ÔøΩd\u0013`\u001aÔøΩ*y\n6XÔøΩ\u0016∆≤ÔøΩ;ÔøΩ\nLÔøΩOÔøΩUÔøΩÔøΩ@VLÔøΩ:ÔøΩÔøΩÔøΩ\u0007UgÔøΩ∆ÇÔøΩÔøΩ\u0019rÔøΩ/ÔøΩ.ÔøΩ*ÔøΩ6ÔøΩ∆óÔøΩÔøΩUÔøΩHÔøΩ\nÔøΩÔøΩ*+ÔøΩjÔøΩÔøΩÔøΩÔøΩs~ÔøΩÔøΩ9SÔøΩg¬§ÔøΩ\u0001ÔøΩÔøΩhÔøΩ'ÔøΩ#ÔøΩÔøΩÔøΩuÔøΩ{ÔøΩÔøΩ\\ÔøΩÔøΩ\u0013wÔøΩwÔøΩÔøΩŸánÔøΩ·∞≥cÔøΩÔøΩÔøΩ”áÔøΩÔøΩu\u0014W}ÔøΩ}\u0005\u0019ÔøΩ,ÔøΩFÔøΩsÔøΩÔøΩÔøΩÔøΩ{ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩz6ÔøΩÔøΩg?=ÔøΩŸèÔøΩ∆óWÔøΩdÔøΩ# ÔøΩn\u0006\u0019s\u0017ÔøΩ\u0018+Ë≤•ÔøΩFÔøΩx\u0013lÔøΩJÔøΩÔøΩ\u0005ÔøΩxÔøΩÔøΩ#ÔøΩ\u0011-ÔøΩÔøΩuÔøΩwÔøΩwWÔøΩsÔøΩÔøΩm_ÔøΩNh_ÔøΩÔøΩÔøΩ{ÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩu9\nÔøΩÔøΩr\ncSÔøΩstfÔøΩWÔøΩÔøΩÔøΩÔøΩ/ÔøΩ/ÔøΩÔøΩÔøΩCÔøΩ≈®\u0002◊°:ÔøΩ:–åÔøΩÔøΩ_WÔøΩ\u0003ÔøΩÔøΩÔøΩLÔøΩoÔøΩÔøΩÔøΩÔøΩ&|ÔøΩÔøΩÔøΩÔøΩRÔøΩ5}D1ÔøΩÔøΩÔøΩtk∆æÔøΩ}9\u000fq\u000f(ÔøΩ7=ÔøΩÔøΩhÔøΩ3ÔøΩÔøΩÔøΩœóNrGÔøΩg4–û—üÔøΩ)\t*t*\u0005*«ãJd5JÔøΩÔøΩÔøΩ9ÔøΩ\u0019ÔøΩÔøΩÔøΩÔøΩ\\VLÔøΩL]^ÔøΩ\n\u001bÔøΩ\nÔøΩ\nq\u0011ÔøΩÔøΩÔøΩC0ÔøΩ2ÔøΩÔøΩ!ÔøΩÔøΩGÔøΩNÔøΩqeÔøΩW ÔøΩ\u0005ÔøΩW\u0000rO\n1\nÔøΩÔøΩÔøΩfÔøΩÔøΩÔøΩÔøΩ?ÔøΩ\nÔøΩJÔøΩyzÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩLz\nKÔøΩs%ÔøΩaGÔøΩ)ÔøΩSf{]2w+r*ÔøΩZqQVA+\neDZq>«∑b\u0007ÔøΩÔøΩYÔøΩS‹ä¬¶bQÔøΩÔøΩX\u0017\u0014«ú\n{ÔøΩÔøΩ,=ÔøΩÔøΩ3sÔøΩ\bÔøΩ\n$qGÔøΩ_ÔøΩtÔøΩMÔøΩ\u0000ÔøΩÔøΩÔøΩÔøΩo>ÔøΩÔøΩÔøΩ{\u000eÔøΩÔøΩÔøΩ}ÔøΩ»∑{6\u000fÔøΩy7\bÔøΩ\n#eÔøΩ\u001646ÔøΩÔøΩÔøΩsÔøΩo%sÔøΩÔøΩÕ∑ÔøΩ:ÔøΩXÔøΩÔøΩÔøΩ;ÔøΩwÔøΩÔøΩÔøΩÔøΩÔøΩhr€ëÔøΩÔøΩÔøΩRÔøΩ=4X?ÔøΩ+<ÔøΩÔøΩ\u0015OÔøΩ|ÔøΩÔøΩ\u001bÔøΩÔøΩ*ÔøΩ\u0001ÔøΩ0\bÔøΩÔøΩ%ÔøΩÔøΩ<ÔøΩ]ÔøΩ\u001bÔøΩpÔøΩÔøΩa!\nÔøΩÔøΩ\n1wÔøΩ\u000fÔøΩÔøΩÔøΩÔøΩx-ŒüÔøΩ_\u0012JÔøΩÔøΩ\n61ÔøΩ9\u0018ÔøΩÔøΩŒ∫ÔøΩs\u0016ÔøΩMKÔøΩÔøΩÔøΩ\u0016ÔøΩÔøΩwÔøΩÔøΩ\u001a\u0005ÔøΩ~ÔøΩk22ÔøΩnÔøΩUÔøΩCÔøΩÔøΩÔøΩd^\u001749ÔøΩ|ÔøΩJÔøΩÔøΩ+ÔøΩ]\u0012\\ ÔøΩÔøΩ”ç\nyÔøΩpz{ÔøΩxÔøΩ\u0007ÔøΩ\u0015ÔøΩ\u0003ÔøΩÔøΩÀéÔøΩdÔøΩ$SÔøΩÔøΩÔøΩ\u0002ÔøΩ\u0001WÔøΩÔøΩ\u0007ÔøΩÔøΩ<LGÔøΩ\u0016ﬁ®-ÔøΩBÔøΩcÔøΩÔøΩÔøΩCÔøΩÔøΩÔøΩÔøΩE|ÔøΩÔøΩI\nÔøΩxqÔøΩ0ÔøΩŸ°FgvVh\u0000ÔøΩÔøΩm6ÔøΩ?\u0006ÔøΩcCÔøΩBÔøΩÔøΩ\nÔøΩÔøΩÃ®ÔøΩBGuB\u0016ÔøΩk6ÔøΩd\u0015\u0006ÔøΩÔøΩ<B4\"\\0aÔøΩÔøΩs_ÔøΩ\u0005ÔøΩZÔøΩ\n^ÔøΩÔøΩÔøΩJÔøΩ\u0012ÔøΩ~ÔøΩÔøΩ\u000e\nÔøΩÔøΩ\n\u0016,ÔøΩÔøΩ4ÔøΩÔøΩÔøΩÔøΩ\u001fÔøΩ^ÔøΩFÔøΩÔøΩdÔøΩLÔøΩÔøΩŒÇÔøΩÔøΩ;ÔøΩ\u0016ÔøΩÔøΩÔøΩ2_ÔøΩÔøΩ`PÔøΩ*ÔøΩKr}ÔøΩ6\u000eFÔøΩ\u0005ÔøΩLÔøΩdÔøΩÔøΩ7K\u000fÔøΩ C8ÔøΩÔøΩÔøΩyÔøΩÔøΩLÔøΩ\\oÔøΩÔøΩÔøΩ\u0010ÔøΩÔøΩ7ÔøΩÔøΩ}ÔøΩÔøΩÔøΩÔøΩ{ÔøΩrÔøΩÔøΩ*gP$[ÔøΩ\nÔøΩ#\n\u0011ÔøΩL\u0006[ÔøΩ@6√π`\n=ÔøΩÔøΩÔøΩ\u0013ÔøΩÔøΩ\"ÔøΩ\u0015XÔøΩ\\ÔøΩÔøΩ*ÔøΩ9ÔøΩ\u0006\u001a\"IyRÔøΩ\n$\nÔøΩ##xD~ÔøΩÔøΩÔøΩHÔøΩHÔøΩ@ÔøΩÔøΩÔøΩIf4ÔøΩzÔøΩOÔøΩNEÔøΩ=ÔøΩ;\u0014w\u0004XÔøΩÔøΩ€±hÔøΩÔøΩÔøΩÔøΩÔøΩN\u0014\bŸëhÔøΩÔøΩa…≥;<>ÔøΩÔøΩ\nÔøΩÔøΩ\u0005KÔøΩBÔøΩ$oÔøΩÔøΩÔøΩ\u0000<\u0005,>s8ÔøΩÔøΩ(\u0003\nÔøΩÔøΩ\"s\u001a1BNÔøΩÔøΩÔøΩ\t3ÔøΩÔøΩ\nJÔøΩ!ÔøΩ~IÔøΩ–ïÔøΩÎíÇÔøΩÔøΩﬁùÔøΩÔøΩWÔøΩ'ÔøΩÔøΩ&ÔøΩÔøΩ0.ÔøΩ\u0018\\hÔøΩÔøΩÔøΩY$E1}b\u0003KG6ÔøΩÔøΩÔøΩ\n.'ÔøΩ\u0000ÔøΩÃßÔøΩÔøΩ9ÔøΩÔøΩÔøΩÔøΩ/ƒªv!zÔøΩ\u0002\u0017RÔøΩ_vÔøΩÔøΩÔøΩUÔøΩŒØÔøΩBƒ≥ÔøΩ\u0007X\u0015b\u0000mÔøΩÔøΩ\u0011myZg!ÔøΩ=ÔøΩ~\b;ÔøΩÔøΩNÔøΩÔøΩÔøΩ¬ó\u001bÔøΩÔøΩÔøΩŸìÔøΩ]ÔøΩÔøΩ}CÔøΩÔøΩÔøΩ\\ﬂÜÔøΩÔøΩ\u0013ÔøΩÔøΩÔøΩÔøΩ\u0017ÔøΩÔøΩÔøΩdÔøΩÔøΩ\u001aGÔøΩ~\tÔøΩcueﬁÇ\u0005ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ1\\ÔøΩ,j/qÔøΩÔøΩﬁ¢\"ÔøΩÔøΩÔøΩuÔøΩÔøΩ◊árÔøΩÔøΩ0ÔøΩ»≠ÔøΩ\u000fÔøΩK\nÔøΩ\u0010-bÔøΩÔøΩÔøΩ6ÔøΩÕ±\u0017ÔøΩÔøΩÔøΩ[ÔøΩÔøΩÔøΩ+ÔøΩCÔøΩCÔøΩ3ÔøΩ?ÔøΩ\u0017ÔøΩ.B7ÔøΩÔøΩK\u001f)yÔøΩÔøΩiÔøΩÔøΩKﬂ±ÔøΩÔøΩ'ÔøΩÔøΩ*&ÔøΩÔøΩM\u0018:\u0017ÔøΩÔøΩ\u0010\u0013ÔøΩÔøΩÔøΩI(ÔøΩ&ÔøΩ<ÔøΩTp\u0015ÔøΩguÔøΩ/\u0011ÔøΩ\u0001ÔøΩÔøΩyÔøΩk<5ﬁΩÔøΩwÔøΩÔøΩ<ÔøΩÔøΩ÷´ÔøΩ<ÿ´+1ÔøΩÔøΩrÔøΩ-ÀëÔøΩÔøΩ\tdGB%ÔøΩÔøΩ5ÔøΩÔøΩÔøΩmÔøΩ\u0007\u0018\u0013HÔøΩÔøΩMxÔøΩÔøΩÔøΩÔøΩÔøΩbÔøΩbÔøΩBiÔøΩÔøΩJ\u001a\u0010kTÔøΩ<ÔøΩÔøΩ5ÔøΩÔøΩ\u0019ÔøΩav‘óÔøΩÔøΩyÔøΩÔøΩnÔøΩÔøΩÔøΩ\u0010*\u001a*⁄ò6ÔøΩEÔøΩ\"oQÔøΩD\u0006ÔøΩI[ÔøΩÔøΩÔøΩHyÔøΩÔøΩ\u001b\u0003ÔøΩÔøΩoÔøΩÔøΩÔøΩÔøΩW:RÔøΩÔøΩÔøΩ{ÔøΩ\u000f=ÔøΩzÔøΩMJ\u0003ÔøΩ\nÔøΩoÔøΩZVÔøΩ\u0007ÔøΩ\\\u0011*+tzÔøΩÔøΩ‡¢¢R6ÔøΩ\u001f(+SÔøΩ\u0004\u0003fs\u000e\u0013\n\u0010ÔøΩÔøΩÔøΩÔøΩ![jÔøΩÔøΩ\u0006ÔøΩH02QU=ÔøΩ<N,_AC!\nÔøΩÔøΩ6ÔøΩbÔøΩ#ÔøΩÔøΩÔøΩnÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩgcmYÔøΩÔøΩÔøΩr\n\u0002ÔøΩ\u0014ÔøΩÔøΩ#{jÔøΩi>ÔøΩ0OLC·∑è\u0016ÔøΩ\u001b\nÔøΩÔøΩWÔøΩÔøΩÔøΩ\u0006ÔøΩ\u0011ÔøΩÔøΩnÔøΩGb…†pÔøΩ.rÔøΩrÔøΩxÔøΩr\n9wYHn\b\u0010ÔøΩÔøΩ,] \u0012ÔøΩ&ÔøΩÔøΩÔøΩrz4\tÔøΩÔøΩhKÔøΩÔøΩÔøΩÔøΩÔøΩ\u0001ÔøΩ\u0003+lvÔøΩÔøΩÔøΩÔøΩ}\n/ÔøΩ-ÔøΩÔøΩt+ÔøΩa≈ºRÔøΩvÔøΩJÔøΩÔøΩx^)ÔøΩ\u0007KqD\u0016*EﬁºÔøΩRÔøΩ(aÔøΩJAÔøΩ\u0019+\n+ÔøΩ~ÔøΩ)\u0010ÔøΩ\u0001ÔÇΩ11N8ÔøΩ`!ÔøΩ'G:ÔøΩwÔøΩÔøΩKzbHÔøΩ7$ÔøΩ\u0003ÔøΩÔøΩÔøΩ€ír\u0005;~œäÔøΩÔøΩÔøΩÔøΩÔøΩ\u001a)ÔøΩÔøΩ5ÔøΩ\u00111ÔøΩÊ©∂\u0007\nÔøΩyÔøΩ&osÔøΩÔøΩÔøΩÔøΩÔøΩx{Cj◊ãÔøΩÔøΩÔøΩ⁄æÔøΩ[ÔøΩÔøΩ\nÔøΩeÔøΩÔøΩ\nÔøΩÔøΩ]ÔøΩNÔøΩ=ÔøΩs\u0015y-ÔøΩÔøΩÔøΩ:ÔøΩ0(\u0014UÔøΩÔøΩ\u001bz6ÔøΩBÔøΩÔøΩÔøΩ\nÔøΩÔøΩnÔøΩddÔøΩÔøΩÔøΩÔøΩd\u001bAÔøΩÔøΩbtÔøΩ\u0010ÔøΩHÔøΩYÔøΩ^ÔøΩ3ÔøΩÔøΩ4i\u0002BÔøΩQÔøΩIÔøΩ\nEf\u0016ÔøΩa\u0010ÔøΩ\u0006zZÔøΩÔøΩÔøΩ+\u0010\nÔøΩÔøΩQÔøΩÔøΩw\nzÔøΩ~DÔøΩÔøΩIÔøΩÔøΩÔøΩ8ÔøΩÔøΩÔøΩÔøΩ!ÔøΩ\nÔøΩ&\\dÔøΩFÔøΩ#ÔøΩa#/ÔøΩÔøΩK\u0012ÔøΩjÔøΩÔøΩ<yÔøΩ6ÔøΩVKÔøΩÔøΩqÔøΩÔøΩŸ®ÔøΩ9;HÔøΩ’£ÔøΩkÔøΩ*ÔøΩÔøΩ(#ÔøΩdÔøΩÔøΩÔøΩ={ÔøΩkcÔøΩFÔøΩÔøΩ\u0014ÔøΩ\u0004ÔøΩÔøΩ[ÔøΩlÔøΩÔøΩÔøΩÔøΩÔøΩPÔøΩbÔøΩNoXR\u0014ÔøΩMmjÔøΩÁ´∏niÔøΩ^o\\\\8oUbSÔøΩÔøΩÔøΩÔøΩÔøΩ4\u00150ÔøΩÔøΩA\n<ÔøΩÔøΩÔøΩÔøΩÔøΩ8ÔøΩ0SÔøΩ\u0006\u0011,ÔøΩÔøΩV5ÔøΩk8ÔøΩ\nFyÔøΩdÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩldq15ÔøΩÔøΩ[ÔøΩ\n≈åP<RÔøΩT`ÔøΩÔøΩÔøΩÔøΩ\"ÔøΩÔøΩÔøΩEÔøΩÔøΩFÔøΩ&ÔøΩÔøΩ\nV\u0015n54ÿö\nÔøΩm\u0005ÕÖqcÔøΩÔøΩÔøΩ\u0015/ÔøΩŸ∏À∂€±ÀµÔøΩpÔøΩÔøΩÔøΩÔøΩ\n\u000fÔøΩ\ns<ÔøΩzÔøΩÔøΩÔøΩÔøΩgsÔøΩÔøΩ\nÔøΩ}ÔøΩÔøΩXŒ∑\nÔøΩÔøΩÔøΩ¬è\n/\u0014\u0016ÔøΩÔøΩIo2ÔøΩ/ÔøΩÔøΩÔøΩ\u0007ÔøΩN\u0014+\u001a3qÔøΩR\u001ft(ÔøΩ“ã\u0004ÔøΩÔøΩÔøΩpÔøΩn[\u0010\u0007ÔøΩ24noÔøΩEÔøΩÔøΩÔøΩÔøΩvX/ÔøΩÔøΩÕ©\u001b9aGÕ¥ÔøΩ\u0011|\u0018ÔøΩÔøΩCÔøΩ<cvC6ÔøΩjÔøΩ[ÔøΩ\nÔøΩÔøΩ\"ÔøΩÔøΩ$ÔøΩa?ÔøΩ=!ÔøΩ{ÔøΩtÔøΩ8[5ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩcÔøΩ\u00042ÔøΩ\nÔøΩÔøΩGÔøΩLÔøΩ9n\nÔøΩÔøΩÔøΩÔøΩIÔøΩB<ÔøΩÔøΩDv%\u0016\u0015ÔøΩÔøΩ7\u0010ÔøΩ9\u0004ÔøΩt\bÔøΩÔøΩÔøΩ\u0016ÔøΩÔøΩÔøΩvQ=cwZJÔøΩLÔøΩd.ÔøΩÀ≤\\{ÔøΩÔøΩ=oÔøΩÔî∑ÔøΩ* æÔøΩoÔøΩÔøΩ\u001f}2ÔøΩÔøΩ:ÔøΩÔøΩÔøΩÔøΩÔøΩ%ÔøΩ^ÔøΩQSÔøΩ\u0001ÔøΩÔøΩSÔøΩÔøΩÔøΩ`/ÔøΩÔøΩzÔøΩÔøΩÔøΩ\u001buÔøΩ+ÔøΩÔøΩÔøΩc'ÔøΩ-ÔøΩs\bÔøΩrÔøΩÔøΩs–ÉBÔøΩ`iÔøΩÔøΩZNY8d\u0011,ÔøΩ ÔøΩ\n1ÔøΩÔøΩL‹çÔøΩAÔøΩ\nÔøΩuÔøΩÔøΩÔøΩ\u0012`7lÔøΩÔøΩÔøΩ\nÔøΩ\u001bÔøΩ@\nÔøΩ\nzl00*\u0006ÔøΩTJ-√¢ÔøΩÔøΩœÄÔøΩZÔøΩÔøΩÔøΩ\nÔøΩÔøΩ,bÔøΩmÔøΩo\u00185p\u0006ÔøΩÔøΩ8ÔøΩÔøΩÔøΩ%\nÔøΩ|\u0016:{ÔøΩ(ÔøΩÔøΩÔøΩÔøΩgÔøΩIÔøΩogÔøΩÔøΩgÔøΩÔøΩEÔøΩ;ÔøΩÔøΩÔøΩÃÖÔøΩsxÔøΩ+ÔøΩÔøΩiÔøΩeQÔøΩ\u001aÔøΩ@ÔøΩeÔøΩÔøΩÔøΩÔøΩpGb\u0011yÔøΩÔøΩÔøΩÕªÔøΩÔøΩÔøΩÔøΩ\u0005ÔøΩÔøΩIv~ƒíÔøΩ”∞~iÔøΩÔøΩÔøΩ\u0010wÔøΩÔøΩDÔøΩ}ÔøΩÔøΩ|j.\u0004ÔøΩÔøΩÔøΩ2ÔøΩ@ÔøΩ\nÔøΩ\u0016›®ÔøΩÔøΩ\u0007ÔøΩ;ÔøΩ\nÔøΩNcCXÔøΩÔøΩN–±ÔøΩÔøΩ\u0003ÔøΩ{ÔøΩ`cÔøΩ,ÔøΩe8V'\u0013VÔøΩÔøΩ~ÔøΩÔøΩ\u0010ÔøΩM~ÔøΩc\u0012?|tTÔøΩ5VÔøΩÔøΩ8ÔøΩ{ÔøΩ2ÔøΩ\u0013ÔøΩÔøΩ3rÔøΩ`ÔøΩ('ÔøΩ^a>@ZÔøΩM\u0007ÔøΩÔøΩsÔøΩÔøΩÔøΩÔøΩJ\nÔøΩBa’åÔøΩ}ÔøΩÔøΩ\nÔøΩ\nÔøΩ\nvc\u00171\u000eÔøΩÔøΩ1ÔøΩÔøΩÔøΩÔøΩ~ÔøΩOÔøΩwÔøΩl,ÕìÔøΩÔøΩ}ÔøΩ-ÔøΩ\n{ÔøΩECfÔøΩ5ÔøΩÔøΩga\u0014ÔøΩ»èÔøΩBÔøΩMÔøΩwe?ÔøΩÔøΩÔøΩuq√≤[ÔøΩ7ÔøΩÔøΩÔøΩ7ÔøΩ3ÔøΩÔøΩÔøΩÔøΩ_ÔøΩTÔøΩÔøΩÔøΩÔøΩÔøΩbÔøΩÔøΩoqÔøΩ-2ÔøΩÔøΩÔøΩBv\nÔøΩ!\n\u0016^ÔøΩ7ÔøΩÔøΩÔøΩrA\u0015ÔøΩÔøΩAÔøΩ{ÔøΩÔøΩlzÔøΩ2<ÔøΩÔøΩÔøΩÔøΩŸåt\u0016ÔøΩ.«Ü\n/ÔøΩ&(ŸåI\n\u00132PP\bÔøΩ\u0004Y!ÔøΩ\u0012$ÔøΩÔøΩÔøΩ &oÔøΩ\\ÔøΩ&ÔøΩ_U3jkÔøΩÔøΩÔøΩÔøΩÔøΩÃæÔøΩ4ÔøΩq2%ÔøΩ8ÔøΩÔøΩ^Û§•ù9ÔøΩÔøΩÔøΩ$*ÔøΩ{ÔøΩ&ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ}NÔøΩÔøΩ\u0015ÔøΩ\u0019ÔøΩ\u0001\u001a%@ÔøΩÔøΩ—äÔøΩ&ÔøΩÔøΩUﬁπÔøΩzÔøΩ\u001bÔøΩÔøΩfÔøΩ=m·¨äL\u0016\u0018\u0006TfÔøΩ^;$ÔøΩÔøΩsÎ©∑ÔøΩ_ŸΩÔøΩÕéÔøΩÔøΩÔøΩÔøΩÔøΩ{ÔøΩÔøΩ'ÔøΩ≈îf,\u001b^ÔøΩt{SusÔøΩsÔøΩ^f\u0000{\u000eÔøΩÔøΩÔøΩÔøΩÒªüªÔøΩÔøΩÔøΩ_O\u000f~\u0007sÔøΩÔøΩÔøΩÔøΩ_ÔøΩ0ÔøΩÔøΩOÔøΩ6\u0017ÔøΩ{\u001bÔøΩÔøΩÔøΩ\u001feÔøΩfaQÔøΩÔøΩÔøΩp\u0007ÔøΩ\u0003ÔøΩ\u0003ÔøΩÔøΩINqÔøΩÔøΩuÔøΩ\u0001›º\u0005\n\bÔøΩ\\6kÔøΩÔøΩÔøΩLÔøΩÔøΩÔøΩ\u0006ÔøΩÔøΩÔøΩIÔøΩ\u0002}b4Vg;ÔøΩ\u0015UÔøΩP\"ÔøΩÔøΩÔøΩvh8ÔøΩÔøΩÔøΩa9`9ÔøΩÔøΩ0ÔøΩ\u0001kÔøΩf\nk`ÔøΩ\\ÔøΩ\nÔøΩ\":ÔøΩÔøΩÔøΩlÔøΩÔøΩ\bf‘ÜÔøΩÔøΩÔøΩHÔøΩ8fÔøΩPÔøΩÔøΩÔøΩ\u0001ÔøΩ]ÔøΩAÔøΩ#ÔøΩÔøΩÔøΩ:Vg\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ–Ü4eÔøΩÔøΩÔøΩ5ÔøΩÔøΩÔøΩxÔøΩeÔøΩÔøΩ]ÔøΩŒìÔøΩnÔøΩ,ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩLÔøΩÔøΩzÔøΩGZ~ÔøΩ\u0000&ÔøΩÔøΩu\nÔøΩÔøΩ\u0015ÔøΩIÔøΩDÔøΩv,ﬁ∑ÔøΩ_<)ÔøΩ\n\u0019Ul\u0004<ÔøΩÔøΩÔøΩ\u00000\bÔøΩ…ìC^kÔøΩÔøΩhN9\u0017ÔøΩ\"ÔøΩ;GÔøΩ πÔøΩ\n\u0002ÔøΩ?ÔøΩQÔøΩYÔøΩ\tÔøΩÔøΩÔøΩÔøΩ\u0000\u001a(xÔøΩpÔøΩ\u000fbtÔøΩ\u0018_ÔøΩÔøΩAÔøΩoÔøΩ\u0017ÔøΩ,qÔøΩc\\ÔøΩuÔøΩÔøΩÔøΩaZÔøΩÔøΩ5ZÔøΩiÔøΩ\u0002r\u0016M=ÔøΩÔøΩw◊ôÔøΩ\u001alÔøΩÔøΩÔøΩÔøΩ-ÔøΩÔøΩKÔøΩL{?ÔøΩWw\u0011ÔøΩI$\u0003hÔøΩÔøΩA2DÔøΩÔøΩ\u001b,XÔøΩÔøΩfÔøΩU\u00170\u0004\n\u0005\\DÔøΩÔøΩ\u0004/\t7YÔøΩÔøΩÔøΩ\u001b\nÔøΩ<ÔøΩ\u001f\tÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩCÔøΩNgÔøΩjÔøΩ<ÔøΩ\"ÔøΩ.ÔøΩ,ÔøΩÔøΩÔøΩÔøΩ9\u0011ÔøΩÔøΩ\u0017aÔøΩ\u0016YÔøΩlf\nQ\u0010ÔøΩ\u0016ÔøΩ\n39ÔøΩ[\u0016ÔøΩ*ÔøΩ/ÔøΩB7ÔøΩAÀ∞5\u0015ÔøΩ\nÔøΩÔøΩÔøΩ<ÔøΩ\nÔøΩ<ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩiÔøΩÔøΩÔøΩDÔøΩ/ÔøΩÔøΩZNÔøΩÔøΩ5ÔøΩÔøΩÔøΩ\u0007Î©íO–ßÔøΩ?GÔøΩÔøΩÔøΩjÔøΩÔøΩ6ÔøΩdÔøΩ\nÔøΩaÔøΩÔøΩÔøΩ]ÔøΩ\u001bÔøΩÔøΩY~\u0016ÔøΩÔøΩÔøΩ\u0011ÔøΩÔøΩÔøΩÔøΩƒ¶y~ÔøΩi\u000fPÔøΩÔøΩJÔøΩ[ÔøΩT]NÔøΩÔøΩÔøΩ&∆´\u0011ÔøΩB\u0016+ÔøΩVÔøΩÔøΩlÔøΩÔøΩÔøΩÔøΩ!ÔøΩÔøΩ#aK\u0018ÔøΩÔøΩÔøΩfÔøΩÔøΩjfTJ%BÔøΩÔøΩ?ÔøΩÔøΩ\\\nbÔøΩ\u001a\u000eÔøΩÔøΩÔøΩ\n{ÔøΩrÔøΩÔøΩB\t.ÔøΩÔøΩ6ÔøΩQÔøΩ=ÔøΩQ#ÔøΩ3ÔøΩe…∑ÔøΩÔøΩÔøΩJSyx\u001aÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\"3zÔøΩt\u0011ÔøΩÔøΩB\u0000ÔøΩ8ÔøΩÔøΩVÔøΩlÔøΩÔøΩ\n{ÔøΩ|ÔøΩ√¢g,ÔøΩXLÔøΩ\u0016cF9RZÔøΩÁºì/ÔøΩÔøΩÕëÔøΩr,}\bÔøΩÔøΩ\u0000SÔøΩu\u0015\n\u001bdÔøΩÔøΩÔøΩ\nÔøΩÔøΩsÔøΩPÏä©ÔøΩvoCd:@vIYÔøΩÔøΩFXÔøΩ?¬ßÔøΩ\bÔøΩHN\nÔøΩÔøΩÔøΩiÔøΩ;gÔøΩOÔøΩÔøΩgÔøΩ3ÔøΩÔøΩ\u0007ÔøΩ\u0005ÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩIÔøΩÔøΩÔøΩÔøΩD\u0016{A\u0016ÔøΩ\u001bhÔøΩÔøΩQÔøΩÔøΩÔøΩ2EÔøΩEfÔøΩÔøΩ2ÔøΩfÔøΩ…†0ÔøΩÔøΩ\u000eÔøΩEnÔøΩ*ÔøΩ\u0016ÔøΩSmÔøΩÔøΩ4\nÔøΩ]-S!ÔøΩIÔøΩB\nÔøΩUgaÔøΩ3ÔøΩ,ÔøΩ>«¨gÔøΩÔøΩÔøΩ,ÔøΩÔøΩÔøΩdÔøΩ’πv5ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ,ÔøΩRÔøΩÌì¨_P!U\u0016B*ÔøΩÔøΩLÔøΩÔøΩÔøΩ,ÔøΩEÔøΩÔøΩÔøΩcÔøΩÔøΩl\u0018F\n\"ÔøΩlVff^^n.«±/)L\u0019\u0019\u000eÔøΩÔøΩ\tÔøΩwÔøΩVÔøΩ0\u001a\nZÔøΩFÔøΩTÔøΩ\u001aÔøΩ”©ÔøΩ*\u0004ÔøΩ\u0002ÔøΩÔøΩÔøΩh2eeeWÔøΩÔøΩ0?ÔøΩÔøΩÔøΩXÔøΩ`ÔøΩutÔøΩY}ÔøΩ]uÔøΩÔøΩyT\u0005ÔøΩT\u000eÔøΩ\b3ÔøΩÔøΩ‚±Ü]tÔøΩœÉp?ÔøΩÔøΩÔøΩÔøΩSÔøΩÔøΩ3?ÔøΩÔøΩÔøΩBUÔøΩÔøΩÔøΩÔøΩYÔøΩÔøΩÔøΩb…äHÔøΩE<wkÔøΩÔøΩ0ÔøΩ9ÔøΩ]VÔøΩÔøΩ*seÔøΩÔøΩƒïfÔøΩÔøΩÔøΩXwÔøΩÔøΩÔøΩteÔøΩÔøΩÔøΩ\u000fÔøΩ\\{\u0011ÔøΩÔøΩk\u001bÔøΩÔøΩÔøΩD\u001bÔøΩÔøΩ~lÔøΩÔøΩÔøΩs€ØÔøΩvÔøΩÔøΩÔøΩ>‹∏jÔøΩ`.ÔøΩÔøΩ\u0006ÔøΩÔøΩ\u0001oÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ6ÔøΩ\u0010cÔøΩ[ÔøΩÔøΩ?ÔøΩ:ÔøΩH-LÔøΩÔøΩÔøΩ¬íÔøΩÏèÖÔøΩ\nOÔøΩÔøΩ\u001f>\u0013ÔøΩﬂ®\u001f0ﬂ•ÔøΩcÔøΩ,ÔøΩ\\?¬úÔøΩÔøΩ,ÔøΩZÔøΩ\u0016\b\nÔøΩhtz^UnEC16\u0014;AÔøΩeÔøΩÔøΩ`‘èÔøΩﬂπÔøΩ\u001aÔøΩÔøΩÔøΩ\u001aÔøΩ7w+@^ÔøΩÔøΩÔøΩ\b\u001b5b(ÔøΩ4ÔøΩ'ÔøΩwÔøΩ\u000e`ÔøΩÔøΩÔøΩfÔøΩZÔøΩW‚ì±*ÔøΩFÔøΩÔøΩÔøΩÔøΩ)ÔøΩ\\ÔøΩpÔøΩÔøΩ;p€ñujÔøΩVÔøΩc6ÔøΩÔøΩ`AMœûÔøΩÔøΩ\n ∑/\u0001ÔøΩ_ÔøΩÔøΩ,ÔøΩÔøΩJlÔøΩÔøΩÔøΩÔøΩÔøΩ8ÔøΩÿ≠ÔøΩ)ÔøΩK\n#+\u0006`kÔøΩÔøΩOÔøΩ1oÔøΩÔøΩﬂ≤ÔøΩl6TÔøΩIiÔøΩ?4ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩHÔøΩÔøΩ\tt!5ÔøΩEcÔøΩR\u000fÔøΩ*ÔøΩ\u0017ÔøΩ\u001a|ÔøΩkÔøΩ\u0016m1BÔøΩÔøΩ\u0006cÔøΩ\u0015ÔøΩnÔøΩL^ÔøΩseÔøΩCF\u0006\n\u0014ÔøΩ ;ÔøΩD#ÔøΩÔøΩfÔøΩÔøΩJeﬂªÔøΩ[ÔøΩWqÔøΩAKI\b8}$ÔøΩÔøΩ\u0007ÔøΩ\u0005ÔøΩB^ÔøΩL\nÔøΩx\u0001pu\nBEÔøΩ`'ÔøΩZÔøΩÔøΩÔøΩY6\u000fÔøΩCYÔøΩL9ÔøΩÔøΩÔøΩ\u001apZÔøΩÔøΩ d ÔøΩCÔøΩÔøΩ\u0000ÔøΩdÔøΩÔøΩ\u001bÔøΩZpKÔøΩÔøΩ\b\nÔøΩW\u0000ÔøΩ\n ≠\u0007ÔøΩ\u0011ÔøΩÔøΩ@\u0018ÔøΩÔøΩÔøΩcÔøΩrÔøΩ@[ÔøΩ\nqÔøΩÔøΩ\u0015ÔøΩqA9ÔøΩÔøΩ!o\u0010ÔøΩ\nÔøΩÔøΩ\u0010ÔøΩ\u000fÔøΩ,'NjÔøΩ9pkÔøΩ]\u0003qAE\nÔøΩ\u0007ÔøΩKÔøΩ%z\nBÔøΩÔøΩ\u0015ÔøΩÔøΩcÔøΩÔøΩ\u001f!ÔøΩÔøΩ8ÔøΩ\u0006hÔøΩfÔøΩÔøΩÔøΩ»∏\u0002ÔøΩ\u0000ÔøΩO\u0006ÔøΩg.E(ÔøΩ4BÔøΩ\bŸ¥ÔøΩÔøΩ>ÔøΩrÔøΩÔøΩÔøΩÔøΩ\u0002!g\u000fBÔøΩ)ÔøΩ\\ÔøΩÔøΩ3ÔøΩÔøΩÔøΩVÔøΩÔøΩ?D(8\u0002ZÔøΩ\u0003»µ\u0016ÔøΩÔøΩÔøΩ\u0010\nÔøΩAÔøΩÔøΩ\u0001ÔøΩÔøΩ? ÔøΩ\u0000ÔøΩ^8ÔøΩÔøΩb(s)ÔøΩ#ÔøΩ\u000fC\u0004ÔøΩ+ÔøΩ\u0010ZÔøΩ:BÔøΩV!ÔøΩÔøΩ\u0003 s\u0017BÔøΩ!ÔøΩ\u001aÔøΩ\u0015»∑a\u00198(k#0∆¶/!ÔøΩ\u0005ÔøΩÔøΩ\u00042j\u001bÔøΩ€æÔøΩÔøΩuÔøΩGÔøΩ\u0019\\ÀèÔøΩÔøΩÔøΩ!z\nÔøΩÔøΩ;\u0011jﬂÅP\nÔøΩ#ÔøΩÔøΩÔøΩÔøΩ\tB;ÔøΩÔøΩÔøΩw(ÔøΩ,C\u001fÔøΩJ\u0014G2ÔøΩÔøΩvaÔøΩ\u0003ÔøΩÔøΩ3ÔøΩÔøΩ%ÔøΩÀéVÔøΩÔøΩhVÔøΩ-Z$ÔøΩ\u00189ÔøΩ\u0013ÔøΩ\u0019ÔøΩgÔøΩ\u0012ÃÇnÔøΩ3\tÔøΩÔøΩÔøΩÔøΩ\u0010\b8\tÔøΩ\u0003ÔøΩÔøΩÔøΩ`\u0005zg\u0006GÔøΩ|ÔøΩ'\u0012ÔøΩBÔøΩÔøΩ\n\u0012ÔøΩÔøΩmV=%ÔøΩ\u001aÔøΩÔøΩ*ÔøΩZÔøΩaÔøΩ`ÔøΩÔøΩ\u0005fÔøΩ\u0004ÔøΩÔøΩuÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\n1\u0018ÔøΩÔøΩ`\u0006)L%\u0012Ã¢rÔøΩUÔøΩÔøΩ982ÔøΩ5ÔøΩÔøΩ\nÔøΩL\u001b$XÔøΩzfpÔøΩ(ÔøΩÔøΩÔøΩ\u0004ÔøΩÔøΩÔøΩoJÔøΩÔøΩ9ÔøΩÔøΩ$XÔøΩÔøΩÔøΩ\u0007%XÔøΩJÕß%XÔøΩn3ÔøΩIÔøΩ\nÔøΩ,ÔøΩK4ÔøΩ\b’µÔøΩZ\nÔøΩ\u0011ÔøΩlÔøΩÔøΩÔøΩÔøΩÔøΩ(ÔøΩÔøΩÔøΩ(ÔøΩÔøΩÔøΩÔøΩ)ÔøΩÔøΩÔøΩHÔøΩÔøΩ1\u0012aqÔøΩDX\n#\u0011ÔøΩÔøΩÔøΩÔøΩc$ÔøΩÔøΩ\u0018ÔøΩÔøΩ8F\",ÔøΩÔøΩ\bÔøΩc$ÔøΩÔøΩ\u0018ÔøΩÔøΩ8F\",ÔøΩÔøΩ\bÔøΩc$ÔøΩÔøΩ\u0018\u0011X=ÔøΩÔøΩ\u001a⁄ó/SX;'^OÔøΩ'(l$}ÔøΩ\nÔøΩ0ÔøΩÔøΩÔøΩayÔøΩÔøΩYsÔøΩi9\"ÔøΩ3'ÔøΩJÔøΩMa;ÔøΩ\u0011ÔøΩÃõÔøΩÔøΩ\u0003{(ÔøΩÔøΩ\u0014.ÔøΩÔøΩY\n\u0017SÔøΩ\u0002ÔøΩÔøΩs⁄ØÔøΩSÔøΩvNÔøΩvN_\nÔøΩ:ÔøΩ*ÔøΩ\u0011\nÔøΩ~\u0014C\n(ÔøΩÔøΩ ÔøΩÔøΩsÔøΩ6ÔøΩ.\nÔøΩÔøΩÔøΩÔøΩ\u0007.%aÔøΩ ÔøΩÔøΩ(\u00010ÔøΩ\u0010ÔøΩM1xÔøΩÔøΩÔøΩ!ÔøΩjh|ÔøΩYRxÔøΩe<jÔøΩÔøΩ\n40ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ'ÔøΩ7\u000fÔøΩ√ø\b*ÔøΩÔøΩ\u0012\u001a[\n9z ÔøΩ\u0000y:ÔøΩ\n)ÔøΩk\u0003ÔøΩÔøΩ\u0004ÔøΩ\u0000ÔøΩ\u001bCÔøΩPG7ÔøΩq<Z\u000fÔøΩ\u0010≈âC\\\u0014ÔøΩÔøΩ\u0006m?i];ÔøΩÔøΩÔøΩ\u0004ÔøΩ\tqqÔøΩÔøΩÔøΩÔøΩxÔøΩÔøΩAÔøΩÔøΩÔøΩÔøΩ\u0014m\ni\t\u000fÔøΩ\u0004'%ÔøΩÔøΩ\tzÕ£\u0006ÔøΩÔøΩG>ZÔøΩ:ÔøΩÔøΩÔøΩ\u000eÔøΩCÔøΩBÔøΩ/\u0006ÔøΩ&i€ªÔøΩÔøΩBWÔøΩÔøΩÔøΩÔøΩÔøΩTÔøΩL'–∏ÔøΩÔøΩÔøΩ\u0002mÔøΩe%gÔøΩNÔøΩÔøΩ\b|ÔøΩÔøΩnw7ÔøΩ#\u0001)IJÔøΩ\u0014ÔøΩgÔøΩ\u0017KÔøΩ-ÔøΩ\u0001zÔøΩ\u000eÔøΩÔøΩCÔøΩ7¬∏ÔøΩ\u0000ÔøΩ\n∆áÔøΩÔøΩ\u0010KÔøΩÔøΩ\u0002ÔøΩÔøΩ4ÔøΩ\u0016b\u001aÔøΩ'#ÔøΩ\u0012JÔøΩÔøΩÔøΩ6Nb7\"\nRSG(ÔøΩM{ÔøΩÔøΩÔøΩ{ÔøΩÔøΩ\"\u0015ÔøΩi+ÔøΩi_\bnzÔøΩ\n3ÔøΩÔøΩÔøΩ@\u00032fÔøΩÔøΩx\u000eÔøΩÔøΩ%ÔøΩ\u0019ÔøΩÔøΩÔøΩQ+M\n\u0006ÔøΩÔøΩÔøΩ:ÔøΩÔøΩipÔøΩx\u000e–ºÔøΩyÔøΩ=ÔøΩxÔøΩR|ÔøΩ%dÔøΩÔøΩPzÔøΩ(gÔøΩh\\'-%F)ÔøΩ\u0007ÔøΩÔøΩx7IÔøΩuAÔøΩ ≈ãC;ÔøΩL#ÔøΩ#÷ôÔøΩ\u001bÔøΩIÔøΩÔøΩ\u0010\nÔøΩ\u0018ÔøΩnÔøΩeÔøΩÔøΩÔøΩÔøΩDÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩÔøΩRÔøΩÔøΩÔøΩÔøΩKÔøΩ\"\u0014ÔøΩÔøΩ)eH*ÔøΩjÔøΩÔøΩK<ÔøΩÔøΩÔøΩÔøΩÔøΩgÔøΩ’≠ÔøΩÔøΩÔøΩI%_mÔøΩÔøΩÔøΩWÔøΩRÔøΩp[ÔøΩ*\\qeÔøΩb<ÔøΩÔøΩ ÔøΩDÔøΩDÔøΩÔøΩ\nÔøΩÔøΩIZZÔøΩÔøΩ\nÔøΩÔøΩÔøΩ'iIÔøΩWÔøΩÔøΩ8NÔøΩÔøΩfB\nÔøΩÔøΩ$-ÔøΩ\nb;h\u000fÔøΩÔøΩ1ÔøΩ%^ÔøΩ\u0012ÔøΩ\u000fÔøΩfÔøΩ%\u0012ÔøΩÔøΩRZÔøΩÔøΩQ*ÔøΩ\u0012sd]ÔøΩ\nvb\u000eﬂäÔøΩKÔøΩ]JÔøΩÔøΩÔøΩÔøΩ=3BbOfÔøΩ\u001bÔøΩ„øìÔøΩÔøΩ\\\tÔøΩ!ÔøΩÔøΩ,f\npEÔøΩ7@)NÔøΩÔøΩÔøΩÿÆÔøΩÔøΩMd\u0006ÔøΩ\u0006ÔøΩÔøΩÔøΩÔøΩÔøΩ#Õ•ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ,ÔøΩÔøΩ}ÔøΩrÔøΩ\bÔøΩIÔøΩÔøΩ >FÔøΩNÔøΩÔøΩÔøΩÔøΩmtTÔøΩ.\u001bÔøΩÔøΩeÔøΩÔøΩ-ÔøΩÔøΩ/N%iÔøΩ$ÔøΩ\u0007\u0001OÔøΩÔøΩi9ÔøΩÔøΩÔøΩ~ÔøΩ<qNÔøΩÔøΩ:(ÔøΩÔøΩÔøΩ\nKÔøΩwÔøΩ8ÔøΩÔøΩ\u0012{ÔøΩÔøΩ2 uÔøΩyÔøΩ\nÔøΩÔøΩeÔøΩÔøΩÔøΩ\u001fÔøΩvÔøΩÔøΩWÔøΩÔøΩ&ÔøΩ0ÔøΩÔøΩÔøΩÔøΩ\u0016ÔøΩÔøΩ!\nT1S\u0002YÔøΩÔøΩ!ÔøΩÔøΩÔøΩ5ÔøΩÔøΩ|ÔøΩ\u0010ÔøΩ\u0017\nÔøΩyÔøΩT\n+ÔøΩ|DlK\u0010ÔøΩo\u0013ÔøΩÔøΩ0#ÔøΩ:\u000fRÔøΩKÔøΩBT\nÔøΩÔøΩZÔøΩÔøΩ@+ ÔøΩÔøΩNF+\u0005-ÔøΩ\u0000\n#\nÔøΩ\"ÔøΩB–èÔøΩg|\u001bÔøΩ|mÔøΩ P\nÔøΩÔøΩC3|!ÔøΩÔøΩ›íÔøΩ%mjÔøΩÔøΩ\u0014ÔøΩ∆∞DÔøΩ\u0004ÔøΩSR*ÔøΩÔøΩÔøΩ)~J\u001aÔøΩÔøΩtÔøΩÔøΩg8ÔøΩÔøΩ3ÔøΩÔøΩÔøΩÔøΩJÔøΩÔøΩ\n=dVÔøΩ])ÔøΩ;ÔøΩ,OÔøΩRÔøΩjFRÔøΩÔøΩi\u001aÔøΩÔøΩÔøΩÔøΩi}fÔøΩÔøΩ ÔøΩÔøΩZÔøΩÔøΩviVÔøΩQ^&\u0012-*ÔøΩÔøΩEsÔøΩhÔøΩÔøΩÔøΩÔøΩrg7ÔøΩÔøΩJXÔøΩÔøΩ+IgÔøΩÔøΩZÔøΩYOÔøΩÔøΩÔøΩ4\u001bÔøΩÔøΩKÔøΩ\u0004ÔøΩ2q6ÔøΩÕ¨EÔøΩt\u0016ÔøΩiÔøΩ._+ÔøΩ|*J%2ÔøΩTÔøΩÔøΩ%\nÔøΩRÔøΩ(eÔøΩÔøΩÔøΩsÔøΩÔøΩÔøΩC\u0013ÔøΩ>ÔøΩÔøΩÔøΩÿ∂4EÔøΩÔøΩLÔûëÔøΩÌóå}ÔøΩÔøΩ\nÔøΩf^ÔøΩÔøΩ/^ÔøΩ\u0001ÔøΩm^ÔøΩK\u0014ÔøΩ]ÔøΩÔøΩ2H\\\u001bÔøΩ6ÔøΩÔøΩ:ÔøΩÔøΩ7ÔøΩÔøΩ\"ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ%ÔøΩÔøΩÔøΩÔøΩ%ÔøΩÔøΩ\u0018ÔøΩÔøΩÔøΩÔøΩ>:ZÔøΩ“™ ÔøΩm\"ÔøΩ\u0006ÔøΩÔøΩViÔøΩÔøΩ\\4H5ÔøΩÔøΩÔøΩ9ÔøΩA%ÔøΩ\\ÔøΩ .ÔøΩÔøΩ\"ÔøΩ]}ÔøΩÔøΩÔøΩÔøΩÿªeÔøΩsDÔøΩÔøΩœ¥_ÔøΩÔøΩÔøΩ\u0019ÔøΩÔøΩÔøΩÔøΩÔøΩ\"œµÔøΩHÔøΩvÔøΩ#iÔøΩ4@ÔøΩ.ÔøΩÔøΩ@ÔøΩÔøΩ\u001aÔøΩ\u0000ÔøΩÔøΩiÔøΩÔøΩÔøΩjÔøΩÔøΩÔøΩÔøΩhÔøΩÔøΩÔøΩÔøΩÔøΩ;ÔøΩÔøΩƒ•ÔøΩ I{ÔøΩ#q]\u0017\n«¥$LH+\u001bÔøΩ]ÔøΩÔøΩÔøΩÔøΩ%Ûá¥ñÃ∏KÔøΩÔøΩŒôÔøΩ ÔøΩ&tÔøΩÔøΩ#LÔøΩNÔøΩRvS).ÔøΩÔøΩmtÔøΩÔøΩiÔøΩÔøΩI'ÔøΩƒ•]EÔøΩ46ÔøΩÔøΩGÔøΩÔøΩ\u0016ÔøΩkÔøΩ5\nÔøΩÔøΩÔøΩ=+OZiﬁùR[E\nÔøΩ“µ\"ÔøΩnÔøΩtÔøΩÔøΩ%ÔøΩÔøΩÔøΩKJk,ÔøΩKdh;ÂØÅKF1]rÔøΩÔøΩzÔøΩsJ\u00135ÔøΩ~:&√ó`ÔøΩKzyÔøΩbÔøΩÔøΩ\u001aÔøΩ\u001fÔøΩ\u0004aÔøΩÔøΩ\nÔøΩÔøΩÔøΩOQI@ÔøΩ\u0015ÔøΩÔøΩ|3-[ÔøΩuÔøΩ|LÔøΩÔøΩB39ÔøΩok\nÔøΩ#ÔøΩÔøΩÔøΩÔøΩ\u0017ÔøΩÔøΩÔøΩ¬óÔøΩÔøΩ3eo\nÔøΩuDÔøΩbÔøΩsÔøΩ∆Æ\u0018ÔøΩ.ÔøΩ\u0017OA\u0014ÔøΩ<ÔøΩÔøΩ'ÔøΩÔøΩÔøΩx\u001fÔøΩÔøΩÔøΩ\u0016ÔøΩkÔøΩÔøΩÔøΩÔøΩA\nÔøΩÔøΩÔøΩÔøΩxÔøΩ\u0000ÔøΩIÔøΩÔøΩ ﬂºÔøΩÔøΩH1x%!ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩŸïJÔøΩ\u001bbÔøΩXb0÷æÔøΩÔøΩ7ÔøΩÔøΩÔøΩ«ÜÔøΩ\nÔøΩÔøΩhﬂ≥ÔøΩÔøΩDÔøΩ=ÔøΩ\u001bMÔøΩÔøΩÔøΩ\nÔøΩ2>\u0011ÔøΩÔøΩNÔøΩbÔøΩX;ÔøΩÔøΩ«ß\u0000uS#ÔøΩ\u0010MÔøΩ>~ÔøΩ:ÔøΩÔøΩÔøΩ#ÔøΩGÔøΩÔøΩÔøΩXO26ÔøΩ\u0005hÔøΩtI\u0015ÔøΩU\nÔøΩÔøΩ\nÔøΩ&*6ÔøΩ\u0012IRzIhQ\t\u001fXÔøΩ›ñÔøΩ'ÔøΩ\nÔøΩ ÔøΩ\u0007tÔøΩÔøΩ–∏nÔøΩÔøΩÔøΩÔøΩÔøΩWÔøΩ^^ÔøΩquÔøΩzÔøΩ~\u0005ÔøΩvÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ|ÔøΩÔøΩ\nÔøΩÔøΩÔøΩjÔøΩo‘©uÔøΩ]ÔøΩI>ÔøΩ&/ÔøΩÔøΩÔøΩÔøΩÔøΩx,ÔøΩ\u001a&ÔøΩÔøΩÔøΩ\u0019ÔøΩ-ﬁôÔøΩÔøΩw\nÔøΩ\u0016w\u0003}\u0006ÔøΩ1ÔøΩuÔøΩ\u001fÔøΩ\u000fÔøΩÔøΩmÔøΩAÔøΩœÅÔøΩÔøΩXÔøΩÔøΩ\u0003ÔøΩÔøΩMÔøΩBÔøΩ|Ow[ÔøΩ\u000f–£ÔøΩÔøΩXÔøΩ7÷ó\nÔøΩMÔøΩÔøΩ+:\u0018ÔøΩ„≠©hw\u001fÔøΩL]ÔøΩ\u0018“´ÔøΩh\"ÔøΩ«∫ÔøΩÔøΩ\u0004ÔøΩﬁùÔøΩÔøΩÔøΩzÔøΩÔøΩÔøΩDÔøΩwÔøΩ]qÔøΩ+ÔøΩ\u0019ÔøΩ(CÔøΩ9ÔøΩÔøΩ\n(ÔøΩÔøΩn\nHAÔøΩÔøΩÔøΩx_lnÔøΩÔøΩÔøΩtÔøΩbÔøΩÔøΩ\n)f2\u0003\nÔøΩ\u0007ÔøΩ=\u0003ÔøΩÔøΩ\nhv2\u0019KÔøΩÔøΩ\nÔøΩ7ÔøΩÔøΩƒíIÔøΩyÔøΩ\nÔøΩ4Ã©8dMÔøΩÔøΩ⁄∫;ÔøΩ€ÆÔøΩ9\u000fTÔøΩKuÔøΩu“ºÔøΩÔøΩÔøΩnÔøΩ-ÔøΩ\n>AÔøΩÔøΩÔøΩD'(mÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ&\nÔøΩJ(ÔøΩP<ÔøΩ3ÔøΩ\u0012\u0019ÔøΩ\u0003hA#ÔøΩCÔøΩ}\u0003ÔøΩ=ÔøΩÔøΩ.R\u000fÔøΩ%ÔøΩÔøΩ7:ÔøΩCÔøΩaÔøΩÔøΩÔøΩ\tÔøΩf)tiEÔøΩ\nÔøΩ;f;\u0017ÔøΩ\u001bÔøΩw\nƒíÔøΩÔøΩÔøΩx_[,ÔøΩ'ÔøΩ !ÔøΩÔøΩ\"'ÔøΩÔøΩ\u0003=ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0007ÔøΩÔøΩ>ÔøΩÔøΩÔøΩÔøΩuÔøΩÔøΩ\u0012GÔøΩÔøΩÔøΩÔøΩ\u0011ÔøΩ\u0005\u0015ÔøΩÔøΩmÔøΩÔøΩ1&\nÔøΩJÔøΩÔøΩzÔøΩÔøΩÔøΩ3\u0019ÔøΩ`¬¥ÔøΩÔøΩ\u0005A=ÔøΩT\u0005AÔøΩÔøΩXÔøΩ\u0017ÔøΩEÔøΩ\u0017\u0006ÔøΩÔøΩÔøΩ\u0016\u0015GÔøΩG\"*’¶:ÔøΩÔøΩÃõ7>ÔøΩ\nK\u0017ÔøΩ\n\u0017ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ]ÔøΩTE8<44\u0014ÔøΩM\u000f|[ÔøΩwÓúàÔøΩ5ÔøΩÔøΩ\u0010ÔøΩ\u0005Lfh\u0014ÔøΩ‘òÔøΩ\u0002o\nCÔøΩ\u0013ÔøΩÔøΩx_\u0011ÔøΩÔøΩÔøΩ-\u0005=X\u001bMÔøΩ\u0013\u0002ÔøΩ+/-ÔøΩÃñÔøΩ2ÔøΩ2ÔøΩ\nÔøΩwt'ÔøΩ)>ÔøΩÔøΩ\u001fÔøΩJBÔøΩÔøΩÔøΩÔøΩÔøΩ\u0019Q(\u0000\nÔøΩÔøΩÔøΩÔøΩa(ÔøΩbCÔøΩÔøΩ(LÔøΩ\"JÔøΩÔøΩÔøΩÔøΩ.ÔøΩÔøΩÔøΩP4…∑«í›ù0ÔøΩB<ÔøΩ:\u0005ÔøΩ\bÔøΩ;–öÔøΩÔøΩ0ÔøΩÔøΩYÔøΩ\u001aÔøΩÔøΩÔøΩ“≥ÔøΩÔøΩ\u0014XÔøΩÔøΩ=ÔøΩÔøΩ∆°\u0001…ÅÔøΩ6`ÔéÅ\n^$h\"Fy,\tÔøΩÔøΩÔøΩ@ÔøΩ:ÔøΩ\t”∂ÔøΩÔøΩOÔøΩCÔøΩÔøΩÔøΩ`ÔøΩ1I\u0000\\F\u0005ÔøΩnÔøΩ\u0004&\n\u0004ÔøΩÔøΩ2ÔøΩÔøΩDÔøΩÔøΩÔøΩ`hQ\u001f\u0014ÔøΩGÔøΩD|ÔøΩÔøΩ\nÔøΩÔøΩÔøΩ›êÔøΩa\u0007ŒÅN∆àÔøΩ\nÔøΩ“ä4\u0015H4\u0018ÔøΩ\u0019$#ÔøΩ1ÔøΩ\u0010ÔøΩ\u0001ÔøΩ\nBÔøΩ9SÔøΩ*#\u0006ÔøΩ-ÔøΩ&ÔøΩÔøΩqR>–≤ÔøΩÔøΩS2ÔøΩpÔøΩ\\;aÔøΩÔøΩ\u0001ÔøΩH\u0003IÔøΩÔøΩ!ÔøΩËè•\u0006ÔøΩT\u00146ÔøΩÔøΩÔøΩ\\D\bOÔøΩÔøΩKfK\nÔøΩA25\nCÔøΩÔøΩ\u0015M\u0010&ÔøΩÔøΩRÔøΩmI\nÔøΩ\u001b\nÔøΩh{ÔøΩ_ÔøΩ…ùÔøΩ\u001fÔøΩ\u001bÔøΩb==ÔøΩÔøΩ=ÔøΩhÔøΩvÔøΩtÔøΩ\u0004nÔøΩ\u000fÔøΩÔøΩÔøΩÔøΩIg<\u000eK\u0005ÔøΩ%ÔøΩ;\nÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩ\nHÔøΩ|ÔøΩ\u001aÔøΩÔøΩLÔøΩ\u0006ÔøΩF;ÔøΩ7ÔøΩDMÔøΩ\\ÔøΩÔøΩÔøΩÔøΩD\u0004K\\ÔøΩÔøΩÔøΩx€ÄÔøΩEÔøΩ\nÔøΩIÔøΩ)\u001a\bÔøΩÔøΩÔøΩÔøΩ(›£ÔøΩ ÔøΩS›§ÔøΩÔøΩÔøΩ2\tÔøΩ]ÔøΩﬁûpoÔøΩ/ÔøΩ\u001b\nÔøΩ&ÔøΩSdÔøΩ\u001f\u0013dÔøΩ\tÔøΩÔøΩ0ÔøΩPÔøΩÔøΩpÔøΩÔøΩÔøΩBÔøΩÔøΩÔøΩƒßÔøΩh=UÔøΩzÔøΩRÔøΩG_\\ÔøΩÔøΩ\u0005ÔøΩ@uÔøΩ\u0001ÔøΩgÔøΩÔøΩNOoÔøΩÔøΩÔøΩM\nÔøΩ\b{ÔøΩÔøΩ&ÔøΩ*ÔøΩcÔøΩqÔøΩ–úÔøΩÔøΩT\nI?ÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩ%ÔøΩÔøΩÔøΩ8\u00077ÔøΩÔøΩÔøΩVrKÔøΩ/\u0007ÔøΩ(}ÔøΩÔøΩ.)?]x\n?ÔøΩ\"ÔøΩ\u0002V\u0003~ÔøΩnÔøΩH\u0019ÔøΩÔøΩÔøΩÔøΩdÔøΩFT[ÔøΩDÔøΩÔøΩÔøΩ\u0004ÔøΩ7ÔøΩÔøΩ#/ÔøΩw\u0002TC_Ã¨ÔøΩ^ÔøΩ\u0002ÔøΩÔøΩÔøΩÔøΩÔøΩn\nƒóÔøΩÔøΩfÔøΩÔøΩÔøΩÔøΩ@ÔøΩAÔøΩ\n\u0012ÔøΩÔøΩ—øÔøΩ.(ÔøΩjÔøΩÔøΩy\u0010\nÔøΩÔøΩ\u0002T\bZÔøΩ<TÔøΩ\u0016ÔøΩ>ÔøΩ\u0000ÔøΩÔøΩUh\u0005\u0014_ÔøΩ6ÔøΩlÔøΩÔøΩ,h\u0002vÔøΩÔøΩÔøΩÃïÔøΩ\u001ahÔøΩVhz\n4ÔøΩ\nÔøΩ\\ÔøΩÔøΩÔøΩu(ÔøΩÔøΩÔøΩÔøΩiÔøΩ\"ÔøΩEÔøΩÔøΩ\u001bx\u00132\"+\nBÔøΩÔøΩÁΩÑ*ÔøΩ\u0017ÔøΩÔøΩ^ÔøΩ=W\u0016ÔøΩÔøΩ\\ÔøΩ\nEÔøΩ\nÔøΩÔøΩ–±ÔøΩ'ÔøΩ_MÔøΩ÷ñ\bÔøΩ\u0010\u0016ÔøΩh8\n\bÔøΩ–Ñq[nÔøΩ7ÔøΩ_1ÔøΩÔøΩQNÔøΩx<ÔøΩNS~9ÔøΩlÔøΩ\u0004,X$\u0002\u0013\u0005ÔøΩ%ÔøΩWÔøΩÔøΩ_ÔøΩÔøΩ\u0004«∞ÔøΩdﬂáÔøΩÔøΩ\\\u0013ÔøΩP…πj\nD`ÔøΩs»Ä1rÔøΩQÔøΩÔøΩÔøΩ\u00188\u0006\tÔøΩ{\u0013\n_…ÅWÔøΩ7!ÔøΩÔøΩÔøΩ\u000fÔøΩb$ÔøΩ\u000fÔøΩuÔøΩ\u0012(ÔøΩ{ÔøΩK(\u00039Ÿ£ÔøΩR ã\u0013zS\tÔøΩNÔøΩÔøΩ ÔøΩNÔøΩ\u0012ÔøΩ)pÔøΩÔøΩq(ÔøΩ~\nÔøΩ\u0006ÔøΩ\u000fÔøΩap\n2ÔøΩÔøΩ\u0004\u0017\u0006WObÿÉÔøΩAhÔøΩÔøΩÔøΩ3-ÔøΩapqpÔøΩÔøΩqh#ÔøΩuÔøΩÔøΩI|ÔøΩYv\u0007 áÔøΩ_$ÔøΩ\u0001!ÔøΩÔøΩÔøΩÔøΩÔøΩOAhÔøΩÔøΩ\u0010ÔøΩÔøΩ\tx&ÔøΩ\u0001ÔøΩÔøΩQ\bIÔøΩ#RÔøΩÔøΩÔøΩ\u0003ÔøΩCRÔøΩ ÔøΩÔøΩ!|\u0000ÔøΩIÔøΩOÔøΩÔøΩ ;@ÔøΩpÔøΩMÔøΩ;ÔøΩÔøΩj\u0007ÔøΩÔøΩÔøΩ\"ÔøΩXÔøΩÔøΩ\u0007ÔøΩ~ ÔøΩÔøΩÔøΩ#4ÔøΩÔøΩÔøΩ\u0002ÔøΩCk:\u0002a\tÔøΩÔøΩb\bÔøΩeÔøΩÔøΩctÀÑÔøΩZ2\n$ÔøΩ\u0005H\nPÔøΩ\u0016ÔøΩÔøΩ-ÔøΩÔøΩ/{s\u001aÔøΩf\u0011ÔøΩÔøΩÔøΩ\u0019pn\u0006ÔøΩÔøΩ\u0001ÔøΩfÔøΩJÔøΩMB}IÔøΩÔøΩÔøΩ\u001bÔøΩÔøΩÔøΩXÔøΩ{\u0012Ëû§\u0017ÔøΩÔøΩÔøΩÔøΩ\u0004|\u0012\u001bÔøΩÔøΩÔøΩÔøΩÔøΩ'v\bÔøΩ\u0018ÔøΩVÔøΩewÔøΩ\u0007ÔøΩÔøΩdÔøΩ\u0013ÔøΩBIÔøΩÔøΩl\u0007ÔøΩZ`;&ÔøΩy%ÔøΩfÔøΩTj¬à\u0010ÔøΩÔøΩ@pc456ÔøΩ“íÔøΩÿÑ-O\n\u0001kgÔøΩÔøΩmC7ÔøΩcP\u0016ÔøΩ\npÔøΩÔøΩ’ÄÔøΩÿ∂qOÔøΩyÔøΩ]ÔøΩzÔøΩHÔøΩ;w3ÔøΩÔøΩÔøΩÔøΩn\u0019\u0017ÔøΩÔøΩ\u0019ÔøΩÔøΩ%ÔøΩAÔøΩÔøΩ%3ÔøΩbT\t\bAgs%^ÿ¢ÔøΩWÔøΩÔøΩXÔøΩÔøΩWETÔøΩÔøΩA%ÔøΩÔøΩÔøΩÔøΩ},ÔøΩdÔøΩl\u0015[ÔøΩ6ÔøΩ2r\nDQQJ\u000eFÔøΩÔøΩWÔøΩÔøΩ◊åjÔøΩ4'4'5ÔøΩ1ÔøΩ\tÔøΩIÔøΩ)ÔøΩ9ÔøΩLÔøΩ\u0018ÔøΩ oÔøΩÔøΩÔøΩGÔøΩÔøΩÔøΩr\u00151ÔøΩƒ¥hÔøΩ5#\u001a÷®ÔøΩ5\u0011ÔøΩÔøΩiÔøΩ»ú\n<ZÔøΩÔøΩm%ÔøΩÔøΩÔøΩ7ÔøΩÔøΩ\u0007ÔøΩ\n\u00074nÔøΩxÔøΩÔøΩ\n\\3ÔøΩF3ÔøΩÔøΩzÔøΩGÔøΩ#x2ÔøΩ;\tÔøΩ)\beÔøΩd\u0000<\u0003ÔøΩ\u0019 ÔøΩ\u0000ÔøΩ\u0006ÔøΩEÔøΩÔøΩÔøΩ\u0006p-ÔøΩÔøΩÔøΩTÔøΩLJ:\u000fÔøΩ?GRX\"ÔøΩ\u0010ÔøΩÔøΩXÔøΩaÔøΩ\u0014ÔøΩÔøΩ\b\u0004n\n<ÔøΩÔøΩI\u0007O:ÔøΩ:ÔøΩ|\u0006-4ÔøΩœÉk\u0000ÔøΩ“∏SÔøΩÔøΩkÔøΩOÔøΩEÔøΩÔøΩ\u0016prÔøΩ~ÔøΩÔøΩÔøΩ\u0004ÔøΩÔøΩÔøΩLÔøΩÔøΩO\u0004ÔøΩX\u0010ÔøΩ\u0006ÔøΩÔøΩ \u0016*ÔøΩÔøΩKÔøΩ|ÔøΩ222ÔøΩÔøΩÔøΩÔøΩÔøΩ@ÔøΩÔøΩ\\ÔøΩ\nÔøΩÔøΩ\u0003ÔøΩzwÔøΩÔøΩ>PÔøΩ4WÔøΩÔøΩV\u0005ÔøΩÔøΩÔøΩÔøΩÔøΩ7\n\b?ÔøΩ9ÔøΩNÔøΩ3ÔøΩ|ÔøΩ€∑ÔøΩÔøΩÔøΩW◊æÔøΩÔøΩk^\u001b_ÔøΩ{-ÔøΩÔøΩÔøΩÔøΩ\u0019/ÔøΩÔøΩÔøΩ0ÔøΩKÔøΩ\u0017«≠ÔøΩÔøΩÔøΩÔøΩÔøΩ%ÔøΩaÔøΩN3ÔøΩ\u0007ÔøΩÔøΩ\u000fÔøΩE\u0006ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ…òÔøΩ4ÔøΩyÔøΩ%GÔøΩGÔøΩÔøΩÔøΩÔøΩÔøΩ \u00179ÔøΩa\u0000ÔøΩ)ÔøΩÔøΩÔøΩ\u00034ÔøΩ@$ÔøΩÔøΩ$ÔøΩÔøΩÔøΩ\u001f\u001aÔøΩ(ÔøΩÔøΩ^\nbÔøΩ\u0019ÔøΩ\u0001p,ÔøΩ}\bÔøΩ\u000fQl\u0011:LÔøΩÔøΩÔøΩ?EÔøΩÔøΩ%ÔøΩQ\u001aOÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\u0010ÔøΩFÔøΩÔøΩ6ÔøΩÔøΩÔøΩP\u0015ÔøΩfpÔøΩÔøΩdÔøΩ-v\nzÔøΩ%ÔøΩÔøΩ\nÔøΩ;ÔøΩÔøΩÔøΩ;\nÔøΩcÔøΩÔøΩÔøΩ-ÔøΩ\u0016ÔøΩyÔøΩwÔøΩ9ÔøΩ\u0016\tÔøΩyÔøΩNÔøΩÔøΩ\u0003+XÔøΩIiÔøΩ62ZÔøΩ\u0005\n~ÔøΩÔøΩ\u000fQ/ÔøΩÔøΩÔøΩÔøΩ\u0011ÔøΩktÔøΩÔøΩÔøΩ}kÔøΩÔøΩÔøΩ5:?\u0000L\u0000ÔøΩe\nÔøΩÔøΩÔøΩ.ASÔøΩ{ÔøΩZW_ÔøΩ\nVÔøΩ43r!\nÔøΩM}9ÔøΩÔøΩÔøΩ_OÔøΩ\"!À•ÔøΩoÔøΩÔøΩ\\ÔøΩ?ÔøΩt_qÔøΩvÔøΩtK\\$_.ÔøΩa\nÔøΩE}\nÔøΩÔøΩ\u0003ÔøΩ_C}ÔøΩÔøΩqÔøΩÔøΩÔøΩmqÔøΩ\u0016:uÔøΩ:ÔøΩ8ÔøΩÔøΩÔøΩ2ÔøΩ;ÔøΩo'>ÔøΩÔøΩ\u0005CÔøΩ\u0001ÔøΩ^ÔøΩ\u001fÔøΩ2ÔøΩcÔøΩxeÔøΩ9ÔøΩ \u001aÔøΩÔøΩÔøΩÔøΩ\u0010LÔøΩWÔøΩÔøΩ`jÔøΩÔøΩq\b>\nÔøΩÔøΩÔøΩÔøΩ\nÔøΩoLÔøΩ6ÔøΩ…∏Á¥≥:\u001bÔøΩ«´9ÔøΩÔøΩ_RÔøΩGÔøΩ\u001a\nÔøΩÔøΩ\nÔøΩÔøΩ\u0010>ÔøΩ*ÔøΩ\u0017¬ßÔøΩ+?OÔøΩÔøΩÔøΩÔøΩÔøΩWQÔøΩÔøΩÔøΩ?ÔøΩ\u001ahÔøΩ\u0003x5ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩVÔøΩÔøΩÔøΩÔøΩaÔøΩÔøΩ\u0011TDk}pÔøΩÔøΩ4ÔøΩÔøΩ7^ÔøΩ\u0017ÔøΩ{«ãz ÔøΩ7ÔøΩ%\nÔøΩ1^YÔøΩ6ÔøΩ\u001f\"a\bn\u001bÔøΩ2ÔøΩ%kÔøΩ\u001aWAÔøΩ=\u0010ÔøΩ\u00143◊é\u0017ÔøΩ\\5ÔøΩÔøΩIÔøΩ|ÔøΩ=\u000f\u0002?iÔøΩ+ÿç\u001ahuÔøΩq7ÔøΩd\nrÔøΩ\"rÔøΩÔøΩ6⁄éÔøΩ4ÔøΩc\u0003mÔøΩ\u000eÔøΩÔøΩP9ÔøΩÔøΩ<ÔøΩ\"ÔøΩ{ÔøΩÔøΩÔøΩ óIÔøΩ—üÔøΩaÔøΩqÔøΩo^ÔøΩÔøΩmÔøΩÔøΩ_ÔøΩÔøΩÔøΩ\u0007ÔøΩo\u001f#ÔøΩ\u001awÔøΩU4ÔøΩÔøΩGÔøΩ?vÔøΩÔøΩ|ÔøΩ3ÔøΩ7ÔøΩ;O\u0014M*!ÔøΩ’¢I\u0006ÔøΩÔøΩ<\u0002D\n\u0003\\\u0006\u001fu\n.ÔøΩt>Ô¶©OÔøΩ!\u0015ÔøΩÔøΩ@eÔøΩÔøΩQÔøΩ6ÔøΩÔøΩ^x\nw~ÔøΩÔøΩ\u0015ÔøΩ\nÔøΩ\n=ÔøΩ\nÔøΩMEKÔøΩk+\u000f:Wx'1$\nÔøΩPÔøΩÔøΩvVÔøΩ\u0013ÔøΩrÔøΩ^4ÔøΩWO\ntÔøΩÔøΩLÔøΩÔøΩDÔøΩÔøΩÔøΩGÔøΩ\u0005PÔøΩÔøΩMÔøΩÔøΩiÔøΩqÔøΩ\n)ÔøΩÔøΩPÔøΩH)Z\u0015ÔøΩ\u0015ÔøΩ(\u0016+J\u0015ÔøΩ\n^ÔøΩÔøΩÔøΩUd)3ÔøΩFÔøΩ^ÔøΩUÔøΩÔøΩJÔøΩ\\ÔøΩ)\u0019%Rf—õ*ÔøΩ\u0017ÔøΩÔøΩF\u0012ÔøΩ9ÔøΩs\u0014&G%1U\u001bAÔøΩ3XÔøΩÔøΩÔøΩ\u0019ÔøΩdÎò∫ÔøΩex,ÔøΩ\u000eÔøΩm\\6ÔøΩÔøΩÔøΩnRqqÔøΩÿ¢¬∫1e√µ[ÔøΩ`ÔøΩÔøΩ&x\u001acÔøΩÔøΩhÔøΩV`P\u0012ÔøΩÔøΩ>ÔøΩAÔøΩ\nb\nÔøΩsÔøΩÔøΩÔøΩ7ÔπßÔøΩ\t◊çÔøΩhCuÔøΩÔøΩÔøΩ'ÔøΩÔøΩ\u000fÔøΩ5ÔøΩÔøΩdÔøΩe\u0016ÔøΩ3XeÔøΩÔøΩXj*_Qs\u0015ÔøΩEÔøΩÔøΩ\n‹∑\\bÔøΩ—í7ÔøΩ@]ÔøΩ÷±ÔøΩÔøΩ5ÔøΩÔøΩ\u0010ÔøΩb^SÔøΩÔøΩÔøΩFÔøΩÔøΩÔøΩ«ò]LÔøΩÔøΩÔøΩ\u0018ÔøΩOÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÃÆÔøΩ\n$\nÔøΩXÔøΩ4ÔøΩÔøΩÔøΩ~@CÔøΩ$ h\u0013(ÔøΩÔøΩÔøΩ|<AÔøΩÔøΩR4`ÔøΩÔøΩ⁄ö#ÔøΩÔøΩ\"ÔøΩkx5A\u0002ÔøΩyÔøΩ\"uÔøΩeyÔøΩ\n(ÔøΩÔøΩ\u0004ÔøΩÔøΩ8ÔøΩÔøΩÔøΩÔøΩa\n\u0004\nÔøΩA,ÔøΩ0ÔøΩ0-ÔøΩ\u0006ZÔøΩAÔøΩhaÔøΩ\u0004ÔøΩÔøΩ\n(E^ÔøΩrdÔøΩ\u0017\u0010ÔøΩx\u0017ÔøΩ‰É≥ÔøΩnÔøΩÿú&ÔøΩÔøΩxq\u0013ÔøΩ\u0007ÔøΩYÔøΩÔøΩÔøΩ\u0003\\ ÔøΩ0JÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ/ÔøΩÔøΩÔøΩÔøΩ'ÔøΩÔøΩhoÔøΩÔøΩÔøΩk[‹µ1p-cw\u000fvYÔøΩFZyÔøΩHÔøΩ/H\u0002?ÔøΩÔøΩZZ€∫H\u0018ÔøΩÔøΩÔøΩÔøΩ\nÔøΩ\u0019kwÔøΩÔøΩGÔøΩmWIn#ÔøΩQwÔøΩ\u0011ÔøΩVÔøΩqÔøΩ6!V3\n\u0015ÔøΩÔøΩÔøΩhMÔøΩÔøΩ3ÔøΩÔøΩÔøΩ]RÔøΩﬁôÔøΩÔøΩÔøΩJaÔøΩIaÔøΩI]ÔøΩÔøΩ]%ÔøΩÔøΩ$?CÔøΩ#u’ëÔøΩÔøΩ\u0011ÔøΩÔøΩuÔøΩmXÔøΩÔøΩ\u001aÔøΩ\nQÔøΩeÔøΩWei8ÔøΩhÔøΩ0\u001fZÏÆ¶e9ÔøΩÔøΩÔøΩtr,vY>g?ÔøΩ!XÔøΩ4ÔøΩMcZÔøΩÔøΩ1\n8ÔøΩT\\]\\MÔøΩ`vÔøΩ$=D\u001bÔøΩ$ÔøΩÔøΩ\u0016ÔøΩÔøΩÔøΩÔøΩRÔøΩ\u0011ÔøΩMÔøΩeÔøΩÔøΩÔøΩÔøΩvÔøΩÔøΩÔøΩO&ÔøΩ)ÔøΩ\u0006\u0006\nÔøΩO\nXh\\\n&ÔøΩÔøΩÔøΩnlÔøΩ5€∂ÔøΩUÔøΩU÷é\t-5MÔøΩFÃÄÔøΩÔøΩ|ÔøΩ`|ÔøΩÔøΩJ&^ÔøΩÔøΩr_ÔøΩÔøΩ√ïÔøΩÔøΩÔøΩ&ÔøΩÔøΩx5ÔøΩÔøΩ|ÔøΩ9?ÔøΩÔøΩ;_ÔøΩÔøΩÔøΩÔøΩÔøΩrÔøΩpÔøΩ÷£BÔøΩÔøΩÔøΩÔøΩg\u0007ÔøΩÔøΩp\nÔøΩjkhÔøΩ\u0003\u0010ÔøΩÔøΩ\u001aHÔøΩ?\u0004\u0015$ÔøΩÔøΩÔøΩ\u0015\u000e\u0014.ÔøΩZÔøΩÔøΩÔøΩ@ÔøΩ≈†ÔøΩ\u0017ÔøΩLpnpÔøΩÔøΩ\u001aÔøΩÔøΩÔøΩwÔøΩÔøΩ\tÔøΩﬂÄÔøΩ/p\nÔøΩ\u0002ÔøΩÔøΩÔøΩ{\u0012ÔøΩ\u0004ÔøΩaÔøΩÔøΩÔøΩZKw\nÔøΩÔøΩÔøΩÔøΩ@ÔøΩÔøΩ-ÔøΩÔøΩÔøΩÔøΩ,ÔøΩÔøΩ0ÔøΩ!ÔøΩÔøΩÔøΩƒ∞vÔøΩ\u0018VVÔøΩX\nÔøΩ*UW\u001bXr6ÔøΩ8ÔøΩ?\u0004ÔøΩ\nÔøΩ?ÔøΩÔøΩ\u0014ÔøΩÔøΩ-aKhÔøΩ\u0003\"ÔøΩ6%QÔøΩ\u0010CÔøΩÔøΩmÔøΩ\u0014ÔøΩ)LÔøΩ`BÔøΩTÔøΩÔøΩ\u0010%≈ª$\u0018FÔøΩZÔøΩÔøΩÔøΩÔøΩ\u0011N¬∂?ÔøΩ`@ \u0000$\u001aÔøΩ$ÔøΩ\u0006HÔøΩÔøΩÔøΩ[ÔøΩÔøΩÔøΩ\nendstream\nendobj\n41 0 obj\n30666\nendobj\n40 0 obj\n44164\nendobj\n42 0 obj\n<</Type/XRef/W[1 4 2]/Size 43/Info 1 0 R/Root 2 0 R/ID[<3036DF5E6B7ECC418E1DD192C3D6C896><3036DF5E6B7ECC418E1DD192C3D6C896>]/Filter/FlateDecode/Length 163>>stream\nxÔøΩc`\u0000ÔøΩÔøΩÔøΩ\u0019\u0019\u0018DÔøΩ\u0018\u0018@ÔøΩ\n\bÔøΩ\u0019LÔøΩ\u0019ÔøΩ)\u0006A\bÔøΩ\u001aD1>`\u0002ÔøΩdÔøΩ!ÔøΩ10%ÔøΩ\nD1ÔøΩ∆Çyr\u0007ÔøΩÔøΩ|1ÔøΩRÔøΩ\u0007ÔøΩÔøΩÔøΩ\u000fÔøΩS,\u0004SJÔøΩ`Jy1ÔøΩRI\u0001SÔøΩ|\u0010J\nLÔøΩpÔøΩÔøΩ;ÔøΩ\tÔøΩIÔøΩ)I\u0011\bÔøΩ\bLIqA(ÔøΩ}\"S!T-DC9ƒÅ\n\u0010SÔøΩÔøΩÔøΩÔøΩW\u0010ÔøΩ\u0012ƒπ.\u0010J\u001bÔøΩÔøΩÔøΩ\u0010ÔøΩ\u0007D1ÔøΩÔøΩÔøΩPÔøΩÔøΩÔøΩÔøΩ(\u0003\u0003\u0000ÔøΩ\u0007\u001f:\nendstream\nendobj\nstartxref\n202773\n%%EOF"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://www.allaboutcircuits.com/industry-articles/fpgas-need-a-new-future/", "title": "FPGAs Need a New Future", "url": "https://www.allaboutcircuits.com/industry-articles/fpgas-need-a-new-future/", "published": "Fri, 19 Dec 2025 10:29:33 +0000", "text_source": "feed", "article_fetch_error": "http_403", "text": "<a href=\"https://news.ycombinator.com/item?id=46324269\">Comments</a>"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://blog.kierangill.xyz/oversight-and-guidance", "title": "Scaling LLMs to Larger Codebases", "url": "https://blog.kierangill.xyz/oversight-and-guidance", "published": "Mon, 22 Dec 2025 15:38:37 +0000", "text_source": "article", "article_fetch_error": null, "text": "How do we scale LLMs to larger codebases? Nobody knows yet. But by understanding how LLMs contribute to engineering, we realize that investments in guidance and oversight are worthwhile.\n- Guidance: The context, the environment.\n- Oversight: The skill set needed to guide, validate, and verify the implementor's1 choices.\nInvesting in guidance\nWhen an LLM can generate a working high-quality implementation in a single try, that is called one-shotting. This is the most efficient form of LLM programming.\nThe opposite of one-shotting is rework. This is when you fail to get a usable output from the LLM and must manually intervene.2 This often takes longer than just doing the work yourself.\nSo how do we create more opportunities for one-shotting? Better guidance.\nBetter guidance\nLLMs are choice generators. Every set of tokens is a choice added to your codebase: how a variable is named, where to organize a function, whether to reuse/extend/or duplicate functionality to solve a problem, whether Postgres should be chosen over Redis, and so on.\nOften, these choices are best left up to the designer (e.g., via the prompt). However, it's not efficient to exhaustively list all of these choices in a prompt. It's also not efficient to rework an LLM output whenever it gets these choices wrong.\nIn the ideal world, the prompt only captures the business requirements of a feature. The rest of the choices are either inferrable or encoded.\nWrite a prompt library\nA prompt library is a set of documentation that can be included as context for an LLM.\nWriting this is simple: collate documentation, best practices, a general map of the codebase, and other context an engineer needs to be productive in your codebase.3\nMaking a prompt library useful requires iteration. Every time the LLM is slightly off target, ask yourself, \"What could've been clarified?\" Then, add that answer back into the prompt library.\nA prompt library needs to strike the right balance between comprehensive and lean.\nThe environment is your context\nA peer at Meta told me that they weren't in a position to make Zuckerberg's engineering automation claims a reality. The reason is their codebase is riddled with technical debt. He wasn't surprised by this. Meta (apparently) historically has not prioritized paying down their debts.\nCompare this to the mentality from the Cursor team:\nI think ultimately the principles of clean software are not that different when you want it to be read by people and by models. When you are trying to write clean code you want to, not repeat yourself, not make things more complicated than they need to be.\nI think taste in code... is actually gonna become even more important as these models get better because it will be easier to write more and more code and so it'll be more and more important to structure it in a tasteful way.\nThis is the garbage in, garbage out principle in action. The utility of a model is bottlenecked by its inputs. The more garbage you have, the more likely hallucinations will occur.\nHere's a LLM literacy dipstick: ask a peer engineer to read some code they're unfamiliar with. Do they understand it? Do they struggle to navigate it? If it's a module, can they quickly understand what all that module exposes? Do they know the implications of using a certain function, the side-effects they must be aware of? No? Then the LLM won't either.\nHere's another dipstick: Ask an LLM agent to tell you how certain functionality works. You should know the answer before asking the LLM. Is their answer right? More importantly, how did they go about answering your question? Follow the LLM's trail and document its snags. You'll notice it tends to grep\n, ls\n, and cat\nto search. How can you give it a map so it isn't left to rediscover the codebase on each new prompt? When a map can't be given, how do you make it easier for them to navigate the codebase?\nHow you make the environment better suited for LLM literacy is dependent on the tech stack and domain. But general principles apply: modularity, simplicity, things are well-named, logic is encapsulated. Be consistent and encode these conventions in your prompt library.\nInvesting in oversight\nWe need guidance and oversight. A 3-ton truck with a middle-schooler behind the wheel puts people in the hospital (and in jail). This is why the mentality of automating engineers is objectionable. We should be fostering our teams, not discarding them.\nRemember, engineers operate on two timelines. As overseers of implementation, we must plan for the future of the codebase. If an LLM makes a choice, the overseer should be able to discern whether it was a good one or a bad one. For example, let's say the LLM opted to use Redis over Postgres to store some metadata. Was that a good choice? The overseer should know.\nAn investment in oversight is an investment in team, alignment, and workflows.\nFor team, it's worth investing in elevating everyone's design capabilities.\nDesign produces architecture. Architecture is a bet on the future. It's a bet that by setting up a program in a certain way, it will make the future feature development easier.\nArchitects are often created through experience. A career of shooting yourself in the foot builds intuition. This intuition shapes new software from having the same mistakes.\nAside: Some thoughts on how to grow design skills\nRead books, blogs, and code. Watch conference talks. Replicate masterworks. Practice by writing programs that you don't know how to build.\nOn replicating masterworks:\n- Student artists are often required to replicate masterworks. A masterwork is an art piece that an expert artist made. Through the process of replicating this masterwork, an artist gains practical experience at the bleeding edge of art. (This experience also builds confidence, which is a bonus.)\n- The same is true for engineering. I've learned more by writing a programming language than I have in reading a hundred blog posts.\n- Why does this work?\n- You understand a layer of abstraction deeper than the layer you're working at (this is a Cantrill-ism, but I can't find the quote).\n- Masters use techniques and workflows that are best learned via practice. Thorsten Ball taught me how to break down large problems into tractable phases. Each phase had a contract and each contract could be tested.\nOn reading code:\n- A good way to expand your vocabulary of solutions.\n- In Steve Ruiz's V1 of TLDraw, I learned the patterns necessary to later implement session-based undo/redo for an internal tool at work.\n- Reading code from leaders in the field is also a good way to build taste.\nOversight is not only about architecture, but also temperament, alignment to values, and workflows. Operators need to be both technical and product experts. Without a deep understanding of the product, it's easy to accidentally build the wrong solution.\nAutomating oversight\nSome design concerns can be checked programmatically.\nMoving more implementation feedback from human to computer helps us improve the chance of one-shotting. Agents can get feedback directly from their environment (e.g., type errors).\nThink of these as bumper rails. You can increase the likelihood of an LLM reaching the bowling pins by making it impossible to land in the gutter.\nOne way to do this is through writing safety checks. But what is safety? Safety is protecting your abstractions. Pierce's Types and Programming Languages has my favorite definition of safety:\nInformally, though, safe languages can be defined as ones that make it impossible to shoot yourself in the foot while programming.\nRefining this intuition a little, we could say that a safe language is one that protects its own abstractions.\nSafety refers to the language's ability to guarantee the integrity of these abstractions and of higher-level abstractions introduced by the programmer using the definitional facilities of the language. For example, a language may provide arrays, with access and update operations, as an abstraction of the underlying memory. A programmer using this language then expects that an array can be changed only by using the update operation on it explicitly‚Äîand not, for example, by writing past the end of some other data structure.\nWe tend to write tests for business-logic but don't always write tests for architecture-logic. Some programming languages have facilities for this built in.\nAddressing verification\nDesign and implementation are only two pieces of a project's lifecycle. Verification, like code review or QA, are necessary for building quality software.\nAs the volume of work increases, our ability to ship that work becomes constrained by our ability to review it.\nHere are some incomplete ideas for addressing the verification bottleneck:\n- Lowering the barrier of entry to perform manual QA (not needing a dev env).\n- Invest in a testing setup that makes it easy to express tests (including setting up tests, e.g., with test data creation) with minimal code.\n- Encode frequent PR feedback into documentation so that there is some level of PR review an LLM can reasonably do.\n- Security is baked in as defaults in the framework, not context.\nThat's it, for now\nThis was the third part of a series on LLMs in software engineering.\nFirst we learned what LLMs and genetics have in common. (part 1) LLMs don't simply improve all facets of engineering. Understanding which areas LLMs do improve (part 2) is important for knowing how to focus our investments. (part 3)\n-\nOr, in today's age, the generator's. ‚Ü©\n-\nNot being able to one-shot prevents adoption from many programmers. Programmers are disposed to seeing a worse solution and wanting to build their own. Oh, I can either pay $10/mo for a subscription to this SaaS tool, or I can build my own..? I choose to build my own, of course! (I am guilty of this). I think mentality partially explains the disparity between LLM skeptics and advocates. ‚Ü©\n-\nTechnical strategy is another form of context you can include in a prompt library. Though, you do risk bloating the context with many words, some of which aren't directly applicable. ‚Ü©\n-\nThis structure is motivated by this post and this video. If you are in the Django ecosystem, I recommend reviewing those. ‚Ü©\n-\nI can't help but make another software comparison to Phillip Ball's How Life Works: \"Modularity: Life never has to start from scratch. Evolution works with what is already there, even if this means redirecting it to new ends. We might (with great caution!) compare it to an electronic engineer who uses preexisting circuit components like diodes and resistors, and standard circuit elements such as oscillators and memory units, to create new devices. Thus life possesses a modular structure. This is most obvious in the way large organisms like us are assemblies of cells, as well as sharing common structures such as hearts and eyes. Modularity is an efficient way to build, since it relies on components that have already been tried and tested and permits the modification or replacement of one part more or less independently from the others.\" ‚Ü©"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://eli.thegreenplace.net/2025/plugins-case-study-mdbook-preprocessors/", "title": "Plugins case study: mdBook preprocessors", "url": "https://eli.thegreenplace.net/2025/plugins-case-study-mdbook-preprocessors/", "published": "Thu, 18 Dec 2025 12:47:05 +0000", "text_source": "article", "article_fetch_error": null, "text": "mdBook is a tool for easily creating books out of Markdown files. It's very popular in the Rust ecosystem, where it's used (among other things) to publish the official Rust book.\nmdBook has a simple yet effective plugin mechanism that can be used to modify the book output in arbitrary ways, using any programming language or tool. This post describes the mechanism and how it aligns with the fundamental concepts of plugin infrastructures.\nmdBook preprocessors\nmdBook's architecture is pretty simple: your contents go into a directory tree of Markdown files. mdBook then renders these into a book, with one file per chapter. The book's output is HTML by default, but mdBook supports other outputs like PDF.\nThe preprocessor mechanism lets us register an arbitrary program that runs on the book's source after it's loaded from Markdown files; this program can modify the book's contents in any way it wishes before it all gets sent to the renderer for generating output.\nThe official documentation explains this process very well.\nSample plugin\nI rewrote my classical \"nacrissist\" plugin for mdBook; the code is available here.\nIn fact, there are two renditions of the same plugin there:\n- One in Python, to demonstrate how mdBook can invoke preprocessors written in any programming language.\n- One in Rust, to demonstrate how mdBook exposes an application API to plugins written in Rust (since mdBook is itself written in Rust).\nFundamental plugin concepts in this case study\nLet's see how this case study of mdBook preprocessors measures against the Fundamental plugin concepts that were covered several times on this blog.\nDiscovery\nDiscovery in mdBook is very explicit. For every plugin we want mdBook to use, it has to be listed in the project's book.toml configuration file. For example, in the code sample for this post, the Python narcissist plugin is noted in book.toml as follows:\n[preprocessor.narcissistpy]\ncommand = \"python3 ../preprocessor-python-narcissist/narcissist.py\"\nEach preprocessor is a command for mdBook to execute in a sub-process. Here it uses Python, but it can be anything else that can be validly executed.\nRegistration\nFor the purpose of registration, mdBook actually invokes the plugin command twice. The first time, it passes the arguments supports <renderer> where <renderer> is the name of the renderer (e.g. html). If the command returns 0, it means the preprocessor supports this renderer; otherwise, it doesn't.\nIn the second invocation, mdBook passes some metadata plus the entire book in JSON format to the preprocessor through stdin, and expects the preprocessor to return the modified book as JSON to stdout (using the same schema).\nHooks\nIn terms of hooks, mdBook takes a very coarse-grained approach. The preprocessor gets the entire book in a single JSON object (along with a context object that contains metadata), and is expected to emit the entire modified book in a single JSON object. It's up to the preprocessor to figure out which parts of the book to read and which parts to modify.\nGiven that books and other documentation typically have limited sizes, this is a reasonable design choice. Even tens of MiB of JSON-encoded data are very quick to pass between sub-processes via stdout and marshal/unmarshal. But we wouldn't be able to implement Wikipedia using this design.\nExposing an application API to plugins\nThis is tricky, given that the preprocessor mechanism is language-agnostic. Here, mdBook offers some additional utilities to preprocessors implemented in Rust, however. These get access to mdBook's API to unmarshal the JSON representing the context metadata and book's contents. mdBook offers the Preprocessor trait Rust preprocessors can implement, which makes it easier to wrangle the book's contents. See my Rust version of the narcissist preprocessor for a basic example of this.\nRenderers / backends\nActually, mdBook has another plugin mechanism, but it's very similar conceptually to preprocessors. A renderer (also called a backend in some of mdBook's own doc pages) takes the same input as a preprocessor, but is free to do whatever it wants with it. The default renderer emits the HTML for the book; other renderers can do other things.\nThe idea is that the book can go through multiple preprocessors, but at the end a single renderer.\nThe data a renderer receives is exactly the same as a preprocessor - JSON encoded book contents. Due to this similarity, there's no real point getting deeper into renderers in this post."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://www.404media.co/flock-exposed-its-ai-powered-cameras-to-the-internet-we-tracked-ourselves/", "title": "Flock Exposed Its AI-Powered Cameras to the Internet. We Tracked Ourselves", "url": "https://www.404media.co/flock-exposed-its-ai-powered-cameras-to-the-internet-we-tracked-ourselves/", "published": "Mon, 22 Dec 2025 16:31:40 +0000", "text_source": "article", "article_fetch_error": null, "text": "I am standing on the corner of Harris Road and Young Street outside of the Crossroads Business Park in Bakersfield, California, looking up at a Flock surveillance camera bolted high above a traffic signal. On my phone, I am watching myself in real time as the camera records and livestreams me‚Äîwithout any password or login‚Äîto the open internet. I wander into the intersection, stare at the camera and wave. On the livestream, I can see myself clearly. Hundreds of miles away, my colleagues are remotely watching me too through the exposed feed.\nFlock left livestreams and administrator control panels for at least 60 of its AI-enabled Condor cameras around the country exposed to the open internet, where anyone could watch them, download 30 days worth of video archive, and change settings, see log files, and run diagnostics.\nUnlike many of Flock‚Äôs cameras, which are designed to capture license plates as people drive by, Flock‚Äôs Condor cameras are pan-tilt-zoom (PTZ) cameras designed to record and track people, not vehicles. Condor cameras can be set to automatically zoom in on people‚Äôs faces as they walk through a parking lot, down a public street, or play on a playground, or they can be controlled manually, according to marketing material on Flock‚Äôs website. We watched Condor cameras zoom in on a woman walking her dog on a bike path in suburban Atlanta; a camera followed a man walking through a Macy‚Äôs parking lot in Bakersfield; surveil children swinging on a swingset at a playground; and film high-res video of people sitting at a stoplight in traffic. In one case, we were able to watch a man rollerblade down Brookhaven, Georgia‚Äôs Peachtree Creek Greenway bike path. The Flock camera zoomed in on him and tracked him as he rolled past. Minutes later, he showed up on another exposed camera livestream further down the bike path. The camera‚Äôs resolution was good enough that we were able to see that, when he stopped beneath one of the cameras, he was watching rollerblading videos on his phone.\nThe exposure was initially discovered by YouTuber and technologist Benn Jordan and was shared with security researcher Jon ‚ÄúGainSec‚Äù Gaines, who recently found numerous vulnerabilities in several other models of Flock‚Äôs automated license plate reader (ALPR) cameras. They shared the details of what they found with me, and I verified many of the details seen in the exposed portals by driving to Bakersfield to walk in front of two cameras there while I watched myself on the livestream. I also pulled Flock‚Äôs contracts with cities for Condor cameras, pulled details from company presentations about the technology, and geolocated a handful of the cameras to cities and towns across the United States. Jordan also filmed himself in front of several of the cameras on the Peachtree Creek Greenway bike path. Jordan said he and Gaines discovered many of the exposed cameras with Shodan, an internet of things search engine that researchers regularly use to identify improperly secured devices.\nAfter finding links to the feed, ‚Äúimmediately, we were just without any username, without any password, we were just seeing everything from playgrounds to parking lots with people, Christmas shopping and unloading their stuff into cars,‚Äù Jordan told me in an interview. ‚ÄúI think it was like the first time that I actually got like immediately scared ‚Ä¶ I think the one that affected me most was as playground. You could see unattended kids, and that‚Äôs something I want people to know about so they can understand how dangerous this is.‚Äù In a YouTube video about his research, Jordan said he was able to use footage pulled from the exposed feed to identify specific people using open source investigation tools in order to show how trivially an exposure like this could be abused."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://github.com/FransFaase/MES-replacement", "title": "Show HN: C-compiler to compile TCC for live-bootstrap", "url": "https://github.com/FransFaase/MES-replacement", "published": "Wed, 17 Dec 2025 23:34:51 +0000", "text_source": "article", "article_fetch_error": null, "text": "The goal of this project is to simplify stage0 of live-bootstrap, which involves implementing a replacement for the GNU Mes compiler by implementing a C-compiler in C that can compile the Tiny C Compiler version 0.9.26.\nThe motivation for this project is given in the presentation Reviewing live-bootstrap.\nFor blog article related to reviewing the live bootstrap project and this project see the section 'Live-bootstrap' on this page.\nThe first stage of this project is to implement said C-compiler for i386.\nThe source of the C-compiler is the file tcc_cc.c\n.\nThis compiler produces intermediate code for a stack based language\ncalled Stack-C. The compiler also includes the file\nstdlib.c\n, that contains a minimal version of the\nC standard library.\nThe intermediage code can be compiled with the program\nstack_c.c\nto M1 assembly or interpreted with the\nprogram stack_c_interpreter.c\n.\nThis stage depends on a number of executables from stage0. Namely:\n- hex2\n- M1\n- blood-elf\n- catm\n- match\n- sha256sum\nThese need to be present in the directory of the repository.\nFurthermore it requires the usual Linux commands and the GNU C compiler.\nA makefile is included to build and test the\nC-compiler tcc_cc\n.\nThe sources of the Tiny C Compiler should be placed in a directory\nwith the name tcc_sources\nthat should also have sub directory\nlib\n.\nThere should also be a directory mes\nwith the contents of the\nGNU Mes compiler, which is needed to build the standard library\nthat the Tiny C Compiler needs.\nTo build and test the Tiny C Compiler, the test.sh\nshell script is provided. This script first compiles the Tiny C\nCompiler with GNU C-compiler, resulting in tcc_g\nand with\ntcc_cc compiler, resulting in tcc_s\n. Next is uses these to\nbootstrap the Tiny C Compiler from the sources. The script\ncompares the results for the various steps using tcc_g\nand\ntcc_c\n.\nRemark: The test.sh\nscript assumes that this repository is cloned\nin the git\ndirectory in the home directory. Please update the\nBINDIR\nto point to the repository containing the repository.\nThis stage has been implemented.\nThe second stage will focus on removing the dependency of the executables from stage0 by building these with the C-compiler.\nFor a first feasability study to analyzing what part of the C-grammar\nthat is needed for compiling TCC. For this I wrote a minimal C preprocessor:\nmin_tcc_preprocessor.cpp\nand CParser.c\nthat is heavily based on\nRawParser.\nThe work in this repository falls under the project Verifying and documenting live-bootstrap, which was funded through the NGI0 Core Fund, a fund established by NLnet with financial support from the European Commission's Next Generation Internet programme, under the aegis of DG Communications Networks, Content and Technology under grant agreement N‚Çí 101092990."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://arxiv.org/abs/2512.14693", "title": "Universal Reasoning Model (53.8% pass 1 ARC1 and 16.0% ARC 2)", "url": "https://arxiv.org/abs/2512.14693", "published": "Mon, 22 Dec 2025 18:59:22 +0000", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 16 Dec 2025]\nTitle:Universal Reasoning Model\nView PDF HTML (experimental)Abstract:Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at this https URL.\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://www.koi.ai/blog/npm-package-with-56k-downloads-malware-stealing-whatsapp-messages", "title": "Lotusbail npm package found to be harvesting WhatsApp messages and contacts", "url": "https://www.koi.ai/blog/npm-package-with-56k-downloads-malware-stealing-whatsapp-messages", "published": "Mon, 22 Dec 2025 22:35:45 +0000", "text_source": "article", "article_fetch_error": null, "text": "The lotusbail\nnpm package presents itself as a WhatsApp Web API library - a fork of the legitimate @whiskeysockets/baileys\npackage. With over 56,000 downloads and functional code that actually works as advertised, it's the kind of dependency developers install without a second thought. The package has been available on npm for 6 months and is still live at the time of writing.\nBehind that working functionality: sophisticated malware that steals your WhatsApp credentials, intercepts every message, harvests your contacts, installs a persistent backdoor, and encrypts everything before sending it to the threat actor's server.\nWhat gets captured:\n- Authentication tokens and session keys\n- Complete message history (past and present)\n- Full contact lists with phone numbers\n- Media files and documents\n- Persistent backdoor access to your WhatsApp account\nHow It Works\nThe Cover Is Real\nMost malicious npm packages reveal themselves quickly - they're typosquats, they don't work, or they're obviously sketchy. This one actually functions as a WhatsApp API. It's based on the legitimate Baileys library and provides real, working functionality for sending and receiving WhatsApp messages.\nObvious malware is easy to spot. Functional malware? That gets installed, tested, approved, and deployed to production.\nThe social engineering here is brilliant: developers don't look for malware in code that works. They look for code that breaks.\nThe Theft and Exfiltration\nThe package wraps the legitimate WebSocket client that communicates with WhatsApp. Every message that flows through your application passes through the malware's socket wrapper first.\nWhen you authenticate, the wrapper captures your credentials. When messages arrive, it intercepts them. When you send messages, it records them. The legitimate functionality continues working normally - the malware just adds a second recipient for everything.\nAll your WhatsApp authentication tokens, every message sent or received, complete contact lists, media files - everything that passes through the API gets duplicated and prepared for exfiltration.\nBut the stolen data doesn't get sent in plain text. The malware includes a complete, custom RSA implementation for encrypting the data before transmission:\nWhy implement custom RSA? Because legitimate WhatsApp libraries don't need custom encryption - WhatsApp already handles end-to-end encryption. The custom crypto exists for one reason: to encrypt stolen data before exfiltration so network monitoring won't catch it.\nThe exfiltration server URL is buried in encrypted configuration strings, hidden inside compressed payloads. The malware uses four layers of obfuscation: Unicode variable manipulation, LZString compression, Base-91 encoding, and AES encryption. The server location isn't hardcoded anywhere visible.\nThe Backdoor\nHere's where it gets particularly nasty. WhatsApp uses pairing codes to link new devices to accounts. You request a code, WhatsApp generates a random 8-character string, you enter it on your new device, and the devices link together.\nThe malware hijacks this process with a hardcoded pairing code. The code is encrypted with AES and hidden in the package:\nThis means the threat actor has a key to your WhatsApp account. When you use this library to authenticate, you're not just linking your application - you're also linking the threat actor's device. They have complete, persistent access to your WhatsApp account, and you have no idea they're there.\nThe threat actor can read all your messages, send messages as you, download your media, access your contacts - full account control. And here's the critical part, uninstalling the npm package removes the malicious code, but the threat actor's device stays linked to your WhatsApp account. The pairing persists in WhatsApp's systems until you manually unlink all devices from your WhatsApp settings. Even after the package is gone, they still have access.\nThey Really Didn't Want You Looking\nThe package includes 27 infinite loop traps that freeze execution if debugging tools are detected:\nThese traps check for debuggers, inspect process arguments, detect sandbox environments, and generally make dynamic analysis painful. They also left helpful comments in their code marking the malicious sections - professional development practices applied to supply chain attacks. Someone probably has a Jira board for this.\nFinal Thoughts\nSupply chain attacks aren't slowing down - they're getting better. We're seeing working code with sophisticated anti-debugging, custom encryption, and multi-layer obfuscation that survives marketplace reviews. The lotusbail\ncase isn't an outlier. It's a preview.\nTraditional security doesn't catch this. Static analysis sees working WhatsApp code and approves it. Reputation systems see 56,000 downloads and trust it. The malware hides in the gap between \"this code works\" and \"this code only does what it claims.\"\nCatching sophisticated supply chain attacks requires behavioral analysis - watching what packages actually do at runtime. When a WhatsApp library implements custom RSA encryption and includes 27 anti-debugging traps, those are signals. But you need systems watching for them.\nThis writeup was authored by the research team at Koi Security. We built Koi to detect threats that pass traditional checks but exhibit malicious behavior at runtime.\nBook a demo to see how behavioral analysis catches what static review misses.\nStay safe out there."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://spectrum.ieee.org/teenage-hackers", "title": "How the RESISTORS put computing into 1960s counter-culture", "url": "https://spectrum.ieee.org/teenage-hackers", "published": "Wed, 17 Dec 2025 19:19:00 +0000", "text_source": "article", "article_fetch_error": null, "text": "How the RESISTORS Put Computing into 1960s Counter-culture\nTeenage nerds in New Jersey were hacking before the PC and the Internet\nIn late April of 1968, a computer conference in Atlantic City, N.J., got off to a rocky start. A strike by telephone operators prevented exhibitors from linking their terminals to off-site computers, as union-sympathetic workers refused to wire up the necessary connections. Companies‚Äô displays were effectively dead.\nThis article is an adapted excerpt from W. Patrick McCray‚Äôs README: A Bookish History of Computing From Electronic Brains to Everything Machines (The MIT Press, 2025).MIT Press\nBut a small cohort of teenage computer enthusiasts from the Princeton, N.J., area flaunted a clever work-around: They borrowed an acoustic coupler‚Äîa forerunner of the computer modem‚Äîand connected it to a nearby pay phone. With this hardware in place, the youngsters dialed in to an off-site minicomputer.\nThe teenagers called themselves the RESISTORS, a retronym (they picked the moniker first and then matched words to the letters) for ‚ÄúRadically Emphatic Students Interested in Science, Technology, Or Research Studies.‚Äù The trade publication Computerworld gave the RESISTORS front-page billing‚Äî‚ÄúStudents Steal Show as Conference Opens‚Äù‚Äîand noted how the group drew a ‚Äúfascinated crowd‚Äù of computer professionals. A reporter even suggested that the RESISTORS represented the vanguard of a small-scale social movement as the teens sought to engage with their counterparts from ‚Äúunderprivileged areas of Trenton‚Äù and introduce them to personal computing.\nRESISTOR Peter Eichenberger works on a DEC PDP-8 computer, which Claude Kagan convinced the company to donate to the group.Chuck Ehrlich\nIn the modern history of computing, a story about a small cohort of teens ‚Äúplaying‚Äù with computers might seem tangential. But the previously untold history of the RESISTORS highlights the fact that, years before there were machines called personal computers, some people regularly accessed computers for activities unrelated to their professional lives. Motives varied, but entertainment as well as the display of technical prowess mattered. Just as important, the story of the RESISTORS expands our sense of the hobbyist community beyond later and better-known groups like the Bay Area‚Äôs Homebrew Computer Club.\nAn early computer club for teens\nFewer than 70 kids claimed membership in the RESISTORS over the group‚Äôs roughly decade-long existence. Nonetheless, a surprisingly large number of them went on to have careers in technology and science. Two members wrote books about computing that would sell millions of copies. Another member cofounded Cisco Systems, which got its start manufacturing Internet routers and other networking hardware and is now a multibillion-dollar business. Others became college professors or professional programmers. And starting around 1969, the RESISTORS became linked to computer pioneer Ted Nelson (more on that later).\nAn engineer named Claude Kagan was the nucleus around which the RESISTORS first organized. Born in 1924 in Orval, France, Kagan moved to the United States as a teen, served in the army, and earned an M.S. from Cornell University in 1950. He took a position with Western Electric, the manufacturing arm of AT&T, and in 1958, he moved to Hopewell Township, N.J., a short drive from Princeton.\nElectrical engineer Claude Kagan [second from left] encouraged the RESISTORS to learn computing, using the large collection of used equipment stored in his barn. Chuck Ehrlich\nKagan‚Äôs specialty was high-level computer languages, such as Fortran and BASIC, in which programmers write code that is largely independent of the particular type of computer. He was also an inveterate collector of old computers and other electronics, which he stored in a large red barn on his property that was also home to some donkeys and malamutes.\nChuck Ehrlich, one of the original RESISTORS and later an entrepreneur and venture capitalist, recalls that in late 1966, he and a small group of ‚Äúbrainy social outcasts‚Äù were looking for some sort of clubhouse. The kids weren‚Äôt interested in smoking pot or social protests, and they were disenchanted with the science classes offered at their local schools. But they were into electronics.\nKagan knew one of the teens‚Äô fathers and offered to let the group use his barn. They soon discovered Kagan‚Äôs collection of artifacts, including a surplus IBM paper tape punch, some analog telephone equipment, and a Friden Flexowriter (a kind of heavy-duty typewriter that could be linked to a computer).\nThe first computer the RESISTORS used was a Burroughs Datatron 205 mainframe, which occupied most of two walls in Kagan‚Äôs barn.David Gesswein\nBut the main attraction for the teens were Kagan‚Äôs computers. The most imposing of these was a Burroughs Datatron 205, a computer first manufactured in the mid-1950s and based on vacuum tubes. The enormous machine weighed several tons, and stories circulated about how Kagan had borrowed a tractor trailer to heroically transport the behemoth from Michigan to New Jersey.\nOnly slightly less imposing was an inoperable Packard Bell PB250, a refrigerator-size computer of more recent vintage that the teens managed to get working. Kagan also allowed the teens to connect to his employer‚Äôs DEC PDP-8 machine via teletype over phone lines so they could run programs written in TRAC (Text Reckoning And Compiling). Developed starting in 1959 by computer scientist Calvin Mooers, TRAC was an efficient language amenable to being run on machines that had relatively little memory. The teens were fond of connecting to the off-site computer and accessing a version of Joseph Weizenbaum‚Äôs ELIZA chatbot program.\nBeing able to work with computers interactively and in real time was generally unavailable to nonprofessional computer users at the time. Kagan eventually persuaded the Digital Equipment Corp. to donate a PDP-8‚Äîno trivial gift, as new models sold for US $15,000 or more‚Äîwhich the RESISTORS worked with in the barn.\nOne of the donkeys in Claude Kagan‚Äôs barn looks on as RESISTOR Doug Timbie works on some equipment.John A. Pietras/The Evening Times; Trenton Free Public Library\nThe bargain Kagan struck with the RESISTORS was unusual for several reasons. First, Kagan was gay, a fact that the teens (and their parents) were aware of but which, by all accounts, bothered no one. When the Hopewell Valley Jaycee-ettes held a house tour in April 1966, the brochure encouraged people to visit Kagan‚Äôs ‚Äúunique bachelor setting‚Äù that he shared with artist George Furnish. Furnish passed away around the time the RESISTORS were forming, and the grieving Kagan assumed multiple roles for the group: guru, mentor, publicity agent, and landlord. Kagan provided the space, while the teens were responsible for maintaining both it and the equipment as well as covering the cost of electricity.\nMost amateur computer clubs of the era were masculine spaces, but photographs of the RESISTORS almost always show one or more young women working at a terminal or solving a programming problem. When it came to deciding whose turn it was to use a machine, Jean Hunter‚Äîlater a professor of biological and environmental engineering at Cornell‚Äîlikened it to social time-sharing that required ‚Äúbeating people over the head to make them give you a turn.‚Äù John R. Levine, who was a RESISTOR before studying computer science at Yale and later coauthoring the bestseller The Internet for Dummies, recalled, ‚ÄúWe were so nerdy that it didn‚Äôt occur to us that girls [would] be any different in terms of what they could do.‚Äù\nThere were also efforts to recruit African American teens from schools in Trenton. One of these kids, Joseph Tulloch, provided quirky, Dr. Seuss-like illustrations for a programming manual that Kagan and the teens assembled and published. Tulloch later became a programmer for the state of New Jersey.\nNew members were initiated into the group by having an omega sign, the engineer‚Äôs symbol for electrical resistance, drawn on their face with a Magic Marker (these were teenagers after all). One of the first things a new member would learn was how to use TRAC to write programs. For his part, Kagan held a dim view of traditional learning as practiced in local classrooms. He instead insisted that the RESISTORS learn by doing. The group‚Äôs pedagogical approach came from the African American motto ‚ÄúEach one, teach one.‚Äù As one member recalled, ‚ÄúIf you want to teach someone how to do something, you had to let them sit at the keyboard.‚Äù\nThe RESISTORS‚Äô location in the Princeton area contributed to their success. Several members had parents employed at nearby technology companies, such as AT&T and RCA. Others, such as Nat Kuhn, had parents who worked at Princeton University. Kuhn‚Äôs father was Thomas Kuhn, a historian and author of The Structure of Scientific Revolutions (1962), the landmark book that introduced ‚Äúparadigm shift‚Äù into the vernacular.\nTwelve-year-old Nat Kuhn was just 10 when he joined the RESISTORS. ‚ÄúI was super geeky,‚Äù he later recalled. David Fox\nAs a kid, Nat built devices from hobbyist electronics kits with his father, a former physicist. Nat joined the RESISTORS after attending an open house the group sponsored in February 1968 at the Princeton Junior Museum. He was just 10 years old at the time. ‚ÄúI was super geeky,‚Äù he recalled, ‚Äúand the computer became my hobby and obsession. You could understand things through it and make things happen.‚Äù\nSoon after Nat had his face inked with an omega sign, another person, much older but just as passionate about personal computing, started showing up at Claude Kagan‚Äôs barn.\nTed Nelson and the birth of hypertext\nTed Nelson had majored in philosophy at Swarthmore College, graduating in 1959, and then studied sociology at the University of Chicago and later Harvard, where he took his first computer course. Nelson‚Äôs 2010 autobiography includes a whole chapter, titled ‚ÄúThe Epiphany of Ted Nelson,‚Äù about this revelatory experience. When he realized that the computer, instead of a dreary number-crunching device, ‚Äúcould be whatever it was programmed to be,‚Äù his ‚Äúworld exploded.‚Äù\nTed Nelson met the RESISTORS in the late 1960s, when he was developing his ideas around hypertext and globally interconnected networks for publishing.Ted Nelson\nNelson had a penchant for writing, and so an even bigger revelation was that computers could handle text by manipulating, storing, printing, and, above all, displaying it on screens. And, if this could be done with text, it could probably also be done with images and sound. ‚ÄúThe future of mankind was at the computer screen,‚Äù he decided, as the ‚Äúinteractive computer would become the workplace of the future.‚Äù\nEqually profound for Nelson was recognizing that once a person had text on a computer screen, they could use it to construct parallel, nonsequential textual passages. These word assemblages could then be linked to one another or branch off in entirely new directions‚Äîa farsighted idea for the time.\nIn 1964, Nelson accepted a teaching position at Vassar College, where his new colleagues invited him to describe how the future of work and artistic creativity would happen on computer screens. In the promotional flyer for the talk, he introduced a new word: hypertext.\nSome of the ideas that Ted Nelson discussed with the RESISTORS later turned up in Nelson‚Äôs opus Computer Lib/Dream Machines.Microsoft Press\nAs Nelson defined it in a 1965 paper, hypertext meant ‚Äúa body of written or pictorial material interconnected in such a complex way that it could not conveniently be presented or represented on paper.‚Äù Almost any topic could, in principle, be represented on a computer screen with ‚Äúlinks‚Äù connecting one entry to another, along with annotation, footnotes, and summaries, while also including ‚Äúevery feature a novelist or absent-minded professor could want.‚Äù\nRELATED VIDEO: Ted Nelson on What Modern Programmers Can Learn From the Past\nNelson imagined that his system of information storage, retrieval, and documentation could ‚Äúgrow indefinitely,‚Äù containing more and more of the world‚Äôs knowledge while revealing important connections between all of the entries.\nNelson soon quit Vassar and started raising money and his professional profile. His goal was to design and implement a universal text handling, publishing, and globally connected electronic library system, which he named Project Xanadu, from Samuel Taylor Coleridge‚Äôs ‚ÄúKubla Khan.‚Äù (It‚Äôs also the name of Charles Foster Kane‚Äôs mansion in Orson Welles‚Äôs 1941 classic, Citizen Kane.) Xanadu would turn into Nelson‚Äôs lifelong obsession.\nA convergence of art and computers\nThe catalyst that brought Nelson together with Claude Kagan and the RESISTORS wasn‚Äôt some new computer but an avant-garde art show. In the fall of 1970, a lavish new exhibition titled Software opened at the Jewish Museum in New York City. Museum director Karl Katz handpicked the influential art theorist Jack Burnham to curate the show. Burnham, in turn, was inspired by Norbert Wiener‚Äôs cybernetic concepts and wanted to explore how conceptual artists might experiment with new computing technologies, such as ‚Äúreal-time computing‚Äù and ‚Äúinteractivity,‚Äù in a gallery setting. The exhibition gave thousands of visitors an opportunity to see, and in some cases use, minicomputers, teletype equipment, high-speed copy machines, and closed-circuit television.\nWhen the Jewish Museum launched an ambitious art and tech exhibition in 1970, members of the RESISTORS collaborated with artists and provided tech support. The Jewish Museum\nA contributor to the show and its technical adviser, Ted Nelson recruited the RESISTORS to help him and some of the artists. As he later wrote in his influential 1974 book Computer Lib/Dream Machines, ‚ÄúSome people are too proud to ask children for information. This is dumb. Information is where you find it.‚Äù For Agnes Denes, a Hungarian-born conceptual artist, the teens coded a minicomputer to animate triangles on a screen for a piece called Trigonal Ballet. For conceptual artist Carl Fernbach-Flarsheim, the teens used the I Ching to program a piece called Conceptual Typewriter. A visitor could select one of several buttons, such as ‚Äúthe silent‚Äù (represented by a circle) or ‚Äúthe providing‚Äù (illustrated by sheaves of wheat), and then use a light pen to alter the image. Both artists provided the initial ideas, but the RESISTORS executed them.\nNelson, working with programmer Ned Woodman, contributed a piece titled Labyrinth. Running on a PDP-8 that DEC provided, Labyrinth was explained as ‚Äúthe first public demonstration of a hypertext system.‚Äù To use it, a visitor would sit at a terminal and begin reading the displayed text. For the passage ‚ÄúThe exhibition you are attending is called Software. It was organized by Jack Burnham,‚Äù you could use keystrokes (such as F for forward) to navigate the text and retrieve a definition of ‚Äúsoftware‚Äù or biographical details about Burnham.\nConceptual artist Agnes Denes [right] programmed her piece Trigonal Ballet at the Jewish Museum with help from RESISTORS [from left] Peter Eichenberger, J Laurence Sarno, and John Levine.The Jewish Museum\nFor many museumgoers, the entire exhibition suggested a technological future where people easily navigated the information-rich realm of what would become known as cyberspace.\nThe RESISTORS, meanwhile, gradually faded throughout the 1970s as its members went off to college and the supply of new recruits dwindled. Nonetheless, members like Nat Kuhn and John Levine recall that ideas they bantered about in bull sessions with Nelson in Kagan‚Äôs barn materialized later in the pages of Computer Lib/Dream Machines. ‚ÄúThere was certainly very little in that book that we hadn‚Äôt already heard about before it appeared,‚Äù Levine said.\nWhen I talked with former RESISTORS, it was surprising to hear how many members remained in touch with one another more than a half-century later. Many of them still included their participation on r√©sum√©s. Courtships formed, and at least two members married each other. Their activities left a long-lasting echo in the world of computing as well. Len Bosack cofounded Cisco Systems. Cynthia Dwork, a professor of computer science at Harvard, made pioneering contributions to cryptography. Steve Kirsch was one of two people to invent the optical mouse and went on to become a successful tech entrepreneur.\nEven as the RESISTORS were fading as a group, massive technological changes were just over the horizon. Personal computers, introduced in the early 1970s, soon became consumer goods found in hundreds of thousands of homes. That technological revolution would be solidified when Time named the PC ‚ÄúMachine of the Year‚Äù in 1982. New computing worlds beckoned to experts and neophytes alike, but it was a future that a group of teens in a New Jersey barn had already seen and lived.\nThis article is adapted from the author‚Äôs new book, README: A Bookish History of Computing from Electronic Brains to Everything Machines (The MIT Press, 2025)."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://dfarq.homeip.net/the-biggest-crt-ever-made-sonys-pvm-4300/", "title": "The biggest CRT ever made: Sony's PVM-4300", "url": "https://dfarq.homeip.net/the-biggest-crt-ever-made-sonys-pvm-4300/", "published": "Mon, 22 Dec 2025 12:54:29 +0000", "text_source": "article", "article_fetch_error": null, "text": "Move over, GE Widescreen 1000. In 1989 in Japan, Sony introduced to the largest Trinitron CRT ever built, the KV-45ED1, also known as the PVM-4300. And in 1990, they imported 20 of them to the United States, just in time for the recession. About 34 years later, one of these enigmatic TVs surfaced.\nSony‚Äôs PVM-4300/KV-45ED1\nSony‚Äôs part number suggests it has a 45 inch tube inside. But in a rare case of truth in advertising, Sony advertised it as a 43-inch model. It weighed about 450 pounds, stood about 27 inches tall, and it wouldn‚Äôt fit through a standard door frame. That‚Äôs probably okay, it‚Äôs not like someone was going to use this as a bedroom TV. This thing was going in your living room.\nIn Japan, it sold for 2.6 million yen, but in the United States, it retailed for $40,000, a significant markup. To be fair, shipping them across the Atlantic and then throughout the United States must have been expensive. And news articles in 1990 said Sony dealers would not allow any bickering. They would throw in a couple of options like the separate tuner or speakers. But no discounts.\nSony said at the time they hoped to sell 80 of them that year, but the recession may have kept that from happening.\nThe biggest conventional CRT ever\nThe Sony PVM-4300 was a conventional CRT, unlike the GE Widescreen 1000, which was a projection set. Projection TVs could be bigger and cheaper. But if you wanted the clearest picture, a big CRT was where it was at.\nIt was a conventional CRT that worked with over the air signals, but like many larger TVs of the era, it used a technology called IDTV to enhance the picture quality. The ‚ÄúID‚Äù stood for ‚Äúimproved definition.‚Äù IDTV sets had a buffer so they would store successive frames and interpolate them rather than interlacing them the way a conventional CRT TV worked. They also had circuitry to detect motion and perform image stabilization to further enhance the image. The result wasn‚Äôt as good as HDTV. But it gave high rollers a better picture until HDTV. HDTV arrived in 1998, but articles at the time estimated 2005. The Chicago Tribune warned in 1990 that these $40,000 TVs would be obsolete in 15 years, but the salesperson countered that every TV would be obsolete in 15 years.\nIt‚Äôs also likely that someone in the market for a $40,000 TV didn‚Äôt worry about obsolescence. In 1990, the GE Widescreen 1000 looked dated and it wasn‚Äôt 15 years old yet.\nWhy so expensive?\nThe KV-45ED1 or PVM-4300 cost about 8 times as much as Sony‚Äôs second most expensive model at the time, which had a 29-inch screen. That‚Äôs largely because the KV-45ED1 had to be built by hand. Sony could mass produce its smaller TVs. This was a product for buyers who weren‚Äôt worried about the price.\nSony continued making CRTs into the 21st century, bowing out with its high-def KD-34XBR970, 36-inch KD-36FS170, 32-inch KV-32FS170 and 27-inch KV27FS170 in February 2006.\nIt is unclear how many of these enormous 43-inch units Sony sold, and some people even questioned if it was ever built. A Chicago area dealer told the Chicago Tribune in 1990 that someone had purchased one, but that the buyer wanted to remain anonymous. That was good enough for me; a TV dealer wasn‚Äôt going to tell a newspaper that they have a $40,000 item and then not have it. That‚Äôs just bad business. Good business is taking the free advertising, having an example on display to show knowing most won‚Äôt buy it, but they may buy one of the smaller units. But luring someone into the store with a lie makes it much more difficult to sell anything.\nAnd the Tribune wasn‚Äôt going to make something like this up. It would anger the TV dealers and risk losing their advertising. And someone who could afford a $40,000 TV was likely a business owner or high-ranking executive who could pull their advertising. In the days of print newspapers, advertisers and potential advertisers held a lot of sway. This could be both good and bad. I‚Äôm not going to say capitalism solves every problem but this was a case where it helped keep people honest.\nShank Mods‚Äô surviving Sony PVM-4300\nOn December 22, 2024, Youtuber Shank Mods released a video telling the story of a Sony PVM-4300 and how he acquired it. One of the photos of a purported surviving unit turned out to be very real. It was taken in a restaurant in Japan, and the owner was actually aware of the photo. Unfortunately the restaurant was having to move, and needed to get rid of the set. Shank Mods was able to contact some people in Japan who could help race against time and remove the TV from the restaurant and then ship it to the United States.\nThe 35-minute video is well worth watching if you have interest in vintage CRTs, or even if you just like stories of strangers coming together and helping each other just for the sake of being helpful. Actually, I take that back. At the end of the video, Shank Mods played a prank on his fellow CRT fans that is absolutely hilarious and makes the video worth watching for that reason alone. I won‚Äôt ruin it for you.\nWe can only guess how many other examples may survive. But we now know that at least one survives and is in the hands of a retro hobbyist.\nDavid Farquhar is a computer security professional, entrepreneur, and author. He has written professionally about computers since 1991, so he was writing about retro computers when they were still new. He has been working in IT professionally since 1994 and has specialized in vulnerability management since 2013. He holds Security+ and CISSP certifications. Today he blogs five times a week, mostly about retro computers and retro gaming covering the time period from 1975 to 2000."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://www.space.com/space-exploration/satellites/satellites-reveal-heat-leaking-from-largest-us-cryptocurrency-mining-center", "title": "Satellites reveal heat leaking from largest US cryptocurrency mining center", "url": "https://www.space.com/space-exploration/satellites/satellites-reveal-heat-leaking-from-largest-us-cryptocurrency-mining-center", "published": "Mon, 22 Dec 2025 23:23:12 +0000", "text_source": "article", "article_fetch_error": null, "text": "Satellites reveal heat leaking from largest US cryptocurrency mining center\n\"The world needs better ways to understand what's actually happening on the ground.\"\nOne of the world's largest Bitcoin mining facilities is seen leaking heat into the environment in a new image captured from orbit by a heat-seeking satellite that was recently released by the U.K.-based company SatVu.\nThe image reveals the thermal footprint of a major Bitcoin-mining data center in Rockdale, Texas, which has been widely criticized for its electricity consumption and carbon footprint.\nSatVu didn't disclose which specific facility is in the image, but Rockdale is home to the Riot Platforms Bitcoin mine. The facility, considered the largest in the U.S., has an energy consumption of 700 megawatts, requiring about as much electricity as 300,000 homes.\nThe satellite image reveals in a resolution of 11.5 feet (3.5 meters) where and how much heat leaks into the environment from the plant. SatVu thinks that the insights that could be gleaned from such images could help regulators and grid operators better understand the impact such facilities have on the environment and local power networks.\n\"Today's data center buildout is moving incredibly quickly, and the world needs better ways to understand what's actually happening on the ground,\" Thomas Cobti, SatVu's VP for Business Development, said in a statement. \"Thermal data gives an objective view of operational activity as it occurs ‚Äî not weeks later through reports or announcements.\"\nAlthough the image was only released on Dec. 17, it was most likely captured already in 2023, before SatVu's HotSat-1 satellites failed in orbit in December that year. SatVu plans to launch its replacement HotSat-2 next year and is already building HotSat-3.\nThe thermal camera aboard these satellites is the best in class, providing an order of a magnitude better resolution than other temperature-measuring devices in orbit.\nBreaking space news, the latest updates on rocket launches, skywatching events and more!\nSatVu released the first HotSat-1 images in October 2023, capturing the heat trails behind locomotives and showing how heat spreads from large sun-drenched concrete parking lots in a city like Las Vegas.\nWith the newly released image, the company shows how satellites could keep an objective eye on a fast-growing and controversial sector.\n\"At a closer level, [the image] reveals which substations and cooling systems are under load ‚Äî clear, physical indicators of real operational behavior,\" the company said in the statement. \"Together, these layers provide a grounded, evidence-based view of how major data center sites are evolving in real time.\"\nA closer inspection of the image shows \"distinct thermal signatures across rooftop chillers, transformers and electrical yards, making clear which parts of the facility are active and which remain dormant,\" SatVu added.\nAccording to the McKinsey consultancy, investment into computing data centers will continue to grow, reaching more than $7 billion by 2030. Global data centers are believed to contribute by about 0.5% to the global carbon dioxide emissions. Bitcoin mining is especially energy intensive, a recent study estimated that one Bitcoin transaction generates about as much carbon dioxide as a gasoline car generates in a 1,600-mile (2,500-kilometer) drive.\nTereza is a London-based science and technology journalist, aspiring fiction writer and amateur gymnast. She worked as a reporter at the Engineering and Technology magazine, freelanced for a range of publications including Live Science, Space.com, Professional Engineering, Via Satellite and Space News and served as a maternity cover science editor at the European Space Agency.\nYou must confirm your public display name before commenting\nPlease logout and then login again, you will then be prompted to enter your display name."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://enzom.dev/b/passkeys/", "title": "Things I learnt about passkeys when building passkeybot", "url": "https://enzom.dev/b/passkeys/", "published": "Mon, 22 Dec 2025 18:58:45 +0000", "text_source": "article", "article_fetch_error": null, "text": "Things I learnt about passkeys when building passkeybot\nI recently released passkeybot.com, a hosted sign in page that allows you to add passkey auth to your site with just a few server side HTTP handlers.\nHere are the things I learnt in the process.\nWhat Secure Enclave Processors (SEP) are\nApple devices have secure enclaves which are like a separate tiny computer living inside the main CPU that has its own isolated encrypted memory and OS. It can create secrets that never leave the secure enclave. The main OS can only prove it has possession of that secret by asking the secure enclave to sign some data and getting the signature as a response (it can only use this message protocol with the SEP).\nWhen the user signs in with their passkey with User Verification = true, the SEP requires a biometric/passcode auth first before signing the data with the private key.\nOther devices have something similar to the SEP, but are branded with different names.\nPhone SIM cards are actually a form of secure element. SIM cards are CPUs that run a stripped down version of Java, and use the same principle of ‚Äúsecrets can never leave the SIM‚Äù and ‚Äúprove possession with message signing‚Äù.\nUser Presence (UP) vs User Verification (UV)\nPresence means ‚Äúthe user tapped a button and was there‚Äù, verification means ‚Äúthe user entered their biometric or passcode‚Äù.\nYou can request which one you require with the JS passkey API.\nThe difference is presence can be faked by anyone with the unlocked device by pressing a button, but verification always requires the re-auth of the user with biometrics or a passcode.\nWhat an authenticator is\nAn authenticator is the hardware and software that holds the private/public key pairs and signs the passkey challenge to prove it has the private key. On Apple devices that is the SEP.\nThe browser asks the user which authenticator they want to use, then uses OS level APIs to interact with the chosen authenticator.\nFor example:\n- User chooses on-device Apple SEP ‚Üí site calls JS API ‚Üí browser uses Swift API for passkey operations.\n- User chooses Yubikey ‚Üí site calls JS API ‚Üí browser uses Yubikey API over USB for passkey operations\nThe interesting thing here is that the JS API normalises all these different possible authenticator APIs. Under the hood the browser implements all the possible API protocols for different authenticators.\nThe Chrome Dev Tools also has a virtual authenticator to bypass reptetive OS password entry for testing.\nWhat attestation is\nSigning proves possession: Being able to sign with the private key proves you have possession of it.\nAttestation proves device hardware used: Attestation proves what hardware and software combination created the passkey pair. It allows enforcing policies for what set of hardware devices are trusted, and which are blocked.\nThe issue is that attestation data also allows fingerprinting as it reveals exactly what hardware the user is using.\nHardware attestation only occurs for the creation of the passkey pair (not on every auth). This creates an issue: if keys are synced to another device, the attestation is no longer valid. So if you require strict attestation that specific hardware is used, you create a new keypair for every device instead of allowing use of a passkey pair that has moved to a non-attested device.\nWithout attestation the secure enclave is still used, it is just not proven to the webauthn client API.\nApple hardware has attestation disabled by default unless you have enterprise device management enabled. This lets enterprises define an allow-list of trusted authenticator hardware.\nPasskeys are just for authentication, not for general signing of intent\nWhen the user authenticates with a passkey, they sign a challenge that is a hash unique to that particular sign in flow. The challenge hash needs to have 16 bytes or more of random data to avoid replays, but it can also include a hash over other metadata.\nThe authenticator GUI only shows ‚Äúsign in to your_domain.com‚Äù. It never allows a more general ‚Äúsign this content for your_domain.com‚Äù. E.g. ‚Äúsign this transaction request to move ¬£50 to Bob's account‚Äù.\nThe JS code must be secure, but it cannot be verified\nIf the JS code of a site is compromised, the attacker can read all personal data. They could also trick the user into signing something with their authenticator (as the authenticator does not show the user what they are actually signing) - this leads to a real private key signing over faked challenge data.\nThe browser has Subresource Integrity (SRI) which only allows executing JS scripts with a given hash. But the root document HTML is not checked in that case, which means the attacker can change the SRI hashes to match their own JS. Chrome Extensions also allow injecting JS.\nIt would be interesting if authenticators could also attest as to what HTML/JS was loaded on the page to rule out that they have been compromised.\nImmediate mediation - an upcoming ‚Äúfast sign in‚Äù API\nThis Chrome origin trial will add an option the passkey API:\nnavigator.credentials.get({mediation: \"immediate\"})\nThis allows you to sign in a user who already has a passkey quickly.\nIf they do not have a passkey, you can decide what to do from JS (instead of having the browser show them a UI to find passkeys on other devices).\nThere is no way to get a list of the user's passkeys from JS - lists are always shown from the browser UI.\nThe immediate mediation option allows your JS to get an immediate ‚Äúthe user has 0 local keys‚Äù message without any user interaction:\n- 0 keys: Immediate JS response with NotAllowedError, JS decides next step.\n- 1 key: Immediately ask the user to sign in with that one.\n- >1 key: Ask the user to choose.\nRelated Origin Requests\nRelated Origin Requests\nallow you as a domain owner to define a list of other domains that can\ncreate passkeys for your domain. They work by having you serve the\nlist from /.well-known/webauthn\n.\nThis is what passkeybot.com uses to allow domain owners to grant permissions to passkeybot.\nBut RORs do not work over HTTP, only HTTPS. The reason is the authenticator requests the well-known file over HTTPS only, so localhost will not work.\nThey are also not supported in iOS 18 or Firefox.\nIf an authenticator creates a passkey for the root domain, that passkey will work for all subdomains. But if it creates it for a subdomain, it only ever works on that specific subdomain.\nThe counter is just a ‚Äúheuristic‚Äù\nAuthenticators store a counter which increments for each passkey usage. In theory this can detect a cloned authenticator. But much of the recommendations say to use the counter as a heuristic rather than evidence of a cloned authenticator because there are many legitimate reasons the counter can be wrong. I think in practice this counter is often ignored.\nUse passkeys stored on nearby devices using Bluetooth\nYou can sign into a public computer that does not have your passkeys by having your own device's authenticator communicate with it over Bluetooth Low Energy (BLE). Bluetooth is used to assert your close proximity with the device you are signing into. Your keys never leave your device, the signing protocol travels between the two devices.\nDeleting passkeys with the Signal API\nThe JS API cannot list or modify the list of passkeys, only the browser GUI or Apple Passwords can do that.\nBut you can asynchronously signal that you want to delete a passkey. It is only a hint, and you will not receive back any confirmation as that may leak user data.\nThe Signal API methods currently are:\nPublicKeyCredential.signalUnknownCredential({ rpId, credentialId }) PublicKeyCredential.signalAllAcceptedCredentials({ rpId, userId, allAcceptedCredentialIds }) PublicKeyCredential.signalCurrentUserDetails({ rpId, userId, name, displayName })\nuser.id and userHandle represent ‚Äúone account‚Äù\nThe user.id\nand userHandle are the same value, but with\ndifferent names in different JS API calls.\nThey are used to map many passkeys to a single logical account. It\nshould be set and stored as passkey APIs require the\nuser.id\n(like the signal APIs above).\nYou can generate one unique user.id\nper new passkey and\njust store the user => passkey relation in your database, but this\nmay prevent ‚Äúper account‚Äù passkey grouping and management in the\nbrowser UI.\ncrypto.subtle.generateKey can create non-extractable keys\ngenerateKey is a JS API that allows you to create new key pairs, where the private key cannot be extracted similar to passkeys.\ncrypto.subtle.generateKey(algorithm, extractable, keyUsages)\nYou can perform general operations like signing with this key pair, but in the case of JS being compromised, the private key cannot be read and moved to a different device. But compromised JS can still sign using that unextractable private key.\nPKCE = \"Proof Key for Code Exchange‚Äù was retrofitted into OAuth\nPKCE is a protocol that works like a one time password: at the start of every sign in flow an actor creates a code_verifier and code_challenge.\nThe code_verifier is a secret random 32 bytes held on the flow initiating actor. The code_challenge is the sha256 hash of those bytes, and is shared with the user going through the sign in flow.\nThe code_challenge is also sent to the auth service that verifies the user and creates a (token, code_challenge).\nPKCE protects this token by only allowing the holder of the code_verifier secret to redeem it by sending (token, code_verifier).\nThis means even if the token is stolen, only the actor that started the flow can redeem it with the auth API.\nPKCE was originally designed for environments that cannot hold static secrets because the source code can be read - like JS or desktop apps. Instead of embedding static secrets, these actors dynamically create secrets at runtime at the start of each sign in flow (the code_verifier and code_challenge pair).\nPasskeybot uses PKCE to avoid having to manage API bearer token secrets for each API client. Note: Passkeybot uses the general principle behind PKCE, but naming and interaction differs from the OAuth standard.\nIt is interesting how they managed to retrofit PKCE into the OAuth standard to solve the ‚Äútoken interception‚Äù problem. It only requires a sha256 hash function so is very easy to implement.\nDigital Credentials API is a browser bridge to the native OS wallet\nThis is a passkey adjacent JS API. The Digital Credentials API will allow you to request things from the user's native OS wallet like IDs, tickets, badges, membership cards. It allows you to do things like prove your age or ability to drive without having to share your actual identity cards."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://yalereview.org/article/in-pursuit-of-clancy-sigal", "title": "In Pursuit of Clancy Sigal (2021)", "url": "https://yalereview.org/article/in-pursuit-of-clancy-sigal", "published": "Mon, 22 Dec 2025 20:21:12 +0000", "text_source": "article", "article_fetch_error": null, "text": "The golden notebook\nIn Pursuit of Clancy Sigal\nA writer‚Äôs radical life\nTodd Gitlini first encountered Clancy Sigal, in a manner of speaking, in 1963, during my last semester in college. That‚Äôs when I picked up, and devoured, Doris Lessing‚Äôs novel The Golden Notebook, which featured a character named Saul Green who was widely known to have been based on Sigal. At the time, I was a fervent and brooding left-wing activist with two years of political organizing under my belt (mostly trying to ban the bomb). So I was, unsurprisingly, enthralled by Lessing‚Äôs book, which took left-wing politics and writing seriously, as human facts, not ‚Äúbackground.‚Äù And I was mesmerized by the broken expat Green, a Communist maudit, former union organizer, blacklisted Hollywood agent, and blocked writer, attached to left-wing ideals despite reasons not to be. I was especially moved as he and Lessing‚Äôs protagonist Anna Wulf plunge into a transformative folie de deux, ‚Äúa cocoon of madness.‚Äù\nSaul can no longer find fellowship with his old comrades, who have settled, ‚Äúall married or successful and having drunken private conversations with themselves.‚Äù He won‚Äôt settle. He tosses up in London, sick and insomniac, a political refugee and scrambling neurotic. In Anna‚Äîherself a blocked novelist trying to dig out of a rubble of Communist faith, bad relationships, and war terror‚Äî he finds a kindred, equally disassembled spirit. She takes him in, impressed by ‚Äúhis jaunty soldier air,‚Äù coiled as he is, ‚Äúhis energies‚Ä¶ absorbed in simply holding himself together,‚Äù ‚Äúhis cool grey eyes on guard.‚Äù But he‚Äôs also, she will conclude, a ‚Äúmonster,‚Äù prone to ‚Äúcold moment[s] of pure hostility,‚Äù ‚Äújeering and sneering‚Äù at her ‚Äúmiddle-class‚Äù origins. He lectures her, and she likes being lectured to. ‚ÄúSaul,‚Äù she says, ‚Äúwe‚Äôre very bad for each other.‚Äù\nHistory having abandoned them, they lurch into each other‚Äôs arms as lovers, accusers, and confessors. She, having compartmentalized her writing into topical segments, hungers for wholeness; he, stuck in a moment when ‚Äúsome kind of guts have gone out of people,‚Äù would ‚Äúgive anything to go back to when I was in the gang of idealist kids on the street corner, believing we could change everything‚Ä¶the only time in my life I‚Äôve been happy.‚Äù They circle each other like ravenous carnivores biting chunks out of each other‚Äôs flesh, their cruelties spurring understandings that feel like misunderstandings.\nI recognized Lessing‚Äôs characters as weathered, burned-out forebears of my New Left crowd. ‚ÄúThe truth for our time was war, the immanence of war,‚Äù writes Anna Wulf. ‚ÄúWar was working in us all, towards fruition.‚Äù This spoke to us, given that the United States and the Soviet Union had just careened into, and almost not out of, the Cuban Missile Crisis. And my circle devoured The Golden Notebook not least because Lessing took women‚Äôs passions and quandaries seriously. Lessing named their disquiet, their longing to be ‚Äúfree women.‚Äù I lent my copy to Casey Hayden, who had recently been evicted from her marriage to Tom Hayden, then the president of Students for a Democratic Society (SDS).\nPolitics was not all that drew me to Saul and Anna. Lessing told me what I was missing in my own life, for I often felt as if I had been dropped onto the earth without an instruction manual, lacking even a vocabulary for what human beings felt about, and wanted from, each other. I admired Anna‚Äôs courageous struggle to unite her fragments, and saw myself in Saul Green‚Äôs lurching helplessness. He was some fifteen years older than I‚Äîthe right age for an archetype. He embodied both the swagger and the fragility of the masculine mystique I aspired to, √† la Humphrey Bogart in Casablanca. Like Saul, I fancied myself summoned by history with a capital H, wanting my bewilderments and failings to add up to something significant.\ngoing away\nsix months after reading The Golden Notebook, I picked up a copy of Going Away, Sigal‚Äôs second book and his major opus. By then, I had been elected the third president of SDS. I was fired up by the updraft of the civil rights movement and John Kennedy‚Äôs moves toward d√©tente. In my bookish way I was seeking a ‚Äúusable past‚Äù while struggling with an uneasy relationship between witnessing history and participating in it. In some way, I intuited that the dilemmas we were facing as young radicals were already built into the situation of being left wing in a country that was not. And I sensed that these were dilemmas about which Clancy Sigal would have something important to say.\nGoing Away is, in a way, the story of what happened to Saul Green before he met Anna Wulf. Lyrical and intellectually serious at once, it maps the political wasteland left by the death of the Old Left. At twenty-nine, the narrator is the son of union organizers (his mother a socialist, his absent father a Communist). It is October 1956. He drives a big, borrowed red-and-white De Soto convertible from L.A. eastward, looking up old buddies and recalling adventures whose meaning has been drained away by America‚Äôs stupor.\nMost of the old militancy has expired; most of his pals are in retreat; a few hold fast to Stalinism for dear life. Mostly he finds embers. In Wyoming, a onetime union leader tells him, ‚ÄúIt‚Äôs not a radical union any more.‚Ä¶ All they talk about is sex, baseball, cars and the lousy Company.‚Äù Another: ‚ÄúThe guys are tired. They‚Äôve a right to be.‚Ä¶ They want a rest. They don‚Äôt want to strike, they don‚Äôt want any trouble. They want to take long uninterrupted fishing trips.‚Äù ‚ÄúAll the people I talked to felt ‚Äòout of it,‚Äô‚Äù Sigal writes, ‚Äúbelieving that they could exercise no real influence over the important decisions in their lives, they were now busily brewing up a blend of wisecracking apathy.‚Äù Drive-by sex substitutes for the revolution.\nStopping off in Reno, Sigal watches people on street corners and motel roofs staring toward the government‚Äôs desert test site. ‚ÄúWhat‚Äôs going on?‚Äù he asks men in Stetson hats. ‚ÄúNuclear device,‚Äù one of them says. ‚ÄúThat was the phrase one of them used, nuclear device. I asked him if he meant an atom bomb, and he looked at me. I was a square.‚Äù These men, stunned into euphemism, talk kiloton ranges, weather, altitude. ‚ÄúI asked again what time the atomic bomb would be exploded. They stared at me again. I had done the square thing. I excused myself and called it a nuclear device.‚Äù One of the Stetsons ‚Äúput his hand on my shoulder and said, ‚ÄòSon, there ain‚Äôt no call for you getting sarcastic. That‚Äôs the only thing that stands between us and the Russians.‚Äô‚Äù\nThe bomb, 380 miles away, lights up the dawning sky. ‚ÄúAn intake of breath swept the crowds‚Ä¶ as the eyes caught the white glare that spread over the whole dawning sky, a sharp, slow splurge of light that brought forth appreciative Ah‚Äôs from the crowd. One of the Stetsons said, ‚ÄòGod, that‚Äôs beautiful.‚Äô‚Äù Later, Sigal wanders ‚Äúthe bleak, blinking streets thinking about Hungary,‚Äù where people are thrillingly, if futilely, rising up against the Stalinist regime.\nEverywhere, America looks ‚Äúrapaciously, lifelessly ‚Äòmodern.‚Äô‚Äù As he drives, revolted by billboards, charged up on whiskey and Dexedrine, a Greek chorus of radio bulletins spits out news of Hungary‚Äîand soon enough, dreadfully, of the Soviet onslaught that crushes what remains of his old-time religion. He is long gone from the Communist Party, but some residue of nostalgia binds him to the comrades with whom he had shared illusions. ‚ÄúIt has been written that one cannot have Socialism. One is a socialist,‚Äù he writes. ‚ÄúIt is true.‚Äù For no apparent reason, he breaks out crying.\nFor him, communism was insurgency and solidarity‚Äîa way of life and a morality, not an economic or political arrangement.\nWhat did I make of this at the time? I loved the narrator‚Äôs loyalties, his let-it-all-hang-out grousing, his penchant for theories of what killed America, his uncertainty about those theories, his honesty about his flaws and flops (‚ÄúI have never been afraid of self-pity‚Äù). I loved his belief that writing matters, that genres are cages, that writing is an art of fluidity, not boxes. I loved the long sections he devoted to political sagas‚Äîthe battle between Communists and socialists over control of the United Auto Workers; running the mimeograph machine for a proletarian poet during a strike in North Carolina‚Äîand his lust for solidarity. Above all, I was touched by his urgent need to know if it was ‚Äúpossible to have a small circle of friends, friends of grace and purpose, not incestuously, but on a basis of mutual respect, work, and a kind of humorous, informal dignity in the United States.‚Äù\nThe narrator was like Woody Guthrie with a college degree and a penchant for political argument. As John Leonard would later write, ‚ÄúIt was as if On the Road had been written by somebody with brains.‚Äù I didn‚Äôt mind much that the book was overlong, more than five hundred pages of wound-licking and self-purging. A rough-hewn cautionary tale felt like a fit tombstone for the Fifties‚Äô fraudulent sanities and grave defeats. It comes as no surprise when Clancy writes, ‚ÄúThis is the chronicle of how I started to go mad.‚Äù The last bulletin he hears, on shipboard, about to depart for Europe, goes, ‚ÄúThis is Budapest. Budapest Radio. Budapest Radio. Help us. Help. Help. Help.‚Äù\nI turned the final page and lay in bed thinking, This will never happen to me. My movement was different. Stalin and McCarthy were dead. The New Left was independent, unillusioned, rambunctious, joyful. Lucky me, I had the sixties to look forward to.\nabandoned by history\nalmost a decade later, I reread Going Away, and this time when I was done, I lay in bed thinking, So it did happen to me.\nI had spent the years in between as part of the New Left‚Äî‚Äúthe movement.‚Äù I had been president of SDS. The movement had been my way of life, the country I lived in. It gave me a calendar of events, reference points, vocabulary, music, jokes, lovers. It made the world cohere, or seem to. But by the early seventies, the Vietnam War was still bleeding on, and Richard Nixon reigned, and the movement had been seized by lunatic fights over who should lead an imaginary revolution. The most acute and ferocious hell wasn‚Äôt right-wingers or liberals: it was other left-wing factions. Even as we were being denounced and surveilled by government agencies, those of us who were independent-minded were also under fire from Leninist factions who labeled us ‚Äúmovement creeps‚Äù and ‚Äúrevisionists.‚Äù Movement solidarity had morphed into a collective hallucination. I was going away, all right‚Äînot only from America but from the movement that was going away from America. Or rather, it was going away from me. Going, going, gone.\nMarooned, disbelieving, burned out, for a time I contemplated a new political start‚Äîa new manifesto, something‚Äîwith movement buddies equally estranged from the clotted rhetoric and revolutionary fantasies that engulfed the sects of the late, no longer so New Left. But the world had found us indigestible and spat us out. Our intellectual ground had veered from early-sixties participatory democracy and brotherhood ideals to a haze of outrage, indebted to the Herbert Marcuse of the Great Refusal, the Guevarism of R√©gis Debray, the Maoism of the Little Red Book, and the R. D. Laing of the divided self. We were unstable selves floating free. Even our successes seemed perishable. What was it to be a movement writer without a sensible movement?\nTo get some distance, I took some time to write a long analytical essay. That‚Äôs what a serious left-wing intellectual did. But the more clearly I saw the origins of our maladies, the less avoidable they seemed. I smoked dope and had panic attacks. I wrote an epic poem. I scrambled to figure out what had happened to me.\nIt occurred to me, then, to write my way through the fog of failed revolution by trying my hand instead at a memoir-cum-fiction. I filled pages for a few days, feeling frightened, wondering if this was what it was like to go crazy. And then, staring at my barely begun book, the thought came that I should reach out to Clancy Sigal.\nI figured that if anyone could understand my life-problem as well as my writerly quandary, he could. I was, after all, the age he had been when he shipped out of America. He had known political defeat and extracted honor. So, in anguish, I wrote him a letter introducing myself. I sketched my situation, paid tribute to Going Away, said I was trying to write something similar, and confessed my fear that writing it would make me lose my mind. I asked him what writing his book had done for him, and mailed my letter from San Francisco in care of his publisher, Houghton Mifflin, in Boston.\nTen days later, I had back an air-letter from London. Houghton Mifflin had done the least every publisher needs to do‚Äîput a reader in touch with a writer. Clancy wrote that he knew of me through Studs Terkel, who had flatteringly blurbed a book I‚Äôd coauthored called Uptown: Poor Whites in Chicago. He told me that he, too, had found himself blocked from writing his big book until he‚Äôd met a third-generation Yorkshire miner, a Communist and a writer to boot, who took him down in the pits and let him hang around. The resulting short, sweet book of reportage, Weekend in Dinlock, was Sigal‚Äôs first book, published before Going Away.\nHe said he finished Going Away and then went crazy. He said this was obviously the subject he was writing about now. He was still making himself up as he went along, confessing, at one point, to a retrograde desire to own a color television. He was avuncular. He said he‚Äôd read a review of a just-published volume of Chekhov‚Äôs letters, said that they sounded like good advice for a young writer. (I rushed out to buy a copy.) He typed out every square inch of the air letter. I had written to the right guy.\ntake back the night\nsigal wasn‚Äôt kidding about going crazy. When I wrote him, he had been suffering from ‚Äúnameless, numbing panics‚Äù and frolicking with fellow-suffering freaks in the London anti-psychiatry world of the Scottish psychoanalyst R. D. Laing. Laing believed that schizophrenia was the goal of psychiatry, and told Sigal to stop ‚Äúbleatin‚Äô and moanin‚Äô like your standard Jewish neurotic‚Äù when ‚Äúye‚Äôve the makin‚Äôs of a first-class schizophrenic‚Äù‚Äîa high compliment from the Lenin of the madness movement. A few years later, Sigal would publish a roman √† clef about his time with Laing called Zone of the Interior, his first book since Going Away. In the preface, Sigal asks himself, ‚ÄúWhy, with so much to politically rethink in the Sixties‚Ä¶ did I seek refuge in near-Fascist irrationalism?‚Äù A good question that he does not answer, though strong clues will be found in the title and substance of his later roman √† clef, The Secret Defector.\nI wrote Clancy again after Zone came out. (It was published only in America, because British libel law, which favors the accuser, meant it would have been legally perilous to publish the book in London while Laing was alive.) ‚ÄúSometimes,‚Äù I wrote, ‚ÄúI felt the book could be more interior.‚Äù In truth, it was more than sometimes. Zone was Fellini shot from a balloon‚Äîa bumpy burlesque, sometimes funny but undeveloped and anticlimactic. Going Away had its longueurs, but they all circled back to the unmistakable center of the novel, the brooding, broken adventurer-narrator who stumbles from dizzy spell to hectic sex as he tries to get down deep with his onetime comrades. Sigal‚Äôs Zone alter ego, by contrast, rarely makes solid connections with his would-be transcendent mates. His ironic distance flattens his prose. About his own forays into ecstatic LSD-boosted effusions, laughing for hours, suddenly aware of ‚Äúthe pointlessness of all human activity,‚Äù he sounds more sardonic than transcendent. ‚ÄúInterior‚Äù was not Clancy‚Äôs strong zone.\nLater that year, I made my first trip to Britain, and my wife and I spent an evening with Clancy and his wife, Margaret Walters, a feminist art historian. Their walk-up on Wigmore Street was diminutive and unprepossessing. A gas heater hung over the kitchen sink. They had spent the day at the Notting Hill Carnival, where Margaret‚Äôs purse had been stolen. I was impressed by her aplomb and Clancy‚Äôs inquisitive but warm manner, which was not the aggressive style of Saul Green. He had hooded eyes and a strong chin. He was pleasant, cocky, impassioned, and wary all at once. He studied me, and my wife, closely. We talked about the general rottenness of politics. We hit it off. He graciously didn‚Äôt ask about my unwritten book.\nWe stayed in touch from then on. In 1978, he visited me in San Francisco, and we strolled around in exuberance during the first ‚ÄúTake Back the Night‚Äù march, chanting through the streets of North Beach, ‚ÄúWomen, unite! Take back the night!‚Äù Nothing ironic about it‚Äîwe were, both of us, fervent feminists. He would write in his later memoir The London Lover how impressed he was with the new women, ‚Äúbasking in their new power‚Ä¶tough, difficult, and like my mum, sassy.‚Ä¶Such is the moth‚Äôs flame that I‚Äôm enchanted‚Äîin the original sense‚Äîby their vivacity and sheer joie de vivre.‚Äù ‚ÄúTake Back the Night‚Äù was sheer exultation. At one point we shouted ‚ÄúJoin us‚Äù to two women watching us from an upstairs apartment window. ‚ÄúJoin us,‚Äù they shouted back, grinning and beckoning. Those were the days.\naction as tranquilizer\nduring the years that followed, Sigal wrote fiercely‚Äîno more writer‚Äôs block that I could see. He wrote literally thousands of articles: journalism, book and movie reviews, BBC scripts. He wrote about political and sociological travels, √† la George Orwell; about stolid Tories and innocent campaigners for nuclear disarmament. He interviewed Samuel Beckett. In 1984 he covered the Los Angeles Olympics, of all things, for The Observer, and not long after that he decided it was time to come back home. He applied for a professorship in the Department of Journalism at the University of Southern California. His cause there was taken up vigorously by A. J. Langguth, the fine former New York Times Saigon bureau chief. I was flattered to be asked to write a recommendation for Clancy, and evidently it did not torpedo his prospects, for he got the job and proceeded‚Äîdevotedly‚Äîto teach long-form journalism there for fifteen years.\nIn the fifties, even as some of his college friends headed to graduate school, Clancy had steered clear of the academy. The intellectual‚Äôs first role, he wrote, was ‚Äúto stand and shout bloody murder.‚Äù The second was ‚Äúnot to break into any kind of Establishment.‚Äù Playing at word games or concept refinement was not his idea of intellectual life. ‚ÄúI have a big mouth but a torpid mind,‚Äù he wrote. So it would have amazed the young Clancy to think that he would end up as a tenured professor, and a dedicated teacher to boot.\nEven so, he was suspicious of corrals, even comfortable ones, and his style was never dented by the academy. When I confessed my own resolve to hold on to my writerly missions even as I climbed the ladder of tenure, he was quick with advice: ‚ÄúWrite one for them and one for yourself.‚Äù (I liked the sound of it, though after I got tenure, I stopped writing the one for them.) He never wrote the one for them. Novel-memoirs, commercial screenplays (in collaboration with his second wife, Janice Tidwell), and journalism were his m√©tiers. Professional intellectuals were not high on his admiration list.\nTo get some distance, I took some time to write a long analytical essay. That‚Äôs what a serious left-wing intellectual did.\nIn part, that was because Clancy loved action. In his 1992 roman √† clef about his time in London, The Secret Defector (his first book after Zone of the Interior), the Doris Lessing character, Rose O‚ÄôMalley, charges the narrator with ‚Äúnever knowing if you‚Äôre Vladimir Ilyich Lenin or Errol Flynn.‚Äù ‚ÄúHold on,‚Äù Clancy‚Äôs character responds. ‚ÄúI knew who I was. Vladimir Flynn.‚Äù\nClancy was not the only radical or would-be revolutionary to wonder whether he was in a movie. Still, he did not posture. During the civil rights movement, he went back to the United States to hang out with the radicals of the Student Nonviolent Coordinating Committee and help register Black voters in the South. And during the Vietnam War he was a stationmaster in a network smuggling American AWOLs and deserters to Sweden, taking plenty of chances.\nHis character in The Secret Defector goes on to tell Rose that ‚Äúwho I didn‚Äôt want to be, what I was in danger of becoming, was that most terrifying caricature, a political adventurer‚Ä¶ living on air, terribly magnetized by seriousness in others while compulsively inconstant themselves.‚Äù If that is who Clancy was, he came by it honestly. He spent his childhood traveling around the country with his turbulent union-organizer mother, Jennie Persily, whom he described lovingly in his next, best-made memoir, A Woman of Uncertain Character: The Amorous and Radical Adventures of My Mother Jennie (Who Always Wanted to Be a Respectable Jewish Mom), by Her Bastard Son. Given that Jennie brought Clancy to union rallies, picket lines, and battles with strikebreaking cops, action was, you might say, his mother‚Äôs milk.\n‚ÄúFear is my tranquilizer,‚Äù he would write later. ‚ÄúBeing at the centre of the action is the only thing that helps to reduce my state of perpetual terror.‚Äù During risky exploits he remembers becoming ‚Äúeuphoric with fear.‚Äù ‚ÄúWhy,‚Äù he asked himself, ‚Äúdo I feel so at ease in an out-of-control crowd?‚Äù Perhaps for the same reason he felt at ease, for a while, in an out-of-control love affair.\na vexatious angel\nover the years, I suppressed the occasional impulse to broach with Clancy the subject of Saul Green. I didn‚Äôt want to pry, but I wondered, What did it mean to Clancy to be outed, so to speak, by his onetime lover? I assumed that he resented what Lessing did with him. But it seems to have been more complicated than that. In a fascinating monograph, Literary Half-Lives: Doris Lessing, Clancy Sigal, and Roman √† Clef, Roberta Rubenstein reports that Clancy recorded in his diary in July 1959 how he felt on learning that Lessing had begun writing a play about their affair: ‚ÄúI was, at first, unconcerned, and then, as I understood, furious. I issued a veiled threat that to continue to write such a play might cleave us inseparably, but I knew she would write it. She would never, for some very good reasons, sacrifice her personal life to art.‚Äù He must have meant the opposite.\nThe diary entry he wrote about discovering that Lessing was lifting his words verbatim is electric. He has just found out that his mother has died. When the call comes from the States, he is up north with ‚Äúsome of the best friends I‚Äôd made in England‚Ä¶having one of those intense political meetings charting the future of the known universe. I said, ‚ÄòMy ma just died.‚Äô No one got up to put an arm around me or to say how sorry they were. Mostly they looked acutely embarrassed as if I‚Äôd farted in public or mistranslated Sartre‚Äôs Being and Nothingness.‚Äù\n‚ÄúArmored against emotion,‚Äù he returns to London. Lessing isn‚Äôt home. He prowls around and stumbles upon her play manuscript. ‚ÄúI let my eye idle over a few lines,‚Äù he records in his diary. ‚ÄúIt hit me between the eyes. The play she was writing, in fact had completed, was about our relationship. But those lines of dialogue! They were exactly what I had said to her, in my worst, most intimate moments of a severe nervous breakdown in the year previous.‚Ä¶ Never, I felt, had I been so profoundly betrayed.‚Ä¶ I wrote a letter to the woman, an angry letter, [to] which she replied in like mind, reminding me how much she had done for me. She was perfectly correct in this.‚Äù Lessing has commandeered him, hexed him, body-snatched him.\nLessing went on to draw on Clancy‚Äôs diaries for The Golden Notebook. He knew she was doing it because he had taken to tying a thin black thread between his bureau drawers so as to tell whether she had peeked, and he half-boasted, half-chortled about having written passages into his diary to nudge her novel this way or that. He would discreetly check her manuscript for signs of her piracy.\nSo he would pass through literary life preceded by her depiction of him as Saul Green, as if the far more famous Lessing was always making rabbit ears with her fingers behind his head. And not only his literary life. He suspects that when people meet him they wonder if the Clancy Sigal they meet masks the fictional character. But notoriety works in complicated ways. Beyond feeling diminished, perhaps he is also weirdly proud to have been ‚Äúimmortalized,‚Äù even perversely, in a literary classic. Over the years, Clancy slams Lessing for invading his diaries. ‚ÄúI thought of slugging my ex-girlfriend (or at least killing her),‚Äù he writes. But then he adds the following sentences, ‚ÄúShe had immortalized me as a neurotic schmegege‚Äîand who wants to be remembered like that? But living well, not suing for libel, is the best revenge. Second best is answering back in kind.‚Äù\nThey were fiercely coupled, twisted together in mutual aid and resentment. Lessing‚Äôs love merged with her jealousy. Clancy‚Äôs shock at her literary appropriations merged with his rivalry. (Before The Golden Notebook, she was already a published and well-regarded author; he, seven years younger, was not.) She would say he tormented her, and she gave as good as she got. In a 1992 article, Clancy tells this story:\nR. D. Laing, a great fan of Mrs. Lessing‚Äôs, and I used to get roaring drunk together and, when totally blotto, exchange profundities about the schizophrenic implications of a divided self being further split by the act of being written about. ‚ÄúThat woman stealing your soul was the luckiest thing that ever happened to you,‚Äù he would insist. ‚ÄúShe‚Äôs emptied you for the Great Task ahead.‚Äù Which was, more or less, the ‚Äúschizoid voyage‚Äù that was our politics then. Sometimes Mrs. Lessing seemed to agree. Once, at a party‚Ä¶ she put her arms around me and boasted to the guests, ‚ÄúI invented Clancy.‚Äù ‚ÄúNo, you didn‚Äôt,‚Äù I said stubbornly, ‚Äúmy mother, Jennie, did that by giving birth to me.‚Äù ‚ÄúShe had,‚Äù Mrs. Lessing said, dismissing my protest, ‚Äúthe easy part.‚Äù\nClancy and Doris stayed in touch over the years. He was like that with his most serious lovers‚Äîplaying a long game. Love, respect, and companionship were points on the same curve. After she died in 2013, I wrote him about how reading The Golden Notebook at age twenty ‚Äúhad some huge & eerie effect on me‚Ä¶told me that the sort of life I was embarking on might not have consequence but it had gravity and honor, and not only honor but a kind of density.‚Ä¶ Unlike the grander ideological novels I‚Äôd been reading, The Golden Notebook was full of the dirt & mess & craziness of everyday life, the back-&-forth of working out politics & lostness & honor on the run, so to speak; & the possibility of scrambling toward a way of writing that would convey the mess & yet the stakes of the struggle & its worth.‚Äù And I asked whether, even if he resented her ‚Äúportrait‚Äù of him, he realized it had made him ‚Äúan inspirational figure not only for me but for my crowd, & could you see how she had honored you? You were her vexatious angel.‚Äù\nkeep the flame lit\nclancy never stopped writing, churning out a succession of romans √† clef and, more vividly, memoirs of chunks of his life (Black Sunset about Hollywood, and the aforementioned The London Lover and A Woman of Uncertain Character), as well as screenplays‚ÄîIn Love and War, based on wartime reminiscences of his beloved Ernest Hemingway, and (in collaboration with Janice) Frida, with the redoubtable Kahlo played by Salma Hayek and featuring Geoffrey Rush as Trotsky and Antonio Banderas as David Alfaro Siqueiros. (I asked him why the movie was lacking what would surely have been a bang-up true-life scene in which Siqueiros, the down-the-line Stalinist and brilliant muralist, did his damnedest to shoot the exiled Trotsky, leaving holes in the wall of Trotsky‚Äôs house but none in Trotsky himself. Clancy told me that he and Janice did write that scene but it didn‚Äôt make it into the movie.)\nHis and Janice‚Äôs screenplay about the Sartre-Beauvoir-Nelson Algren triangle (Clancy and Algren were Chicago buddies) never got produced, though Annette Bening had signed on. All the while, he kept scribbling for the British press and wrote political op-eds, many of them published on the far-left Counterpunch website, as well as longer pieces of reportage that didn‚Äôt always find venues. One that saw the light of day only in a much truncated version was a remarkable account of his visit to Nixon henchman H. R. Haldeman in the federal prison in Lompoc, California, where Haldeman was serving a shortened sentence of eighteen months for Watergate crimes. Haldeman and his partner-in-crime John Ehrlichman had been UCLA contemporaries of Clancy‚Äôs in the early fifties. The future Nixon co-conspirators were already right-wingers when Sigal was the Big Commie on Campus and the editor of the school paper; they tried to get the student government to take the paper away from him. ‚ÄúIt was never personal,‚Äù Haldeman told him at Lompoc.\nEven our successes seemed perishable. What was it to be a movement writer without a sensible movement?\nClancy‚Äôs late style is jagged, suited to delicate evasion, full of meaningful pauses, his writing stripped down, sometimes to aphorism. Cinematic, in fact. This was not strictly or even primarily an aesthetic choice. (It was largely, according to his widow, because during his later years he was in tremendous pain from an agitated sciatic nerve, and so had to write standing up.) Quoting Elmore Leonard (‚ÄúTry to leave out the part that readers tend to skip‚Äù), in his late books he has traded in the longueurs of Going Away and Zone of the Interior for staccato suggestions and hints toward what need not be dwelt upon or cannot even be hazarded‚Äîor perhaps is best left to dangle in ambiguity.\nClancy‚Äôs radicalism was more consistent than his novelistic ambition. Through it all, until he died at ninety in 2017, he tried to keep the eternal flame lit. Stay in fighting trim. Choose sides. Make the most of defeat. He had no illusions about Stalinism. (In The Secret Defector, he spoke of ‚Äúmy dream of a non-Communist independent left.‚Äù) Nor did he have any love for ‚Äúarmed struggle‚Äù terrorism. In 1982 he had attended ‚Äúa Sunday concert in Regents Park when the military band was blown to pieces by IRA shrapnel, and a few weeks later a flying chunk of iron from a Piccadilly postbox with an IRA letter bomb inside had almost decapitated me.‚Äù But when I told him once that I was not just non-Communist but anti-Communist, he was genuinely shocked. It wasn‚Äôt that he disagreed with me on the merits. It was a matter of primal identity. For him, communism was insurgency and solidarity‚Äîa way of life and a morality, not an economic or political arrangement. ‚ÄúIt was the essence of what we believed that Communism was men standing up on their own two feet and, for the first time in history, ordering their lives in imperfect consultation but in perfect awareness.‚Äù\nTo him, anticommunism was once and for all desertion. It was McCarthy, John Foster Dulles, the domino theory. It was the stupidities of the Cold War. It was the Bomb. I argued that anticommunism was more general but also simpler than that‚Äîit was the conviction that communism was a rotten political system, even if scoundrels thought so too. I thought it morally necessary as well as tactically wise to clarify that there was more than one kind of monstrosity in the world. We agreed to disagree.\nWe also agreed to disagree about Democratic presidents. Clancy was ever on the lookout for signs of submission. In the fifties, he found the speeches of Adlai Stevenson ‚Äúmealymouthed and disingenuous in the extreme, even cowardly.‚Äù He thought liberals as guilty as Communists of ‚Äúthe abdication of thought to prayer.‚Äù During his later years, he grew, if anything, more vehement, riveted to a single metaphor: bad marriage. In 2006, he emailed me: ‚ÄúMore and more I think we should get a divorce from the Dems so we can fall in love all over again.‚Äù In 2010, he accused ‚Äúthe American left (what there is of it)‚Ä¶[of ] trail[ing] poodle-like after Barack Obama‚Ä¶ and isn‚Äôt it nice for a change to have a president who can parse a complicated sentence?‚Ä¶ One scary look at Obama‚Äôs yowling enemies‚Äîracist and crazy about Palin‚Äî‚Ä¶ was enough to send us whimpering back to our kennels.‚Äù\nHis dogma was not mine. I tried to convince him that given America‚Äôs two-party presidential system and first-past-the-post elections, supporting the best available Democrats was not marriage, bad or otherwise, but hygiene, like brushing your teeth. ‚ÄúWe live in the country we live in, not the one we wish we lived in,‚Äù I preached. He was not swayed. I had read Clancy rightly when, decades earlier, I asked his advice about how to make sense of the overpowering experience of thinking big and falling short. I had grasped his feeling for the defeated, the sobriety of his intelligent grief, his loyalty to the proles of his youth, his understanding that self-pity was shallow and that recovery was more than a matter of will‚Äîhis insistence on being earnest. He had no time for the left-wing glitterati who soaked up much more attention. He knew that to move humanity toward wholeness, to respect the effort wrapped in the tragedy, and the tragedy in the effort, you had to have been immersed. To be indefatigable, you had to have lost everything but your honor. You could only burn out if you had burned."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://github.com/S-FM/faim-python-client", "title": "Show HN: Python SDK ‚Äì forecasting with foundation time-series and tabular models", "url": "https://github.com/S-FM/faim-python-client", "published": "Thu, 18 Dec 2025 11:33:28 +0000", "text_source": "article", "article_fetch_error": null, "text": "Production-ready Python SDK for FAIM (Foundation AI Models) - a unified platform for time-series forecasting and tabular inference powered by foundation models.\n- üöÄ Multiple Foundation Models:\n- Time-Series: FlowState, Amazon Chronos 2.0, TiRex\n- Tabular: LimiX (classification & regression)\n- üîí Type-Safe API: Full type hints with Pydantic validation\n- ‚ö° High Performance: Optimized Apache Arrow serialization with zero-copy operations\n- üéØ Probabilistic & Deterministic: Point forecasts, quantiles, samples, and probabilistic predictions\n- üîÑ Async Support: Built-in async/await support for concurrent requests\n- üìä Rich Error Handling: Machine-readable error codes with detailed diagnostics\n- üß™ Battle-Tested: Production-ready with comprehensive error handling\npip install faim-sdk\nGet your API key at https://faim.it.com/\nfrom faim_sdk import ForecastClient\n# Initialize client with your API key\nclient = ForecastClient(api_key=\"your-api-key\")\nimport numpy as np\nfrom faim_sdk import ForecastClient, Chronos2ForecastRequest\n# Initialize client\nclient = ForecastClient(api_key=\"your-api-key\")\n# Prepare your time-series data\n# Shape: (batch_size, sequence_length, features)\ndata = np.random.randn(32, 100, 1).astype(np.float32)\n# Create probabilistic forecast request\nrequest = Chronos2ForecastRequest(\nx=data,\nhorizon=24, # Forecast 24 steps ahead\noutput_type=\"quantiles\",\nquantiles=[0.1, 0.5, 0.9] # 10th, 50th (median), 90th percentiles\n)\n# Generate forecast - model inferred automatically from request type\nresponse = client.forecast(request)\n# Access predictions\nprint(response.quantiles.shape) # (32, 24, 3, 1)\nprint(response.metadata) # Model version, inference time, etc.\nAll time-series models require 3D input arrays:\n# Shape: (batch_size, sequence_length, features)\nx = np.array([\n[[1.0], [2.0], [3.0]], # Series 1\n[[4.0], [5.0], [6.0]] # Series 2\n]) # Shape: (2, 3, 1)\n- batch_size: Number of independent time series\n- sequence_length: Historical data points (context window)\n- features: Number of variables per time step (use 1 for univariate)\nImportant: 2D input will raise a validation error. Always provide 3D arrays.\nTabular models require 2D input arrays:\n# Shape: (n_samples, n_features)\nX_train = np.array([\n[1.0, 2.0, 3.0], # Sample 1\n[4.0, 5.0, 6.0], # Sample 2\n]) # Shape: (2, 3)\n- n_samples: Number of training/test samples\n- n_features: Number of input features per sample\nPoint Forecasts (3D):\nresponse.point # Shape: (batch_size, horizon, features)\nQuantile Forecasts (4D):\nresponse.quantiles # Shape: (batch_size, horizon, num_quantiles, features)\n# Example: (32, 24, 5, 1) = 32 series, 24 steps ahead, 5 quantiles, 1 feature\nPredictions (1D):\nresponse.predictions # Shape: (n_samples,)\n# Classification: class labels or indices\n# Regression: continuous values\nClassification Probabilities (2D):\nresponse.probabilities # Shape: (n_samples, n_classes) - classification only\n# Probability for each class\n- Chronos2: ‚úÖ Supports multivariate forecasting (multiple features)\n- FlowState:\n‚ö†Ô∏è Univariate only - automatically transforms multivariate input - TiRex:\n‚ö†Ô∏è Univariate only - automatically transforms multivariate input\nChoose your client and model based on your task:\n| Task | Client | Models | Input | Output |\n|---|---|---|---|---|\n| Time-Series Forecasting | ForecastClient |\nFlowState, Chronos2, TiRex | 3D: (batch, seq_len, features) |\n3D/4D point/quantiles |\n| Tabular Classification | TabularClient |\nLimiX | 2D: (n_samples, n_features) |\n1D predictions + 2D probabilities |\n| Tabular Regression | TabularClient |\nLimiX | 2D: (n_samples, n_features) |\n1D continuous predictions |\nfrom faim_sdk import FlowStateForecastRequest\nrequest = FlowStateForecastRequest(\nx=data,\nhorizon=24,\nmodel_version=\"latest\",\noutput_type=\"point\",\nscale_factor=1.0, # Optional: normalization factor, for details check: https://huggingface.co/ibm-granite/granite-timeseries-flowstate-r1\nprediction_type=\"mean\" # Options: \"mean\", \"median\"\n)\nresponse = client.forecast(request)\nprint(response.point.shape) # (batch_size, 24, features)\nfrom faim_sdk import Chronos2ForecastRequest\n# Quantile-based probabilistic forecast\nrequest = Chronos2ForecastRequest(\nx=data,\nhorizon=24,\noutput_type=\"quantiles\",\nquantiles=[0.05, 0.25, 0.5, 0.75, 0.95] # Full distribution\n)\nresponse = client.forecast(request)\nprint(response.quantiles.shape) # (batch_size, 24, 5)\nfrom faim_sdk import TiRexForecastRequest\nrequest = TiRexForecastRequest(\nx=data,\nhorizon=24,\noutput_type=\"point\"\n)\nresponse = client.forecast(request)\nprint(response.point.shape) # (batch_size, 24, features)\nThe SDK also supports LimiX, a foundation model for tabular classification and regression:\nfrom faim_sdk import TabularClient, LimiXPredictRequest\nimport numpy as np\n# Initialize tabular client\nclient = TabularClient(api_key=\"your-api-key\")\n# Prepare tabular data (2D arrays)\nX_train = np.random.randn(100, 10).astype(np.float32)\ny_train = np.random.randint(0, 2, 100).astype(np.float32)\nX_test = np.random.randn(20, 10).astype(np.float32)\n# Create classification request\nrequest = LimiXPredictRequest(\nX_train=X_train,\ny_train=y_train,\nX_test=X_test,\ntask_type=\"Classification\", # or \"Regression\"\nuse_retrieval=False # Set to True for retrieval-augmented inference\n)\n# Generate predictions\nresponse = client.predict(request)\nprint(response.predictions.shape) # (20,)\nprint(response.probabilities.shape) # (20, n_classes) - classification only\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n# Load dataset\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n# Convert to float32\nX_train = X_train.astype(np.float32)\nX_test = X_test.astype(np.float32)\ny_train = y_train.astype(np.float32)\n# Create and send request\nrequest = LimiXPredictRequest(\nX_train=X_train,\ny_train=y_train,\nX_test=X_test,\ntask_type=\"Classification\"\n)\nresponse = client.predict(request)\n# Evaluate\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, response.predictions.astype(int))\nprint(f\"Accuracy: {accuracy:.4f}\")\nfrom sklearn.datasets import fetch_california_housing\n# Load dataset\nhouse_data = fetch_california_housing()\nX, y = house_data.data, house_data.target\n# Split data (50/50 for demo)\nsplit_idx = len(X) // 2\nX_train, X_test = X[:split_idx].astype(np.float32), X[split_idx:].astype(np.float32)\ny_train, y_test = y[:split_idx].astype(np.float32), y[split_idx:].astype(np.float32)\n# Create and send request\nrequest = LimiXPredictRequest(\nX_train=X_train,\ny_train=y_train,\nX_test=X_test,\ntask_type=\"Regression\"\n)\nresponse = client.predict(request)\n# Evaluate\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(y_test, response.predictions))\nprint(f\"RMSE: {rmse:.4f}\")\nFor better accuracy on small datasets, enable retrieval-augmented inference:\nrequest = LimiXPredictRequest(\nX_train=X_train,\ny_train=y_train,\nX_test=X_test,\ntask_type=\"Classification\",\nuse_retrieval=True # Enable RAI (slower but more accurate)\n)\nresponse = client.predict(request)\nTime-series forecasts return a ForecastResponse\nobject with predictions and metadata:\nresponse = client.forecast(request)\n# Access predictions based on output_type\nif response.point is not None:\npredictions = response.point # Shape: (batch_size, horizon, features)\nif response.quantiles is not None:\nquantiles = response.quantiles # Shape: (batch_size, horizon, num_quantiles)\n# Lower quantiles for uncertainty bounds\nlower_bound = quantiles[:, :, 0] # 10th percentile\nmedian = quantiles[:, :, 1] # 50th percentile (median)\nupper_bound = quantiles[:, :, 2] # 90th percentile\nif response.samples is not None:\nsamples = response.samples # Shape: (batch_size, horizon, num_samples)\n# Access metadata\nprint(response.metadata)\n# {'model_name': 'chronos2', 'model_version': '1.0', 'inference_time_ms': 123}\nThe SDK provides error codes for robust error handling:\nfrom faim_sdk import (\nForecastClient,\nChronos2ForecastRequest,\nValidationError,\nAuthenticationError,\nRateLimitError,\nModelNotFoundError,\nErrorCode\n)\ntry:\nrequest = Chronos2ForecastRequest(x=data, horizon=24, quantiles=[0.1, 0.5, 0.9])\nresponse = client.forecast(request)\nexcept AuthenticationError as e:\n# Handle authentication failures (401, 403)\nprint(f\"Authentication failed: {e.message}\")\nprint(f\"Request ID: {e.error_response.request_id}\")\nexcept ValidationError as e:\n# Handle invalid request parameters (422)\nif e.error_code == ErrorCode.INVALID_SHAPE:\nprint(f\"Shape error: {e.error_response.detail}\")\n# Fix shape and retry\nelif e.error_code == ErrorCode.MISSING_REQUIRED_FIELD:\nprint(f\"Missing field: {e.error_response.detail}\")\nexcept RateLimitError as e:\n# Handle rate limiting (429)\nprint(\"Rate limit exceeded - implementing exponential backoff\")\nretry_after = e.error_response.metadata.get('retry_after', 60)\ntime.sleep(retry_after)\nexcept ModelNotFoundError as e:\n# Handle model/version not found (404)\nprint(f\"Model not found: {e.message}\")\nFAIMError (base)\n‚îú‚îÄ‚îÄ APIError\n‚îÇ ‚îú‚îÄ‚îÄ AuthenticationError (401, 403)\n‚îÇ ‚îú‚îÄ‚îÄ InsufficientFundsError (402)\n‚îÇ ‚îú‚îÄ‚îÄ ModelNotFoundError (404)\n‚îÇ ‚îú‚îÄ‚îÄ PayloadTooLargeError (413)\n‚îÇ ‚îú‚îÄ‚îÄ ValidationError (422)\n‚îÇ ‚îú‚îÄ‚îÄ RateLimitError (429)\n‚îÇ ‚îú‚îÄ‚îÄ InternalServerError (500)\n‚îÇ ‚îî‚îÄ‚îÄ ServiceUnavailableError (503, 504)\n‚îú‚îÄ‚îÄ NetworkError\n‚îú‚îÄ‚îÄ SerializationError\n‚îú‚îÄ‚îÄ TimeoutError\n‚îî‚îÄ‚îÄ ConfigurationError\nThe SDK supports async operations for concurrent requests:\nimport asyncio\nfrom faim_sdk import ForecastClient, Chronos2ForecastRequest\nasync def forecast_multiple_series():\nclient = ForecastClient(\napi_key=\"your-api-key\"\n)\n# Create multiple requests\nrequests = [\nChronos2ForecastRequest(x=data1, horizon=24),\nChronos2ForecastRequest(x=data2, horizon=24),\nChronos2ForecastRequest(x=data3, horizon=24),\n]\n# Execute concurrently\nasync with client:\ntasks = [\nclient.forecast_async(req)\nfor req in requests\n]\nresponses = await asyncio.gather(*tasks)\nreturn responses\n# Run async forecasts\nresponses = asyncio.run(forecast_multiple_series())\nSee the examples/\ndirectory for complete Jupyter notebook examples:\ntoy_example.ipynb\n- Get started with FAIM and generate both point and probabilistic forecastsairpassengers_dataset.ipynb\n- End-to-end example with AirPassengers dataset\n-\nlimix_classification_example.ipynb\n- Binary classification on breast cancer dataset -\nlimix_regression_example.ipynb\n- Regression on California housing dataset\n- Python >= 3.10\n- numpy >= 1.26.0\n- pyarrow >= 11.0.0\n- httpx >= 0.23.0\n- pydantic >= 2.0.0\n-\nBatch Processing: Process multiple time series in a single request for optimal throughput\n# Good: Single request with 32 series data = np.random.randn(32, 100, 1) # Less efficient: 32 separate requests # for series in data: client.forecast(...)\n-\nCompression: Use\ncompression=\"zstd\"\nfor large payloads (default, recommended) -\nAsync for Concurrent Requests: Use\nforecast_async()\nwithasyncio.gather()\nfor parallel processing\n- Connection Pooling: Reuse client instances across requests instead of creating new ones\n- Email: support@faim.it.com\nApache License 2.0 - See LICENSE file for details.\nIf you use FAIM in your research, please cite:\n@software{faim_sdk,\ntitle = {FAIM SDK: Foundation AI Models for Time Series Forecasting},\nauthor = {FAIM Team},\nyear = {2024},\nurl = {https://github.com/S-FM/faim-python-client}\n}"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://github.com/ahoward/tc", "title": "Tc ‚Äì Theodore Calvin's language-agnostic testing framework", "url": "https://github.com/ahoward/tc", "published": "Mon, 22 Dec 2025 22:03:07 +0000", "text_source": "article", "article_fetch_error": null, "text": "language-agnostic testing for unix hackers\ntheodore \"tc\" calvin - helicopter pilot, testing framework namesake, legend\n|=o=o=o=o=o=o=o=o=o=o=o=| tc v1.0.0 - island hopper\n| testing any language, anywhere\n___/ \\___ (o) üöÅ fly safe, test well\n(( tc ))======\\\n\\_______/ (o)\n^ ^\n^-----------^\nWhat: Language-agnostic test framework. Write tests once, run against any language (bash, python, rust, go, whatever).\nHow: Tests are directories. Your code reads input.json\nfrom stdin, writes expected.json\nto stdout. That's it.\nGet Started:\n# clone and install\ngit clone https://github.com/ahoward/tc.git\ncd tc\n# IMPORTANT: Add to PATH (avoids conflict with Unix traffic control command)\nexport PATH=\"$PWD/tc/bin:$PATH\"\n# verify\ntc --version\n# try the hello-world example\ntc examples/hello-world\n# create your first test\ntc new tests/my-feature\nThat's it. See full docs for advanced features.\ntc\nconflicts with the Unix traffic control command. You MUST add this project's tc\nto your PATH.\n# Add to PATH for current session\nexport PATH=\"$PWD/tc/bin:$PATH\"\n# Add to shell config for persistence (optional)\necho 'export PATH=\"$PWD/tc/bin:$PATH\"' >> ~/.bashrc # or ~/.zshrc\nVerify:\nwhich tc # should show: ./tc/bin/tc (NOT /usr/sbin/tc)\ntc --version # should show: tc v1.0.0 - island hopper\ntc is a dead-simple testing framework that lets you:\n- test any language with the same test suite\n- organize tests as directories with json input/output\n- run tests with zero dependencies (just jq)\n- port code between languages without rewriting tests\nsimple ‚Ä¢ portable ‚Ä¢ language-agnostic ‚Ä¢ unix ‚Ä¢ spec-driven\nü§ñ In the AI age, specifications and tests are permanent while implementations are disposable.\nTests are the spec. Code is a build artifact. Port languages freely, keep tests forever.\nSee projects/\nand examples/multi-lang-dao/\nfor a working example of identical DAO interfaces in 5 languages (Ruby, Go, Python, JavaScript, Rust) all passing the same test suite.\nVision: Disposable applications. Swap languages freely, keep tests forever.\nSee docs/THEORY.md for the full system adapter pattern vision.\n# test execution\ntc # run all tests (KISS!)\ntc <suite-path> # run single test suite\ntc <path> --all # run all suites in directory tree\ntc <path> --tags TAG # run suites matching tag\ntc <path> --parallel # run all suites in parallel (auto CPU detection)\ntc <path> --parallel N # run with N parallel workers\n# test generation\ntc new <test-path> # generate new test suite\ntc init [directory] # initialize test directory with README\n# discovery & metadata\ntc list [path] # list all test suites with metadata\ntc tags [path] # show all available tags\ntc explain <suite> # explain what a test suite does\n# info\ntc --version # show version\ntc --help # show help\nTTY mode (terminal): Clean single-line status with üöÅ spinner, fail-fast behavior\nNon-TTY mode (CI/CD): Traditional verbose output with full logs\nOverride: TC_FANCY_OUTPUT=true/false\n‚Üí full docs | ‚Üí tc new guide | ‚Üí system adapter theory (WIP)\nmy-feature/\n‚îú‚îÄ‚îÄ run # executable: reads input.json, writes json to stdout\n‚îî‚îÄ‚îÄ data/\n‚îî‚îÄ‚îÄ scenario-1/\n‚îú‚îÄ‚îÄ input.json # test input\n‚îî‚îÄ‚îÄ expected.json # expected output\ntc my-feature # ‚úì pass or ‚úó fail\ntc supports simple pattern matching in expected.json\nfor dynamic values:\n{\n\"id\": \"<uuid>\",\n\"status\": \"pending\",\n\"created_at\": \"<timestamp>\",\n\"count\": \"<number>\",\n\"message\": \"<string>\"\n}\nPatterns:\n<uuid>\n- validates UUID v4 format<timestamp>\n- validates ISO 8601 timestamp (YYYY-MM-DDTHH:MM:SS)<number>\n- any JSON number<string>\n- any string value<boolean>\n- true or false<null>\n- null value<any>\n- matches anything\nWorks everywhere:\n- Nested objects\n- Array elements\n- Mixed with exact values\nNo configuration needed - patterns are auto-detected.\nDefine your own patterns via TC_CUSTOM_PATTERNS\n:\nexport TC_CUSTOM_PATTERNS=\"email:^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\nipv4:^([0-9]{1,3}\\.){3}[0-9]{1,3}$\nphone:^\\+?[0-9]{10,15}$\"\nThen use in expected.json:\n{\n\"email\": \"<email>\",\n\"server\": \"<ipv4>\",\n\"contact\": \"<phone>\"\n}\nFormat: pattern_name:regex\n(one per line, standard regex syntax)\ntest execution:\n- run single test suite\n- semantic json comparison (order-independent)\n- pattern matching (\n<uuid>\n,<timestamp>\n,<number>\n,<string>\n, etc.) - timeout management\n- result persistence (tc-result files)\n- hierarchical test discovery (--all flag)\n- tag-based filtering (--tags flag)\n- parallel execution (--parallel flag, auto-detect CPU cores)\n- single-line animated status (TTY mode: helicopter üöÅ, spinner, colors)\n- fail-fast on first error (TTY mode stops immediately, shows log path)\n- final stats summary (colored counts: passed/failed/errors, cumulative time)\n- traditional verbose output (non-TTY mode for CI/CD)\n- machine-readable logs (JSONL format in\ntc/tmp/report.jsonl\n)\ntest generation:\n- scaffold generation (\ntc new\n) - test directory initialization (\ntc init\n) - metadata flags (--tags, --priority, --description)\n- template system (--from, --list-examples)\n- TDD workflow (tests fail until implemented)\ndiscovery & metadata:\n- list all tests (\ntc list\n) - show available tags (\ntc tags\n) - explain test suite (\ntc explain\n) - AI-friendly metadata format\nquality:\n- dogfooding (tc tests itself!)\nroadmap:\n- pattern-based selection\n- distributed test execution\n‚Üí tc-kit: AI-driven testing\ntc-kit integrates with spec-kit for automatic test generation from specifications. Perfect for AI-assisted development workflows where specs and tests are permanent while implementations are disposable.\nQuick start:\n# Generate tests from spec\n/tc.specify\n# Implement to pass tests\nedit tc/tests/my-feature/user-story-01/run\n# Validate & refine\n/tc.validate\n/tc.refine\nSee AI.md for full documentation.\nPrerequisites: bash 4.0+, jq\n# Install jq\nbrew install jq # macOS\nsudo apt-get install jq # Ubuntu/Debian\n# Clone tc\ngit clone https://github.com/ahoward/tc.git\ncd tc\n# Add to PATH\nexport PATH=\"$PWD/tc/bin:$PATH\"\n# Verify\ntc --version\nSee the TL;DR section above for PATH setup details.\nmit license - see LICENSE\nmade with ‚òï and helicopters\n\"the chopper's fueled up and ready to go. let's test some code.\" ‚Äî tc\nüöÅ fly safe, test well\nan #n5 joint üö¨"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://news.ycombinator.com/rss", "feed_title": "Hacker News", "id": "https://www.sciencedirect.com/science/article/pii/S0160412025002181", "title": "Diesel pollution particles impair lysosomal functions of iPSC-derived microglia", "url": "https://www.sciencedirect.com/science/article/pii/S0160412025002181", "published": "Tue, 23 Dec 2025 02:56:50 +0000", "text_source": "feed", "article_fetch_error": "http_403", "text": "<a href=\"https://news.ycombinator.com/item?id=46361917\">Comments</a>"}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.16953v1", "title": "Navigating Taxonomic Expansions of Entity Sets Driven by Knowledge Bases", "url": "https://arxiv.org/abs/2512.16953", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 17 Dec 2025]\nTitle:Navigating Taxonomic Expansions of Entity Sets Driven by Knowledge Bases\nView PDFAbstract:Recognizing similarities among entities is central to both human cognition and computational intelligence. Within this broader landscape, Entity Set Expansion is one prominent task aimed at taking an initial set of (tuples of) entities and identifying additional ones that share relevant semantic properties with the former -- potentially repeating the process to form increasingly broader sets. However, this ``linear'' approach does not unveil the richer ``taxonomic'' structures present in knowledge resources. A recent logic-based framework introduces the notion of an expansion graph: a rooted directed acyclic graph where each node represents a semantic generalization labeled by a logical formula, and edges encode strict semantic inclusion. This structure supports taxonomic expansions of entity sets driven by knowledge bases. Yet, the potentially large size of such graphs may make full materialization impractical in real-world scenarios. To overcome this, we formalize reasoning tasks that check whether two tuples belong to comparable, incomparable, or the same nodes in the graph. Our results show that, under realistic assumptions -- such as bounding the input or limiting entity descriptions -- these tasks can be implemented efficiently. This enables local, incremental navigation of expansion graphs, supporting practical applications without requiring full graph construction.\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.16969v1", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "url": "https://arxiv.org/abs/2512.16969", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 18 Dec 2025]\nTitle:Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows\nView PDF HTML (experimental)Abstract:Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.\nCurrent browse context:\ncs.AI\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.16970v1", "title": "PAACE: A Plan-Aware Automated Agent Context Engineering Framework", "url": "https://arxiv.org/abs/2512.16970", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 18 Dec 2025]\nTitle:PAACE: A Plan-Aware Automated Agent Context Engineering Framework\nView PDF HTML (experimental)Abstract:Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.\nCurrent browse context:\ncs.AI\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.17041v1", "title": "Security Risks of Agentic Vehicles: A Systematic Analysis of Cognitive and Cross-Layer Threats", "url": "https://arxiv.org/abs/2512.17041", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 18 Dec 2025]\nTitle:Security Risks of Agentic Vehicles: A Systematic Analysis of Cognitive and Cross-Layer Threats\nView PDF HTML (experimental)Abstract:Agentic AI is increasingly being explored and introduced in both manually driven and autonomous vehicles, leading to the notion of Agentic Vehicles (AgVs), with capabilities such as memory-based personalization, goal interpretation, strategic reasoning, and tool-mediated assistance. While frameworks such as the OWASP Agentic AI Security Risks highlight vulnerabilities in reasoning-driven AI systems, they are not designed for safety-critical cyber-physical platforms such as vehicles, nor do they account for interactions with other layers such as perception, communication, and control layers. This paper investigates security threats in AgVs, including OWASP-style risks and cyber-attacks from other layers affecting the agentic layer. By introducing a role-based architecture for agentic vehicles, consisting of a Personal Agent and a Driving Strategy Agent, we will investigate vulnerabilities in both agentic AI layer and cross-layer risks, including risks originating from upstream layers (e.g., perception layer, control layer, etc.). A severity matrix and attack-chain analysis illustrate how small distortions can escalate into misaligned or unsafe behavior in both human-driven and autonomous vehicles. The resulting framework provides the first structured foundation for analyzing security risks of agentic AI in both current and emerging vehicle platforms.\nCurrent browse context:\ncs.AI\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.17043v1", "title": "UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering", "url": "https://arxiv.org/abs/2512.17043", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 18 Dec 2025]\nTitle:UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering\nView PDF HTML (experimental)Abstract:Knowledge Graph Question Answering (KGQA) has traditionally focused on entity-centric queries that return a single answer entity. However, real-world queries are often relational, seeking to understand how entities are associated. In this work, we introduce relation-centric KGQA, a complementary setting where the answer is a subgraph capturing the semantic connections among entities rather than an individual entity. The main challenge lies in the abundance of candidate subgraphs, where trivial or overly common connections often obscure the identification of unique and informative answers. To tackle this, we propose UniRel-R1, a unified framework that integrates subgraph selection, multi-stage graph pruning, and an LLM fine-tuned with reinforcement learning. The reward function is designed to encourage compact and specific subgraphs with more informative relations and lower-degree intermediate entities. Extensive experiments show that UniRel-R1 achieves significant gains in connectivity and reward over Vanilla baselines and generalizes effectively to unseen entities and relations.\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.17066v1", "title": "Realistic threat perception drives intergroup conflict: A causal, dynamic analysis using generative-agent simulations", "url": "https://arxiv.org/abs/2512.17066", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 18 Dec 2025]\nTitle:Realistic threat perception drives intergroup conflict: A causal, dynamic analysis using generative-agent simulations\nView PDF HTML (experimental)Abstract:Human conflict is often attributed to threats against material conditions and symbolic values, yet it remains unclear how they interact and which dominates. Progress is limited by weak causal control, ethical constraints, and scarce temporal data. We address these barriers using simulations of large language model (LLM)-driven agents in virtual societies, independently varying realistic and symbolic threat while tracking actions, language, and attitudes. Representational analyses show that the underlying LLM encodes realistic threat, symbolic threat, and hostility as distinct internal states, that our manipulations map onto them, and that steering these states causally shifts behavior. Our simulations provide a causal account of threat-driven conflict over time: realistic threat directly increases hostility, whereas symbolic threat effects are weaker, fully mediated by ingroup bias, and increase hostility only when realistic threat is absent. Non-hostile intergroup contact buffers escalation, and structural asymmetries concentrate hostility among majority groups.\nSubmission history\nFrom: Suhaib Abdurahman [view email][v1] Thu, 18 Dec 2025 21:06:07 UTC (16,000 KB)\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.17086v1", "title": "Value Under Ignorance in Universal Artificial Intelligence", "url": "https://arxiv.org/abs/2512.17086", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 18 Dec 2025]\nTitle:Value Under Ignorance in Universal Artificial Intelligence\nView PDF HTML (experimental)Abstract:We generalize the AIXI reinforcement learning agent to admit a wider class of utility functions. Assigning a utility to each possible interaction history forces us to confront the ambiguity that some hypotheses in the agent's belief distribution only predict a finite prefix of the history, which is sometimes interpreted as implying a chance of death equal to a quantity called the semimeasure loss. This death interpretation suggests one way to assign utilities to such history prefixes. We argue that it is as natural to view the belief distributions as imprecise probability distributions, with the semimeasure loss as total ignorance. This motivates us to consider the consequences of computing expected utilities with Choquet integrals from imprecise probability theory, including an investigation of their computability level. We recover the standard recursive value function as a special case. However, our most general expected utilities under the death interpretation cannot be characterized as such Choquet integrals.\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.17093v1", "title": "A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving", "url": "https://arxiv.org/abs/2512.17093", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 18 Dec 2025]\nTitle:A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving\nView PDF HTML (experimental)Abstract:The rise of large language models (LLMs) has sparked interest in coding assistants. While general-purpose programming languages are well supported, generating code for domain-specific languages remains a challenging problem for LLMs. In this paper, we focus on the LLM-based generation of code for Answer Set Programming (ASP), a particularly effective approach for finding solutions to combinatorial search problems. The effectiveness of LLMs in ASP code generation is currently hindered by the limited number of examples seen during their initial pre-training phase.\nIn this paper, we introduce a novel ASP-solver-in-the-loop approach for solver-guided instruction-tuning of LLMs to addressing the highly complex semantic parsing task inherent in ASP code generation. Our method only requires problem specifications in natural language and their solutions. Specifically, we sample ASP statements for program continuations from LLMs for unriddling logic puzzles. Leveraging the special property of declarative ASP programming that partial encodings increasingly narrow down the solution space, we categorize them into chosen and rejected instances based on solver feedback. We then apply supervised fine-tuning to train LLMs on the curated data and further improve robustness using a solver-guided search that includes best-of-N sampling. Our experiments demonstrate consistent improvements in two distinct prompting settings on two datasets.\nSubmission history\nFrom: Timo Pierre Schrader [view email][v1] Thu, 18 Dec 2025 21:45:45 UTC (10,488 KB)\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.17102v1", "title": "Reinforcement Learning for Self-Improving Agent with Skill Library", "url": "https://arxiv.org/abs/2512.17102", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 18 Dec 2025]\nTitle:Reinforcement Learning for Self-Improving Agent with Skill Library\nView PDF HTML (experimental)Abstract:Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.17145v2", "title": "Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty", "url": "https://arxiv.org/abs/2512.17145", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 19 Dec 2025 (v1), last revised 22 Dec 2025 (this version, v2)]\nTitle:Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty\nView PDF HTML (experimental)Abstract:Reasoning under uncertainty is a key challenge in AI, especially for real-world tasks, where problems with sparse data demands systematic generalisation. Existing approaches struggle to balance accuracy and simplicity when evaluating multiple candidate solutions. We propose a Solomonoff-inspired method that weights LLM-generated hypotheses by simplicity and predictive fit. Applied to benchmark (Mini-ARC) tasks, our method produces Solomonoff-weighted mixtures for per-cell predictions, yielding conservative, uncertainty-aware outputs even when hypotheses are noisy or partially incorrect. Compared to Bayesian Model Averaging (BMA), Solomonoff scoring spreads probability more evenly across competing hypotheses, while BMA concentrates weight on the most likely but potentially flawed candidates. Across tasks, this highlights the value of algorithmic information-theoretic priors for interpretable, reliable multi-hypothesis reasoning under uncertainty.\nSubmission history\nFrom: Josh Barber [view email][v1] Fri, 19 Dec 2025 00:43:49 UTC (172 KB)\n[v2] Mon, 22 Dec 2025 02:19:00 UTC (466 KB)\nCurrent browse context:\ncs.AI\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.17194v1", "title": "MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation", "url": "https://arxiv.org/abs/2512.17194", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 19 Dec 2025]\nTitle:MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation\nView PDF HTML (experimental)Abstract:Multi-modal Retrieval-Augmented Generation (MMRAG) enables highly credible generation by integrating external multi-modal knowledge, thus demonstrating impressive performance in complex multi-modal scenarios. However, existing MMRAG methods fail to clarify the reasoning logic behind retrieval and response generation, which limits the explainability of the results. To address this gap, we propose to introduce reinforcement learning into multi-modal retrieval-augmented generation, enhancing the reasoning capabilities of multi-modal large language models through a two-stage reinforcement fine-tuning framework to achieve explainable multi-modal retrieval-augmented generation. Specifically, in the first stage, rule-based reinforcement fine-tuning is employed to perform coarse-grained point-wise ranking of multi-modal documents, effectively filtering out those that are significantly irrelevant. In the second stage, reasoning-based reinforcement fine-tuning is utilized to jointly optimize fine-grained list-wise ranking and answer generation, guiding multi-modal large language models to output explainable reasoning logic in the MMRAG process. Our method achieves state-of-the-art results on WebQA and MultimodalQA, two benchmark datasets for multi-modal retrieval-augmented generation, and its effectiveness is validated through comprehensive ablation experiments.\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.17196v1", "title": "UmniBench: Unified Understand and Generation Model Oriented Omni-dimensional Benchmark", "url": "https://arxiv.org/abs/2512.17196", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 19 Dec 2025]\nTitle:UmniBench: Unified Understand and Generation Model Oriented Omni-dimensional Benchmark\nView PDF HTML (experimental)Abstract:Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. However, evaluations of unified multimodal models (UMMs) remain decoupled, assessing their understanding and generation abilities separately with corresponding datasets. To address this, we propose UmniBench, a benchmark tailored for UMMs with omni-dimensional evaluation. First, UmniBench can assess the understanding, generation, and editing ability within a single evaluation process. Based on human-examined prompts and QA pairs, UmniBench leverages UMM itself to evaluate its generation and editing ability with its understanding ability. This simple but effective paradigm allows comprehensive evaluation of UMMs. Second, UmniBench covers 13 major domains and more than 200 concepts, ensuring a thorough inspection of UMMs. Moreover, UmniBench can also decouple and separately evaluate understanding, generation, and editing abilities, providing a fine-grained assessment. Based on UmniBench, we benchmark 24 popular models, including both UMMs and single-ability large models. We hope this benchmark provides a more comprehensive and objective view of unified models and logistical support for improving the performance of the community model.\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.17250v1", "title": "Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction", "url": "https://arxiv.org/abs/2512.17250", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 19 Dec 2025]\nTitle:Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction\nView PDF HTML (experimental)Abstract:Real-time sequential control agents are often bottlenecked by inference latency. Even modest per-step planning delays can destabilize control and degrade overall performance. We propose a speculation-and-correction framework that adapts the predict-then-verify philosophy of speculative execution to model-based control with TD-MPC2. At each step, a pretrained world model and latent-space MPC planner generate a short-horizon action queue together with predicted latent rollouts, allowing the agent to execute multiple planned actions without immediate replanning. When a new observation arrives, the system measures the mismatch between the encoded real latent state and the queued predicted latent. For small to moderate mismatch, a lightweight learned corrector applies a residual update to the speculative action, distilled offline from a replanning teacher. For large mismatch, the agent safely falls back to full replanning and clears stale action queues. We study both a gated two-tower MLP corrector and a temporal Transformer corrector to address local errors and systematic drift. Experiments on the DMC Humanoid-Walk task show that our method reduces the number of planning inferences from 500 to 282, improves end-to-end step latency by 25 percent, and maintains strong control performance with only a 7.1 percent return reduction. Ablation results demonstrate that speculative execution without correction is unreliable over longer horizons, highlighting the necessity of mismatch-aware correction for robust latency reduction.\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.17266v1", "title": "ScoutGPT: Capturing Player Impact from Team Action Sequences Using GPT-Based Framework", "url": "https://arxiv.org/abs/2512.17266", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 19 Dec 2025]\nTitle:ScoutGPT: Capturing Player Impact from Team Action Sequences Using GPT-Based Framework\nView PDF HTML (experimental)Abstract:Transfers play a pivotal role in shaping a football club's success, yet forecasting whether a transfer will succeed remains difficult due to the strong context-dependence of on-field performance. Existing evaluation practices often rely on static summary statistics or post-hoc value models, which fail to capture how a player's contribution adapts to a new tactical environment or different teammates. To address this gap, we introduce EventGPT, a player-conditioned, value-aware next-event prediction model built on a GPT-style autoregressive transformer. Our model treats match play as a sequence of discrete tokens, jointly learning to predict the next on-ball action's type, location, timing, and its estimated residual On-Ball Value (rOBV) based on the preceding context and player identity. A key contribution of this framework is the ability to perform counterfactual simulations. By substituting learned player embeddings into new event sequences, we can simulate how a player's behavioral distribution and value profile would change when placed in a different team or tactical structure. Evaluated on five seasons of Premier League event data, EventGPT outperforms existing sequence-based baselines in next-event prediction accuracy and spatial precision. Furthermore, we demonstrate the model's practical utility for transfer analysis through case studies-such as comparing striker performance across different systems and identifying stylistic replacements for specific roles-showing that our approach provides a principled method for evaluating transfer fit.\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.17308v1", "title": "Large Language Models as Pok\\'emon Battle Agents: Strategic Play and Content Generation", "url": "https://arxiv.org/abs/2512.17308", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 19 Dec 2025]\nTitle:Large Language Models as Pok√©mon Battle Agents: Strategic Play and Content Generation\nView PDF HTML (experimental)Abstract:Strategic decision-making in Pok√©mon battles presents a unique testbed for evaluating large language models. Pok√©mon battles demand reasoning about type matchups, statistical trade-offs, and risk assessment, skills that mirror human strategic thinking. This work examines whether Large Language Models (LLMs) can serve as competent battle agents, capable of both making tactically sound decisions and generating novel, balanced game content. We developed a turn-based Pok√©mon battle system where LLMs select moves based on battle state rather than pre-programmed logic. The framework captures essential Pok√©mon mechanics: type effectiveness multipliers, stat-based damage calculations, and multi-Pok√©mon team management. Through systematic evaluation across multiple model architectures we measured win rates, decision latency, type-alignment accuracy, and token efficiency. These results suggest LLMs can function as dynamic game opponents without domain-specific training, offering a practical alternative to reinforcement learning for turn-based strategic games. The dual capability of tactical reasoning and content creation, positions LLMs as both players and designers, with implications for procedural generation and adaptive difficulty systems in interactive entertainment.\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.17373v1", "title": "Dialectics for Artificial Intelligence", "url": "https://arxiv.org/abs/2512.17373", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 19 Dec 2025]\nTitle:Dialectics for Artificial Intelligence\nView PDF HTML (experimental)Abstract:Can artificial intelligence discover, from raw experience and without human supervision, concepts that humans have discovered? One challenge is that human concepts themselves are fluid: conceptual boundaries can shift, split, and merge as inquiry progresses (e.g., Pluto is no longer considered a planet). To make progress, we need a definition of \"concept\" that is not merely a dictionary label, but a structure that can be revised, compared, and aligned across agents. We propose an algorithmic-information viewpoint that treats a concept as an information object defined only through its structural relation to an agent's total experience. The core constraint is determination: a set of parts forms a reversible consistency relation if any missing part is recoverable from the others (up to the standard logarithmic slack in Kolmogorov-style identities). This reversibility prevents \"concepts\" from floating free of experience and turns concept existence into a checkable structural claim. To judge whether a decomposition is natural, we define excess information, measuring the redundancy overhead introduced by splitting experience into multiple separately described parts. On top of these definitions, we formulate dialectics as an optimization dynamics: as new patches of information appear (or become contested), competing concepts bid to explain them via shorter conditional descriptions, driving systematic expansion, contraction, splitting, and merging. Finally, we formalize low-cost concept transmission and multi-agent alignment using small grounds/seeds that allow another agent to reconstruct the same concept under a shared protocol, making communication a concrete compute-bits trade-off.\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.17470v1", "title": "Translating the Rashomon Effect to Sequential Decision-Making Tasks", "url": "https://arxiv.org/abs/2512.17470", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 19 Dec 2025]\nTitle:Translating the Rashomon Effect to Sequential Decision-Making Tasks\nView PDF HTML (experimental)Abstract:The Rashomon effect describes the phenomenon where multiple models trained on the same data produce identical predictions while differing in which features they rely on internally. This effect has been studied extensively in classification tasks, but not in sequential decision-making, where an agent learns a policy to achieve an objective by taking actions in an environment. In this paper, we translate the Rashomon effect to sequential decision-making. We define it as multiple policies that exhibit identical behavior, visiting the same states and selecting the same actions, while differing in their internal structure, such as feature attributions. Verifying identical behavior in sequential decision-making differs from classification. In classification, predictions can be directly compared to ground-truth labels. In sequential decision-making with stochastic transitions, the same policy may succeed or fail on any single trajectory due to randomness. We address this using formal verification methods that construct and compare the complete probabilistic behavior of each policy in the environment. Our experiments demonstrate that the Rashomon effect exists in sequential decision-making. We further show that ensembles constructed from the Rashomon set exhibit greater robustness to distribution shifts than individual policies. Additionally, permissive policies derived from the Rashomon set reduce computational requirements for verification while maintaining optimal performance.\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.17559v1", "title": "Towards Explainable Conversational AI for Early Diagnosis with Large Language Models", "url": "https://arxiv.org/abs/2512.17559", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 19 Dec 2025]\nTitle:Towards Explainable Conversational AI for Early Diagnosis with Large Language Models\nView PDF HTML (experimental)Abstract:Healthcare systems around the world are grappling with issues like inefficient diagnostics, rising costs, and limited access to specialists. These problems often lead to delays in treatment and poor health outcomes. Most current AI and deep learning diagnostic systems are not very interactive or transparent, making them less effective in real-world, patient-centered environments. This research introduces a diagnostic chatbot powered by a Large Language Model (LLM), using GPT-4o, Retrieval-Augmented Generation, and explainable AI techniques. The chatbot engages patients in a dynamic conversation, helping to extract and normalize symptoms while prioritizing potential diagnoses through similarity matching and adaptive questioning. With Chain-of-Thought prompting, the system also offers more transparent reasoning behind its diagnoses. When tested against traditional machine learning models like Naive Bayes, Logistic Regression, SVM, Random Forest, and KNN, the LLM-based system delivered impressive results, achieving an accuracy of 90% and Top-3 accuracy of 100%. These findings offer a promising outlook for more transparent, interactive, and clinically relevant AI in healthcare.\nSubmission history\nFrom: Maliha Tabassum Tabassum [view email][v1] Fri, 19 Dec 2025 13:28:50 UTC (1,344 KB)\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.17637v1", "title": "About Time: Model-free Reinforcement Learning with Timed Reward Machines", "url": "https://arxiv.org/abs/2512.17637", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 19 Dec 2025]\nTitle:About Time: Model-free Reinforcement Learning with Timed Reward Machines\nView PDF HTML (experimental)Abstract:Reward specification plays a central role in reinforcement learning (RL), guiding the agent's behavior. To express non-Markovian rewards, formalisms such as reward machines have been introduced to capture dependencies on histories. However, traditional reward machines lack the ability to model precise timing constraints, limiting their use in time-sensitive applications. In this paper, we propose timed reward machines (TRMs), which are an extension of reward machines that incorporate timing constraints into the reward structure. TRMs enable more expressive specifications with tunable reward logic, for example, imposing costs for delays and granting rewards for timely actions. We study model-free RL frameworks (i.e., tabular Q-learning) for learning optimal policies with TRMs under digital and real-time semantics. Our algorithms integrate the TRM into learning via abstractions of timed automata, and employ counterfactual-imagining heuristics that exploit the structure of the TRM to improve the search. Experimentally, we demonstrate that our algorithm learns policies that achieve high rewards while satisfying the timing constraints specified by the TRM on popular RL benchmarks. Moreover, we conduct comparative studies of performance under different TRM semantics, along with ablations that highlight the benefits of counterfactual-imagining.\nCurrent browse context:\ncs.AI\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.17898v1", "title": "Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally", "url": "https://arxiv.org/abs/2512.17898", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 19 Dec 2025]\nTitle:Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally\nView PDF HTML (experimental)Abstract:Over a billion users across the globe interact with AI systems engineered with increasing sophistication to mimic human traits. This shift has triggered urgent debate regarding Anthropomorphism, the attribution of human characteristics to synthetic agents, and its potential to induce misplaced trust or emotional dependency. However, the causal link between more humanlike AI design and subsequent effects on engagement and trust has not been tested in realistic human-AI interactions with a global user pool. Prevailing safety frameworks continue to rely on theoretical assumptions derived from Western populations, overlooking the global diversity of AI users. Here, we address these gaps through two large-scale cross-national experiments (N=3,500) across 10 diverse nations, involving real-time and open-ended interactions with an AI system. We find that when evaluating an AI's human-likeness, users focus less on the kind of theoretical aspects often cited in policy (e.g., sentience or consciousness), but rather applied, interactional cues like conversation flow or understanding the user's perspective. We also experimentally demonstrate that humanlike design levers can causally increase anthropomorphism among users; however, we do not find that humanlike design universally increases behavioral measures for user engagement and trust, as previous theoretical work suggests. Instead, part of the connection between human-likeness and behavioral outcomes is fractured by culture: specific design choices that foster self-reported trust in AI-systems in some populations (e.g., Brazil) may trigger the opposite result in others (e.g., Japan). Our findings challenge prevailing narratives of inherent risk in humanlike AI design. Instead, we identify a nuanced, culturally mediated landscape of human-AI interaction, which demands that we move beyond a one-size-fits-all approach in AI governance.\nSubmission history\nFrom: Robin Schimmelpfennig [view email][v1] Fri, 19 Dec 2025 18:57:53 UTC (2,185 KB)\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.17901v1", "title": "When Reasoning Meets Its Laws", "url": "https://arxiv.org/abs/2512.17901", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Artificial Intelligence\n[Submitted on 19 Dec 2025]\nTitle:When Reasoning Meets Its Laws\nView PDF HTML (experimental)Abstract:Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: this https URL\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2507.15118v1", "title": "Graph Attention Networks for Detecting Epilepsy from EEG Signals Using Accessible Hardware in Low-Resource Settings", "url": "https://arxiv.org/abs/2507.15118", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Electrical Engineering and Systems Science > Signal Processing\n[Submitted on 20 Jul 2025]\nTitle:Graph Attention Networks for Detecting Epilepsy from EEG Signals Using Accessible Hardware in Low-Resource Settings\nView PDF HTML (experimental)Abstract:Goal: Epilepsy remains under-diagnosed in low-income countries due to scarce neurologists and costly diagnostic tools. We propose a graph-based deep learning framework to detect epilepsy from low-cost Electroencephalography (EEG) hardware, tested on recordings from Nigeria and Guinea-Bissau. Our focus is on fair, accessible automatic assessment and explainability to shed light on epilepsy biomarkers. Methods: We model EEG signals as spatio-temporal graphs, classify them, and identify interchannel relationships and temporal dynamics using graph attention networks (GAT). To emphasize connectivity biomarkers, we adapt the inherently node-focused GAT to analyze edges. We also designed signal preprocessing for low-fidelity recordings and a lightweight GAT architecture trained on Google Colab and deployed on RaspberryPi devices. Results: The approach achieves promising classification performance, outperforming a standard classifier based on random forest and graph convolutional networks in terms of accuracy and robustness over multiple sessions, but also highlighting specific connections in the fronto-temporal region. Conclusions: The results highlight the potential of GATs to provide insightful and scalable diagnostic support for epilepsy in underserved regions, paving the way for affordable and accessible neurodiagnostic tools.\nCurrent browse context:\neess.SP\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.16925v1", "title": "V-Agent: An Interactive Video Search System Using Vision-Language Models", "url": "https://arxiv.org/abs/2512.16925", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Computer Vision and Pattern Recognition\n[Submitted on 4 Nov 2025]\nTitle:V-Agent: An Interactive Video Search System Using Vision-Language Models\nView PDF HTML (experimental)Abstract:We introduce V-Agent, a novel multi-agent platform designed for advanced video search and interactive user-system conversations. By fine-tuning a vision-language model (VLM) with a small video preference dataset and enhancing it with a retrieval vector from an image-text retrieval model, we overcome the limitations of traditional text-based retrieval systems in multimodal scenarios. The VLM-based retrieval model independently embeds video frames and audio transcriptions from an automatic speech recognition (ASR) module into a shared multimodal representation space, enabling V-Agent to interpret both visual and spoken content for context-aware video search. This system consists of three agents-a routing agent, a search agent, and a chat agent-that work collaboratively to address user intents by refining search outputs and communicating with users. The search agent utilizes the VLM-based retrieval model together with an additional re-ranking module to further enhance video retrieval quality. Our proposed framework demonstrates state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark, highlighting its potential for both academic research and real-world applications.\nCurrent browse context:\ncs.CV\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.16927v1", "title": "Optimizing Text Search: A Novel Pattern Matching Algorithm Based on Ukkonen's Approach", "url": "https://arxiv.org/abs/2512.16927", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Data Structures and Algorithms\n[Submitted on 29 Nov 2025]\nTitle:Optimizing Text Search: A Novel Pattern Matching Algorithm Based on Ukkonen's Approach\nView PDFAbstract:In the realm of computer science, the efficiency of text-search algorithms is crucial for processing vast amounts of data in areas such as natural language processing and bioinformatics. Traditional methods like Naive Search, KMP, and Boyer-Moore, while foundational, often fall short in handling the complexities and scale of modern datasets, such as the Reuters corpus and human genomic sequences. This study rigorously investigates text-search algorithms, focusing on optimizing Suffix Trees through methods like Splitting and Ukkonen's Algorithm, analyzed on datasets including the Reuters corpus and human genomes. A novel optimization combining Ukkonen's Algorithm with a new search technique is introduced, showing linear time and space efficiencies, outperforming traditional methods like Naive Search, KMP, and Boyer-Moore. Empirical tests confirm the theoretical advantages, highlighting the optimized Suffix Tree's effectiveness in tasks like pattern recognition in genomic sequences, achieving 100% accuracy. This research not only advances academic knowledge in text-search algorithms but also demonstrates significant practical utility in fields like natural language processing and bioinformatics, due to its superior resource efficiency and reliability.\nCurrent browse context:\ncs.DS\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
{"fetched_at_utc": "2025-12-23T04:38:42.905183+00:00", "feed_url": "https://rss.arxiv.org/atom/cs.AI", "feed_title": "cs.AI updates on arXiv.org", "id": "oai:arXiv.org:2512.16950v1", "title": "Enhancing Tree Species Classification: Insights from YOLOv8 and Explainable AI Applied to TLS Point Cloud Projections", "url": "https://arxiv.org/abs/2512.16950", "published": "2025-12-22T00:00:00-05:00", "text_source": "article", "article_fetch_error": null, "text": "Computer Science > Computer Vision and Pattern Recognition\n[Submitted on 17 Dec 2025]\nTitle:Enhancing Tree Species Classification: Insights from YOLOv8 and Explainable AI Applied to TLS Point Cloud Projections\nView PDFAbstract:Classifying tree species has been a core research area in forest remote sensing for decades. New sensors and classification approaches like TLS and deep learning achieve state-of-the art accuracy but their decision processes remain unclear. Methods such as Finer-CAM (Class Activation Mapping) can highlight features in TLS projections that contribute to the classification of a target species, yet are uncommon in similar looking contrastive tree species. We propose a novel method linking Finer-CAM explanations to segments of TLS projections representing structural tree features to systemically evaluate which features drive species discrimination. Using TLS data from 2,445 trees across seven European tree species, we trained and validated five YOLOv8 models with cross-validation, reaching a mean accuracy of 96% (SD = 0.24%). Analysis of 630 saliency maps shows the models primarily rely on crown features in TLS projections for species classification. While this result is pronounced in Silver Birch, European Beech, English oak, and Norway spruce, stem features contribute more frequently to the differentiation of European ash, Scots pine, and Douglas fir. Particularly representations of finer branches contribute to the decisions of the models. The models consider those tree species similar to each other which a human expert would also regard as similar. Furthermore, our results highlight the need for an improved understanding of the decision processes of tree species classification models to help reveal data set and model limitations, biases, and to build confidence in model predictions.\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."}
